Proceedings O
of O
the O
60th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
2134 O
- O
2148 O
May O
22 O
- O
27 O
, O
2022 O
c O

 O
2022 O
Association O
for O
Computational O
Linguistics O
bert2BERT B-MethodName
: O
Towards O
Reusable O
Pretrained O
Language O
Models O
Cheng O
Chen1† O
, O
Yichun O
Yin2 O
, O
Lifeng O
Shang2 O
, O
Xin O
Jiang2 O
, O
Yujia O
Qin1 O
, O
Fengyu O
Wang1 O
, O
Zhi O
Wang3,4‡ O
, O
Xiao O
Chen2 O
, O
Zhiyuan O
Liu1 O
, O
Qun O
Liu2 O
1Department O
of O
Computer O
Science O
and O
Technology O
, O
Tsinghua O
University O
2Huawei O
Noah O
’s O
Ark O
Lab,3Tsinghua O
Shenzhen O
International O
Graduate O
School O
4Peng O
Cheng O
Laboratory O
{ O
c-chen19,qyj20,wangfy20}@mails.tsinghua.edu.cn O
{ O
yinyichun,shang.lifeng,jiang.xin,chen.xiao2,qun.liu}@huawei.com O

wangzhi@sz.tsinghua.edu.cn,liuzy@tsinghua.edu.cn O
Abstract O

In O
recent O
years O
, O
researchers O
tend O
to O
pre O
- O
train O
ever O
- O
larger O
language O
models O
to O
explore O
the O
upper O
limit O
of O
deep O
models O
. O

However O
, O
large O
language O
model O
pre O
- O
training O
costs O
intensive O
computational O
resources O
, O
and O
most O
of O
the O
models O
are O
trained O
from O
scratch O
without O
reusing O
the O
existing O
pre O
- O
trained O
models O
, O
which O
is O
wasteful O
. O

In O
this O
paper O
, O
we O
propose O
bert2BERT1 B-MethodName
, O
which O
can O
effectively O
transfer O
the O
knowledge O
of O
an O
existing O
smaller O
pre O
- O
trained O
model O
to O
a O
large O
model O
through O
parameter O
initialization O
and O
significantly O
improve O
the O
pre O
- O
training O
efficiency O
of O
the O
large O
model O
. O

Specifically O
, O
we O
extend O
the O
previous O
function O
- O
preserving O
( O
Chen O
et O
al O
. O
, O
2016 O
) O
method O
proposed O
in O
computer O
vision O
on O
the O
Transformer B-MethodName
- O
based O
language O
model O
, O
and O
further O
improve O
it O
by O
proposing O
a O
novel O
method O
, O
advanced O
knowledge O
for O
the O
large O
model O
’s O
initialization O
. O

In O
addition O
, O
a O
two O
- O
stage O
learning O
method O
is O
proposed O
to O
further O
accelerate O
the O
pre O
- O
training O
. O

We O
conduct O
extensive O
experiments O
on O
representative O
PLMs O
( O
e.g. O
, O
BERT B-MethodName
and O
GPT B-MethodName
) O
and O
demonstrate O
that O
( O
1 O
) O
our O
method O
can O
save O
a O
significant O
amount O
of O
training O
cost O
compared O
with O
baselines O
including O
learning O
from O
scratch O
, O
StackBERT B-MethodName
( O
Gong O
et O
al O
. O
, O
2019 O
) O
and O
MSLT B-MethodName
( O
Yang O
et O
al O
. O
, O
2020 O
) O
; O
( O
2 O
) O
our O
method O
is O
generic O
and O
applicable O
to O
different O
types O
of O
pretrained O
models O
. O

In O
particular O
, O
bert2BERT B-MethodName
saves O
about O
45 O
% O
and O
47 O
% O
computational O
cost O
of O
pretraining O
BERT B-MethodName
BASE I-MethodName
and O
GPT B-MethodName
BASE I-MethodName
by O
reusing O
the O
models O
of O
almost O
their O
half O
sizes O
. O

1 O
Introduction O
Pre O
- O
trained O
language O
models O
( O
PLMs O
) O
, O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
GPT B-MethodName
( O
Radford O
et O
al O
. O
, O
2018 O
, O
2019 O
; O
Brown O
et O
al O
. O
, O
2020 O
) O
, O
ELECTRA B-MethodName
( O
Clark O
et O
al O
. O
, O
2020 O
) O
, O
XLNet B-MethodName
( O
Yang O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
have O
achieved O
great O
†This O
work O
is O
done O
when O
Cheng O
Chen O
is O
an O
intern O
at O
Huawei O
Noah O
’s O
Ark O
Lab O
. O

‡Corresponding O
author O
. O

1Our O
code O
is O
available O
at O
https://github.com/ O
huawei O
- O
noah O
/ O
Pretrained O
- O
Language O
- O
Model O
. O

0 O
1 O
2 O
3 O
4 O
5 O
6 O
7 O
8 O
FLOPs O
( O
1e19)1.41.61.82.02.22.42.6MLM O
Loss O
4 O
5 O
6 O
71.401.421.441.461.48 O
1.437 O
100 O
% O
75.7 O
% O
54.8%BERTBASE O
StackBERTbert2BERTFigure O
1 O
: O
Loss O
curves O
of O
bert2BERT O
and O
baselines O
. O
StackBERT O
( O
Gong O
et O
al O
. O
, O
2019 O
) O
is O
based O
on O
the O
progressive O
training O
setting O
. O

More O
details O
are O
shown O
in O
Table O
2 O
. O
success O
in O
natural O
language O
processing O
( O
NLP O
) O
. O

However O
, O
the O
pre O
- O
training O
process O
of O
large O
PLMs O
can O
be O
extremely O
computationally O
expensive O
and O
produces O
huge O
carbon O
footprints O
. O

For O
example O
, O
GPT-3 B-MethodName
uses O
3.1E+6 O
GPU O
hours O
for O
training O
, O
at O
an O
estimated O
cost O
of O
$ O
4.6 O
million2 O
, O
consuming O
a O
lot O
of O
computing O
resources O
. O

Therefore O
, O
how O
to O
reduce O
the O
training O
cost O
of O
PLM O
is O
of O
great O
importance O
to O
Green O
AI O
( O
Schwartz O
et O
al O
. O
, O
2020 O
) O
. O

Recently O
, O
there O
is O
a O
trend O
of O
training O
extremely O
large O
models O
to O
explore O
the O
upper O
limits O
of O
PLMs O
. O

For O
example O
, O
large O
pre O
- O
trained O
models O
, O
including O
GPT-3 B-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
) O
( O
175B O
) O
, O
PanGuα(Zeng O
et O
al O
. O
, O
2021 O
) O
( O
200B O
) O
and O
Switch B-MethodName
Transformers I-MethodName
( O
Fedus O
et O
al O
. O
, O
2021 O
) O
( O
1571B O
) O
, O
have O
been O
proved O
promising O
in O
language O
understanding O
and O
generation O
. O

However O
, O
these O
models O
are O
all O
pre O
- O
trained O
from O
scratch O
independently O
without O
utilizing O
the O
knowledge O
of O
smaller O
ones O
that O
have O
already O
been O
trained O
. O

On O
the O
other O
hand O
, O
our O
empirical O
studies O
show O
that O
the O
pre O
- O
trained O
models O
of O
different O
scales O
could O
share O
similar O
knowledge O
, O
for O
example O
in O
Figure O
2 O
, O
the O
attention O
patterns O
of O
the O
two O
PLMs O
with O
different O
sizes O
are O
similar O
. O

To O
save O
the O
training O
cost O
of O
large O
models O
, O
we O
2https://lambdalabs.com/blog/ O
demystifying O
- O
gpt-3/2134L2 O
H1 O
  O
L2 O
H2 O
  O
L2 O
H4 O
  O
L2 O
H5 O
  O
L12 O
H4 O
  O
L12 O
H5 O
  O
L12 O
H10 O
  O
L12 O
H11 O
L2 O
H8 O
  O
L2 O
H2 O
  O
L2 O
H4 O
  O
L2 O
H5 O
  O
L12 O
H3 O
  O
L12 O
H8 O
  O
L12 O

H5 O
  O
L12 O
H7Figure O
2 O
: O
The O
comparisons O
of O
attention O
patterns O
between O
small O
and O
large O
PLMs O
. O

The O
upper O
ones O
are O
the O
attention O
patterns O
of O
BERT O
BASE O
model O
whose O
architecture O
is O
{ O
L=12,D=768 O
} O
, O
and O
the O
lower O
ones O
are O
the O
attention O
patterns O
of O
one O
small O
BERT O
model O
whose O
architecture O
is O
{ O
L=12,D=512 O
} O
. O

We O
find O
that O
there O
are O
a O
large O
number O
of O
similar O
attention O
patterns O
in O
the O
same O
layer O
of O
the O
two O
models O
, O
indicating O
the O
possibility O
of O
reusing O
parameters O
of O
trained O
small O
PLMs O
to O
speed O
up O
the O
pre O
- O
training O
of O
large O
PLMs O
. O

The O
attention O
maps O
of O
PLMs O
with O
different O
layers O
are O
also O
similar O
, O
which O
is O
visualized O
in O
previous O
work O
( O
Gong O
et O
al O
. O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2020 O
) O
. O
propose O
the O
bert2BERT B-MethodName
method O
, O
which O
can O
efficiently O
transfer O
the O
learned O
knowledge O
of O
the O
smaller O
model O
to O
the O
large O
model O
. O

bert2BERT B-MethodName
consists O
of O
two O
components O
: O
( O
1 O
) O
For O
parameter O
initialization O
, O
we O
first O
extend O
the O
function O
preserving O
training O
( O
Chen O
et O
al O
. O
, O
2016 O
) O
to O
PLMs O
by O
duplicating O
and O
stacking O
the O
parameters O
of O
the O
existing O
smaller O
PLM O
, O
which O
we O
call O
function O
- O
preserving O
initialization O
( O
FPI O
) O
. O

FPI O
ensures O
that O
the O
initialized O
large O
model O
has O
almost O
the O
same O
behavior O
as O
the O
small O
model O
, O
so O
that O
the O
large O
model O
has O
a O
good O
starting O
point O
for O
later O
optimization O
. O

We O
also O
find O
that O
duplicating O
the O
weights O
of O
the O
upper O
layer O
to O
the O
current O
layer O
can O
further O
accelerate O
the O
convergence O
of O
the O
large O
model O
, O
which O
we O
call O
advanced O
knowledge O
initialization O
( O
AKI O
) O
. O

Although O
the O
AKI O
somewhat O
violates O
the O
principle O
of O
function O
preserving O
, O
we O
find O
that O
empirically O
it O
also O
has O
a O
good O
starting O
point O
as O
shown O
in O
Table O
1 O
, O
which O
leads O
to O
a O
faster O
convergence O
rate O
and O
achieves O
higher O
training O
efficiency O
. O

( O
2 O
) O
Secondly O
, O
a O
two O
- O
stage O
training O
strategy O
is O
further O
applied O
to O
the O
large O
model O
to O
accelerate O
the O
training O
process O
. O

To O
demonstrate O
the O
superiority O
of O
our O
method O
, O
we O
conduct O
extensive O
experiments O
on O
two O
representative O
PLMs O
: O
BERT B-MethodName
and O
GPT B-MethodName
, O
with O
different O
source O
model O
sizes O
. O

The O
results O
show O
that O
: O
( O
1 O
) O
our O
method O
can O
save O
a O
significant O
amount O
of O
computation O
in O
pre O
- O
training O
compared O
to O
the O
traditional O
way O
of O
learning O
from O
scratch O
and O
progressive O
stacking O
methods O
such O
as O
StackBERT B-MethodName
( O
Gong O
et O
al O
. O
, O
2019 O
) O
and O
MSLT B-MethodName
( O
Yang O
et O
al O
. O
, O
2020 O
) O
; O
( O
2 O
) O
our O
method O
is O
model O
- O
agnostic O
, O
which O
can O
be O
applied O
on O
a O
wide O
range O
of O
Transformer O
- O
based O
PLMs O
. O

One O
typical O
example O
is O
that O
, O
when O
using O
a O
small O
pre O
- O
trainedmodel O
with O
half O
the O
size O
of O
BERT B-MethodName
BASE I-MethodName
for O
initialization O
, O
bert2BERT B-MethodName
saves O
45 O
% O
computation O
cost O
of O
the O
original O
BERT B-MethodName
BASE I-MethodName
pre O
- O
training O
. O

In O
general O
, O
our O
contributions O
are O
summarized O
as O
follows O
: O
( O
1 O
) O
We O
explore O
a O
new O
direction O
for O
the O
efficient O
pre O
- O
training O
by O
reusing O
the O
trained O
parameters O
of O
small O
models O
to O
initialize O
the O
large O
model O
; O
( O
2 O
) O
We O
successfully O
extend O
function O
preserving O
method O
( O
Chen O
et O
al O
. O
, O
2016 O
) O
on O
BERT B-MethodName
and O
further O
propose O
advanced O
knowledge O
initialization O
, O
which O
can O
effectively O
transfer O
the O
knowledge O
of O
the O
trained O
small O
model O
to O
the O
big O
model O
and O
improve O
the O
pre O
- O
training O
efficiency O
; O
( O
3 O
) O
The O
proposed O
method O
outperforms O
other O
training O
methods O
and O
achieves O
45 O
% O
computation O
reduction O
on O
BERT B-MethodName
BASE I-MethodName
; O
( O
4 O
) O
Our O
method O
is O
generic O
, O
effective O
for O
both O
the O
BERT B-MethodName
and O
GPT B-MethodName
models O
, O
and O
have O
great O
potential O
to O
become O
an O
energy O
- O
efficient O
solution O
for O
pre O
- O
training O
super O
large O
- O
scale O
language O
models O
. O

2 O
Related O
Work O
Efficient O
Pre O
- O
training O
in O
NLP O
. O

The O
efficiency O
of O
pre O
- O
training O
has O
been O
explored O
by O
previous O
work O
. O

Some O
works O
( O
Gong O
et O
al O
. O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2020 O
; O
Gu O
et O
al O
. O
, O
2021 O
) O
propose O
progressive O
learning O
to O
accelerate O
the O
pre O
- O
training O
, O
which O
are O
motivated O
by O
the O
fact O
that O
different O
layers O
have O
some O
similar O
knowledge O
( O
e.g. O
, O
attention O
patterns O
) O
. O

They O
start O
pre O
- O
training O
a O
small O
model O
with O
fewer O
Transformer O
layers O
, O
and O
then O
iteratively O
expand O
the O
model O
by O
stacking O
the O
already O
trained O
layers O
on O
the O
top O
. O

Another O
line O
of O
work O
proposes O
to O
“ O
back O
distill O
” O
the O
knowledge O
of O
the O
small O
models O
into O
large O
models O
, O
which O
is O
termed O
as O
knowledge O
inheritance O
( O
Qin O
et O
al O
. O
, O
2021 O
) O
. O

Some O
works O
focus O
on O
the O
data O
effi-2135ciency O

( O
Wu O
et O
al O
. O
, O
2021 O
) O
and O
take O
notes O
for O
rare O
words O
during O
the O
pre O
- O
training O
process O
to O
help O
the O
model O
understand O
them O
when O
they O
occur O
next O
. O

ELECTRA B-MethodName
( O
Clark O
et O
al O
. O
, O
2020 O
) O
proposes O
a O
task O
of O
replaced O
token O
detection O
to O
predict O
whether O
each O
token O
in O
the O
input O
was O
replaced O
or O
not O
, O
which O
improves O
the O
pre O
- O
training O
efficiency O
. O

Our O
method O
is O
orthogonal O
to O
this O
kind O
of O
work O
and O
the O
combination O
of O
ELECTRA B-MethodName
and O
bert2BERT B-MethodName
could O
achieve O
better O
efficiency O
. O

In O
addition O
, O
there O
are O
several O
other O
orthogonal O
techniques O
for O
efficient O
pre O
- O
training O
: O
mixed O
- O
precision O
training O
( O
Shoeybi O
et O

al O
. O
, O
2019 O
) O
, O
large O
batch O
optimization O
( O
You O
et O
al O
. O
, O
2020 O
) O
, O
model O
architecture O
innovation O
( O
Lan O
et O
al O
. O
, O
2020 O
) O
, O
layer O
dropping O
technique O
( O
Zhang O
and O
He O
, O
2020 O
) O
, O
etc O
. O

Reusable O
Neural O
Network O
. O

Reusable O
neural O
network O
, O
a O
topic O
related O
to O
transfer O
learning O
( O
Pan O
and O
Yang O
, O
2010 O
) O
, O
is O
introduced O
to O
accelerate O
the O
model O
training O
in O
computer O
vision O
. O

One O
classical O
work O
is O
Net2Net B-MethodName
( O
Chen O
et O
al O
. O
, O
2016 O
) O
, O
which O
first O
proposes O
the O
concept O
of O
the O
function O
- O
preserving O
transformation O
to O
make O
neural O
networks O
reusable O
. O

However O
, O
Net2Net B-MethodName
randomly O
selects O
the O
neurons O
to O
be O
split O
. O

To O
handle O
this O
problem O
, O
some O
works O
( O
Wu O
et O
al O
. O
, O
2019 O
, O
2020b O
; O
Wang O
et O
al O
. O
, O
2019b O
; O
Wu O
et O
al O
. O
, O
2020a O
) O
leverage O
a O
functional O
steepest O
descent O
idea O
to O
decide O
the O
optimal O
subset O
of O
neurons O
to O
be O
split O
. O

The O
pruning O
technique O
( O
Han O
et O
al O
. O
, O
2015 O
) O
is O
also O
introduced O
for O
reusable O
neural O
networks O
( O
Feng O
and O
Panda O
, O
2020 O
) O
. O

In O
this O
paper O
, O
we O
study O
the O
reusable O
pre O
- O
trained O
language O
model O
and O
propose O
a O
new O
method O
, O
bert2BERT B-MethodName
, O
to O
accelerate O
the O
pre O
- O
training O
of O
BERT B-MethodName
and O
GPT B-MethodName
. O

3 O
Preliminary O
BERT B-MethodName
consists O
of O
one O
embedding O
layer O
and O
multiple O
Transformer B-MethodName
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
layers O
. O

3.1 O
Embedding O
Layer O
The O
embedding O
layer O
first O
maps O
the O
tokens O
in O
a O
sentence O
into O
vectors O
with O
an O
embedding O
matrix O
WE O
. O

Then O
one O
normalization O
layer O
is O
employed O
to O
produce O
the O
initial O
hidden O
states O
H0 O
. O

3.2 O
Transformer O
Layer O
The O
hidden O
states O
are O
iteratively O
processed O
by O
multiple O
Transformer O
layers O
as O
follows O
: O
Hl= O
Transformer O
l(Hl−1 O
) O
, O
l∈[1 O
, O
L O
] O
( O
1 O
) O
where O
Ldenotes O
the O
number O
of O
Transformer O
layers O
, O
each O
including O
a O
multi O
- O
head O
attention O
( O
MHA O
) O
and O
a O
feed O
- O
forward O
network O
( O
FFN).MHA O
. O

It O
is O
composed O
of O
multiple O
parallel O
selfattention O
heads O
. O

The O
hidden O
states O
of O
the O
previous O
layer O
are O
fed O
into O
each O
head O
and O
then O
the O
outputs O
of O
all O
heads O
are O
summed O
to O
obtain O
the O
final O
output O
as O
follows O
: O
Qi O
, O
Ki O
, O
Vi O
= O
Hl−1WQ O
l O
, O
i O
, O
Hl−1WK O
l O
, O
i O
, O
Hl−1WV O
l O
, O
i O
, O
HHEAD O
l O
, O
i O
= O
softmax(QiKiT O
√dk)ViWO O
l O
, O
i O
, O
MHA(Hl−1 O
) O

= O
aX O
i=1HHEAD O
l O
, O
i O
, O
HMHA O
l O
= O
LayerNorm O
( O
Hl−1 O
+ O
MHA O
( O
Hl−1 O
) O
) O
. O

( O
2 O
) O
Hl−1is O
linearly O
projected O
to O
queries O
( O
Qi O
) O
, O
keys O
( O
Ki O
) O
and O
values O
( O
Vi O
) O
using O
WQ O
l O
, O
i O
, O
WK O
l O
, O
i O
, O
WV O
l O
, O
irespectively O
. O

HHEAD O
l O
, O
iindicates O
the O
context O
- O
aware O
vector O
which O
is O
obtained O
by O
the O
scaled O
dot O
- O
product O
of O
queries O
and O
keys O
in O
the O
i O
- O
th O
attention O
head O
. O

a O
represents O
the O
number O
of O
self O
- O
attention O
heads O
. O

dk O
is O
the O
head O
dimension O
acting O
as O
the O
scaling O
factor O
. O

FFN O
. O

It O
consists O
of O
two O
linear O
layers O
and O
one O
GeLU O
activation O
function O
( O
Hendrycks O
and O
Gimpel O
, O
2016 O
) O
, O
that O
is O
: O
HFFN O
l= O
GeLU O
( O
HMHA O
lW1 O
l+b1 O
l)W2 O
l+b2 O
l O
, O
Hl= O
LayerNorm O
( O
HMHA O
l O
+ O
HFFN O
l O
) O
. O

( O
3 O
) O
Layer O
Normalization O
. O

Both O
the O
modules O
of O
MHA O
and O
FFN O
have O
one O
layer O
normalization O
( O
Ba O
et O
al O
. O
, O
2016 O
) O
that O
stabilizes O
the O
dynamics O
of O
the O
hidden O
state O
in O
the O
Transformer O
. O

Formally O
, O
it O
is O
written O
as O
: O
LayerNorm O
( O
H O
) O
= O
( O
H−µH O
σH)⊙WLN+bLN O
, O
( O
4 O
) O
where O
⊙means O
the O
element O
- O
wise O
multiplication O
. O

The O
statistics O
of O
µHandσHare O
the O
mean O
and O
variance O
of O
hidden O
states O
Hrespectively O
. O
4 O
Methodology O
4.1 O
Problem O
Statement O
We O
aim O
to O
accelerate O
the O
pre O
- O
training O
of O
target O
model O
T(Lt O
, O
Dt)by O
transferring O
the O
knowledge O
of O
an O
existing O
pre O
- O
trained O
source O
model O
S(Ls O
, O
Ds O
) O
, O
where O
Ls|tmeans O
the O
numbers O
of O
Transformer O
layer O
and O
Ds|tmeans O
the O
model O
width O
( O
i.e. O
, O
hidden O
size O
) O
, O
satisfying O
Ls≤LtandDs≤Dt O
. O

Formally O
, O
our O
problem O
is O
two O
- O
fold O
: O
( O
1 O
) O
how O
to O
perform O
an O
effective O
parameter O
initialization O
for O
Tby O
reusing O
the O
trained O
parameters O
of O
S O
, O
and O
( O
2 O
) O
how O
to O
efficiently2136train O
the O
initialized O
T O
, O
so O
that O
Tcan O
have O
a O
faster O
convergence O
rate O
in O
pre O
- O
training O
. O

4.2 O
Overview O
Targeting O
the O
above O
problems O
, O
bert2BERT B-MethodName
first O
initializes O
the O
target O
model O
Twith O
the O
parameters O
of O
the O
existing O
model O
Sby O
the O
width O
- O
wise O
expansion O
( O
Ds→Dt O
) O
and O
depth O
- O
wise O
expansion O
( O
Ls→Lt O
) O
. O

Through O
this O
expansion O
, O
the O
knowledge O
contained O
in O
the O
parameters O
of O
the O
source O
model O
is O
directly O
transferred O
to O
the O
target O
model O
. O

Then O
we O
further O
pre O
- O
train O
the O
initialized O
target O
model O
with O
a O
twostage O
pre O
- O
training O
method O
. O

The O
overall O
workflow O
is O
illustrated O
in O
Section O
4.5 O
. O

Essentially O
, O
the O
width O
- O
wise O
expansion O
can O
be O
decomposed O
into O
expansions O
of O
parameter O
matrices O
( O
or O
vectors3 O
) O
. O

As O
illustrated O
in O
Figure O
3 O
, O
the O
matrix O
expansion O
enlarges O
W∈ O
Rdw O
in∗dw O
outofSto O
U∈ O
Rdu O
in∗du O

outofTby O
two O
kinds O
of O
operations O
: O
in O
- O
dimension O
and O
out O
- O
dimension O
expansion O
. O

In O
the O
following O
sections O
, O
we O
first O
introduce O
two O
strategies O
of O
width O
- O
wise O
expansion O
: O
functionpreserving O
and O
advanced O
knowledge O
initialization O
. O

Then O
, O
we O
introduce O
the O
depth O
- O
wise O
expansion O
and O
detail O
the O
two O
- O
stage O
pre O
- O
training O
process O
. O

4.3 O
Width O
- O
wise O
Expansion O
For O
the O
paper O
clarity O
, O
we O
introduce O
two O
index O
mapping O
functions O
: O
ginandgout O
, O
where O
gin(i)means O
thei O
- O
th O
in O
- O
dimension O
of O
Ureuses O
the O
gin(i)-th O
indimension O
parameters O
of O
W O
, O
gout(j)means O
the O
j O
- O
th O
out O
- O
dimension O
of O
Ureuses O
the O
gout(j)-th O
outdimension O
parameters O
of O
W. O
Both O
our O
two O
methods O
are O
defined O
with O
these O
two O
mapping O
functions O
. O

W(i O
, O
j)means O
the O
parameter O
element O
, O
iandjrefer O
to O
the O
i O
- O
th O
in O
- O
dimension O
index O
and O
j O
- O
th O
outdimension O
index O
respectively O
. O

As O
shown O
in O
Figure O
3 O
, O
the O
i O
- O
th O
in O
- O
dimension O
parameters O
of O
Ware O
the O
parameters O
of O
the O
i O
- O
th O
input O
neuron O
of O
Wor O
the O
i O
- O
th O
column O
of O
W. O
4.3.1 O
Function O
Preserving O
Initialization O
Function O
preserving O
initialization O
( O
FPI O
) O
( O
Chen O
et O
al O
. O
, O
2016 O
) O
aims O
to O
make O
the O
initialized O
target O
model O
have O
the O
same O
function O
as O
the O
source O
model O
, O
which O
means O
that O
given O
the O
same O
input O
, O
the O
initialized O
target O
model O
has O
the O
same O
output O
as O
the O
source O
model O
. O

In O
this O
paper O
, O
we O
extend O
FPI O
on O
a O
different O
architecture O
, O
Transformer O
- O
based O
pre O
- O
trained O
language O
model O
. O

We O
give O
an O
example O
in O
Figure O
3 O
to O
illustrate O
3We O
omit O
the O
expansion O
of O
bias O
( O
vector O
) O
for O
simplicity O
. O

It O
follows O
a O
similar O
process O
as O
the O
matrix O
expansion O
. O

h1 O
𝑑out𝑤𝑑in𝑤❶ O
❷h2 O
x1 O
x2y1 O
y2 O
𝑜h1 O
h2 O
x1 O
x2y1 O
y2 O
x1h1 O

h2 O
x1 O
x2y1 O
y2 O
x1h2 O
𝑔in O
{ O
1:1,2:2,𝟑:𝟏}𝑔out O
{ O
1:1,2:2,𝟑:𝟐 O
} O
𝑝 O
𝑞𝑟𝑜𝑞𝑝 O

𝑟𝑎𝑏𝑐 O
𝑑 O
𝑜 O
2𝑞 O
2𝑞 O
2𝑜 O
2 O
𝑞 O
2𝑟𝑞 O

2 O
𝑜 O
2 O
𝑞 O
2𝑟𝑝𝑜 O
2𝑞 O
2𝑞 O
2𝑟𝑞 O
2𝑜 O
2𝑝𝑜 O
2 O
𝑞 O
2𝑟𝑞 O
2𝑑in𝑢𝑑in𝑢 O
𝑑out𝑢𝑏 O
2𝑏 O
2 O
𝑑 O
2𝑑 O
2❸ O
COPY O
& O
RE O
- O
SCALECOPY O
𝑾𝑼 O
𝑼~ O
Change O
. O

20211112𝑞 O
2𝑟𝑞 O
2Figure O
3 O
: O
Overview O
of O
the O
function O
preserving O
initialization O
( O
FPI O
) O
. O

Given O
the O
same O
input O
{ O
x1,x2 O
} O
, O
FPI O
ensures O
the O
initialized O
target O
model O
has O
the O
same O
output O
{ O
y1,y2 O
} O
with O
the O
source O
model O
. O

The O
first O
and O
the O
second O
steps O
are O
expanding O
the O
in O
- O
dimension O
and O
out O
- O
dimension O
of O
the O
parameter O
matrix O
according O
to O
mapping O
functions O
ginandgoutrespectively O
. O

After O
we O
expand O
the O
matrix O
WintoU O
, O
we O
use O
the O
in O
- O
dimension O
expansion O
on O
the O
upper O
parameter O
matrix O
again O
to O
ensure O
the O
output O
{ O
y1 O
, O
y2 O
} O
same O
as O
the O
original O
one O
. O

From O
the O
view O
of O
neurons O
, O
FPI O
copies O
the O
corresponding O
input O
and O
output O
neurons O
to O
expand O
the O
neural O
network O
. O
FPI O
. O

Formally O
, O
the O
mapping O
functions O
are O
defined O
as O
follows O
: O
gin(i O
) O
=( O

i O
i O
∈[1 O
, O
dw O
in O
] O
f({1,2 O
, O
... O
, O
dw O
in})i∈(dw O
in O
, O
du O
in],(5 O
) O
gout(j O
) O
=( O
j O
j O
∈[1 O
, O
dw O
out O
] O
f({1,2 O
, O
... O
, O
dw O
out})j∈(dw O
out O
, O
du O
out O
] O
, O
( O
6 O
) O
where O
f(·)is O
uniform O
sampling O
. O

We O
denote O
the O
weight O
expansion O
as O
U= O
EXPN O
( O
W;gin O
, O
gout O
) O
, O
which O
includes O
in O
- O
dimension O
expansion O
( O
Eq O
. O
7 O
) O
and O
out O
- O
dimension O
expansion O
( O
Eq O
. O
8) O
: O
Cgin(i)=du O
inX O
i′=1I(gin(i′ O
) O
= O
gin(i O
) O
) O
eU(i,∗)=1 O
Cgin(i)W(gin(i),∗),(7 O
) O
U(∗,j)=eU(∗,gout(j O
) O
) O
, O
( O
8) O
where O
I(·)is O
an O
indicator O
function O
, O
and O
Cgin(i)is O
the O
count O
of O
gin(i)in O
the O
values O
of O
gin O
( O
· O
) O
, O
which O
is O
used O
to O
re O
- O
scale O
the O
original O
parameters O
to O
keep O
the O
function O
preserving O
property O
. O

Expansion O
for O
All O
Modules O
. O

We O
apply O
FPI O
for O
all O
modules O
of O
BERT O
via O
matrix O
expansion O
EXPN O
( O
· O
) O
. O

Specifically O
, O
for O
the O
embedding O
matrix O
WE O
, O
we O
only O
conduct O
the O
out O
- O
dimension O
expansion O
: O
UE O
( O
∗,j)=WE O
( O
∗,ge O
out(j O
) O
) O
. O

( O
9 O
) O
MHA O
module O
can O
be O
decomposed O
into O
multiple O
parallel O
self O
- O
attention O
heads O
and O
we O
conduct O
the O
head O
- O
wise O
expansion O
for O
this O
module O
, O
which O
means2137increasing O
the O
number O
of O
attention O
heads O
. O

The O
headwise O
expansion O
is O
formulated O
as O
: O
UQ|K|V|O= O
EXPN O
( O
WQ|K|V|O;gq|k|v|o O
in O
, O
gq|k|v|o O
out O
) O
. O

( O
10 O
) O
Specifically O
, O
the O
head O
- O
wise O
expansion O
means O
that O
we O
reuse O
the O
head O
group O
parameters O
to O
construct O
the O
new O
matrices O
. O

The O
i O
- O
th O
head O
group O
in O
l O
- O
th O
layer O
contains O
WQ O
l O
, O
i|WK O
l O
, O
i|WV O
l O
, O
i|WO O
l O
, O
iin O
Eq O
. O

2 O
and O
the O
outdimension O
expansion O
for O
WQ O
l O
, O
i|WK O
l O
, O
i|WV O
l O
, O
iis O
: O
gq|k|v O
out(j O
) O
=( O
j O
j O
∈[1 O
, O
as O
] O
f({1,2 O
, O
... O
, O
as})j∈(as O
, O
at O
] O
, O
( O
11 O
) O
where O
jis O
the O
head O
index O
and O
as|tmean O
the O
head O
numbers O
of O
source O
model O
and O
target O
model O
respectively O
. O

The O
module O
has O
three O
constraints O
: O
{ O
ge O
out O
= O
gq|k|v O
in;gq|k|v O
out O
= O
go O
in;gq|k|v O
in O
= O
go O
out O
} O
, O
with O
the O
first O
two O
constraints O
for O
hidden O
dimension O
consistency O
( O
Wen O
et O
al O
. O
, O
2018 O
; O
Chen O
et O
al O
. O
, O
2021 O
) O
and O
the O
third O
one O
for O
residual O
connection O
( O
Eq O
. O
2 O
) O
. O

For O
the O
FFN O
module O
, O
we O
perform O
the O
expansion O
on O
the O
parameter O
matrices O
W1|2(Eq O
. O

3 O
) O
as O
follows O
: O
U1|2= O
EXPN O
( O
W1|2;g1|2 O
in O
, O
g1|2 O
out O
) O
. O

( O
12 O
) O
Similar O
to O
the O
MHA O
module O
, O
the O
mapping O
functions O
of O
FFN O
also O
have O
three O
constraints O
: O
{ O
go O
out O
= O
g1 O
in O
; O
g1 O
out O
= O
g2 O
in;g1 O
in O
= O
g2 O
out O
} O
. O

For O
the O
layer O
normalization O
, O
we O
take O
the O
layer O
normalization O
of O
FFN O
as O
an O
example O
, O
its O
expansion O
is O
formulated O
as O
: O
ULN O
j O
= O
WLN O
g2 O
out(j O
) O
. O

( O
13 O
) O
Note O
that O
in O
layer O
normalization O
( O
Eq O
. O
4 O
) O
, O
the O
mean O
µ O
and O
variance O
σare O
calculated O
based O
on O
the O
hidden O
representations O
H. O
Thus O
, O
the O
expansion O
of O
this O
parameter O
inevitably O
induces O
a O
gap O
and O
prevents O
the O
target O
model O
from O
strictly O
following O
the O
function O
preserving O
principle O
. O

However O
, O
we O
empirically O
find O
that O
the O
gap O
is O
so O
small O
that O
it O
can O
hardly O
affect O
the O
initialization O
and O
convergence O
of O
the O
target O
model O
. O

Thus O
we O
ignore O
this O
discrepancy O
. O

We O
have O
validated O
the O
effectiveness O
of O
the O
adapted O
FPI O
in O
different O
settings O
in O
Table O
1 O
. O

The O
results O
show O
that O
the O
initialized O
model O
Tachieves O
almost O
the O
same O
loss O
as O
S O
, O
demonstrating O
that O
FPI O
successfully O
retains O
the O
knowledge O
of O
the O
small O
model O
when O
performing O
parameter O
expansion O
. O

4.3.2 O
Advanced O
Knowledge O
Initialization O
To O
further O
improve O
the O
convergence O
rate O
of O
the O
pretraining O
target O
model O
, O
we O
propose O
the O
advanced O
knowledge O
initialization O
( O
AKI O
) O
, O
which O
expands O
newMethod O
S(12,384 O
) O
S(12,512 O
) O

Original O
1.89 O
1.67 O
Rand O
10.40 O
10.42 O
DirectCopy O
9.05 O
6.45 O
FPI O
1.89 O
1.70 O
AKI O
2.08 O
1.96 O
Table O
1 O
: O
The O
comparison O
of O
MLM O
losses O
between O
FPI O
and O
baselines O
. O

“ O
Original O
” O
refers O
to O
the O
MLM O
losses O
of O
source O
pre O
- O
trained O
models O
S. O
“ O
Rand O
” O
refers O
to O
the O
MLM O
losses O
of O
randomly O
initialized O
target O
models O
. O

“ O
DirectCopy O
” O
refers O
to O
a O
naive O
method O
that O
directly O
copies O
the O
source O
model O
to O
the O
target O
model O
and O
the O
unfilled O
part O
is O
randomly O
initialized O
, O
“ O
FPI O
” O
represents O
the O
function O
preserving O
method O
. O

We O
expand O
both O
models O
to O
the O
target O
model O
T(12,768 O
) O
and O
find O
that O
FPI O
can O
make O
the O
target O
model O
have O
similar O
losses O
with O
these O
trained O
source O
models O
. O

The O
loss O
gap O
between O
FPI O
and O
Original O
is O
brought O
by O
layer O
normalization O
. O

“ O
AKI O
” O
represents O
the O
advanced O
knowledge O
initialization O
method O
. O

matrices O
based O
on O
not O
only O
the O
parameters O
of O
the O
same O
layer O
but O
also O
the O
parameters O
of O
the O
upper O
layer O
in O
the O
source O
model O
. O

The O
intuition O
is O
based O
on O
previous O
findings O
( O
Jawahar O
et O
al O
. O
, O
2019 O
; O
Clark O
et O
al O
. O
, O
2019 O
) O
that O
adjacent O
Transformer O
layers O
have O
similar O
functionality O
, O
which O
ensures O
that O
it O
will O
not O
damage O
the O
knowledge O
contained O
in O
the O
parameters O
of O
the O
current O
layer O
. O

Moreover O
, O
the O
knowledge O
that O
comes O
from O
adjacent O
layers O
can O
break O
the O
symmetry O
( O
Chen O
et O
al O
. O
, O
2016 O
) O
appeared O
in O
FPI O
, O
which O
has O
been O
demonstrated O
beneficial O
. O

We O
give O
an O
illustrative O
example O
in O
Figure O
4 O
and O
formulate O
AKI O
as O
: O
Ul= O
EXPN O
( O
Wl O
, O
Wl+1;gl|l+1 O
in O
, O
gl O
out).(14 O
) O

Specifically O
, O
we O
first O
do O
the O
in O
- O
dimension O
expansion O
forWl|l+1 O
. O

Here O
we O
take O
Wlas O
an O
example O
: O
Cgl O
in(i)=du O
inX O
i′=1I(gl O
in(i′ O
) O
= O
gl O
in(i O
) O
) O

eUl O
( O
i,∗)=1 O
Cgl O
in(i)Wl O
( O
gl O
in(i),∗).(15 O
) O
It O
is O
similar O
with O
Eq O
. O

7 O
. O
Then O
we O
stack O
the O
expanded O
matrices O
of O
eUlandeUl+1to O
construct O
the O
final O
matrix O
: O

Ul O
( O
∗,j)=(eUl O
( O
∗,j)j∈[1 O
, O
dw O
out O
] O
eUl+1 O
( O
∗,gl O
out(j))j∈(dw O
out O
, O
du O
out].(16 O
) O
We O
directly O
copy O
the O
expanded O
eUlas O
the O
top O
part O
of O
the O
new O
matrix O
and O
place O
the O
sampled O
parameters O
fromeUl+1on O
the O
bottom O
of O
the O
new O
matrix O
. O

We O
aggregate O
upper O
- O
layer O
information O
into O
a O
new O
matrix O
for O
two O
intuitions O
: O
( O
1 O
) O
it O
breaks O
the O
FPI O
symmetry O
that O
hinders O
model O
convergence O
( O
Chen O
et O
al O
. O

, O
2138𝑜𝑝 O
𝑞𝑟 O
𝑎𝑏 O
𝑐𝑑𝑔in𝑙 O
{ O
1:1,2:2,𝟑:𝟏}𝑜 O
2 O
𝑞 O
2𝑟𝑝𝑜 O
2𝑞 O
2 O
𝑏 O
2𝑑 O
2𝑐𝑎𝑏 O
2𝑑 O
2𝑔in𝑙+1 O
{ O
1:1,2:2,𝟑:𝟐}𝑜 O
2 O
𝑞 O
2𝑟𝑝𝑜 O
2𝑞 O
2𝑔out𝑙 O
{ O
1:1,2:2,𝟑:𝟐 O
} O
𝑑 O
2𝑐𝑑 O
2❶ O
❷ O
Cheng O
20211113COPY𝑑in𝑤 O
𝑑out𝑤𝑑in𝑢𝑑in𝑢 O
𝑑out𝑢 O
𝑾𝑙+1𝑾𝑙 O
෩𝑼𝑙 O
෩𝑼𝑙+1𝑼𝑙Figure O
4 O
: O
Overview O
of O
AKI O
. O

It O
first O
performs O
the O
indimension O
expansion O
on O
both O
the O
matrixes O
of O
current O
and O
upper O
layers O
. O

Then O
it O
uses O
the O
widened O
matrix O
of O
the O
current O
layer O
as O
the O
top O
part O
of O
the O
new O
matrix O
and O
samples O
the O
row O
of O
the O
widened O
matrix O
of O
the O
upper O
layer O
as O
the O
bottom O
part O
of O
the O
new O
matrix O
. O
2016 O
) O
. O

For O
example O
, O
FPI O
makes O
the O
attention O
patterns O
in O
the O
same O
layer O
repeated O
, O
which O
is O
redundant O
and O
called O
symmetry O
; O
( O
2 O
) O
upper O
- O
layer O
information O
can O
be O
used O
as O
similar O
but O
high O
- O
level O
knowledge O
to O
guide O
the O
model O
to O
converge O
faster O
. O

We O
display O
the O
attention O
patterns O
of O
the O
target O
model O
initialized O
by O
AKI O
in O
Appendix O
E O
and O
find O
that O
the O
target O
model O
can O
maintain O
the O
attention O
patterns O
of O
both O
current O
and O
upper O
layers O
very O
well O
. O

Expansion O
for O
All O
Modules O
. O

For O
embedding O
matrix O
, O
we O
only O
do O
the O
out O
- O
dimension O
expansion O
as O
Eq O
. O
9 O
in O
the O
FPI O
. O

Both O
the O
modules O
of O
MHA O
and O
FFN O
do O
the O
matrix O
expansion O
by O
following O
the O
defined O
operation O
in O
Eq O
. O
15 O
and O
Eq O
. O

16 O
. O

The O
constraints O
of O
mapping O
functions O
follow O
the O
setting O
of O
FPI O
. O

Empirically O
, O
we O
find O
that O
the O
AKI O
method O
outperforms O
FPI O
, O
while O
the O
performance O
is O
worse O
if O
we O
build O
a O
new O
matrix O
based O
on O
the O
matrix O
of O
the O
lower O
layer O
( O
or O
low O
- O
level O
knowledge O
) O
. O

How O
to O
construct O
the O
optimal O
initialization O
for O
the O
target O
model O
with O
the O
parameters O
of O
different O
layers O
remains O
an O
open O
question O
and O
we O
leave O
it O
as O
future O
work O
. O

For O
more O
details O
, O
we O
give O
a O
clear O
illustration O
of O
the O
FPI O
and O
AKI O
process O
in O
Appendix O
F. O
4.4 O
Depth O
- O
wise O
Expansion O
After O
the O
width O
- O
wise O
expansion O
, O
we O
obtain O
a O
widened O
model O
with O
the O
same O
width O
as O
the O
target O
model O
. O

To O
bridge O
the O
depth O
gap O
, O
we O
perform O
depthwise O
expansion O
to O
increase O
model O
depth O
to O
the O
depth O
of O
the O
target O
model O
. O

We O
illustrate O
this O
process O
in O
Algorithm O
1 O
and O
the O
main O
idea O
is O
to O
iteratively O
stack O
the O
widened O
model O
until O
its O
depth O
is O
equal O
to O
the O
target O
model O
( O
Gong O
et O
al O
. O
, O
2019 O
) O
. O

4.5 O
Two O
- O
stage O
Pre O
- O
training O
To O
further O
improve O
the O
pre O
- O
training O
efficiency O
of O
initialized O
target O
model O
, O
we O
propose O
a O
two O
- O
stage O
training O
method O
: O
( O
1 O
) O
train O
sub O
- O
models O
with O
differentAlgorithm O
1 O
Target O
Model O
Initialization O
Input O
: O
the O
target O
model O
T(Lt O
, O
Dt)and O
the O
source O
model O
S(Ls O
, O
Ds O
) O
. O

1 O
: O
T1(Ls O
, O
Dt)←do O
AKI O
or O
FPI O
with O
S(Ls O
, O
Ds O
) O
2 O
: O
k← O
⌊Lt O
/ O
Ls⌋ O
3 O
: O
fort= O
2→kdo O
4 O
: O
Tt(Ls·t O
, O
Dt)←stackT1on O
top O
of O
Tt−1 O
5 O
: O
end O
for O
6 O
: O
T O
← O
stack O
top O
Lt−Ls·klayers O
of O
T1 O
. O

Output O
: O
the O
initialized O
model O
T(Lt O
, O
Dt O
) O
Algorithm O
2 O
Two O
- O
stage O
Pre O
- O
training O
Input O
: O
the O
initialized O
model O
T O
, O
large O
- O
scale O
unsupervised O
dataset O
D O
, O
the O
epoch O
number O
of O
submodel O
training O
Eband O
the O
epoch O
number O
of O
whole O
training O
process O
E O
, O
the O
layer O
number O
lb O
. O
1 O
: O
Construct O
sub O
- O
models O
and O
these O
models O
have O
the O
layer O
numbers O
of O
{ O
lb,2·lb O
, O
. O
. O
. O

, O
Lt O
} O
. O

2 O
: O
fore= O
1→Ebdo O
3 O
: O
forbatch O
inDdo O
4 O
: O
T′←sample O
one O
sub O
- O
model O
. O
5 O
: O
Perform O
forward O
and O
backward O
of O
T′. O
6 O
: O
Update O
only O
top O
lblayers O
of O
T′. O
7 O
: O
end O
for O
8 O
: O
end O
for O
9 O
: O
fore O
= O
Eb→Edo O
10 O
: O
forbatch O
inDdo O
11 O
: O
Perform O
forward O
and O
backward O
of O
T. O
12 O
: O
Update O
whole O
model O
T. O
13 O
: O
end O
for O
14 O
: O
end O
for O
Output O
: O
the O
pre O
- O
trained O
model O
T O
layers O
in O
a O
random O
manner O
to O
make O
the O
complete O
model O
converge O
at O
a O
low O
cost O
. O

These O
sub O
- O
models O
are O
built O
with O
bottom O
Transformer O
layers O
of O
the O
initialized O
target O
model O
and O
share O
one O
classification O
layer O
. O

At O
each O
optimization O
step O
, O
we O
randomly O
sample O
one O
sub O
- O
model O
and O
only O
update O
its O
top O
Transformer O
layers O
and O
the O
shared O
classification O
layer O
. O

( O
2 O
) O
After O
the O
sub O
- O
structure O
training O
, O
we O
further O
perform O
the O
traditional O
full O
- O
model O
training O
. O

The O
details O
of O
our O
method O
are O
displayed O
in O
Algorithm O
2 O
. O
5 O
Experiment O
5.1 O
Experimental O
Setup O
Pre O
- O
training O
Details O
. O

We O
use O
the O
English B-DatasetName
Wikipedia I-DatasetName
and O
Toronto B-DatasetName
Book I-DatasetName
Corpus I-DatasetName
( O
Zhu O
et O
al O
. O
, O
2015 O
) O
as O
the O
pre O
- O
training O
data O
. O

The O
settings O
of O
pretraining O
are O
: O
peak O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-4 B-HyperparameterValue
, O
warmup2139Model O
FLOPs O
Ratio O
Loss O
SQuADv1.1 O
SST-2 O
MNLI O
MRPC O
CoLA O
QNLI O
QQP O
STS O
- O
B O
Avg O
. O
( O
×1e19 O
) O
( O
Saving O
) O
( O
MLM O
) O
( O
F1 O
) O
( O
Acc O
) O
( O
Acc O
) O
( O
Acc O
) O
( O
Mcc O
) O
( O
Acc O
) O
( O
Acc O
) O
( O
Acc O
) O
BERT O
BASE O
( O
Google O
) O
- O
- O
- O
88.4(0.1 O
) O
93.6(0.2 O
) O
84.7(0.1 O
) O
87.9(0.9 O
) O
59.6(1.5 O
) O
91.6(0.1 O
) O
91.4(0.1 O
) O
89.6(0.5 O
) O
85.8(0.1 O
) O
BERT O
BASE†(Ours O
) O
7.3 O
0 O
% O
1.437 O
89.6(0.1 O
) O
92.7(0.2 O
) O
84.6(0.2 O
) O
88.6(0.5 O
) O
57.3(4.0 O
) O
90.6(0.7 O
) O
90.6(0.1 O
) O
89.9(0.3 O
) O
85.5(0.5 O
) O
Progressive O
Training O
MSLT† O
6.5 O
10.7 O
% O
1.436 O
90.4(0.2 O
) O
92.9(0.2 O
) O
85.1(0.2 O
) O
87.9(2.1 O
) O
55.6(4.1 O
) O
90.7(0.2 O
) O
90.6(0.2 O
) O
88.2(0.6 O
) O
85.2(0.7 O
) O
StackBERT† O
5.5 O
24.3 O
% O
1.433 O
90.4(0.2 O
) O
92.6(0.4 O
) O
85.3(0.1 O
) O
88.2(1.0 O
) O
63.2(0.9 O
) O
91.0(0.4 O
) O
91.0(0.1 O
) O
86.7(0.7 O
) O
86.0(0.2 O
) O
bert2BERT O
: O
S(12 O
, O
512 O
) O
→ O
T O
( O
12 O
, O
768 O
) O
DirectCopy O
6.4 O
12.2 O
% O
1.436 O
89.8(0.2 O
) O
92.9(0.3 O
) O
84.7(0.2 O
) O
86.2(0.6 O
) O
62.2(0.7 O
) O
90.2(0.6 O
) O
90.4(0.1 O
) O
89.2(0.1 O
) O
85.7(0.1 O
) O
FPI O
5.1 O
30.4 O
% O
1.436 O
90.0(0.2 O
) O
92.6(0.4 O
) O
85.2(0.1 O
) O
87.1(0.5 O
) O
61.5(0.9 O
) O
90.9(0.6 O
) O
90.8(0.2 O
) O
89.7(0.2 O
) O
86.0(0.1 O
) O
AKI O
4.5 O
38.4 O
% O
1.434 O
90.4(0.1 O
) O
92.5(0.4 O
) O
85.3(0.4 O
) O
87.8(0.9 O
) O
61.0(1.4 O
) O
91.2(0.2 O
) O
90.5(0.1 O
) O
89.5(0.2 O
) O
86.0(0.2 O
) O
bert2BERT O
4.0 O
45 O
.2 O
% O
1.433 O
90.0(0.2 O
) O
92.9(0.1 O
) O
85.1(0.1 O
) O
87.7(0.7 O
) O
60.0(1.2 O
) O
90.5(0.8 O
) O
90.4(0.1 O
) O
89.2(0.2 O
) O
85.7(0.4 O
) O
Table O
2 O
: O
Comparison O
between O
bert2BERT B-MethodName
and O
baselines O
. O

We O
report O
mean O
( O
and O
standard O
deviation O
) O
performance O
over O
3 O
runs O
on O
the O
dev O
set O
. O

bert2BERT B-MethodName
means O
the O
combination O
of O
AKI O
and O
two O
- O
stage O
pre O
- O
training O
here O
. O

FPI O
and O
AKI O
mean O
that O
the O
function O
preserving O
initialization O
, O
advanced O
knowledge O
initialization O
respectively O
. O

†means O
the O
re O
- O
implemented O
results O
, O
where O
the O
BERT B-MethodName
BASE I-MethodName
and O
StackBERT B-MethodName
achieve O
similar O
results O
with O
the O
original O
paper O
, O
and O
the O
MSLT B-MethodName
result O
is O
different O
from O
the O
original O
paper O
may O
be O
due O
to O
the O
different O
training O
settings O
( O
e.g. O
, O
in O
the O
original O
paper O
, O
it O
uses O
the O
LAMB O
optimizer O
( O
You O
et O
al O
. O
, O
2020 O
) O
and O
only O
trains O
the O
corpus O
with O
a O
max O
sequence O
length O
of O
128 O
) O
. O

steps B-HyperparameterName
of O
10k B-HyperparameterValue
, O
training O
epochs B-HyperparameterName
of O
E=40 B-HyperparameterValue
, O
batch B-HyperparameterName
size I-HyperparameterName
of O
512 B-HyperparameterValue
, O
sub B-HyperparameterName
- I-HyperparameterName
model I-HyperparameterName
training I-HyperparameterName
epochs I-HyperparameterName
of O
Eb=5 B-HyperparameterValue
, O
layer B-HyperparameterName
number I-HyperparameterName
of O
lb=3 B-HyperparameterValue
. O

Unless O
otherwise O
noted O
, O
all O
methods O
including O
bert2BERT B-MethodName
and O
baselines O
use O
the O
same O
pre O
- O
training O
settings O
for O
fair O
comparisons O
. O

In O
the O
settings O
of O
bert2BERT B-MethodName
, O
the O
target O
model O
has O
a O
BERT B-MethodName
BASE I-MethodName
architecture O
of O
T(12,768 O
) O
and O
the O
source O
model O
has O
an O
architecture O
of O
S(12,512 O
) O
. O

Fine O
- O
tuning O
Details O
. O

For O
the O
evaluation O
, O
we O
use O
tasks O
from O
GLUE B-DatasetName
benchmark O
( O
Wang O
et O
al O
. O
, O
2019a O
) O
and O
SQuADv1.1 B-DatasetName
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
. O

We O
report O
F1 B-MetricName
for O
SQuADv1.1 B-DatasetName
, O
Matthews B-MetricName
correlation I-MetricName
coefficient I-MetricName
( O
Mcc O
) O
for O
CoLA B-DatasetName

( O
Warstadt O
et O
al O
. O
, O
2019 O
) O
and O
accuracy B-MetricName
( O
Acc O
) O
for O
other O
tasks O
. O

For O
the O
GLUE B-DatasetName
tasks O
fine O
- O
tuning O
, O
we O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
32 B-HyperparameterValue
, O
choose O
the O
learning B-HyperparameterName
rate I-HyperparameterName
from O
{ O
5e-6 B-HyperparameterValue
, O
1e-5 B-HyperparameterValue
, O
2e-5 B-HyperparameterValue
, O
3e-5 B-HyperparameterValue
} O
and O
epochs B-HyperparameterName
from O
{ O
4 B-HyperparameterValue
, O
5 B-HyperparameterValue
, O
10 B-HyperparameterValue
} O
. O

For O
the O
SQuADv1.1 B-DatasetName
finetuning O
, O
we O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
16 B-HyperparameterValue
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
3e-5 B-HyperparameterValue
, O
and O
the O
number B-HyperparameterName
of I-HyperparameterName
training I-HyperparameterName
epochs I-HyperparameterName
to O
4 B-HyperparameterValue
. O

All O
results O
are O
the O
average O
of O
3 O
runs O
on O
the O
dev O
set O
. O
Baselines O
. O

We O
first O
introduce O
a O
naive O
bert2BERT B-MethodName
baseline O
named O
DirectCopy O
, O
which O
directly O
copies O
the O
small O
model O
to O
the O
target O
model O
and O
randomly O
initializes O
the O
unfilled O
parameters O
. O

StackBERT B-MethodName
( O
Gong O
et O
al O
. O
, O
2019 O
) O
and O
MSLT B-MethodName
( O
Yang O
et O
al O
. O
, O
2020 O
) O
are O
also O
included O
as O
the O
baselines O
. O

Both O
of O
them O
are O
trained O
in O
a O
progressive O
manner O
. O

Following O
the O
original O
setting O
, O
for O
the O
StackBERT B-MethodName
, O
we O
first O
train O
the O
3 O
- O
layer O
BERT B-MethodName
for O
5 B-HyperparameterValue
epochs B-HyperparameterName
, O
stack O
it O
twice O
into O
a O
6 O
- O
layer O
BERT B-MethodName
and O
then O
train O
it O
for O
7 B-HyperparameterValue
epochs B-HyperparameterName
. O

In O
the O
final O
step O
, O
we O
stack O
the O
6 O
- O
layer O
model O
into O
BERT B-MethodName
BASE I-MethodName
and O
further O
train O
it O
with O
28 B-HyperparameterValue
epochs B-HyperparameterName
. O

For O
MSLT B-MethodName
, O
we O
first O
perform O
4 O
- O
stage O
training O
. O

In O
each O
stage O
, O
we O
add O
the O
top O
3 O
layers O
of O
the O
model O
already O
trained O
to O
the O
top O
of O
the O
model O
and O
then O
pre O
- O
train O
the O
new O
model O
by O
partially O
updatingthe O
top O
3 O
layers O
. O

Each O
stage O
of O
the O
partial O
training O
process O
has O
8 B-HyperparameterValue
epochs B-HyperparameterName
. O

Finally O
, O
we O
further O
perform O
20 O
full O
- O
model O
training O
epochs4to O
achieve O
the O
same O
loss O
as O
BERT B-MethodName
BASE I-MethodName
trained O
from O
scratch O
. O

The O
baselines O
are O
trained O
using O
the O
same O
optimizer O
, O
training B-HyperparameterName
steps I-HyperparameterName
, O
and O
warmup B-HyperparameterName
steps I-HyperparameterName
as O
the O
bert2BERT B-MethodName
. O
5.2 O
Results O
and O
Analysis O
We O
demonstrate O
the O
effectiveness O
of O
the O
proposed O
method O
on O
the O
SQuAD B-DatasetName
and O
GLUE B-DatasetName
benchmark O
. O

The O
results O
are O
shown O
in O
Table O
2 O
. O

We O
also O
represent O
the O
loss O
curves O
in O
Figure O
1 O
and O

Appendix O
A. O

The O
results O
show O
that O
: O
( O
1 O
) O
DirectCopy O
only O
saves O
12.2 O
% O
computational O
costs O
, O
which O
indicates O
this O
naive O
method O
of O
directly O
copying O
the O
trained O
parameters O
of O
the O
source O
model O
to O
the O
target O
model O
is O
not O
effective O
; O
( O
2 O
) O
our O
proposed O
methods O
, O
FPI O
and O
AKI O
, O
achieve O
better O
performances O
than O
the O
baselines O
. O

Although O
AKI O
does O
not O
follow O
the O
function O
preserving O
, O
it O
has O
a O
bigger O
loss O
than O
FPI O
at O
the O
start O
of O
training O
, O
AKI O
achieves O
a O
faster O
convergence O
rate O
by O
using O
the O
advanced O
knowledge O
and O
breaking O
the O
symmetry O
; O
( O
3 O
) O
by O
performing O
the O
two O
- O
stage O
pre O
- O
training O
on O
the O
target O
model O
initialized O
by O
AKI O
, O
we O
can O
save O
45.2 O
% O
computational O
costs O
. O

Note O
that O
the O
total O
parameters O
of O
the O
source O
model O
are O
half O
of O
those O
of O
the O
target O
model O
( O
54 O
M O
vs. O
110 O
M O
) O
. O

The O
loss O
of O
bert2BERT B-MethodName
in O
Figure O
1 O
is O
high O
at O
the O
stage O
of O
sub O
- O
model O
training O
because O
it O
represents O
the O
average O
loss O
of O
all O
submodels O
. O

We O
also O
compare O
the O
attention O
patterns O
of O
the O
target O
models O
initialized O
by O
DirectCopy O
, O
FPI O
, O
and O
AKI O
. O

The O
attention O
patterns O
and O
their O
discussions O
are O
displayed O
in O
Appendix O
E. O
4We O
have O
tried O
the O
same O
setting O
as O
the O
original O
paper O
with O
8 O
epoch O
full O
- O
model O
running O
but O
it O
does O
not O
achieve O
the O
same O
loss O
with O
BERT O
BASE O
( O
1.511 O
vs. O
1.437).2140bert2BERT O
with O
Smaller O
Source O
Model O
. O

We O
also O
evaluate O
bert2BERT B-MethodName
on O
different O
settings O
, O
where O
the O
source O
model O
S(6 O
, O
512 O
) O
, O
S(8 O
, O
512 O
) O
, O
S(10 O
, O
512 O
) O
are O
significantly O
smaller O
than O
the O
target O
model O
( O
35 O
M O
| O
42 O
M O
| O
48 O
M O
vs. O
110 O
M O
) O
. O

The O
results O
are O
shown O
in O
Table O
3 O
and O
loss O
curves O
are O
displayed O
in O
Appendix O

B. O

We O
observe O
that O
DirectCopy O
for O
S(6 O
, O
512 O
) O
achieves O
no O
efficiency O
improvement O
over O
the O
original O
pre O
- O
training O
, O
which O
indicates O
that O
the O
significant O
size O
gap O
between O
the O
source O
and O
target O
model O
greatly O
reduces O
the O
benefit O
of O
DirectCopy O
methods O
. O

Compared O
with O
DirectCopy O
, O
our O
proposed O
method O
reduces O
the O
computation O
cost O
by O
23.3 O
% O
, O
which O
again O
demonstrates O
the O
effectiveness O
of O
bert2BERT B-MethodName
. O

The O
results O
show O
that O
the O
smaller O
the O
size O
gap O
between O
the O
source O
model O
and O
target O
model O
, O
the O
greater O
the O
cost O
savings O
of O
bert2BERT B-MethodName
. O

We O
also O
note O
that O
it O
is O
more O
challenging O
to O
speed O
up O
the O
target O
model O
with O
a O
small O
source O
model O
S(6 O
, O
512 O
) O
. O

We O
encourage O
future O
work O
to O
explore O
to O
transfer O
the O
knowledge O
from O
smaller O
source O
models O
to O
improve O
the O
pre O
- O
training O
efficiency O
of O
the O
target O
model O
. O

Settings O
Model O
FLOPs O
Ratio O
Loss O
Avg O
. O
( O
×1e19 O
) O
( O
Saving O
) O
( O
MLM O
) O
S(6 O
, O
512)DirectCopy O
7.3 O
0 O
% O
1.440 O
89.1 O
bert2BERT O
5.6 O
23.3 O
% O
1.435 O
89.3 O
S(8 O
, O
512 O
) O

bert2BERT O
4.6 O
36.8 O
% O
1.435 O
89.2 O
S(10 O
, O
512 O
) O
bert2BERT O
4.2 O
42.7 O
% O
1.434 O
89.1 O
Table O
3 O
: O
bert2BERT O
with O
smaller O
source O
model O
. O

Avg O
means O
the O
average O
score O
of O
SST-2 O
/ O
MNLI O
/ O
SQuADv1.1 O
. O

Effect O
of O
Sub O
- O
model O
Training O
Epochs O
. O

Our O
training O
procedure O
includes O
two O
stages O
: O
sub O
- O
model O
training O
and O
full O
- O
model O
training O
. O

Here O
, O
we O
study O
the O
effect O
of O
the O
number O
of O
sub O
- O
model O
training O
epochs O
by O
performing O
bert2BERT O
on O
the O
different O
settings O
ofEb={0 O
, O
5 O
, O
10 O
, O
20 O
} O
. O

The O
results O
are O
presented O
in O
Table O
4 O
and O
the O
loss O
curves O
are O
displayed O
in O
Appendix O
C. O

We O
observe O
that O
our O
method O
achieves O
the O
best O
efficiency O
when O
the O
epoch O
number O
is O
set O
to O
5 O
, O
while O
a O
larger O
or O
smaller O
epoch O
number O
will O
bring O
a O
negative O
impact O
. O

Model O
FLOPs O
Ratio O
Loss O
Avg O
. O
( O
×1e19 O
) O
( O
Saving O
) O
( O
MLM O
) O
bert2BERT O
: O
S(12 O
, O
512 O
) O
→ O
T O
( O
12 O
, O
768 O
) O
bert2BERT O
( O
Eb= O
0 O
) O
4.5 O
38.4 O
% O
1.434 O
89.4 O
bert2BERT O
( O
Eb= O
5 O
) O
4.0 O
45 O
.2 O
% O
1.433 O
89.3 O
bert2BERT O
( O
Eb= O
10 O
) O
4.1 O
43.9 O
% O
1.436 O
89.3 O
bert2BERT O
( O
Eb= O
20 O
) O
5.4 O
25.4 O
% O
1.448 O
89.1 O
Table O
4 O
: O
Effect O
of O
sub O
- O
model O
training O
epochs O
. O

Avg O
means O
the O
average O
score O
of O
SST-2 O
/ O
MNLI O
/ O
SQuADv1.1.5.3 O
Application O
on O
GPT B-MethodName
Datasets O
. O

To O
demonstrate O
that O
our O
method O
is O
generic O
, O
following O
the O
BERT O
setting O
, O
we O
also O
use O
the O
English O
Wikipedia O
and O
Book O
Corpus O
in O
the O
GPTtraining O
. O

For O
the O
evaluation O
, O
we O
use O
the O
datasets O
of O
WikiText-2 O
, O
PTB O
, O
and O
WikiText103 O
and O
evaluate O
these O
models O
under O
the O
zero O
- O
shot O
setting O
without O
fine O
- O
tuning O
on O
the O
training O
set O
. O

Implementation O
Details O
. O

We O
use O
the O
architecture O
of O
{ O
L=12,D=768 O
} O
for O
the O
GPT O
target O
model O
, O
and O
pre O
- O
train O
it O
with O
the O
learning O
rate O
of O
1e-4 O
, O
training O
epochs O
of O
20 O
. O

For O
bert2BERT O
, O
we O
use O
the O
source O
model O
with O
an O
architecture O
of O
{ O
L=12 O
, O
D=512 O
} O
, O
initialize O
the O
target O
model O
with O
AKI O
, O
and O
pre O
- O
train O
it O
by O
the O
full O
- O
model O
training O
( O
Eb=0 O
) O
. O
Results O
and O
Analysis O
. O

We O
compare O
the O
original O
pre O
- O
training O
method O
and O
bert2BERT O
, O
the O
results O
are O
shown O
in O
Table O
5 O
and O

Appendix O
D. O

We O
observe O
that O
the O
proposed O
method O
saves O
47 O
% O
computation O
cost O
of O
GPT O
pre O
- O
training O
, O
exhibiting O
a O
similar O
trend O
to O
BERT O
pre O
- O
training O
. O

Although O
GPT O
and O
BERT O
have O
different O
architectures O
( O
e.g. O
, O
postLN O
and O
pre O
- O
LN O
( O
Xiong O
et O
al O
. O
, O
2020 O
) O
) O
and O
are O
pretrained O
with O
different O
tasks O
, O
bert2BERT O
saves O
a O
significant O
amount O
of O
training O
cost O
on O
both O
these O
two O
models O
, O
which O
shows O
that O
the O
proposed O
method O
is O
generic O
and O
is O
effective O
for O
different O
kinds O
of O
PLMs O
. O

Model O
FLOPs O
PTB O
WikiText-2 O
WikiText103 O
( O
×1e19 O
) O
( O
w/o O
FT O
) O
( O
w/o O
FT O
) O
( O
w/o O
FT O
) O
bert2BERT O
: O
S(12 O
, O
512 O
) O
→ O
T O
( O
12 O
, O
768 O
) O
GPT O
4.9 O
133.8 O
47.0 O
53.5 O
bert2BERT O
2.6(47%↓ O
) O
132.1 O
47.9 O
53.0 O
Table O
5 O
: O
Experiments O
on O
GPT O
. O

We O
report O
the O
perplexity O
for O
these O
tasks O
. O

“ O
w/o O
FT O
” O
means O
that O
the O
pre O
- O
trained O
model O
is O
directly O
evaluated O
on O
the O
test O
set O
without O
finetuning O
on O
the O
train O
set O
. O

5.4 O
Application O
on O
T5 B-MethodName
Datasets O
. O

To O
demonstrate O
that O
our O
method O
can O
be O
used O
to O
train O
larger O
models O
, O
we O
use O
the O
Baidu B-DatasetName
Wikipedia I-DatasetName
, O
Sougou B-DatasetName
Wikipedia I-DatasetName
, O
and O
Zhihu B-DatasetName
to O
train O
the O
T5 B-MethodName
model O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
. O

For O
the O
evaluation O
, O
we O
use O
the O
dataset O
of O
the O
original O
Chinese B-TaskName
natural I-TaskName
language I-TaskName
inference I-TaskName
task O
( O
OCNLI B-DatasetName
) O

( O
Hu O
et O
al O
. O
, O
2020 O
) O
. O

Implementation O
Details O
. O

Since O
the O
bert2BERT B-MethodName
method O
is O
suitable O
for O
BERT B-MethodName
and O
GPT B-MethodName
, O
it O
can O
also O
be O
used O
for O
the O
T5 B-MethodName
model O
, O
which O
consists O
of O
an O
encoder O
and O
a O
decoder O
. O

The O
target O
T5 B-MethodName
model O
’s O
architecture O
is O
{ O
Le=12,Ld=12,D=1024 O
, O
A=16 O
} O
, O
where2141LeandLdmeans O
the O
numbers O
of O
encoder O
and O
decoder O
Transformer O
layers O
respectively O
, O
Dmeans O
the O
hidden O
size O
, O
Ameans O
the O
number O
of O
attention O
heads O
. O

We O
pre O
- O
train O
it O
with O
the O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-4 B-HyperparameterValue
, O
batch B-HyperparameterName
size I-HyperparameterName
of O
1024 B-HyperparameterValue
. O

For O
bert2BERT B-MethodName
, O
we O
use O
the O
source O
model O
with O
an O
architecture O
of O
{ O
Le=12 O
, O
Ld=12,D=256 O
, O
A=4 O
} O
, O
initialize O
the O
target O
model O
with O
FPI O
, O
and O
pre O
- O
train O
it O
by O
the O
full O
- O
model O
training O
( O
Eb=0 O
) O
. O

Note O
that O
the O
scale O
gap O
between O
the O
source O
model O
and O
the O
target O
model O
is O
over O
10 O
times O
( O
31 O
M O
vs. O
360 O
M O
) O
, O
which O
is O
a O
challenging O
setting O
. O
Results O
and O
Analysis O
. O

We O
compare O
the O
original O
pre O
- O
training O
method O
and O
bert2BERT B-MethodName
method O
on O
the O
T5 B-MethodName
model O
, O
the O
results O
are O
shown O
in O
Table O
6 O
. O

We O
observe O
that O
the O
proposed O
method O
saves O
at O
least O
25 O
% O
computation O
cost O
of O
T5 B-MethodName
pre O
- O
training O
. O

It O
demonstrates O
the O
effectiveness O
of O
the O
method O
on O
larger O
models O
. O

Model O
FLOPs O
Loss O
OCNLI O
( O
×1e20 O
) O
( O
MLM O
) O
( O
Acc O
) O
bert2BERT O
: O
S(12 O
, O
12 O
, O
256 O
, O
4 O
) O
→ O
T O
( O
12 O
, O
12 O
, O
1024 O
, O
16 O
) O
T5 O
1.6 O
1.90 O
72.03 O
bert2BERT O
1.2(25%↓ O
) O
1.90 O
72.75 O
Table O
6 O
: O
Experiments O
on O
the O
T5 O
model O
. O

6 O
Conclusion O
and O
Future O
Work O
This O
paper O
proposes O
an O
efficient O
pre O
- O
training O
method O
, O
bert2BERT B-MethodName
, O
which O
reuses O
the O
parameters O
of O
the O
small O
trained O
model O
as O
the O
initialization O
parameters O
of O
the O
large O
model O
. O

We O
employ O
the O
proposed O
method O
in O
BERT B-MethodName
and O
GPT B-MethodName
under O
different O
settings O
of O
model O
sizes O
. O

The O
extensive O
results O
show O
that O
bert2BERT B-MethodName
is O
generic O
to O
Transformerbased O
models O
and O
saves O
a O
significant O
amount O
of O
computation O
cost O
. O

Moreover O
, O
the O
detailed O
analysis O
shows O
that O
our O
techniques O
, O
function O
- O
preserving O
, O
advanced O
knowledge O
initialization O
, O
and O
two O
- O
stage O
pre O
- O
training O
, O
are O
all O
effective O
. O

In O
the O
future O
, O
we O
will O
apply O
bert2BERT B-MethodName
on O
training O
super O
largescale O
language O
models O
( O
e.g. O
, O
use O
the O
10B O
source O
model O
to O
train O
the O
100B O
target O
model O
) O
and O
extends O
its O
scope O
to O
other O
PLMs O
such O
as O
ELECTRA B-MethodName
and O
BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
. O

Acknowledgements O
This O
work O
is O
supported O
in O
part O
by O
NSFC O
( O
Grant O
No O
. O
61872215 O
) O
, O
and O
Shenzhen O
Science O
and O
Technology O
Program O
( O
Grant O
No O
. O
RCYX20200714114523079 O
) O
. O

We O
would O
like O
to O
thank O
Yifeng O
Liu O
, O
Binbin O
Deng O
, O
Ziliang O
Yang O
, O
Jiaxin O
Shi O
for O
their O
support O
of O
this O
work O
. O

References O
Jimmy O
Lei O
Ba O
, O
Jamie O
Ryan O
Kiros O
, O
and O
Geoffrey O
E. O
Hinton O
. O
2016 O
. O

Layer O
normalization O
. O

ArXiv O
preprint O
, O
abs/1607.06450 O
. O

Tom O
B. O
Brown O
, O
Benjamin O
Mann O
, O
Nick O
Ryder O
, O
Melanie O
Subbiah O
, O
Jared O
Kaplan O
, O
Prafulla O
Dhariwal O
, O
Arvind O
Neelakantan O
, O
Pranav O
Shyam O
, O
Girish O
Sastry O
, O
Amanda O
Askell O
, O
Sandhini O
Agarwal O
, O
Ariel O
Herbert O
- O
V O
oss O
, O
Gretchen O
Krueger O
, O
Tom O
Henighan O
, O
Rewon O
Child O
, O
Aditya O
Ramesh O
, O
Daniel O
M. O
Ziegler O
, O
Jeffrey O
Wu O
, O
Clemens O
Winter O
, O
Christopher O
Hesse O
, O
Mark O
Chen O
, O
Eric O
Sigler O
, O
Mateusz O
Litwin O
, O
Scott O
Gray O
, O
Benjamin O
Chess O
, O
Jack O
Clark O
, O
Christopher O
Berner O
, O
Sam O
McCandlish O
, O
Alec O
Radford O
, O
Ilya O
Sutskever O
, O
and O
Dario O
Amodei O
. O
2020 O
. O

Language O
models O
are O
few O
- O
shot O
learners O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
33 O
: O
Annual O
Conference O
on O
Neural O
Information O
Processing O
Systems O
2020 O
, O
NeurIPS O
2020 O
, O
December O
6 O
- O
12 O
, O
2020 O
, O
virtual O
. O

Cheng O
Chen O
, O
Yichun O
Yin O
, O
Lifeng O
Shang O
, O
Zhi O
Wang O
, O
Xin O
Jiang O
, O
Xiao O
Chen O
, O
and O
Qun O
Liu O
. O
2021 O
. O

Extract O
then O
distill O
: O
Efficient O
and O
effective O
task O
- O
agnostic O
BERT O
distillation O
. O

In O
Artificial O
Neural O
Networks O
and O
Machine O
Learning O
- O
ICANN O
2021 O
- O
30th O
International O
Conference O
on O
Artificial O
Neural O
Networks O
, O
Bratislava O
, O
Slovakia O
, O
September O
14 O
- O
17 O
, O
2021 O
, O
Proceedings O
, O
Part O
III O
, O
volume O
12893 O
of O
Lecture O
Notes O
in O
Computer O
Science O
, O
pages O
570–581 O
. O

Springer O
. O

Tianqi O
Chen O
, O
Ian O
J. O
Goodfellow O
, O
and O
Jonathon O
Shlens O
. O
2016 O
. O

Net2net B-MethodName
: O

Accelerating O
learning O
via O
knowledge O
transfer O
. O

In O
ICLR O
. O

Kevin O
Clark O
, O
Urvashi O
Khandelwal O
, O
Omer O
Levy O
, O
and O
Christopher O
D. O
Manning O
. O

2019 O
. O

What O
does O
BERT O
look O
at O
? O

an O
analysis O
of O
BERT B-MethodName
’s O
attention O
. O

In O
Proceedings O
of O
the O
2019 O
ACL O
Workshop O
BlackboxNLP O
: O
Analyzing O
and O
Interpreting O
Neural O
Networks O
for O
NLP O
, O
pages O
276–286 O
, O
Florence O
, O
Italy O
. O

Association O
for O
Computational O
Linguistics O
. O

Kevin O
Clark O
, O
Minh O
- O
Thang O
Luong O
, O
Quoc O
V O
. O

Le O
, O
and O
Christopher O
D. O
Manning O
. O

2020 O
. O

ELECTRA B-MethodName
: O
pretraining O
text O
encoders O
as O
discriminators O
rather O
than O
generators O
. O

In O
8th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2020 O
, O
Addis O
Ababa O
, O
Ethiopia O
, O
April O
26 O
- O
30 O
, O
2020 O
. O

OpenReview.net O
. O
Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT B-MethodName
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
, O
Minneapolis O
, O
Minnesota O
. O

Association O
for O
Computational O
Linguistics O
. O

William O
Fedus O
, O
Barret O
Zoph O
, O
and O
Noam O
Shazeer O
. O

2021 O
. O

Switch O
transformers O
: O
Scaling O
to O
trillion O
parameter O
models O
with O
simple O
and O
efficient O
sparsity O
. O

CoRR O
.2142A. O

Feng O
and O
P. O
Panda O
. O

2020 O
. O

Energy O
- O
efficient O
and O
robust O
cumulative O
training O
with O
net2net O
transformation O
. O

In O
IJCNN O
. O

Linyuan O
Gong O
, O
Di O
He O
, O
Zhuohan O
Li O
, O
Tao O
Qin O
, O
Liwei O
Wang O
, O
and O
Tie O
- O
Yan O
Liu O
. O
2019 O
. O

Efficient O
training O
of O
BERT O
by O
progressively O
stacking O
. O

In O
Proceedings O
of O
the O
36th O
International O
Conference O
on O
Machine O
Learning O
, O
ICML O
2019 O
, O
9 O
- O
15 O
June O
2019 O
, O
Long O
Beach O
, O
California O
, O
USA O
, O
volume O
97 O
of O
Proceedings O
of O
Machine O
Learning O
Research O
, O
pages O
2337–2346 O
. O

PMLR O
. O

Xiaotao O
Gu O
, O
Liyuan O
Liu O
, O
Hongkun O
Yu O
, O
Jing O
Li O
, O
Chen O
Chen O
, O
and O
Jiawei O
Han O
. O
2021 O
. O

On O
the O
transformer O
growth O
for O
progressive O
BERT O
training O
. O

In O
Proceedings O
of O
the O
2021 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
5174–5180 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Song O
Han O
, O
Jeff O
Pool O
, O
John O
Tran O
, O
and O
William O
Dally O
. O
2015 O
. O

Learning O
both O
weights O
and O
connections O
for O
efficient O
neural O
network O
. O

Advances O
in O
neural O
information O
processing O
systems O
, O
28 O
. O
Dan O
Hendrycks O
and O
Kevin O
Gimpel O
. O

2016 O
. O

Gaussian O
error O
linear O
units O
( O
gelus O
) O
. O

ArXiv O
preprint O
, O
abs/1606.08415 O
. O

Hai O
Hu O
, O
Kyle O
Richardson O
, O
Liang O
Xu O
, O
Lu O
Li O
, O
Sandra O
Kübler O
, O
and O
Lawrence O
S. O
Moss O
. O
2020 O
. O

OCNLI B-DatasetName
: O
original O
chinese O
natural O
language O
inference O
. O

In O
Findings O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
EMNLP O
2020 O
, O
Online O
Event O
, O
16 O
- O
20 O
November O
2020 O
, O
volume O
EMNLP O
2020 O
of O
Findings O
of O
ACL O
, O
pages O
3512–3526 O
. O

Association O
for O
Computational O
Linguistics O
. O

Ganesh O
Jawahar O
, O
Benoît O
Sagot O
, O
and O
Djamé O
Seddah O
. O
2019 O
. O

What O
does O
BERT O
learn O
about O
the O
structure O
of O
language O
? O

In O
ACL O
, O
pages O
3651–3657 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O

Zhenzhong O
Lan O
, O
Mingda O
Chen O
, O
Sebastian O
Goodman O
, O
Kevin O
Gimpel O
, O
Piyush O
Sharma O
, O
and O
Radu O
Soricut O
. O
2020 O
. O

ALBERT B-MethodName
: O

A O
lite O
BERT O
for O
self O
- O
supervised O
learning O
of O
language O
representations O
. O

In O
8th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2020 O
, O
Addis O
Ababa O
, O
Ethiopia O
, O
April O
26 O
- O
30 O
, O
2020 O
. O

OpenReview.net O
. O

Mike O
Lewis O
, O
Yinhan O
Liu O
, O
Naman O
Goyal O
, O
Marjan O
Ghazvininejad O
, O
Abdelrahman O
Mohamed O
, O
Omer O
Levy O
, O
Veselin O
Stoyanov O
, O
and O
Luke O
Zettlemoyer O
. O

2020 O
. O

BART B-MethodName
: O

Denoising O
sequence O
- O
to O
- O
sequence O
pre O
- O
training O
for O
natural O
language O
generation O
, O
translation O
, O
and O
comprehension O
. O

In O
ACL O
, O
pages O
7871–7880 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Yinhan O
Liu O
, O
Myle O
Ott O
, O
Naman O
Goyal O
, O
Jingfei O
Du O
, O
Mandar O
Joshi O
, O
Danqi O
Chen O
, O
Omer O
Levy O
, O
Mike O
Lewis O
, O
Luke O
Zettlemoyer O
, O
and O
Veselin O
Stoyanov O
. O

2019 O
. O

Roberta B-MethodName
: O
A O
robustly O
optimized O
BERT O
pretraining O
approach O
. O

CoRR O
.Sinno O

Jialin O
Pan O
and O
Qiang O
Yang O
. O

2010 O
. O

A O
survey O
on O
transfer O
learning O
. O

TKDE O
. O

Yujia O
Qin O
, O
Yankai O
Lin O
, O
Jing O
Yi O
, O
Jiajie O
Zhang O
, O
Xu O
Han O
, O
Zhengyan O
Zhang O
, O
YuSheng O
Su O
, O
Zhiyuan O
Liu O
, O
Peng O
Li O
, O
Maosong O
Sun O
, O
and O
Jie O
Zhou O
. O

2021 O
. O

Knowledge O
inheritance O
for O
pre O
- O
trained O
language O
models O
. O

CoRR O
. O

Alec O
Radford O
, O
Karthik O
Narasimhan O
, O
Tim O
Salimans O
, O
and O
Ilya O
Sutskever O
. O

2018 O
. O

Improving O
language O
understanding O
by O
generative O
pre O
- O
training O
. O

Alec O
Radford O
, O
Jeffrey O
Wu O
, O
Rewon O
Child O
, O
David O
Luan O
, O
Dario O
Amodei O
, O
and O
Ilya O
Sutskever O
. O
2019 O
. O

Language O
models O
are O
unsupervised O
multitask O
learners O
. O

OpenAI O
blog O
, O
1(8):9 O
. O

Colin O
Raffel O
, O
Noam O
Shazeer O
, O
Adam O
Roberts O
, O
Katherine O
Lee O
, O
Sharan O
Narang O
, O
Michael O
Matena O
, O
Yanqi O
Zhou O
, O
Wei O
Li O
, O
and O
Peter O
J. O
Liu O
. O
2020 O
. O

Exploring O
the O
limits O
of O
transfer O
learning O
with O
a O
unified O
text O
- O
to O
- O
text O
transformer O
. O

J. O
Mach O
. O

Learn O
. O

Res O
. O
, O
21:140:1–140:67 O
. O

Pranav O
Rajpurkar O
, O
Jian O
Zhang O
, O
Konstantin O
Lopyrev O
, O
and O
Percy O
Liang O
. O

2016 O
. O

SQuAD B-DatasetName
: O
100,000 O
+ O
questions O
for O
machine O
comprehension O
of O
text O
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2383–2392 O
, O
Austin O
, O
Texas O
. O
Association O
for O
Computational O
Linguistics O
. O
Roy O
Schwartz O
, O
Jesse O
Dodge O
, O
Noah O
A O
Smith O
, O
and O
Oren O
Etzioni O
. O

2020 O
. O

Green O
ai O
. O

Communications O
of O
the O
ACM O
, O
63(12):54–63 O
. O

Mohammad O
Shoeybi O
, O
Mostofa O
Patwary O
, O
Raul O
Puri O
, O
Patrick O
LeGresley O
, O
Jared O
Casper O
, O
and O
Bryan O
Catanzaro O
. O

2019 O
. O

Megatron O
- O
lm O
: O
Training O
multi O
- O
billion O
parameter O
language O
models O
using O
model O
parallelism O
. O

CoRR O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N. O
Gomez O
, O
Lukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
30 O
: O
Annual O
Conference O
on O
Neural O
Information O
Processing O
Systems O
2017 O
, O
December O
4 O
- O
9 O
, O
2017 O
, O
Long O
Beach O
, O
CA O
, O
USA O
, O
pages O
5998–6008 O
. O

Alex O
Wang O
, O
Amanpreet O
Singh O
, O
Julian O
Michael O
, O
Felix O
Hill O
, O
Omer O
Levy O
, O
and O
Samuel O
R. O
Bowman O
. O
2019a O
. O
GLUE B-DatasetName
: O

A O
multi O
- O
task O
benchmark O
and O
analysis O
platform O
for O
natural O
language O
understanding O
. O

In O
7th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2019 O
, O
New O
Orleans O
, O
LA O
, O
USA O
, O
May O
6 O
- O
9 O
, O
2019 O
. O

OpenReview.net O
. O

Dilin O
Wang O
, O
Meng O
Li O
, O
Lemeng O
Wu O
, O
Vikas O
Chandra O
, O
and O
Qiang O
Liu O
. O
2019b O
. O

Energy O
- O
aware O
neural O
architecture O
optimization O
with O
fast O
splitting O
steepest O
descent O
. O

CoRR O
. O

Alex O
Warstadt O
, O
Amanpreet O
Singh O
, O
and O
Samuel O
R. O
Bowman O
. O

2019 O
. O

Neural O
network O
acceptability O
judgments O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
7:625–641.2143Wei O
Wen O
, O
Yuxiong O
He O
, O
Samyam O
Rajbhandari O
, O
Minjia O
Zhang O
, O
Wenhan O
Wang O
, O
Fang O
Liu O
, O
Bin O
Hu O
, O
Yiran O
Chen O
, O
and O
Hai O
Li O
. O

2018 O
. O

Learning O
intrinsic O
sparse O
structures O
within O
long O
short O
- O
term O
memory O
. O

In O
6th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2018 O
, O
Vancouver O
, O
BC O
, O
Canada O
, O
April O
30 O
- O
May O
3 O
, O
2018 O
, O
Conference O
Track O
Proceedings O
. O

OpenReview.net O
. O

Lemeng O
Wu O
, O
Bo O
Liu O
, O
Peter O
Stone O
, O
and O
Qiang O
Liu O
. O
2020a O
. O

Firefly O
neural O
architecture O
descent O
: O
a O
general O
approach O
for O
growing O
neural O
networks O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
33 O
: O
Annual O
Conference O
on O
Neural O
Information O
Processing O
Systems O
2020 O
, O
NeurIPS O
2020 O
, O
December O
6 O
- O
12 O
, O
2020 O
, O
virtual O
. O

Lemeng O
Wu O
, O
Dilin O
Wang O
, O
and O
Qiang O
Liu O
. O
2019 O
. O

Splitting O
steepest O
descent O
for O
growing O
neural O
architectures O
. O

InNeurIPS O
, O
pages O
10655–10665 O
. O

Lemeng O
Wu O
, O
Mao O
Ye O
, O
Qi O
Lei O
, O
Jason O
D. O
Lee O
, O
and O
Qiang O
Liu O
. O

2020b O
. O

Steepest O
descent O
neural O
architecture O
optimization O
: O

Escaping O
local O
optimum O
with O
signed O
neural O
splitting O
. O

CoRR O
. O

Qiyu O
Wu O
, O
Chen O
Xing O
, O
Yatao O
Li O
, O
Guolin O
Ke O
, O
Di O
He O
, O
and O
Tie O
- O
Yan O
Liu O
. O
2021 O
. O

Taking O
notes O
on O
the O
fly O
helps O
language O
pre O
- O
training O
. O

In O
ICLR O
. O

Ruibin O
Xiong O
, O
Yunchang O
Yang O
, O
Di O
He O
, O
Kai O
Zheng O
, O
Shuxin O
Zheng O
, O
Chen O
Xing O
, O
Huishuai O
Zhang O
, O
Yanyan O
Lan O
, O
Liwei O
Wang O
, O
and O
Tie O
- O
Yan O
Liu O
. O
2020 O
. O

On O
layer O
normalization O
in O
the O
transformer O
architecture O
. O

In O
Proceedings O
of O
the O
37th O
International O
Conference O
on O
Machine O
Learning O
, O
ICML O
2020 O
, O
13 O
- O
18 O
July O
2020 O
, O
Virtual O
Event O
, O
volume O
119 O
of O
Proceedings O
of O
Machine O
Learning O
Research O
, O
pages O
10524–10533 O
. O

PMLR O
. O

Cheng O
Yang O
, O
Shengnan O
Wang O
, O
Chao O
Yang O
, O
Yuechuan O
Li O
, O
Ru O
He O
, O
and O
Jingqiao O
Zhang O
. O

2020 O
. O

Progressively O
stacking O
2.0 O
: O
A O
multi O
- O
stage O
layerwise O
training O
method O
for O
bert O
training O
speedup O
. O

arXiv O
preprint O
arXiv:2011.13635 O
. O

Zhilin O
Yang O
, O
Zihang O
Dai O
, O
Yiming O
Yang O
, O
Jaime O
G. O
Carbonell O
, O
Ruslan O
Salakhutdinov O
, O
and O
Quoc O
V O
. O

Le O
. O
2019 O
. O

Xlnet B-MethodName
: O

Generalized O
autoregressive O
pretraining O
for O
language O
understanding O
. O

In O
NeurIPS O
, O
pages O
5754–5764 O
. O

Yang O
You O
, O
Jing O
Li O
, O
Sashank O
J. O
Reddi O
, O
Jonathan O
Hseu O
, O
Sanjiv O
Kumar O
, O
Srinadh O
Bhojanapalli O
, O
Xiaodan O
Song O
, O
James O
Demmel O
, O
Kurt O
Keutzer O
, O
and O
Cho O
- O
Jui O
Hsieh O
. O
2020 O
. O

Large O
batch O
optimization O
for O
deep O
learning O
: O
Training O
BERT O
in O
76 O
minutes O
. O

In O
8th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2020 O
, O
Addis O
Ababa O
, O
Ethiopia O
, O
April O
26 O
- O
30 O
, O
2020 O
. O

OpenReview.net O
. O

Wei O
Zeng O
, O
Xiaozhe O
Ren O
, O
Teng O
Su O
, O
Hui O
Wang O
, O
Yi O
Liao O
, O
Zhiwei O
Wang O
, O
Xin O
Jiang O
, O
ZhenZhang O
Yang O
, O
Kaisheng O
Wang O
, O
Xiaoda O
Zhang O
, O
Chen O
Li O
, O
Ziyan O
Gong O
, O
Yifan O
Yao O
, O
Xinjing O
Huang O
, O
Jun O
Wang O
, O
Jianfeng O
Yu O
, O
Qi O
Guo O
, O
Yue O
Yu O
, O
Yan O
Zhang O
, O
Jin O
Wang O
, O
Hengtao O
Tao O
, O
Dasen O
Yan O
, O
Zexuan O
Yi O
, O
Fang O
Peng O
, O
Fangqing O
Jiang O
, O
Han O
Zhang O
, O
Lingfeng O
Deng O
, O
Yehong O
Zhang O
, O
Zhe O
Lin O
, O
Chao O
Zhang O
, O
Shaojie O
Zhang O
, O
Mingyue O
Guo O
, O
Shanzhi O
Gu O
, O
Gaojun O
Fan O
, O
Yaowei O
Wang O
, O
Xuefeng O
Jin O
, O
Qun O
Liu O
, O
and O
Yonghong O
Tian O
. O
2021 O
. O

Panguα O
: O
Large O
- O
scale O
autoregressive O
pretrained O
chinese O
language O
models O
with O
auto O
- O
parallel O
computation O
. O

Minjia O
Zhang O
and O
Yuxiong O
He O
. O
2020 O
. O

Accelerating O
training O
of O
transformer O
- O
based O
language O
models O
with O
progressive O
layer O
dropping O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
33 O
: O
Annual O
Conference O
on O
Neural O
Information O
Processing O
Systems O
2020 O
, O
NeurIPS O
2020 O
, O
December O
6 O
- O
12 O
, O
2020 O
, O
virtual O
. O

Yukun O
Zhu O
, O
Ryan O
Kiros O
, O
Richard O
S. O
Zemel O
, O
Ruslan O
Salakhutdinov O
, O
Raquel O
Urtasun O
, O
Antonio O
Torralba O
, O
and O
Sanja O
Fidler O
. O

2015 O
. O

Aligning O
books O
and O
movies O
: O
Towards O
story O
- O
like O
visual O
explanations O
by O
watching O
movies O
and O
reading O
books O
. O

In O
2015 O
IEEE O
International O
Conference O
on O
Computer O
Vision O
, O
ICCV O
2015 O
, O
Santiago O
, O
Chile O
, O
December O
7 O
- O
13 O
, O
2015 O
, O
pages O
19–27 O
. O

IEEE O
Computer O
Society.2144A O
Ablation O
Study O
of O
bert2BERT O
0 O
1 O
2 O
3 O
4 O
5 O
6 O
7 O
8 O
FLOPs O
( O
1e19)1.41.61.82.02.22.42.6MLM O
Loss O
4 O
5 O
6 O
71.401.421.441.461.48 O
1.437 O
100 O
% O
87.8 O
% O
69.6 O
% O
61.6 O
% O
54.8%BERTBASE O
DirectCopyFPI O
AKIbert2BERT O
Figure O
5 O
: O
Ablation O
study O
of O
bert2BERT O
. O

bert2BERT O
means O
the O
combination O
of O
AKI O
and O
two O
- O
stage O
pretraining O
. O

The O
ablation O
study O
of O
bert2BERT O
is O
displayed O
in O
Table O
5 O
. O

From O
the O
table O
, O
we O
observe O
that O
: O
( O
1 O
) O
all O
the O
proposed O
methods O
is O
better O
than O
the O
original O
pre O
- O
training O
method O
and O
DirectCopy O
; O
( O
2 O
) O
although O
AKI O
has O
a O
worse O
initialization O
than O
FPI O
, O
it O
achieves O
faster O
convergence O
rate O
than O
FPI O
; O
( O
3 O
) O
the O
two O
- O
stage O
pre O
- O
training O
furthers O
reduce O
the O
cost O
from O
61.6 O
% O
to O
54.8 O
% O
; O
( O
4 O
) O
the O
FPI O
curve O
has O
an O
upward O
trend O
at O
the O
beginning O
. O

We O
conjecture O
that O
it O
is O
due O
to O
the O
symmetry O
brought O
by O
FPI O
and O
the O
model O
needs O
some O
optimization O
time O
to O
break O
this O
symmetry O
. O

B O
bert2BERT O
with O
smaller O
source O
model O
0 O
1 O
2 O
3 O
4 O
5 O
6 O
7 O
8 O
FLOPs O
( O
1e19)1.41.61.82.02.22.42.6MLM O
Loss O
4 O
5 O
6 O
71.401.421.441.461.48 O
1.437 O
100 O
% O
76.7 O
% O
63.2 O
% O
57.3%DirectCopy_L6 O
bert2BERT_L6bert2BERT_L8 O
bert2BERT_L10BERTBASE O
Figure O
6 O
: O
Loss O
curves O
of O
bert2BERT O
and O
baselines O
with O
smaller O
source O
models O
. O

We O
test O
bert2BERT O
with O
different O
source O
models O
and O
the O
loss O
curves O
are O
represented O
in O
Figure O
6 O
. O

C O
Effect O
of O
sub O
- O
model O
training O
epochs O
We O
study O
the O
effect O
of O
sub O
- O
model O
training O
epochs O
on O
the O
pre O
- O
training O
efficiency O
. O

The O
loss O
curves O
are O
represented O
in O
Figure O
7 O
. O

Note O
that O
the O
setting O
Eb= O
20has O
not O
achieved O
the O
same O
loss O
( O
1.437 O
) O
as O
the O
baseline O
BERT O
BASE O
in O
the O
40 O
training O
epochs O
. O

D O
Application O
on O
GPT B-MethodName
The O
loss O
curve O
of O
our O
method O
on O
GPT B-MethodName
application O
is O
displayed O
in O
Figure O
8 O
. O
0 O
1 O
2 O
3 O
4 O
5 O
6 O
7 O
8 O
FLOPs O
( O
1e19)1.41.61.82.02.22.42.6MLM O
Loss O
4 O
5 O
6 O
71.401.421.441.461.48 O
1.437 O
100 O
% O
61.6 O
% O
54.8%BERTBASE O
Eb=0Eb=5 O
Eb=10Eb=20Figure O
7 O
: O
Loss O
curves O
of O
bert2BERT O
with O
different O
submodel O
training O
epochs O
. O

0 O
1 O
2 O
3 O
4 O
5 O
FLOPs O
( O
1e19)2.83.03.23.43.63.84.0Loss O
2.5 O
3.0 O
3.5 O
4.0 O
4.5 O
5.02.842.862.882.90 O
2.870 O
100 O
% O
52.4%GPT O
bert2BERT O
Figure O
8 O
: O
Pre O
- O
training O
loss O
curves O
of O
GPT O
. O
E O
Comparisons O
of O
Attention O
Patterns O
We O
take O
the O
source O
model O
S(4,256 O
) O
and O
target O
model O
T(4,512 O
) O
as O
an O
example O
to O
analyze O
the O
attention O
patterns O
of O
DirectCopy O
in O
Figure O
10 O
, O
FPI O
in O
Figure O
11 O
and O
AKI O
in O
Figure O
12 O
. O

We O
display O
the O
attention O
patterns O
of O
the O
source O
model O
S(4,256 O
) O
in O
Figure O
9 O
. O

Compared O
with O
the O
source O
model O
, O
we O
observe O
that O
the O
newly O
added O
attention O
patterns O
of O
DirectCopy O
are O
messy O
, O
and O
the O
randomly O
initialized O
parameters O
destroy O
the O
attention O
patterns O
of O
the O
source O
model O
. O

The O
proposed O
FPI O
method O
makes O
the O
new O
model O
have O
the O
same O
attention O
patterns O
as O
the O
source O
model O
, O
thus O
the O
knowledge O
of O
the O
source O
model O
is O
preserved O
. O

However O
, O
FPI O
always O
induces O
symmetrical O
attention O
patterns O
in O
the O
same O
layer O
. O

This O
symmetry O
will O
hinder O
the O
convergence O
. O

To O
handle O
this O
problem O
, O
we O
use O
AKI O
method O
to O
reuse O
the O
parameters O
of O
the O
upper O
layer O
( O
advanced O
knowledge O
) O
to O
break O
the O
symmetry O
, O
and O
meanwhile O
make O
the O
knowledge O
in O
the O
same O
layer O
richer O
. O

Through O
the O
AKI O
method O
, O
the O
attention O
patterns O
of O
the O
upper O
layer O
can O
be O
also O
maintained O
well O
in O
the O
target O
model O
. O

For O
example O
, O
as O
shown O
in O
Figure O
12 O
, O
the O
newly O
added O
attention O
patterns O
of O
the O
1st O
layer O
in O
the O
target O
model O
are O
similar O
to O
the O
ones O
of O
the O
2nd O
layer O
in O
the O
source O
model O
. O

F O
Illustration O
of O
FPI O
and O
AKI O
process O
We O
illustrate O
the O
process O
of O
FPI O
and O
AKI O
in O
Figure O
13 O
and O
14 O
respectively.2145L1 O
H0 O
  O
L1 O
H1 O
  O
L1 O
H2 O
  O
L1 O
H3 O
L2 O
H0 O
  O
L2 O
H1 O
  O
L2 O
H2 O
  O
L2 O
H3 O
L3 O
H0 O
  O
L3 O
H1 O
  O
L3 O
H2 O
  O
L3 O
H3 O
L4 O

H0 O
  O
L4 O
H1 O
  O
L4 O
H2 O
  O
L4 O
H3Figure O
9 O
: O
Attention O
patterns O
of O
the O
source O
model O
S(4,256 O
) O
, O
which O
has O
4 O
attention O
heads O
in O
each O
layer O
. O

L1 O
H0 O
  O
L1 O
H1 O
  O
L1 O
H2 O
  O
L1 O
H3 O
  O
L1 O
H4 O
  O
L1 O
H5 O
  O
L1 O

H6 O
  O
L1 O
H7 O
L2 O
H0 O
  O
L2 O
H1 O
  O
L2 O
H2 O
  O
L2 O
H3 O
  O
L2 O
H4 O
  O
L2 O
H5 O
  O
L2 O
H6 O
  O
L2 O
H7 O
L3 O
H0 O
  O
L3 O
H1 O
  O
L3 O
H2 O
  O
L3 O
H3 O
  O
L3 O
H4 O
  O
L3 O
H5 O
  O
L3 O

H6 O
  O
L3 O
H7 O
L4 O
H0 O
  O
L4 O
H1 O
  O
L4 O
H2 O
  O
L4 O
H3 O
  O
L4 O
H4 O
  O
L4 O
H5 O
  O
L4 O
H6 O
  O
L4 O
H7 O
Figure O
10 O
: O
Attention O
patterns O
of O
the O
target O
model O
T(4,512 O
) O
based O
on O
the O
baseline O
DirectCopy O
method O
. O

The O
first O
4 O
attention O
patterns O
( O
H0 O
- O
H3 O
) O
in O
each O
row O
correspond O
to O
the O
source O
model O
’s O
attention O
patterns O
, O
and O
the O
last O
4 O
attention O
patterns O
( O
H4 O
- O
H7 O
) O
are O
newly O
added.2146L1 O
H0 O
  O
L1 O
H1 O
  O
L1 O
H2 O
  O
L1 O
H3 O
  O
L1 O
H4 O
  O
L1 O

H5 O
  O
L1 O

H6 O
  O
L1 O
H7 O
L2 O
H0 O
  O
L2 O
H1 O
  O
L2 O
H2 O
  O
L2 O
H3 O
  O
L2 O
H4 O
  O
L2 O
H5 O
  O
L2 O
H6 O
  O
L2 O
H7 O
L3 O
H0 O
  O
L3 O
H1 O
  O
L3 O
H2 O
  O
L3 O
H3 O
  O
L3 O
H4 O
  O
L3 O
H5 O
  O
L3 O

H6 O
  O
L3 O
H7 O
L4 O
H0 O
  O
L4 O
H1 O
  O
L4 O
H2 O
  O
L4 O
H3 O
  O
L4 O
H4 O
  O
L4 O
H5 O
  O
L4 O
H6 O
  O
L4 O
H7Figure O
11 O
: O
Attention O
patterns O
of O
the O
target O
model O
T(4,512 O
) O
based O
on O
our O
FPI O
method O
. O

The O
last O
4 O
attention O
patterns O
( O
H4 O
- O
H7 O
) O
in O
each O
row O
are O
obtained O
by O
FPI O
expansion O
. O

L1 O
H0 O
  O
L1 O
H1 O
  O
L1 O
H2 O
  O
L1 O
H3 O
  O
L1 O
H4 O
  O
L1 O
H5 O
  O
L1 O

H6 O
  O
L1 O
H7 O
L2 O
H0 O
  O
L2 O
H1 O
  O
L2 O
H2 O
  O
L2 O
H3 O
  O
L2 O
H4 O
  O
L2 O
H5 O
  O
L2 O
H6 O
  O
L2 O
H7 O
L3 O
H0 O
  O
L3 O
H1 O
  O
L3 O
H2 O
  O
L3 O
H3 O
  O
L3 O
H4 O
  O
L3 O
H5 O
  O
L3 O

H6 O
  O
L3 O
H7 O
L4 O
H0 O
  O
L4 O
H1 O
  O
L4 O
H2 O
  O
L4 O
H3 O
  O
L4 O
H4 O
  O
L4 O
H5 O
  O
L4 O
H6 O
  O
L4 O
H7 O
Figure O
12 O
: O
Attention O
patterns O
of O
the O
target O
model O
T(4,512 O
) O
based O
on O
our O
AKI O
method O
. O

The O
last O
4 O
attention O
patterns O
( O
H4 O
- O
H7 O
) O
in O
each O
row O
are O
obtained O
by O
AKI O
expansion.2147abc O

d O
efg O

h O

i O
j O
abc O

d O
ef O
a O
bcg/2 O
Layer O
Norm:𝑢=𝐻.𝑚𝑒𝑎𝑛 O
𝑠=(𝐻−𝑢)2.𝑚𝑒𝑎𝑛LayerNorm O
会出现难以避 O
免的不同但影响不大。当大klm O
nClassifier O
EmbeddingMulti O
-Head O
  O
Attention O
( O
MHA)Add O
& O
NormFeed O
Forward O
  O
Network O
( O
FFN)Add O
& O
Norm O

N O
xop O
q O
rst O

uv O
g/2 O
h O
i/2i/2jk O
lm O
nk O
lo/2o/2 O
pq/2 O
q/2rs O
tu O
vs O
tab O
c O
defa O
b O
c O
g/2g/2 O
hi/2 O
i/2 O
jg/2 O
hg/2k/2k/2 O
lm/2 O
m/2 O
nk/2 O
lk/2o/2o/2 O
pq/2 O
q/2 O
ro/2 O
po/2s/2s/2 O
tu/2 O
u/2vs/2 O
ts/2ab O
c O
defa O
b O
c O
ade O
fbc O
w1 O
w2w1 O
w2w1 O
w2 O
w1 O
w1 O
w2 O
w1 O
w1 O
w2W1/2 O
w2W1/2 O
w1 O
w1 O
w2w1 O
w1 O
w2w2W1/2 O
W1/2MHA O
/ O
FFN O
Expansion O
step O
1 O
MHA O
Expansion O
step O
2 O
FFN O
Expansion O
step O
2 O
𝑊𝑙𝑄|𝐾|𝑉 O
𝑊𝐸𝑀𝐵𝑊𝑙𝑂 O
𝑊𝐿𝑁 O
abc O

d O
ef O
a O
bc𝑊𝐿𝑁𝑊𝑙1𝑊𝑙2𝑊𝐿𝑁𝑊𝐸𝑀𝐵𝑇 O
𝑔𝑖𝑛={1,2,1}FFN O
MHA O
abc O

d O
ef O
a O
bcg/2g/2 O
hi/2 O
i/2 O
jk O
lm O
nk O
lo/2o/2 O
pq/2 O
q/2rs O
tu O
vs O
tab O
c O
defa O
b O
c O
w1 O
w1 O
w2 O
w1 O
w1 O
w2W1/2 O
w2W1/2FPI O
hidden O
neuron O
embedding O
neuron O
different O
heads O
’ O
neuron O
FFN O
neuronFigure O
13 O
: O
FPI O
process O
. O

We O
use O
the O
FPI O
method O
to O
widen O
the O
source O
model O
with O
a O
width O
of O
2 O
into O
a O
target O
model O
with O
a O
width O
of O
3 O
. O

In O
the O
example O
, O
the O
source O
model O
and O
the O
target O
model O
have O
2 O
and O
3 O
attention O
heads O
respectively O
. O

And O
the O
head O
dimension O
is O
1 O
. O

To O
facilitate O
the O
illustration O
, O
we O
reduce O
the O
number O
of O
neurons O
in O
the O
FFN O
layer O
. O

We O
also O
note O
that O
since O
the O
MLM O
classifier O
of O
BERT O
is O
a O
transposition O
of O
the O
Embedding O
layer O
, O
they O
share O
a O
parameter O
matrix O
. O

Therefore O
, O
in O
step O
1 O
, O
we O
expand O
the O
MLM O
classifier O
by O
re O
- O
scaling O
the O
parameter O
values O
of O
the O
LN O
layer O
below O
the O
MLM O
classifier O
instead O
of O
following O
formula O
7 O
. O

Layer O
Norm:𝑢=𝐻.𝑚𝑒𝑎𝑛 O
𝑠=(𝐻−𝑢)2.𝑚𝑒𝑎𝑛LayerNorm O
会出现难以避 O
免的不同但影响不大。当大Classifier O
EmbeddingMulti O
-Head O
  O
Attention O
( O
MHA)Add O
& O
NormFeed O
Forward O
  O
Network O
( O
FFN)Add O
& O
Norm O
N O
x O
g/2g/2 O
hi/2 O
i/2 O
jg’/2 O
h’g’/2k/2k’/2 O
lm/2 O
m’/2 O
nk/2 O
lk’/2o/2o/2 O
pq/2 O
q/2 O
ro’/2 O
p’o’/2s/2s’/2 O
tu/2 O
u’/2vs/2 O
ts’/2ab O
c O
defa O
b O
c O
w1 O
w1 O
w2w1 O
w1 O
w2w2W1/2 O
W1/2MHA O
Expansion O
step O
2FFN O
Expansion O
step O
2 O
abc O
d O
ef O
a O
bc O
𝑔𝑖𝑛={1,2,1}FFN O
MHA O
abc O

d O
ef O
a O
bcg/2g/2 O
hi/2 O
i/2 O
jk O
lm O
nk O
lo/2o/2 O
pq/2 O
q/2rs O
tu O
vs O
tab O
c O
defa O
b O
c O
w1 O
w1 O
w2 O
w1 O
w1 O
w2W1/2 O
w2W1/2 O
𝑊𝑙1𝑊𝑙2 O
𝑊𝑙𝑄|𝐾|𝑉𝑊𝑙𝑂 O
g’/2g’/2 O
h’i’/2 O
i’/2 O
j’k’l’m’n’k O
’ O
l’o’/2o’/2 O
p’q’/2 O
q’/2 O
r O
’s O
’ O
t’u O
’ O
v O
’s O
’ O
t O
’ O
w’1 O
w’1 O
w’2𝑊𝑙+11𝑊𝑙+12 O

𝑊𝑙+1𝑄|𝐾|𝑉𝑊𝑙+1𝑂𝑈𝑙2 O
𝑈𝑙1 O
𝑈𝑙𝑄|𝐾|𝑉𝑈𝑙𝑂AKI O
hidden O
neuron O
embedding O
neuron O
different O
heads O
’ O
neurondifferent O
layers O
’ O
FFN O
neuron O
𝒍-thlayer O
Transformer O
𝒍+𝟏 O
-thlayer O

Transformer O
Figure O
14 O
: O
AKI O
process O
. O

We O
ignore O
its O
first O
step O
because O
it O
is O
the O
same O
as O
FPI O
’s O
first O
step O
. O

The O
main O
difference O
between O
AKI O
and O
FPI O
is O
that O
in O
step O
2 O
, O
AKI O
copies O
some O
attention O
heads O
in O
MHA O
and O
some O
parameters O
in O
FFN O
of O
the(l+ O
1 O
) O
-th O
layer O
instead O
of O
only O
copying O
the O
l O
- O
th O
layer O
to O
construct O
the O
new O
l O
- O
th O
layer O
Transformer.2148 O

