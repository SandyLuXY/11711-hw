Proceedings O
of O
the O
60th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
46 O
- O
56 O
May O
22 O
- O
27 O
, O
2022 O
c O

 O
2022 O
Association O
for O
Computational O
Linguistics O
AlephBERT B-MethodName
: O
Language O
Model O
Pre O
- O
training O
and O
Evaluation O
from O
Sub O
- O
Word O
to O
Sentence O
Level O
Amit O
Seker O
, O
Elron O
Bandel O
, O
Dan O
Bareket O
, O
Idan O
Brusilovsky O
, O
Refael O
Shaked O
Greenfeld O
, O
Reut O
Tsarfaty O
Department O
of O
Computer O
Science O
, O
Bar O
Ilan O
University O
, O
Ramat O
- O
Gan O
, O
Israel O
{ O
aseker00,elronbandel O
, O
dbareket O
, O
brusli1 O
, O
shakedgreenfeld,reut.tsarfaty}@gmail.com O
Abstract O
Large O
Pre O
- O
trained O
Language O
Models O
( O
PLMs O
) O
have O
become O
ubiquitous O
in O
the O
development O
of O
language O
understanding O
technology O
and O
lie O
at O
the O
heart O
of O
many O
artificial O
intelligence O
advances O
. O

While O
advances O
reported O
for O
English O
using O
PLMs O
are O
unprecedented O
, O
reported O
advances O
using O
PLMs O
for O
Hebrew O
are O
few O
and O
far O
between O
. O

The O
problem O
is O
twofold O
. O

First O
, O
so O
far O
, O
Hebrew O
resources O
for O
training O
large O
language O
models O
are O
not O
of O
the O
same O
magnitude O
as O
their O
English O
counterparts O
. O

Second O
, O
most O
benchmarks O
available O
to O
evaluate O
progress O
in O
Hebrew O
NLP O
require O
morphological O
boundaries O
which O
are O
not O
available O
in O
the O
output O
of O
PLMs O
. O

In O
this O
work O
we O
remedy O
both O
aspects O
. O

We O
present O
AlephBERT B-MethodName
, O
a O
large O
PLM O
for O
Modern O
Hebrew O
, O
trained O
on O
larger O
vocabulary O
and O
a O
larger O
dataset O
than O
any O
Hebrew O
PLM O
before O
. O

Moreover O
, O
we O
introduce O
a O
novel O
neural O
architecture O
that O
recovers O
the O
morphological O
segments O
encoded O
in O
contextualized O
embedding O
vectors O
. O

Based O
on O
this O
new O
morphological O
component O
we O
offer O
an O
evaluation O
suite O
consisting O
of O
multiple O
tasks O
and O
benchmarks O
that O
cover O
sentencelevel O
, O
word O
- O
level O
andsub O
- O
word O
level O
analyses O
. O

On O
all O
tasks O
, O
AlephBERT B-MethodName
obtains O
state O
- O
of O
- O
theart O
results O
beyond O
contemporary O
Hebrew O
stateof O
- O
the O
- O
art O
models O
. O

We O
make O
our O
AlephBERT B-MethodName
model O
, O
the O
morphological O
extraction O
component O
, O
and O
the O
Hebrew O
evaluation O
suite O
publicly O
available O
, O
for O
future O
investigations O
and O
evaluations O
of O
Hebrew O
PLMs O
. O

1 O
Introduction O
Contextualized O
word O
representations O
provided O
by O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
GPT3 B-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
) O
, O
T5 O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
and O
more O
, O
were O
shown O
in O
recent O
years O
to O
be O
a O
critical O
component O
for O
obtaining O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
a O
wide O
range O
of O
Natural O
Language O
Processing O
( O
NLP O
) O
tasks O
, O
from O
surface O
syntactic O
tasks O
as O
tagging B-TaskName
and O
parsing B-TaskName
, O
to O
downstream O
semantic O
tasks O
as O
question B-TaskName
answering I-TaskName
, O
information B-TaskName
extraction I-TaskName
and O
text B-TaskName
summarization I-TaskName
. O

While O
advances O
reported O
for O
English O
using O
such O
models O
are O
unprecedented O
, O
previously O
reported O
results O
using O
PLMs O
in O
Modern O
Hebrew O
are O
far O
from O
satisfactory O
. O

Specifically O
, O
the O
BERT O
- O
based O
Hebrew O
section O
of O
multilingual B-MethodName
- I-MethodName
BERT I-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
( O
henceforth O
, O
mBERT B-MethodName
) O
, O
did O
not O
provide O
a O
similar O
boost O
in O
performance O
as O
observed O
by O
the O
English O
section O
of O
mBERT B-MethodName
. O

In O
fact O
, O
for O
several O
reported O
tasks O
, O
the O
results O
of O
the O
mBERT B-MethodName
model O
are O
on O
a O
par O
with O
pre O
- O
neural O
models O
or O
neural O
models O
based O
on O
non O
- O
contextual O
embeddings O
( O
Tsarfaty O
et O
al O
. O
, O
2020 O
; O
Klein O
and O
Tsarfaty O
, O
2020 O
) O
. O

An O
additional O
Hebrew O
BERT O
- O
based O
model O
, O
HeBERT B-MethodName
( O
Chriqui O
and O
Yahav O
, O
2021 O
) O
, O
has O
been O
recently O
released O
, O
yet O
without O
empirical O
evidence O
of O
performance O
improvements O
on O
key O
components O
of O
the O
Hebrew O
NLP O
pipeline O
. O

The O
challenge O
of O
developing O
PLMs O
for O
morphologically O
- O
rich O
andmedium O
- O
resourced O
languages O
such O
as O
Modern O
Hebrew O
is O
twofold O
. O

First O
, O
contextualized O
word O
representations O
are O
obtained O
by O
pre O
- O
training O
a O
large O
language O
model O
on O
massive O
quantities O
of O
unlabeled O
texts O
. O

In O
Hebrew O
, O
the O
size O
of O
published O
texts O
available O
for O
training O
is O
relatively O
small O
. O

To O
wit O
, O
Hebrew B-DatasetName
Wikipedia I-DatasetName
( O
300 O
K O
articles O
) O
used O
for O
training O
mBERT B-MethodName
is O
orders O
of O
magnitude O
smaller O
compared O
to O
English B-DatasetName
Wikipedia I-DatasetName
( O
6 O
M O
articles O
) O
. O

Second O
, O
commonly O
accepted O
benchmarks O
for O
evaluating O
Hebrew O
models O
, O
via O
Morpho B-TaskName
- I-TaskName
Syntactic I-TaskName
Tagging I-TaskName
and I-TaskName
Parsing I-TaskName
( O
Sadde O
et O
al O
. O
, O
2018 O
) O
, O
or O
Named B-TaskName
Entity I-TaskName
Recognition I-TaskName
( O
Bareket O
and O
Tsarfaty O
, O
2020 O
) O
require O
decomposition O
of O
words O
into O
morphemes O
, O
1 O
which O
are O
distinct O
of O
the O
sub O
- O
words O
( O
a.k.a O
. O
wordpieces O
) O
provided O
by O
standard O
PLMs O
. O

Such O
morphemes O
are O
as O
of O
yet O
not O
readily O
available O
in O
the O
PLMs O
’ O
output O
embeddings O
. O

Evaluating O
BERT O
- O
based O
models O
on O
morphemelevel O
tasks O
is O
thus O
non O
- O
trivial O
due O
to O
the O
mismatch O
between O
the O
sub O
- O
word O
tokens O
used O
as O
sub O
- O
word O
1These O
morphemes O
are O
affixes O
and O
clitics O
bearing O
their O
own O
POS O
. O

They O
are O
termed O
syntactic O
words O
in O
UD O
( O
Zeman O
et O
al O
. O
, O
2018 O
) O
, O
or O
segments O
in O
previous O
literature O
on O
Hebrew O
NLP.46Figure O
1 O
: O
PLM O
Morphological O
Extraction O
Pipeline O
. O

The O
two O
- O
word O
phrase O
“ O
/ֹדנֵfנ O
/ O
דרֹוuנfלביתהלבלַנִ O
, O
” O
transliterated O
as O
“ O
lbit O
hlbn O
” O
, O
mapped O
to O
word O
- O
pieces O
which O
are O
consumed O
by O
a O
PLM O
to O
generate O
contextualized O
vectors O
and O
extract O
the O
sub O
- O
word O
morphological O
units O
. O

In O
this O
example O
the O
WordPiece O
Tokenizer O
splits O
the O
first O
word O
, O
“ O
lbit O
” O
, O
into O
two O
pieces O
while O
leaving O
the O
second O
word O
, O
“ O
hlbn O
” O
, O
intact O
. O

Consequently O
, O
AlephBERT B-MethodName
generates O
3 O
embedded O
vectors O
- O
the O
vectors O
associated O
with O
the O
split O
word O
pieces O
are O
averaged O
to O
form O
a O
single O
contextualized O
vector O
. O

Finally O
, O
the O
resulting O
two O
word O
vectors O
are O
used O
by O
the O
Morphological O
Extraction O
Model O
that O
generates O
the O
disambiguated O
morphological O
segments O
. O

input O
units O
used O
by O
the O
PLMs O
and O
the O
sub O
- O
word O
morphological O
units O
needed O
for O
evaluation O
. O

PLMs O
employ O
sub O
- O
word O
tokenization O
mechanisms O
such O
as O
WordPiece O
or O
Byte O
- O
Pair O
Encoding O
( O
BPE O
) O
for O
the O
purposes O
of O
minimizing O
Out O
- O
Of O
- O
V O
ocabulary O
words O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
. O

These O
sub O
- O
word O
tokens O
are O
generated O
in O
a O
pre O
- O
processing O
step O
, O
without O
utilization O
of O
any O
linguistic O
information O
, O
and O
passed O
as O
input O
to O
the O
PLM O
. O

Crucially O
, O
such O
word O
- O
pieces O
do O
not O
reflect O
morphological O
units O
. O

Extracting O
morphological O
units O
from O
contextualized O
vectors O
provided O
by O
PLMs O
is O
challenging O
yet O
necessary O
in O
order O
to O
enable O
morphological O
- O
level O
evaluation O
of O
Hebrew O
PLMs O
on O
standard O
benchmarks O
. O

In O
this O
paper O
we O
introduce O
AlephBERT B-MethodName
, O
a O
Hebrew O
PLM O
trained O
on O
more O
data O
and O
a O
larger O
vocabulary O
than O
any O
Hebrew O
PLM O
before.2Moreover O
, O
we O
propose O
a O
novel O
architecture O
that O
extracts O
the O
morphological O
sub O
- O
word O
units O
implicitly O
encoded O
in O
the O
contextualized O
vectors O
outputted O
by O
PLMs O
. O

Using O
AlephBERT B-MethodName
and O
the O
proposed O
morphological O
extraction O
model O
we O
enable O
evaluation O
on O
allexisting O
Hebrew O
benchmarks O
. O

We O
thus O
present O
a O
processing O
and O
evaluation O
pipeline O
tailored O
to O
fit O
Morphologically O
Rich O
Languages O
( O
MRLs O
) O
, O
i.e. O
, O
covering O
2We O
make O
our O
PLM O
https://huggingface.co/ O
onlplab O
/ O
alephbert O
- O
base O
and O
demo O
https://nlp O
. O

biu.ac.il/~amitse/alephbert/ O
publicly O
available O
, O
to O
qualitatively O
assess O
present O
and O
future O
Hebrew O
PLMs.sentence O
- O
level O
, O
word O
- O
level O
and O
most O
importantly O
sub O
- O
word O
morphological O
- O
level O
tasks O
( O
Segmentation B-TaskName
, O
Part B-TaskName
- I-TaskName
of I-TaskName
- I-TaskName
Speech I-TaskName
Tagging I-TaskName
, O
full B-TaskName
Morphological I-TaskName
Tagging I-TaskName
, O
Dependency B-TaskName
Parsing I-TaskName
, O
Named B-TaskName
Entity I-TaskName
Recognition I-TaskName
( O
NER B-TaskName
) O
andSentiment B-TaskName
Analysis I-TaskName
) O
, O
and O
present O
new O
and O
improved O
SOTA O
for O
Modern O
Hebrew O
on O
all O
of O
these O
tasks O
. O

2 O
Previous O
Work O
Contextualized O
word O
embedding O
vectors O
are O
a O
major O
driver O
for O
improved O
performance O
of O
deep O
learning O
models O
on O
many O
Natural B-TaskName
Language I-TaskName
Understanding I-TaskName
( O
NLU B-TaskName
) O
tasks O
. O

Initially O
, O
ELMo B-MethodName
( O
Peters O
et O
al O
. O
, O
2018 O
) O
and O
ULMFit B-MethodName
( O
Howard O
and O
Ruder O
, O
2018 O
) O
introduced O
contextualized O
word O
embedding O
frameworks O
by O
training O
LSTM B-MethodName
- O
based O
models O
on O
massive O
amounts O
of O
texts O
. O

The O
linguistic O
quality O
encoded O
in O
these O
models O
was O
demonstrated O
over O
6 O
tasks O
: O
Question B-TaskName
Answering I-TaskName
, O
Textual B-TaskName
Entailment I-TaskName
, O
Semantic B-TaskName
Role I-TaskName
labeling I-TaskName
, O
Coreference B-TaskName
Resolution I-TaskName
, O
Name B-TaskName
Entity I-TaskName
Extraction I-TaskName
, O
and O
Sentiment B-TaskName
Analysis I-TaskName
. O

The O
next O
big O
leap O
was O
obtained O
with O
the O
introduction O
of O
the O
GPT-1 B-MethodName
framework O
by O
Radford O
and O
Sutskever O
( O
2018 O
) O
. O

Instead O
of O
using O
LSTM B-MethodName
layers O
, O
GPT B-MethodName
is O
based O
on O
12 O
layers O
of O
Transformer B-MethodName
decoders O
with O
each O
decoder O
layer O
composed O
of O
a O
768 B-HyperparameterValue
- O
dimensional O
feed B-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
layer I-HyperparameterName
and O
12 B-HyperparameterValue
self B-HyperparameterName
- I-HyperparameterName
attention I-HyperparameterName
heads I-HyperparameterName
. O

Devlin O
et O
al O
. O

( O
2019 O
) O
followed O
along O
the O
same O
lines O
and O
implemented O
Bidirectional B-MethodName
Encoder I-MethodName
Representations I-MethodName
from I-MethodName
Transformers I-MethodName
, O
or O
BERT B-MethodName
in O
short O
. O

BERT B-MethodName
attends O
to O
the O
input O
tokens O
in O
both O
forward O
and O
backward O
directions O
while O
optimizing O
a O
Masked O
Language O
Model O
and O
a O
Next O
Sentence O
Prediction O
objective O
objectives O
. O

BERT B-MethodName
Benchmarks O
An O
integral O
part O
involved O
in O
developing O
various O
PLMs O
is O
providing O
NLU O
multitask O
benchmarks O
used O
to O
demonstrate O
the O
linguistic O
abilities O
of O
new O
models O
and O
approaches O
. O

English O
BERT B-MethodName
models O
are O
evaluated O
on O
3 O
standard O
major O
benchmarks O
. O

The O
Stanford B-DatasetName
Question I-DatasetName
Answering I-DatasetName
Dataset O
( O
SQuAD B-DatasetName
) O
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
is O
used O
for O
testing O
paragraph O
- O
level O
reading O
comprehension O
abilities O
. O

Wang O
et O
al O
. O

( O
2018 O
) O
selected O
a O
diverse O
and O
relatively O
hard O
set O
of O
sentence O
and O
sentence O
- O
pair O
tasks O
which O
comprise O
the O
General B-DatasetName
Language I-DatasetName
Understanding I-DatasetName
Evaluation I-DatasetName
( O
GLUE B-DatasetName
) O
benchmark O
. O

The O
SWAG B-DatasetName
( O
Situations B-DatasetName
With I-DatasetName
Adversarial I-DatasetName
Generations I-DatasetName
) O
dataset O
( O
Zellers O
et O
al O
. O
, O
2018 O
) O
presents O
models O
with O
partial O
description O
of O
grounded O
situations O
to O
see O
if O
they O
can O
consistently O
predict O
subsequent O
scenarios O
, O
thus O
indicating O
abilities O
of O
commonsense O
reasoning.47When O
evaluating O
Hebrew O
PLMs O
, O
one O
of O
the O
key O
pitfalls O
is O
that O
there O
are O
no O
Hebrew O
versions O
for O
these O
benchmarks O
. O

Furthermore O
, O
none O
of O
the O
suggested O
benchmarks O
account O
for O
examining O
the O
capacity O
of O
PLMs O
for O
encoding O
the O
word O
- O
internal O
morphological O
structures O
which O
are O
inherent O
in O
MRLs O
. O

In O
this O
work O
we O
enable O
a O
generic O
morphological O
- O
level O
evaluation O
pipeline O
that O
is O
suited O
for O
PLMs O
of O
MRLs O
. O

Multilingual O
vs. O
Monolingual O
BERT B-MethodName
Devlin O
et O
al O
. O

( O
2019 O
) O
produced O
2 O
BERT B-MethodName
models O
, O
for O
English O
and O
Chinese O
. O

To O
support O
other O
languages O
, O
they O
trained O
a O
multilingual B-MethodName
BERT I-MethodName
( O
mBERT B-MethodName
) O
model O
combining O
texts O
covering O
over O
100 O
languages O
, O
in O
the O
hoped O
to O
benefit O
low O
- O
resource O
languages O
with O
the O
linguistic O
information O
obtained O
from O
languages O
with O
larger O
datasets O
. O

In O
reality O
, O
however O
, O
mBERT B-MethodName
performance O
on O
specific O
languages O
has O
not O
been O
as O
successful O
as O
English O
. O

Consequently O
, O
several O
research O
efforts O
focused O
on O
building O
monolingual O
BERT B-MethodName
models O
as O
well O
as O
providing O
languagespecific O
evaluation O
benchmarks O
. O

Liu O
et O
al O
. O

( O
2019 O
) O
trained O
CamemBERT B-MethodName
, O
a O
French O
BERT B-MethodName
model O
evaluated O
on O
syntactic O
and O
semantic O
tasks O
in O
addition O
to O
natural O
language O
inference O
tasks O
. O

Rybak O
et O

al O
. O
( O
2020 O
) O
trained O
HerBERT B-MethodName
, O
a O
BERT B-MethodName
PLM O
for O
Polish O
. O

They O
evaluated O
it O
on O
a O
diverse O
set O
of O
existing O
NLU B-TaskName
benchmarks O
as O
well O
as O
a O
new O
dataset O
for O
sentiment B-TaskName
analysis I-TaskName
for O
the O
e O
- O
commerce O
domain O
. O

Polignano O
et O
al O
. O

( O
2019 O
) O
created O
Alberto B-MethodName
, O
a O
BERT B-MethodName
model O
for O
Italian O
, O
using O
a O
massive O
tweet O
collection O
. O

They O
tested O
it O
on O
several O
NLU B-TaskName
tasks O
— O
subjectivity O
, O
polarity O
( O
sentiment O
) O
and O
irony O
detection O
in O
tweets O
. O

In O
order O
to O
obtain O
a O
large O
enough O
training O
corpus O
in O
low O
- O
resources O
languages O
, O
such O
as O
Finnish O
( O
Virtanen O
et O
al O
. O
, O
2019 O
) O
and O
Persian O
( O
Farahani O
et O
al O
. O
, O
2020 O
) O
, O
a O
great O
deal O
of O
effort O
went O
into O
filtering O
and O
cleaning O
text O
samples O
obtained O
from O
web O
crawls O
. O

BERT B-MethodName
for O
MRLs O
Languages O
with O
rich O
morphology O
introduce O
another O
challenge O
involving O
the O
identification O
and O
extraction O
of O
sub O
- O
word O
morphological O
information O
. O

In O
many O
MRLs O
words O
are O
composed O
of O
sub O
- O
word O
morphological O
units O
, O
with O
each O
unit O
acting O
as O
a O
single O
syntactic O
unit O
bearing O
as O
single O
POS O
tag O
( O
mimicking O
‘ O
words O
’ O
in O
English O
) O
. O

Antoun O
et O
al O
. O

( O
2020 O
) O
addressed O
this O
for O
Arabic O
, O
a O
Semitic O
MRLs O
, O
by O
pre O
- O
processing O
the O
training O
data O
using O
a O
morphological O
segmenter O
, O
producing O
morphological O
segments O
to O
be O
used O
for O
training O
AraBERT B-MethodName
instead O
of O
the O
actual O
words O
. O

By O
doing O
so O
, O
they O
were O
able O
to O
produce O
output O
vectors O
that O
corre O
- O
Language O
Oscar O
( O
duped O
) O

Size O
Wikipedia O
Articles O
English O
2.3 O
T O
6,282,774 O
Russian O
1.2 O
T O
1,713,164 O
Chinese O
508 O
G O
1,188,715 O
French O
282 O
G O
2,316,002 O
Arabic O
82 O
G O
1,109,879 O
Hebrew O
20 O
G O
292,201 O
Table O
1 O
: O
Corpora O
Size O
Comparison O
: O
Resource O
- O
savvy O
languages O
vs. O
Hebrew O
. O

spond O
to O
morphological O
segments O
rather O
than O
the O
original O
space O
- O
delimited O
word O
- O
tokens O
. O

However O
, O
this O
approach O
requires O
the O
application O
of O
the O
same O
segmenter O
at O
inference O
time O
as O
well O
, O
and O
like O
any O
pipeline O
approach O
, O
this O
setup O
is O
susceptible O
to O
error O
propagation O
. O

This O
risk O
is O
magnified O
as O
words O
in O
MRLs O
may O
be O
morphologically O
ambiguous O
, O
and O
the O
predicted O
segments O
might O
not O
represent O
the O
correct O
interpretation O
of O
the O
words O
. O

As O
a O
result O
, O
the O
quality O
of O
the O
PLM O
depends O
on O
the O
accuracy O
achieved O
by O
the O
segmenting O
component O
. O

A O
particular O
novelty O
of O
this O
work O
is O
notmaking O
any O
changes O
to O
the O
input O
, O
letting O
the O
PLM O
encode O
morphological O
information O
associated O
with O
complete O
Hebrew O
tokens O
. O

Instead O
, O
transforming O
the O
resulting O
contextualized O
word O
vectors O
into O
morphological O
- O
level O
segments O
via O
a O
novel O
neural O
architecture O
which O
we O
discuss O
shortly O
. O

Evaluating O
PLMs O
for O
MRLs O
Across O
all O
of O
the O
above O
- O
mentioned O
language O
- O
specific O
PLMs O
, O
evaluation O
was O
performed O
on O
the O
word-,sentence- O
or O
paragraph O
- O
level O
. O

Non O
examined O
the O
capacity O
of O
PLMs O
to O
encode O
sub O
- O
word O
morphological O
- O
level O
information O
which O
we O
focus O
on O
in O
this O
work O
. O

¸ O

Sahin O
et O
al O
. O

( O
2019 O
) O
probed O
various O
information O
types O
encoded O
in O
embedded O
word O
vectors O
. O

Similarly O
to O
us O
, O
they O
focused O
on O
languages O
with O
rich O
morphology O
where O
linguistic O
signals O
are O
encoded O
at O
the O
morphological O
, O
subword O
level O
. O

Their O
work O
is O
more O
about O
explainability O
— O
showing O
high O
positive O
correlation O
of O
probing O
tasks O
to O
the O
downstream O
tasks O
, O
especially O
for O
morphologically O
rich O
languages O
. O

Unlike O
us O
, O
they O
assume O
a O
single O
POS O
tag O
and O
set O
of O
features O
per O
word O
in O
their O
probing O
tasks O
. O

In O
Hebrew O
, O
Arabic O
and O
other O
MRLs O
, O
tokens O
may O
carry O
multiple O
POS O
per O
word O
, O
and O
are O
required O
to O
be O
segmented O
for O
further O
processing O
. O

We O
provide O
a O
framework O
that O
extracts O
subword O
morphological O
units O
given O
contextualized O
word O
vectors O
, O
that O
enables O
to O
evaluate O
PLMs O
on O
morphologically O
- O
aware O
datasets O
where O
words O
can O
have O
multiple O
POS O
tags O
and O
feature O
- O
bundles.48Corpus O
File O
Size O
Sentences O
Words O
Oscar B-DatasetName
( O
deduped O
) O
9.8 O
GB O
20.9 O
M O
1,043 O
M O
Twitter B-DatasetName
6.9 O
GB O
71.5 O
M O
774 O
M O
Wikipedia B-DatasetName
1.1 O
GB O
6.3 O
M O
127 O
M O
Total O
17.9 O
GB O
98.7 O
M O
1.9B O
Table O
2 O
: O
AlephBERT B-MethodName
’s O
Training O
Data O
. O

3 O
AlephBERT B-MethodName
Pre O
- O
Training O
Data O
The O
PLM O
termed O
AlephBERT B-MethodName
that O
we O
provide O
herein O
is O
trained O
on O
a O
larger O
dataset O
and O
a O
larger O
vocabulary O
than O
any O
Hebrew O
BERT B-MethodName
instantiation O
before O
. O

The O
data O
we O
train O
on O
is O
listed O
in O
Table O
2 O
. O

Concretely O
, O
we O
employ O
the O
following O
datasets O
for O
pre O
- O
training O
: O
( O
i O
) O
Oscar B-DatasetName
: O

Deduplicated O
Hebrew O
portion O
extracted O
from O
Common O
Crawl O
via O
language O
classification O
, O
filtering O
and O
cleaning O
( O
Ortiz O
Suárez O
et O
al O
. O
, O
2020 O
) O
. O

( O
ii O
) O
Wikipedia B-DatasetName
: O

Texts O
from O
all O
of O
Hebrew O
Wikipedia O
, O
extracted O
using O
Attardi O
( O
2015 O
) O
. O

( O
iii O
) O
Twitter B-DatasetName
: O
Hebrew O
tweets O
collected O
between O
2014 O
- O
09 O
- O
28 O
and O
2018 O
- O
03 O
- O
07 O
. O

We O
removed O
markers O
( O
“ O
RT O
: O
” O
, O
“ O
@ O
” O
user O
mentions O
and O
URLs O
) O
, O
and O
eliminated O
duplicates O
. O

For O
data O
statistics O
, O
see O
Table O
2 O
. O

The O
Hebrew O
portions O
of O
Oscar B-DatasetName
andWikipedia O
provide O
us O
with O
a O
training O
- O
set O
size O
orders O
- O
ofmagnitude O
smaller O
compared O
with O
resource O
- O
savvy O
languages O
, O
as O
shown O
in O
Table O
1 O
. O

In O
order O
to O
build O
a O
strong O
PLM O
we O
need O
a O
considerable O
boost O
in O
the O
amount O
of O
sentences O
the O
PLM O
can O
learn O
from O
, O
which O
in O
our O
case O
comes O
form O
massive O
amounts O
of O
tweets O
added O
to O
the O
training O
set O
. O

We O
acknowledge O
the O
potential O
inherent O
concerns O
associated O
with O
this O
data O
source O
( O
population O
bias O
, O
behavior O
patterns O
, O
bot O
masquerading O
as O
humans O
etc O
. O
) O
and O
note O
that O
we O
have O
not O
made O
any O
explicit O
attempt O
to O
identify O
these O
cases O
. O

Honoring O
ethical O
and O
legal O
constraints O
we O
have O
not O
manually O
analyzed O
nor O
published O
this O
data O
source O
. O

While O
the O
free O
form O
language O
expressed O
in O
tweets O
might O
differ O
significantly O
from O
the O
text O
found O
in O
Oscar B-DatasetName
and O
Wikipedia B-DatasetName
, O
the O
sheer O
volume O
of O
tweets O
helps O
us O
close O
the O
resource O
gap O
substantially O
with O
minimal O
effort.3 O
Model O
We O
used O
the O
Transformers O
training O
framework O
of O
Huggingface O
( O
Wolf O
et O
al O
. O
, O
2020 O
) O
and O
trained O
two O
different O
models O
— O
a O
small O
model O
with O
6 O
hidden O
layers O
learned O
from O
the O
Oscar O
portion O
of O
our O
dataset O
, O
and O
a O
base O
model O
with O
12 O
hidden O
layers O
which O
was O
trained O
on O
the O
entire O
dataset O
. O

The O
processing O
units O
used O
are O
wordpieces O
generated O
by O
training O
BERT B-MethodName
tokenizers O
over O
the O
respective O
3For O
more O
details O
and O
an O
ethical O
discussion O
, O
see O
Section O
8.datasets O
with O
a O
vocabulary O
size O
of O
52 O
K O
in O
both O
cases O
. O

Following O
the O
work O
on O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
we O
optimize O
AlephBERT B-MethodName
with O
a O
masked O
- O
token O
prediction O
loss O
. O

We O
deploy O
the O
default O
masking O
configuration O
where O
15 O
% O
of O
word O
piece O
tokens O
are O
masked O
. O

In O
80 O
% O
of O
the O
cases O
, O
they O
are O
replaced O
by O
[ O
MASK O
] O
, O
in O
10 O
% O
of O
the O
cases O
, O
they O
are O
replaced O
by O
a O
random O
token O
and O
in O
the O
remaining O
cases O
, O
the O
masked O
tokens O
are O
left O
as O
is O
. O

Operation O
To O
optimize O
GPU O
utilization O
and O
decrease O
training O
time O
we O
split O
the O
dataset O
into O
4 O
chunks O
based O
on O
the O
number O
of O
tokens O
in O
a O
sentence O
and O
consequently O
we O
are O
able O
to O
increase O
batch B-HyperparameterName
sizes I-HyperparameterName
and O
dramatically O
shorten O
training O
time O
. O

chunk1 O
chunk2 O
chunk3 O
chunk4 O
max O
tokens O
0>32 O
32>64 O
64>128 O
128>512 O
num O
sentences O

70 O
M O
20 O
M O
5 O
M O
2 O
M O
We O
trained O
for O
5 B-HyperparameterValue
epochs B-HyperparameterName
with O
learning B-HyperparameterName
rate I-HyperparameterName
1e4 B-HyperparameterValue
followed O
by O
an O
additional O
5 B-HyperparameterValue
epochs B-HyperparameterName
with O
learning B-HyperparameterName
rate I-HyperparameterName
at O
5e-5 B-HyperparameterValue
for O
a O
total O
of O
10 B-HyperparameterValue
epochs B-HyperparameterName
. O

We O
trained O
AlephBERT B-MethodName
baseover O
the O
entire O
dataset O
on O
an O
NVidia O
DGX O
server O
with O
8 O
V100 O
GPUs O
which O
took O
8 O
days O
. O

AlephBERT B-MethodName
small I-MethodName
was O
trained O
over O
the O
Oscar B-DatasetName
portion O
only O
, O
using O
4 O
GTX O
2080ti O
GPUs O
taking O
5 O
days O
in O
total O
. O

4 O
The O
Morphological O
Extraction O
Model O
Modern O
Hebrew O
is O
a O
Semitic O
language O
with O
rich O
morphology O
and O
complex O
orthography O
. O

As O
a O
result O
, O
the O
basic O
processing O
units O
in O
the O
language O
are O
typically O
smaller O
than O
raw O
space O
- O
delimited O
tokens O
. O

Subsequently O
, O
most O
standard O
evaluation O
tasks O
require O
knowledge O
of O
the O
internal O
morphological O
boundaries O
within O
the O
raw O
tokens O
. O

To O
accommodate O
this O
granularity O
requirement O
we O
developed O
a O
neural O
model O
designed O
to O
produce O
the O
disambiguated O
morphological O
segments O
for O
each O
token O
in O
context O
. O

These O
linguistic O
segmentations O
are O
distinct O
of O
the O
word O
- O
pieces O
employed O
by O
the O
PLM O
. O

In O
the O
morphological O
extraction O
neural O
model O
, O
each O
input O
token O
is O
represented O
by O
( O
one O
or O
more O
) O
contextualized O
word O
- O
vectors O
produced O
by O
the O
PLM O
. O

Each O
word O
- O
piece O
token O
is O
associated O
with O
a O
vector O
, O
and O
for O
each O
space O
- O
delimited O
token O
, O
we O
average O
the O
word O
- O
piece O
vectors O
. O

We O
feed O
the O
resulting O
vector O
into O
a O
seq2seq O
model O
and O
encode O
the O
surface O
token O
as O
a O
sequence O
of O
characters O
using O
a O
BiLSTM B-MethodName
, O
followed O
by O
a O
decoder O
that O
generates O
an O
output O
sequence O
of O
characters O
, O
using O
space O
as O
a O
special O
symbol O
signaling O
morphological O
boundaries.49Raw O
input O
/ֹדנֵfנ O
/ O
דרֹוuנfלביתהלבלַנִ O
( O
lbit O
hlbn O
) O
Space O
- O
delimited O
words O
/ֹדנֵfנ O
/ O
דרֹוuנfהלבלַנִ O
( O
hlbn O
) O
/ֹדנֵfלביתדרֹו O

( O
lbit O
) O
Index B-TaskName
5 O
4 O
3 O
2 O
1 O
Segmentation B-TaskName
/ֹדנֵfנ O
/ O
דרֹוuנfלבלַנִ(lbn O
) O
white O
/ֹדנֵfהדרֹו(h O
) O
the O
/ֹדנֵfביתדרֹו(bit O
) O
house O
/ֹדנֵfהדרֹו(h O
) O

the O
/ֹדנֵfלדרֹו(l O
) O
to O
POS B-TaskName

ADJ O
DET O
NOUN O

DET O
ADP O
Morphology B-TaskName
Gender O
= O
Masc O
|Number O
= O
Sing O
PronType O
= O
Art O
Gender O
= O
Masc O
|Number O
= O
Sing O
PronType O
= O
Art O
Dependencies B-TaskName
3 O
/ O
amod O
5 O
/ O
det O
1 O
/ O
obj O
3 O
/ O
def O
0 O
/ O
ROOT O
Word B-TaskName
- I-TaskName
level I-TaskName
NER I-TaskName
E O
- O
ORG O
B O
- O
ORG O
Morpheme B-TaskName
- I-TaskName
level I-TaskName
NER I-TaskName
E O
- O
ORG O
I O
- O
ORG O
I O
- O
ORG O
B O
- O
ORG O
O O
Table O
3 O
: O
Illustration O
of O
Evaluated O
Word O
- O
Based O
and O
Morpheme O
- O
Based O
Downstream O
Tasks O
. O

The O
two O
- O
word O
input O
phrase O
“ O
/ֹדנֵfנ O
/ O
דרֹוuנfלביתהלבלַנִ O
, O
” O
transliterated O
as O
“ O
lbit O
hlbn O
” O
( O
to O
the O
White O
House O
) O
, O
decompose O
into O
five O
morphological O
segments O
( O
‘ O
to O
- O
the O
- O
house O
the O
- O
white O
’ O
) O
. O

The O
Hebrew O
text O
goes O
from O
right O
to O
left O
. O

Figure O
2 O
: O
Illustration O
of O
the O
Morphological O
Extraction O
Model O
. O

The O
embedded O
vectors O
associated O
with O
the O
wordpieces O
( O
v1 O
and O
v2 O
representing O
word O
- O
piece O
vectors O
generated O
in O
Figure O
1 O
) O
are O
combined O
( O
averaged O
) O
to O
produce O
a O
single O
word O
context O
vector O
. O

This O
context O
vector O
initializes O
the O
hidden O
( O
forward O
and O
backward O
) O
state O
of O
a O
BiLSTM B-MethodName
that O
encodes O
the O
characters O
of O
the O
origin O
word O
. O

The O
decoder O
LSTM B-MethodName
outputs O
a O
sequence O
of O
characters O
, O
where O
a O
special O
empty O
symbol O
indicates O
a O
morphological O
segment O
boundary O
. O

In O
multi O
- O
task O
setup O
, O
a O
fully O
connected O
linear O
layer O
is O
used O
to O
predict O
a O
label O
whenever O
a O
segment O
boundary O
is O
detected O
. O

For O
tasks O
involving O
both O
segments O
and O
labels O
( O
Part B-TaskName
- I-TaskName
of I-TaskName
- I-TaskName
Speech I-TaskName
Tagging I-TaskName
, O
Morphological B-TaskName
- I-TaskName
Features I-TaskName
Tagging I-TaskName
, O
Named B-TaskName
- I-TaskName
Entity I-TaskName
Recognition I-TaskName
) O
we O
expand O
this O
network O
in O
a O
multi O
- O
task O
learning O
setup O
; O
when O
generating O
an O
end O
- O
of O
- O
segment O
( O
space O
) O
symbol O
, O
the O
model O
also O
predicts O
task O
label O
, O
and O
we O
combine O
the O
segment O
- O
label O
losses O
. O

The O
complete O
morphological O
extraction O
architecture O
is O
illustrated O
in O
Figure O
2 O
. O
5 O
Experimental O
Setup O
Goal O

In O
order O
to O
empirically O
gauge O
the O
effect O
of O
model O
size O
and O
data O
quantity O
on O
the O
quality O
of O
the O
language O
model O
, O
we O
compare O
the O
performance O
of O
AlephBERT B-MethodName
( O
both O
small O
andbase O
) O
with O
all O
existing O
Hebrew O
BERT B-MethodName
instantiations O
. O

In O
this O
Section O
, O
we O
detail O
the O
tasks O
and O
evaluation O
metrics O
. O

In O
the O
nextSection O
, O
we O
present O
and O
analyze O
the O
results O
. O

5.1 O
Sentence O
- O
Based O
Modeling O
Sentiment B-TaskName
Analysis I-TaskName
We O
first O
report O
on O
a O
sentence O
classification O
task O
, O
assigning O
a O
sentence O
with O
one O
of O
three O
sentiment O
values O
: O
negative O
, O
positive O
, O
neutral O
. O

Sentence O
- O
level O
predictions O
are O
achieved O
by O
directly O
fine O
- O
tuning O
the O
PLM O
using O
an O
additional O
sentenceclassification O
head O
The O
sentence O
- O
level O
embedding O
vector O
representation O
is O
the O
one O
associated O
with O
the O
special O

[ O
CLS O
] O
BERT B-MethodName
token O
. O

We O
used O
a O
version O
of O
the O
Hebrew O
Facebook B-DatasetName
Sentiment I-DatasetName
dataset O
( O
henceforth O
FB O
) O
of O
Amram O
et O
al O
. O
( O
2018 O
) O
which O
we O
corrected O
by O
removing O
leaked O
samples.4We O
fine O
- O
tuned O
all O
models O
for O
15 B-HyperparameterValue
epochs B-HyperparameterName
with O
5 O
different O
seeds O
, O
and O
report O
mean O
accuracy O
. O

5.2 O
Word O
- O
Based O
Modeling O
Named B-TaskName
Entity I-TaskName
Recognition I-TaskName

In O
this O
setup O
we O
assume O
a O
sequence O
labeling O
task O
based O
on O
spacedelimited O
word O
- O
tokens O
. O

The O
input O
comprises O
of O
the O
sequence O
of O
words O
in O
the O
sentence O
, O
and O
the O
output O
contains O
BIOES O
tags O
indicating O
entity O
spans O
. O

Word O
- O
level O
NER O
predictions O
are O
achieved O
by O
directly O
fine O
- O
tuning O
the O
PLMs O
using O
an O
additional O
token O
- O
classification O
head O
In O
cases O
where O
a O
word O
is O
split O
into O
multiple O
word O
pieces O
by O
the O
PLM O
tokenizer O
, O
we O
employ O
common O
practice O
and O
use O
the O
first O
word O
- O
piece O
vector O
. O

We O
evaluate O
this O
model O
on O
two O
corpora O
. O

( O
i O
) O
The O
Ben B-DatasetName
- I-DatasetName
Mordecai I-DatasetName
( O
BMC B-DatasetName
) O
corpus O
( O
Ben O
Mordecai O
and O
Elhadad O
, O
2005 O
) O
, O
which O
contains O
3294 O
sentences O
with O
4600 O
entities O
and O
seven O
different O
entity O
categories O
( O
Date O
, O
Location O
, O
Money O
, O
Organization O
, O
Person O
, O
Percent O
, O
Time O
) O
. O

To O
remain O
compatible O
with O
the O
original O
work O
we O
train O
and O
test O
the O
models O
on O
3 O
4This O
version O
has O
a O
total O
of O
8,465 O
samples O
and O
is O
publicly O
available O
here O
: O
https://github.com/OnlpLab/ O
Hebrew O
- O
Sentiment O
- O
Data50different O
splits O
as O
in O
Bareket O
and O
Tsarfaty O
( O
2020 O
) O
. O

( O
ii O
) O
The O
Named B-DatasetName
Entities I-DatasetName
and I-DatasetName
MOrphology I-DatasetName
( O
NEMO B-DatasetName
) O
corpus5(Bareket O
and O
Tsarfaty O
, O
2020 O
) O
which O
is O
an O
extension O
of O
the O
SPMRL O
dataset O
with O
Named O
Entities O
. O

The O
NEMO B-DatasetName
corpus O
contains O
6220 O
sentences O
with O
7713 O
entities O
of O
nine O
entity O
types O
( O
Language O
, O
Product O
, O
Event O
, O
Facility O
, O
Geo O
- O
Political O
Entity O
, O
Location O
, O
Organization O
, O
Person O
, O
Work O
- O
Of O
- O
Art O
) O
. O

We O
trained O
both O
models O
for O
15 B-HyperparameterValue
epochs B-HyperparameterName
with O
5 O
different O
seeds O
and O
report O
mean O
F1 O
scores O
on O
entity O
spans O
. O

5.3 O
Morpheme O
- O
Based O
Modeling O
Finally O
, O
to O
probe O
the O
PLM O
capacity O
to O
accurately O
predict O
word O
- O
internal O
structure O
, O
we O
test O
all O
models O
on O
five O
tasks O
that O
require O
knowledge O
of O
the O
internal O
morphology O
of O
raw O
words O
. O

The O
input O
to O
all O
these O
tasks O
is O
a O
Hebrew O
sentence O
represented O
as O
a O
raw O
sequence O
of O
space O
- O
delimited O
words O
: O
( O
i O
) O
Segmentation B-TaskName
: O

Generating O
a O
sequence O
of O
morphological O
segments O
representing O
the O
basic O
processing O
units O
. O

These O
units O
comply O
with O
the O
2 O
- O
level O
representation O
of O
tokens O
defined O
by O
UD O
, O
each O
unit O
with O
a O
single O
POS O
tag.6 O
( O
ii O
) O
Part B-TaskName
- I-TaskName
of I-TaskName
- I-TaskName
Speech I-TaskName
( O
POS B-TaskName
) O
Tagging I-TaskName
: O
Tagging O
each O
segment O
with O
a O
single O
POS O
. O

( O
iii O
) O
Morphological B-TaskName
Tagging I-TaskName
: O
Tagging O
each O
segment O
with O
a O
single O
POS O
and O
a O
set O
of O
features O
. O

Equivalent O
to O
the O
AllTags O
evaluation O
defined O
in O
the O
CoNLL18 B-DatasetName
shared O
task.7 O
( O
iv O
) O
Morpheme B-TaskName
- I-TaskName
Based I-TaskName
NER I-TaskName
: O
Tagging O
each O
segment O
with O
a O
BIOES O
and O
its O
entity O
- O
type O
. O

( O
v O
) O
Dependency B-TaskName
Parsing I-TaskName
: O
Use O
each O
segment O
as O
a O
node O
in O
the O
predicted O
dependency O
tree O
. O

We O
train O
and O
test O
all O
morphologically O
- O
aware O
models O
using O
two O
available O
morphologically O
- O
aware O
Hebrew O
resources O
: O
•The O
Hebrew O
Section O
of O
the O
SPMRL O
Task O
( O
Seddah O
et O

al O
. O
, O
2013 O
) O
. O

•The O
Hebrew O
Section O
of O
the O
UD O
treebanks O
collection O
( O
Sadde O
et O
al O
. O
, O
2018 O
) O

All O
models O
were O
trained O
for O
15 B-HyperparameterValue
epochs I-HyperparameterValue
with O
5 O
different O
seeds O
and O
we O
report O
two O
variants O
of O
mean O
F1 B-MetricName
scores I-MetricName
as O
described O
next O
. O

5Available O
here O
: O
https://github.com/OnlpLab/ O
NEMO O
- O
Corpus O
6https://universaldependencies.org/u/ O
overview O
/ O
tokenization.html O
7https://universaldependencies.org/ O
conll18 O
/ O
results O
- O
alltags.htmlFor O
tasks O
( O
i)–(iv O
) O
we O
use O
the O
morphological O
extraction O
model O
( O
Section O
4 O
) O
to O
extract O
the O
morphological O
segments O
of O
each O
word O
in O
context O
and O
also O
predict O
the O
labels O
via O
Multitask O
training O
. O

For O
task O
( O
iv O
) O
the O
NER B-TaskName
task O
, O
we O
use O
the O
morphologically O
- O
annotated O
data O
files O
of O
the O
aforementioned O
SPMRL B-DatasetName
- I-DatasetName
based I-DatasetName
NEMO I-DatasetName
corpus O
( O
Bareket O
and O
Tsarfaty O
, O
2020 O
) O
. O

In O
addition O
to O
the O
multi O
- O
task O
setup O
described O
earlier O
, O
we O
design O
another O
setup O
in O
which O
we O
first O
only O
segment O
the O
text O
, O
and O
then O
perform O
fine O
- O
tuning O
with O
a O
token O
classification O
attention O
head O
directly O
applied O
to O
the O
PLM O
output O
for O
the O
segmented O
tokens O
( O
similar O
to O
the O
way O
we O
fine O
- O
tune O
the O
PLM O
for O
the O
word O
- O
based O
NER B-TaskName
task O
described O
in O
the O
previous O
section O
) O
. O

We O
acknowledge O
that O
we O
are O
fine O
- O
tuning O
the O
PLM O
on O
morphological O
segments O
the O
model O
was O
not O
originally O
pre O
- O
trained O
on O
, O
however O
, O
as O
we O
shall O
see O
shortly O
, O
this O
seemingly O
unintuitive O
strategy O
performs O
surprisingly O
well O
. O

For O
task O
( O
v O
) O
we O
set O
up O
a O
dependency O
parsing O
evaluation O
pipeline O
using O
the O
standalone O
Hebrew O
parser O
offered O
by O
More O
et O

al O
. O

( O
2019 O
) O
( O
a.k.a O
YAP O
) O
which O
was O
trained O
to O
produce O
SPMRL O
dependency O
labels O
. O

The O
morphological O
information O
for O
each O
word O
( O
namely O
the O
segments O
and O
POS O
tags O
) O
is O
recovered O
by O
our O
morphological O
extraction O
model O
, O
and O
is O
used O
as O
input O
features O
for O
the O
YAP O
standalone O
dependency O
parser O
. O

5.4 O
Morpheme O
- O
Based O
Evaluation O
Metrics O
Aligned O
Segment O
The O
CoNLL18 B-MetricName
Shared I-MetricName
Task I-MetricName
evaluation I-MetricName
campaign8reports O
scores O
for O
segmentation O
and O
POS O
tagging9for O
all O
participating O
languages O
. O

For O
multi O
- O
segment O
words O
, O
the O
gold O
and O
predicted O
segments O
are O
aligned O
by O
their O
Longest O
Common O
Sub O
- O
sequence O
, O
and O
only O
matching O
segments O
are O
counted O
as O
true O
positives O
. O

We O
use O
the O
script O
to O
compare O
aligned O
segment O
and O
tagging O
scores O
between O
oracle O
( O
gold O
) O
segmentation O
and O
realistic O
( O
predicted O
) O
segmentation O
. O

Aligned O
Multi O
- O
Set O
In O
addition O
to O
the O
CoNLL18 O
metrics O
, O
we O
compute O
F1 B-MetricName
scores I-MetricName
, O
with O
a O
slight O
but O
important O
difference O
from O
the O
shared O
task O
, O
as O
defined O
by O
More O
et O

al O
. O

( O
2019 O
) O
and O
Seker O
and O
Tsarfaty O
( O
2020 O
) O
. O

For O
each O
word O
, O
counts O
are O
based O
on O
multiset O
intersections O
of O
the O
gold O
and O
predicted O
labels O
ignoring O
the O
order O
of O
the O
segments O
while O
account8https://universaldependencies.org/ O
conll18 O
/ O
results.html O
9respectively O
referred O
to O
as O
’ O
Segmented O
Words O
’ O
and O
’ O
UPOS O
’ O
in O
the O
CoNLL18 O
evaluation O
script51Task O
NER O
( O
Word O
) O
Sentiment O

Corpus O
NEMO O
BMC O
FB O
Prev O
. O

SOTA O
77.75 O
85.22 O
NA O
mBERT O
79.07 O
87.77 O
79.07 O
HeBERT O
81.48 O
89.41 O
81.48 O
AlephBERT B-MethodName
small O
78.69 O
89.07 O
78.69 O
AlephBERT B-MethodName
base O
84.91 O
91.12 O
84.91 O
Table O
4 O
: O
Word O
- O
based O
NER O
F1 O
. O

Previous O
SOTA O
on O
both O
corpora O
reported O
by O
the O
NEMO O
models O
of O
Bareket O
and O
Tsarfaty O
( O
2020 O
) O
. O

Sentiment O
Analysis O
accuracy O
on O
the O
corrected O
version O
of O
the O
corpus O
of O
Amram O
et O
al O
. O

( O
2018 O
) O
. O

ing O
for O
the O
number O
of O
each O
segment O
. O

Aligned O
mset O
is O
based O
on O
set O
difference O
which O
acknowledges O
the O
possible O
undercover O
of O
covert O
morphemes O
which O
is O
an O
appropriate O
measure O
of O
morphological O
accuracy O
. O

Discussion O
To O
illustrate O
the O
difference O
between O
aligned O
segment O
andaligned O
mset O
, O
let O
us O
take O
for O
example O
the O
gold O
segmented O
tag O
sequence O
: O
b O
/ O
IN O
, O
h O
/ O
DET O
, O
bit O
/ O
NOUN O
and O
the O
predicted O
segmented O
tag O
sequence O
b O
/ O
IN O
, O
bit O
/ O
NOUN O
. O

According O
to O
aligned O
segment O
, O
the O
first O
segment O
( O
b O
/ O
IN O
) O
is O
aligned O
and O
counted O
as O
a O
true O
positive O
, O
the O
second O
segment O
however O
is O
considered O
as O
a O
false O
positive O
( O
bit O
/ O
NOUN O
) O
and O
false O
negative O
( O
h O
/ O
DET O
) O
while O
the O
third O
gold O
segment O
is O
also O
counted O
as O
a O
false O
negative O
( O
bit O
/ O
NOUN O
) O
. O

On O
the O
other O
hand O
with O
aligned O
multi O
- O
set O
both O
b O
/ O
IN O
andbit O
/ O
NOUN O
exist O
in O
the O
gold O
and O
predicted O
sets O
and O
counted O
as O
true O
positives O
, O
while O
h O
/ O
DET O
is O
mismatched O
and O
counted O
as O
a O
false O
negative O
. O

In O
both O
cased O
the O
total O
counts O
across O
words O
in O
the O
entire O
datasets O
are O
incremented O
accordingly O
and O
finally O
used O
for O
computing O
Precision B-MetricName
, O
Recall B-MetricName
and O
F1 B-MetricName
. O

6 O
Results O
Sentence O
- O
Level O
Task O
Sentiment O
analysis O
accuracy O
results O
are O
provided O
in O
Table O
4 O
. O

All O
BERTbased O
models O
substantially O
outperform O
the O
original O
CNN B-MethodName
Baseline O
reported O
by O
Amram O
et O
al O
. O

( O
2018 O
) O
. O

AlephBERT B-MethodName
baseis O
setting O
a O
new O
SOTA O
. O

Word O
- O
Based O
Task O
On O
our O
two O
NER B-TaskName
benchmarks O
, O
we O
report O
F1 B-MetricName
scores I-MetricName
on O
the O
word O
- O
based O
fine O
- O
tuned O
model O
in O
Table O
4 O
. O

While O
we O
see O
noticeable O
improvements O
for O
the O
mBERT B-MethodName
and O
HeBert B-MethodName
variants O
over O
the O
current O
SOTA O
, O
the O
most O
significant O
increase O
is O
achieved O
by O
AlephBERT B-MethodName
base O
, O
setting O
a O
new O
and O
improved O
SOTA O
on O
this O
task O
. O

Morpheme O
- O
Level O
Tasks O
As O
a O
particular O
novelty O
of O
this O
work O
, O
we O
report O
BERT B-MethodName
- O
based O
results O
on O
sub O
- O
Task O
Segment O
POS O
Features O
UAS O
LAS O
Prev O
. O

SOTA O
NA O
90.49 O
85.98 O
75.73 O
69.41 O
mBERT O
97.36 O
93.37 O
89.36 O
80.17 O
74.9 O
HeBERT O
97.97 O
94.61 O
90.93 O
81.86 O
76.54 O
AlephBERT B-MethodName
small O
97.71 O
94.11 O
90.56 O
81.5 O
76.07 O
AlephBERT B-MethodName
base O
98.10 O
94.90 O
91.41 O
82.07 O
76.9 O
Table O
5 O
: O
Morpheme O
- O
Based O
results O
on O
the O
SPMRL O
corpus O
. O

Aligned O
MultiSet O
( O
mset O
) O
F1 O
for O
Segmentation O
, O
POS O
tags O
and O
Morphological O
Features O
- O
previous O
SOTA O
reported O
by O
Seker O
and O
Tsarfaty O
( O
2020 O
) O
( O
POS O
) O
and O
More O
et O
al O
. O

( O
2019 O
) O
( O
features O
) O
. O

Labeled O
and O
Unlabeled O
Accuracy O
Scores O
for O
morphological O
- O
level O
Dependency O
Parsing O
- O
previous O
SOTA O
reported O
by O
More O
et O
al O
. O

( O
2019 O
) O
( O
uninfused O
/ O
realistic O
scenario O
) O
Task O
Segment O
POS O
Features O
Prev O
. O

SOTA O
NA O
94.02 O
NA O
mBERT O
97.70 O
94.76 O
90.98 O
HeBERT O
98.05 O
96.07 O
92.53 O
AlephBERT B-MethodName
small O
97.86 O
95.58 O
92.06 O
AlephBERT B-MethodName
base O
98.20 O
96.20 O
93.05 O
Table O
6 O
: O
Morpheme O
- O
Based O
Aligned O
MultiSet O
( O
mset O
) O
F1 O
results O
on O
the O
UD O
corpus O
. O

Previous O
SOTA O
reported O
by O
Seker O
and O
Tsarfaty O
( O
2020 O
) O
( O
POS O
) O
word O
( O
segment O
- O
level O
) O
information O
. O

Specifically O
, O
we O
evaluate O
word B-TaskName
segmentation I-TaskName
, O
POS B-TaskName
, O
Morphological B-TaskName
Features I-TaskName
, O
NER B-TaskName
and O
dependencies O
compared O
against O
morphologically O
- O
labeled O
test O
sets O
. O

In O
all O
cases O
, O
we O
use O
raw O
space O
- O
delimited O
tokens O
as O
input O
and O
produce O
morphological O
segments O
with O
our O
morphological O
extraction O
model O
. O

Table O
5 O
presents O
evaluation O
results O
for O
the O
SPRML B-DatasetName
dataset O
, O
compared O
against O
the O
previous O
SOTA O
of O
More O
et O
al O
. O

( O
2019 O
) O
. O

For O
segmentation B-TaskName
, O
POS B-TaskName
tagging I-TaskName
, O
and O
morphological B-TaskName
tagging I-TaskName
we O
report O
aligned O
multiset O
F1 B-MetricValue
scores I-MetricValue
. O

BERT O
- O
based O
segmentations O
are O
similar O
, O
all O
scoring O
in O
the O
high O
range O
of O
97 B-MetricValue
- O
98 B-MetricValue
F1 B-MetricName
, O
which O
are O
hard O
to O
improve O
further.10 O
For O
POS B-TaskName
tagging I-TaskName
and O
morphological B-TaskName
features I-TaskName
, O
all O
BERT O
- O
based O
models O
considerably O
outperform O
the O
previous O
SOTA O
. O

For O
syntactic O
dependencies O
we O
report O
labeled O
and O
unlabeled O
accuracy B-MetricName
scores O
of O
the O
trees O
generated O
by O
YAP O
( O
More O
et O
al O
. O
, O
2019 O
) O
on O
our O
predicted O
segmentation O
. O

Here O
we O
see O
impressive O
improvement O
compared O
to O
the O
previous O
SOTA O
of O
a O
joint O
morpho O
- O
syntactic O
framework O
. O

It O
confirms O
that O
morphological O
errors O
early O
in O
the O
pipeline O
negatively O
impact O
downstream O
tasks O
, O
and O
highlight O
the O
importance O
of O
morphologically O
- O
driven O
benchmarks O
10According O
to O
error O
analysis O
, O
most O
of O
these O
errors O
are O
annotation O
errors O
or O
truly O
ambiguous O
cases.52Task O
Segment O
POS O
Features O
Prev O
. O

SOTA O
96.03 O
93.75 O
91.24 O
mBERT O
97.17 O
94.27 O
90.51 O
HeBERT O
97.54 O
95.60 O
92.15 O
AlephBERT B-MethodName
small O
97.31 O
95.13 O
91.65 O
AlephBERT B-MethodName
base O
97.70 O
95.84 O
92.71 O
Table O
7 O
: O
Morpheme O
- O
Based O
Aligned O
( O
CoNLL O
shared O
task O
) O
F1 O
on O
the O
UD O
corpus O
. O

Previous O
SOTA O
reported O
by O
Minh O
Van O
Nguyen O
and O
Nguyen O
( O
2021 O
) O
Architecture O
Pipeline O
Pipeline O
MultiTask O
Segmentation O
( O
Oracle O
) O
( O
Predicted O
) O
Task O
Seg O
NER O
Seg O
NER O
Seg O
NER O
Prev O
. O

SOTA O
100.00 O
79.10 O
95.15 O
69.52 O
97.05 O
77.11 O
mBERT O
100.00 O
77.92 O
97.68 O
72.72 O
97.24 O
72.97 O
HeBERT O
100.00 O
82 O
98.15 O
76.74 O
97.92 O
74.86 O
AlephBERT B-MethodName
small O
100.00 O
79.44 O
97.78 O
73.08 O
97.74 O
72.46 O
AlephBERT B-MethodName
base O
100.00 O
83.94 O
98.29 O
80.15 O
98.19 O
79.15 O
Table O
8 O
: O
Morpheme O
- O
Based O
NER O
F1 O
on O
the O
NEMO O
corpus O
. O

Previous O
SOTA O
reported O
by O
Bareket O
and O
Tsarfaty O
( O
2020 O
) O
for O
the O
Pipeline O
( O
Oracle O
) O
, O
Pipeline O
( O
Predicted O
) O
and O
a O
Hybrid O
( O
almost O
- O
joint O
) O
scenarios O
, O
respectively O
. O

as O
an O
integral O
part O
of O
PLM O
evaluation O
for O
MRLs O
. O

All O
in O
all O
we O
see O
a O
repeating O
trend O
placing O
AlephBERT B-MethodName
basefirst O
on O
all O
morphological O
tasks O
, O

indicating O
the O
depth O
of O
the O
model O
and O
a O
larger O
pretraining O
dataset O
improve O
the O
ability O
of O
the O
PLM O
to O
capture O
word O
- O
internal O
structure O
. O

These O
trends O
are O
replicated O
on O
the O
UD O
Hebrew O
corpus O
, O
for O
two O
different O
evaluation O
metrics O
— O
the O
Aligned B-MetricName
MultiSet I-MetricName
F1 I-MetricName
Scores I-MetricName
as O
in O
previous O
work O
on O
Hebrew O
( O
More O
et O
al O
. O
, O
2019 O
) O
, O
( O
Seker O
and O
Tsarfaty O
, O
2020 O
) O
, O
and O
the O
Aligned B-MetricName
Segment I-MetricName
F1 I-MetricName
scores I-MetricName
metrics O
as O
described O
in O
the O
UD O
shared O
task O
( O
Zeman O
et O
al O
. O
, O
2018 O
) O
— O
reported O
in O
Tables O
6 O
and O
7 O
respectively O
. O

Morpheme O
- O
Level O
NER B-TaskName
results O
Earlier O
in O
this O
section O
we O
considered O
NER B-TaskName
a O
word O
- O
level O
task O
that O
simply O
requires O
fine O
- O
tuning O
on O
the O
word O
level O
. O

However O
, O
this O
setup O
is O
not O
accurate O
enough O
and O
less O
useful O
for O
downstream O
tasks O
, O
since O
the O
exact O
entity O
boundaries O
are O
often O
word O
internal O
( O
Bareket O
and O
Tsarfaty O
, O
2020 O
) O
. O

We O
hence O
report O
morpheme O
- O
based O
NER B-TaskName
evaluation O
, O
respecting O
the O
exact O
boundaries O
of O
entity O
mentions O
. O

To O
obtain O
morpheme O
- O
based O
labeled O
- O
span O
of O
Named O
Entities O
, O
we O
could O
either O
employ O
a O
pipeline O
, O
first O
predicting O
segmentation O
and O
then O
applying O
a O
fine O
- O
tuned O
labeling O
model O
directly O
on O
the O
segments O
, O
or O
employ O
a O
multi O
- O
task O
model O
and O
predict O
NER B-TaskName
labels O
while O
performing O
segmentation O
. O

Table O
8 O
presents O
segmentation O
and O
NER B-TaskName
results O
for O
3 O
different O
scenarios O
: O
( O
i O
) O
a O
pipeline O
as O
- O
suming O
gold O
segmentation O
( O
ii O
) O
a O
pipeline O
assuming O
predicted O
segmentation O
( O
iii O
) O
segmentation O
and O
NER B-TaskName
labels O
obtained O
jointly O
in O
a O
multi O
- O
task O
setup O
. O

AlephBERT B-MethodName
baseconsistently O
scores O
highest O
in O
all O
3 O
. O

Looking O
at O
the O
Pipeline O
- O
Predicted O
scores O
, O
there O
is O
a O
clear O
correlation O
between O
a O
higher O
segmentation O
quality O
of O
a O
PLM O
and O
its O
ability O
to O
produce O
better O
NER O
results O
. O

Moreover O
, O
the O
differences O
in O
NER O
scores O
are O
considerable O
( O
unlike O
the O
subtle O
differences O
in O
segmentation O
, O
POS O
and O
morphological O
features O
scores O
) O
and O
draw O
our O
attention O
to O
the O
relationship O
between O
the O
size O
of O
the O
PLM O
, O
the O
size O
of O
the O
pre O
- O
training O
data O
and O
the O
quality O
of O
the O
final O
NER O
models O
. O

Specifically O
, O
HeBERT B-MethodName
and O
AlephBERT B-MethodName
smallwere O
both O
pre O
- O
trained O
on O
similar O
datasets O
and O
comparable O
vocabulary O
sizes O
( O
heBERT B-MethodName
with O
30 O
K O
and O
AlephBERT B-MethodName
- O
small O
with O
52 O
K O
) O
but O
HeBERT O
, O
with O
its O
12 B-HyperparameterValue
hidden B-HyperparameterName
layers I-HyperparameterName
, O
performs O
better O
compared O
to O
AlephBERT B-MethodName
smallwhich O
is O
composed O
of O
only O
6 O
hidden O
layers O
. O

It O
thus O
appears O
that O
semantic O
information O
is O
learned O
in O
those O
deeper O
layers O
, O
helping O
in O
both O
discriminating O
entities O
and O
improving O
the O
morphological O
segmentation O
capacity O
. O

In O
addition O
, O
comparing O
AlephBERT B-MethodName
base O
and O
HeBERT B-MethodName
we O
note O
that O
they O
are O
both O
modeled O
with O
the O
same O
12 B-HyperparameterValue
hidden B-HyperparameterName
layer I-HyperparameterName
architecture O
— O
the O
only O
differences O
between O
them O
are O
in O
the O
size O
of O
their O
vocabularies O
( O
30 O
K O
vs O
52 O
K O
respectively O
) O
and O
the O
size O
of O
the O
training O
data O
( O
Oscar B-DatasetName
- O
Wikipedia B-DatasetName
vs O
OscarWikipedia B-DatasetName
- O
Tweets B-DatasetName
) O
. O

The O
improvements O
exhibited O
by O
AlephBERT B-MethodName
base O
, O
compared O
to O
HeBERT B-MethodName
, O
suggest O
large O
amounts O
of O
training O
data O
and O
larger O
vocabulary O
are O
invaluable O
. O

By O
exposing O
AlephBERT B-MethodName
baseto O
a O
substantially O
larger O
amount O
of O
text O
we O
increased O
the O
ability O
of O
the O
PLM O
to O
encode O
syntactic O
and O
semantic O
signals O
associated O
with O
Named O
Entities O
. O

Our O
NER O
experiments O
further O
suggest O
that O
a O
pipeline O
composed O
of O
our O
accurate O
morphological O
segmentation O
model O
followed O
by O
AlephBERT B-MethodName
base O
with O
a O
token O
classification O
head O
is O
the O
best O
strategy O
for O
generating O
morphologically O
- O
aware O
NER O
labels O
. O

Finally O
, O
we O
observe O
that O
while O
AlephBERT B-MethodName
excels O
at O
morphosyntactic O
tasks O
, O
on O
tasks O
with O
a O
more O
semantic O
flavor O
there O
is O
room O
for O
improvement O
. O

7 O
Conclusion O
Modern O
Hebrew O
, O
a O
morphologically O
- O
rich O
and O
medium O
- O
resourced O
language O
, O
has O
for O
long O
suffered O
from O
a O
gap O
in O
the O
resources O
available O
for O
NLP O
applications O
, O
and O
lower O
level O
of O
empirical O
results O
than O
observed O
in O
other O
, O
resource O
- O
rich O
languages O
. O

This53work O
provides O
the O
first O
step O
in O
remedying O
the O
situation O
, O
by O
making O
available O
a O
large O
Hebrew O
PLM O
, O
named O
AlephBERT B-MethodName
, O
with O
larger O
vocabulary O
and O
larger O
training O
set O
than O
any O
Hebrew O
PLM O
before O
, O
and O
with O
clear O
evidence O
as O
to O
its O
empirical O
advantages O
. O

Crucially O
, O
we O
augment O
the O
PLM O
with O
a O
morphological O
disambiguation O
component O
that O
matches O
the O
input O
granularity O
of O
the O
downstream O
tasks O
. O

Our O
system O
does O
not O
presuppose O
Hebrewspecific O
linguistic O
- O
rules O
, O
and O
can O
be O
transparently O
applied O
to O
any O
language O
for O
which O
2 O
- O
level O
segmentation O
data O
( O
i.e. O
, O
the O
standard O
UD O
benchmarks O
) O
exists O
. O

AlephBERT B-MethodName
baseobtains O
state O
- O
of O
- O
the O
- O
art O
results O
on O
morphological B-TaskName
segmentation I-TaskName
, O
POS B-TaskName
tagging I-TaskName
, O
morphological B-TaskName
feature I-TaskName
extraction I-TaskName
, O
dependency B-TaskName
parsing I-TaskName
, O
named B-TaskName
- I-TaskName
entity I-TaskName
recognition I-TaskName
, O
and O
sentiment B-TaskName
analysis I-TaskName
, O
outperforming O
all O
existing O
Hebrew O
PLMs O
. O

Our O
proposed O
morphologically O
- O
driven O
pipeline11serves O
as O
a O
solid O
foundation O
for O
future O
evaluation O
of O
Hebrew O
PLMs O
and O
of O
MRLs O
in O
general O
. O

8 O
Ethical O
Statement O
We O
follow O
Bender O
and O
Friedman O
( O
2018 O
) O
regarding O
professional O
practice O
for O
NLP O
technology O
and O
address O
ethical O
issues O
that O
result O
from O
the O
use O
of O
data O
in O
the O
development O
of O
the O
models O
in O
our O
work O
. O

Pre O
- O
Training O
Data O
. O

The O
two O
initial O
data O
sources O
we O
used O
to O
pre O
- O
train O
the O
language O
models O
are O
Oscar B-DatasetName
and O
Wikipedia B-DatasetName
. O

In O
using O
the O
Wikipedia B-DatasetName
and O
Oscar B-DatasetName
we O
followed O
standard O
language O
model O
training O
efforts O
, O
such O
as O
BERT B-MethodName
and O
RoBERTa B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
) O
. O

We O
use O
the O
languagespecific O
Oscar O
data O
according O
to O
the O
terms O
specified O
in O
Ortiz O
Suárez O
et O
al O
. O

( O
2020 O
) O

and O
we O
extract O
texts O
from O
language O
- O
specific O
Wikipedia O
dumps O
. O

On O
top O
of O
that O
, O
a O
big O
portion O
of O
the O
data O
used O
to O
train O
AlephBERT B-MethodName
originates O
from O
the O
Twitter B-DatasetName
sample O
stream.12 O
As O
shown O
in O
Table O
2 O
, O
this O
data O
set O
includes O
70 O
M O
Hebrew O
tweets O
which O
were O
collected O
over O
a O
period O
of O
4 O
years O
( O
2014 O
to O
2018 O
) O
. O

We O
acknowledge O
the O
potential O
concerns O
inherently O
associated O
with O
Twitter O
data O
( O
population O
bias O
, O
behavior O
patterns O
, O
bot O
masquerading O
as O
humans O
etc O
. O
) O
and O
note O
that O
we O
have O
not O
made O
any O
explicit O
attempt O
to O
identify O
these O
cases O
. O

We O
only O
used O
the O
text O
field O
of O
the O
tweets O
and O
completely O
discard O
any O
other O
information O
included O
11Available O
at O
https://github.com/OnlpLab/ O
AlephBERT B-MethodName
12https://developer.twitter.com/en/ O
docs O
/ O
twitter O
- O
api O
/ O
tweets O
/ O
volume O
- O
streams/ O
api O
- O
reference O
/ O
get O
- O
tweets O
- O
sample O
- O
streamin O
the O
stream O
( O
such O
as O
identities O
, O
followers O
, O
structure O
of O
threads O
, O
date O
of O
publication O
, O
etc O
) O
. O

We O
have O
not O
made O
any O
effort O
to O
identify O
or O
filter O
out O
any O
samples O
based O
on O
user O
properties O
such O
as O
age O
, O
gender O
and O
location O
nor O
have O
we O
made O
any O
effort O
to O
identify O
content O
characteristics O
such O
as O
genre O
or O
topic O
. O

To O
reduce O
exposure O
of O
private O
information O
we O
cleaned O
up O
all O
user O
mentions O
and O
URLs O
from O
the O
text O
. O

Honoring O
ethical O
and O
legal O
constraints O
we O
have O
not O
manually O
analyzed O
nor O
published O
this O
data O
source O
. O

While O
the O
free O
- O
form O
language O
expressed O
in O
tweets O
might O
differ O
significantly O
from O
the O
text O
found O
in O
Oscar O
/ O
Wikipedia O
, O
the O
sheer O
volume O
of O
tweets O
helps O
us O
close O
the O
substantial O
resource O
gap O
. O

Training O
and O
Evaluation O
Benchmarks O
. O

The O
SPMRL O
( O
Seddah O
et O
al O
. O
, O
2013 O
) O
and O
UD O
( O
Sadde O
et O
al O
. O
, O
2018 O
) O
datasets O
we O
used O
for O
evaluating O
segmentation O
, O
tagging O
and O
parsing O
, O
were O
used O
to O
both O
train O
our O
morphological O
extraction O
model O
as O
well O
as O
provide O
us O
with O
the O
test O
data O
to O
evaluate O
on O
morphological O
level O
tasks O
. O

Both O
datasets O
are O
publicly O
available O
and O
widely O
used O
in O
research O
and O
industry O
. O

The O
NEMO B-DatasetName
corpus O
( O
Bareket O
and O
Tsarfaty O
, O
2020 O
) O
used O
to O
train O
and O
evaluate O
word O
and O
morpheme O
level O
NER B-TaskName
is O
an O
extension O
of O
the O
SPMRL B-DatasetName
dataset O
augmented O
with O
entities O
and O
follows O
the O
same O
license O
terms O
. O

The O
BMC B-DatasetName
dataset O
used O
for O
training O
and O
evaluating O
word O
- O
level O
NER B-TaskName
was O
created O
and O
published O
by O
Ben O
Mordecai O
and O
Elhadad O
( O
2005 O
) O
and O
it O
is O
publicly O
available O
for O
NER O
evaluation O
. O

We O
used O
the O
sentiment O
analysis O
dataset O
of O
Amram O
et O
al O
. O

( O
2018 O
) O
for O
training O
and O
evaluating O
AlephBERT B-MethodName
on O
a O
sentence O
level O
task O
, O
and O
we O
follow O
their O
terms O
of O
use O
. O

As O
mentioned O
, O
this O
dataset O
had O
some O
flows O
, O
and O
we O
describe O
carefully O
the O
steps O
we O
’ve O
taken O
to O
fix O
them O
before O
using O
this O
corpus O
in O
our O
experiments O
for O
internal O
evaluation O
purposes O
. O

We O
make O
our O
in O
- O
house O
cleaning O
scripts O
and O
split O
information O
publicly O
available O
. O

Acknowledgements O
This O
research O
was O
funded O
by O
the O
European O
Research O
Council O
( O
ERC O
grant O
agreement O
no O
. O
677352 O
) O
and O
by O
a O
research O
grant O
from O
the O
Ministry O
of O
Science O
and O
Technology O
( O
MOST O
) O
of O
the O
Israeli O
Government O
, O
for O
which O
we O
are O
grateful O
. O

References O
Adam O
Amram O
, O
Anat O
Ben O
- O
David O
, O
and O
Reut O
Tsarfaty O
. O
2018 O
. O

Representations O
and O
architectures O
in O
neu-54ral O
sentiment O
analysis O
for O
morphologically O
rich O
languages O
: O
A O
case O
study O
from O
modern O
hebrew O
. O

In O
Proceedings O
of O
the O
27th O
International O
Conference O
on O
Computational O
Linguistics O
, O
COLING O
2018 O
, O
Santa O
Fe O
, O
New O
Mexico O
, O
USA O
, O
August O
20 O
- O
26 O
, O
2018 O
, O
pages O
2242 O
– O
2252 O
. O

Wissam O
Antoun O
, O
Fady O
Baly O
, O
and O
Hazem O
Hajj O
. O

2020 O
. O

AraBERT O
: O
Transformer O
- O
based O
model O
for O
Arabic O
language O
understanding O
. O

In O
Proceedings O
of O
the O
4th O
Workshop O
on O
Open O
- O
Source O
Arabic O
Corpora O
and O
Processing O
Tools O
, O
with O
a O
Shared O
Task O
on O
Offensive O
Language O
Detection O
, O
pages O
9–15 O
, O
Marseille O
, O
France O
. O

European O
Language O
Resource O
Association O
. O

Giusepppe O
Attardi O
. O

2015 O
. O

Wikiextractor O
. O

https:// O
github.com/attardi/wikiextractor O
. O

Dan O
Bareket O
and O
Reut O
Tsarfaty O
. O

2020 O
. O

Neural O
modeling O
for O
named O
entities O
and O
morphology O
( O
nemoˆ2 O
) O
. O

CoRR O
, O
abs/2007.15620 O
. O

Naama O
Ben O
Mordecai O
and O
Michael O
Elhadad O
. O

2005 O
. O

Hebrew O
named O
entity O
recognition O
. O

Emily O
M. O
Bender O
and O
Batya O
Friedman O
. O

2018 O
. O

Data O
statements O
for O
natural O
language O
processing O
: O
Toward O
mitigating O
system O
bias O
and O
enabling O
better O
science O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
6:587–604 O
. O

Tom O
Brown O
, O
Benjamin O
Mann O
, O
Nick O
Ryder O
, O
Melanie O
Subbiah O
, O
Jared O
D O
Kaplan O
, O
Prafulla O
Dhariwal O
, O
Arvind O
Neelakantan O
, O
Pranav O
Shyam O
, O
Girish O
Sastry O
, O
Amanda O
Askell O
, O
Sandhini O
Agarwal O
, O
Ariel O
Herbert O
- O
V O
oss O
, O
Gretchen O
Krueger O
, O
Tom O
Henighan O
, O
Rewon O
Child O
, O
Aditya O
Ramesh O
, O
Daniel O
Ziegler O
, O
Jeffrey O
Wu O
, O
Clemens O
Winter O
, O
Chris O
Hesse O
, O
Mark O
Chen O
, O
Eric O
Sigler O
, O
Mateusz O
Litwin O
, O
Scott O
Gray O
, O
Benjamin O
Chess O
, O
Jack O
Clark O
, O
Christopher O
Berner O
, O
Sam O
McCandlish O
, O
Alec O
Radford O
, O
Ilya O
Sutskever O
, O
and O
Dario O
Amodei O
. O

2020 O
. O

Language O
models O
are O
few O
- O
shot O
learners O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
volume O
33 O
, O
pages O
1877–1901 O
. O

Curran O
Associates O
, O
Inc. O
Avihay O
Chriqui O
and O
Inbal O
Yahav O
. O
2021 O
. O

Hebert O
| O
& O
hebemo O
: O
a O
hebrew O
bert O
model O
and O
a O
tool O
for O
polarity O
analysis O
and O
emotion O
recognition O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT B-MethodName
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
, O
Minneapolis O
, O
Minnesota O
. O

Association O
for O
Computational O
Linguistics O
. O

Mehrdad O
Farahani O
, O
Mohammad O
Gharachorloo O
, O
Marzieh O
Farahani O
, O
and O
Mohammad O
Manthouri O
. O
2020 O
. O

Parsbert O
: O
Transformer O
- O
based O
model O
for O
persian O
language O
understanding O
. O

Jeremy O
Howard O
and O
Sebastian O
Ruder O
. O

2018 O
. O

Universal O
language O
model O
fine O
- O
tuning O
for O
text O
classification O
. O

InProceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
328–339 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O

Stav O
Klein O
and O
Reut O
Tsarfaty O
. O

2020 O
. O

Getting O
the O
# O
# O
life O
out O
of O
living O
: O
How O
adequate O
are O
word O
- O
pieces O
for O
modelling O
complex O
morphology O
? O

In O
Proceedings O
of O
the O
17th O
SIGMORPHON O
Workshop O
on O
Computational O
Research O
in O
Phonetics O
, O
Phonology O
, O
and O
Morphology O
, O
SIGMORPHON O
2020 O
, O
Online O
, O
July O
10 O
, O
2020 O
, O
pages O
204–209 O
. O

Yinhan O
Liu O
, O
Myle O
Ott O
, O
Naman O
Goyal O
, O
Jingfei O
Du O
, O
Mandar O
Joshi O
, O
Danqi O
Chen O
, O
Omer O
Levy O
, O
Mike O
Lewis O
, O
Luke O
Zettlemoyer O
, O
and O
Veselin O
Stoyanov O
. O

2019 O
. O
RoBERTa O
: O
A O
Robustly O
Optimized O
BERT O
Pretraining O
Approach O
. O

Amir O
Pouran O
Ben O
Veyseh O
Minh O
Van O
Nguyen O
, O
Viet O
Lai O
and O
Thien O
Huu O
Nguyen O
. O
2021 O
. O

Trankit O
: O

A O
lightweight O
transformer O
- O
based O
toolkit O
for O
multilingual O
natural O
language O
processing O
. O

In O
Proceedings O
of O
the O
16th O
Conference O
of O
the O
European O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
System O
Demonstrations O
. O

Amir O
More O
, O
Amit O
Seker O
, O
Victoria O
Basmova O
, O
and O
Reut O
Tsarfaty O
. O

2019 O
. O

Joint O
transition O
- O
based O
models O
for O
morpho O
- O
syntactic O
parsing O
: O
Parsing O
strategies O
for O
mrls O
and O
a O
case O
study O
from O
modern O
hebrew O
. O

Trans O
. O

Assoc O
. O

Comput O
. O

Linguistics O
, O
7:33–48 O
. O

Pedro O
Javier O
Ortiz O
Suárez O
, O
Laurent O
Romary O
, O
and O
Benoît O
Sagot O
. O

2020 O
. O

A O
monolingual O
approach O
to O
contextualized O
word O
embeddings O
for O
mid O
- O
resource O
languages O
. O

InProceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
1703 O
– O
1714 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Matthew O
E. O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O

2018 O
. O

Deep O
contextualized O
word O
representations O
. O

In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
Papers O
) O
, O
pages O
2227–2237 O
, O
New O
Orleans O
, O
Louisiana O
. O
Association O
for O
Computational O
Linguistics O
. O

Marco O
Polignano O
, O
Pierpaolo O
Basile O
, O
Marco O
de O
Gemmis O
, O
Giovanni O
Semeraro O
, O
and O
Valerio O
Basile O
. O

2019 O
. O

Alberto O
: O
Italian O
bert O
language O
understanding O
model O
for O
nlp O
challenging O
tasks O
based O
on O
tweets O
. O

Alec O
Radford O
and O
Ilya O
Sutskever O
. O

2018 O
. O

Improving O
language O
understanding O
by O
generative O
pre O
- O
training O
. O
Inarxiv O
. O

Colin O
Raffel O
, O
Noam O
Shazeer O
, O
Adam O
Roberts O
, O
Katherine O
Lee O
, O
Sharan O
Narang O
, O
Michael O
Matena O
, O
Yanqi O
Zhou O
, O
Wei O
Li O
, O
and O
Peter O
J. O
Liu O
. O
2020 O
. O

Exploring O
the55limits O
of O
transfer O
learning O
with O
a O
unified O
text O
- O
to O
- O
text O
transformer O
. O

Journal O
of O
Machine O
Learning O
Research O
, O
21(140):1–67 O
. O

Pranav O
Rajpurkar O
, O
Jian O
Zhang O
, O
Konstantin O
Lopyrev O
, O
and O
Percy O
Liang O
. O

2016 O
. O

SQuAD O
: O
100,000 O
+ O
questions O
for O
machine O
comprehension O
of O
text O
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2383–2392 O
, O
Austin O
, O
Texas O
. O
Association O
for O
Computational O
Linguistics O
. O
Piotr O
Rybak O
, O
Robert O
Mroczkowski O
, O
Janusz O
Tracz O
, O
and O
Ireneusz O
Gawlik O
. O

2020 O
. O

KLEJ O
: O
Comprehensive O
benchmark O
for O
Polish O
language O
understanding O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
1191 O
– O
1201 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Shoval O
Sadde O
, O
Amit O
Seker O
, O
and O
Reut O
Tsarfaty O
. O
2018 O
. O

The O
hebrew O
universal O
dependency O
treebank O
: O

Past O
present O
and O
future O
. O

In O
Proceedings O
of O
the O
Second O
Workshop O
on O
Universal O
Dependencies O
, O
UDW@EMNLP O
2018 O
, O
Brussels O
, O
Belgium O
, O
November O
1 O
, O
2018 O
, O
pages O
133–143 O
. O

Gözde O
Gül O
¸ O
Sahin O
, O
Clara O
Vania O
, O
Ilia O
Kuznetsov O
, O
and O
Iryna O
Gurevych O
. O

2019 O
. O

LINSPECTOR O
: O
multilingual O
probing O
tasks O
for O
word O
representations O
. O

CoRR O
, O
abs/1903.09442 O
. O
Djamé O
Seddah O
, O
Reut O
Tsarfaty O
, O
Sandra O
Kübler O
, O
Marie O
Candito O
, O
Jinho O
D. O
Choi O
, O
Richárd O
Farkas O
, O
Jennifer O
Foster O
, O
Iakes O
Goenaga O
, O
Koldo O
Gojenola O
Galletebeitia O
, O
Yoav O
Goldberg O
, O
Spence O
Green O
, O
Nizar O
Habash O
, O
Marco O
Kuhlmann O
, O
Wolfgang O
Maier O
, O
Joakim O
Nivre O
, O
Adam O
Przepiórkowski O
, O
Ryan O
Roth O
, O
Wolfgang O
Seeker O
, O
Yannick O
Versley O
, O
Veronika O
Vincze O
, O
Marcin O
Wolin O

ski O
, O
Alina O
Wróblewska O
, O
and O
Éric O
Villemonte O
de O
la O
Clergerie O
. O

2013 O
. O

Overview O
of O
the O
SPMRL O
2013 O
shared O
task O
: O
A O
cross O
- O
framework O
evaluation O
of O
parsing O
morphologically O
rich O
languages O
. O

In O
Proceedings O
of O
the O
Fourth O
Workshop O
on O
Statistical O
Parsing O
of O
Morphologically O
- O
Rich O
Languages O
, O
SPMRL@EMNLP O
2013 O
, O
Seattle O
, O
Washington O
, O
USA O
, O
October O
18 O
, O
2013 O
, O
pages O
146–182 O
. O
Amit O
Seker O
and O
Reut O
Tsarfaty O
. O

2020 O
. O

A O
pointer O
network O
architecture O
for O
joint O
morphological O
segmentation O
and O
tagging O
. O

In O
Findings O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
EMNLP O
2020 O
, O
pages O
4368–4378 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Rico O
Sennrich O
, O
Barry O
Haddow O
, O
and O
Alexandra O
Birch O
. O
2016 O
. O

Neural O
machine O
translation O
of O
rare O
words O
with O
subword O
units O
. O

In O
Proceedings O
of O
the O
54th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1715–1725 O
, O
Berlin O
, O
Germany O
. O

Association O
for O
Computational O
Linguistics O
. O

Reut O
Tsarfaty O
, O
Dan O
Bareket O
, O
Stav O
Klein O
, O
and O
Amit O
Seker O
. O
2020 O
. O

From O
SPMRL O
to O
NMRL O
: O
what O
did O
we O
learn(and O
unlearn O
) O
in O
a O
decade O
of O
parsing O
morphologicallyrich O
languages O
( O
mrls O
) O
? O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
ACL O
2020 O
, O
Online O
, O
July O
5 O
- O
10 O
, O
2020 O
, O
pages O
7396–7408 O
. O
Antti O
Virtanen O
, O
Jenna O
Kanerva O
, O
Rami O
Ilo O
, O
Jouni O
Luoma O
, O
Juhani O
Luotolahti O
, O
Tapio O
Salakoski O
, O
Filip O
Ginter O
, O
and O
Sampo O
Pyysalo O
. O

2019 O
. O

Multilingual O
is O
not O
enough O
: O
Bert O
for O
finnish O
. O

Alex O
Wang O
, O
Amanpreet O
Singh O
, O
Julian O
Michael O
, O
Felix O
Hill O
, O
Omer O
Levy O
, O
and O
Samuel O
Bowman O
. O

2018 O
. O

GLUE O
: O
A O
multi O
- O
task O
benchmark O
and O
analysis O
platform O
for O
natural O
language O
understanding O
. O

In O
Proceedings O
of O
the O
2018 O
EMNLP O
Workshop O
BlackboxNLP O
: O
Analyzing O
and O
Interpreting O
Neural O
Networks O
for O
NLP O
, O
pages O
353–355 O
, O
Brussels O
, O
Belgium O
. O

Association O
for O
Computational O
Linguistics O
. O

Thomas O
Wolf O
, O
Lysandre O
Debut O
, O
Victor O
Sanh O
, O
Julien O
Chaumond O
, O
Clement O
Delangue O
, O
Anthony O
Moi O
, O
Pierric O
Cistac O
, O
Tim O
Rault O
, O
Rémi O
Louf O
, O
Morgan O
Funtowicz O
, O
Joe O
Davison O
, O
Sam O
Shleifer O
, O
Patrick O
von O
Platen O
, O
Clara O
Ma O
, O
Yacine O
Jernite O
, O
Julien O
Plu O
, O
Canwen O
Xu O
, O
Teven O
Le O
Scao O
, O
Sylvain O
Gugger O
, O
Mariama O
Drame O
, O
Quentin O
Lhoest O
, O
and O
Alexander O
M. O
Rush O
. O

2020 O
. O

Transformers O
: O
State O
- O
of O
- O
the O
- O
art O
natural O
language O
processing O
. O

In O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
: O
System O
Demonstrations O
, O
pages O
38–45 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Rowan O
Zellers O
, O
Yonatan O
Bisk O
, O
Roy O
Schwartz O
, O
and O
Yejin O
Choi O
. O

2018 O
. O

SWAG O
: O

A O
large O
- O
scale O
adversarial O
dataset O
for O
grounded O
commonsense O
inference O
. O

In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
93–104 O
, O
Brussels O
, O
Belgium O
. O

Association O
for O
Computational O
Linguistics O
. O

Daniel O
Zeman O
, O
Jan O
Haji O
ˇc O
, O
Martin O
Popel O
, O
Martin O
Potthast O
, O
Milan O
Straka O
, O
Filip O
Ginter O
, O
Joakim O
Nivre O
, O
and O
Slav O
Petrov O
. O

2018 O
. O

CoNLL O
2018 O
shared O
task O
: O
Multilingual O
parsing O
from O
raw O
text O
to O
Universal O
Dependencies O
. O

In O
Proceedings O
of O
the O
CoNLL O
2018 O
Shared O
Task O
: O
Multilingual O
Parsing O
from O
Raw O
Text O
to O
Universal O
Dependencies O
, O
pages O
1–21 O
, O
Brussels O
, O
Belgium O
. O

Association O
for O
Computational O
Linguistics.56 O

