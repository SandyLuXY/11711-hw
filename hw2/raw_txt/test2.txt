Proceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
5882–5893 O
August O
1–6 O
, O
2021 O
. O

© O
2021 O
Association O
for O
Computational O
Linguistics5882SMedBERT B-MethodName
: O
A O
Knowledge O
- O
Enhanced O
Pre O
- O
trained O
Language O
Model O
with O
Structured O
Semantics O
for O
Medical O
Text O
Mining O
Taolin O
Zhang1;2;3 O
, O
Zerui O
Cai4 O
, O
Chengyu O
Wang2 O
, O
Minghui O
Qiu2 O
Bite O
Yang5,Xiaofeng O
He4;6 O
1School O
of O
Software O
Engineering O
, O
East O
China O
Normal O
University2Alibaba O
Group O
3Shanghai O
Key O
Laboratory O
of O
Trsustworthy O
Computing O
4School O
of O
Computer O
Science O
and O
Technology O
, O
East O
China O
Normal O
University5DXY O
6Shanghai O
Research O
Institute O
for O
Intelligent O
Autonomous O
Systems O
zhangtl0519@gmail.com O
, O
zrcai O
flow@126.com O
, O
yangbt@dxy.cn O
fchengyu.wcy O
, O
minghui.qmh O
g@alibaba-inc.com O
, O
hexf@cs.ecnu.edu.cn O
Abstract O

Recently O
, O
the O
performance O
of O
Pre O
- O
trained O
Language O
Models O
( O
PLMs O
) O
has O
been O
signiﬁcantly O
improved O
by O
injecting O
knowledge O
facts O
to O
enhance O
their O
abilities O
of O
language O
understanding O
. O

For O
medical O
domains O
, O
the O
background O
knowledge O
sources O
are O
especially O
useful O
, O
due O
to O
the O
massive O
medical O
terms O
and O
their O
complicated O
relations O
are O
difﬁcult O
to O
understand O
in O
text O
. O

In O
this O
work O
, O
we O
introduce O
SMedBERT B-MethodName
, O
a O
medical O
PLM O
trained O
on O
large O
- O
scale O
medical O
corpora O
, O
incorporating O
deep O
structured O
semantics O
knowledge O
from O
neighbours O
of O
linked O
- O
entity O
. O

In O
SMedBERT B-MethodName
, O
the O
mention O
- O
neighbour O
hybrid O
attention O
is O
proposed O
to O
learn O
heterogeneousentity O
information O
, O
which O
infuses O
the O
semantic O
representations O
of O
entity O
types O
into O
the O
homogeneous O
neighbouring O
entity O
structure O
. O

Apart O
from O
knowledge O
integration O
as O
external O
features O
, O
we O
propose O
to O
employ O
the O
neighbors O
of O
linked O
- O
entities O
in O
the O
knowledge O
graph O
as O
additional O
global O
contexts O
of O
text O
mentions O
, O
allowing O
them O
to O
communicate O
via O
shared O
neighbors O
, O
thus O
enrich O
their O
semantic O
representations O
. O

Experiments O
demonstrate O
that O
SMedBERT B-MethodName
significantly O
outperforms O
strong O
baselines O
in O
various O
knowledge O
- O
intensive O
Chinese O
medical O
tasks O
. O

It O
also O
improves O
the O
performance O
of O
other O
tasks O
such O
as O
question B-TaskName
answering I-TaskName
, O
question B-TaskName
matching I-TaskName
and O
natural B-TaskName
language I-TaskName
inference.1 I-TaskName
1 O
Introduction O
Pre O
- O
trained O
Language O
Models O
( O
PLMs O
) O
learn O
effective O
context O
representations O
with O
self O
- O
supervised O
tasks O
, O
spotlighting O
in O
various O
NLP O
tasks O
( O
Wang O
et O
al O
. O
, O
2019a O
; O

Nan O
et O
al O
. O
, O
2020 O
; O
Liu O
et O
al O
. O
, O
2020a O
) O
. O

In O
addition O
, O
Knowledge O
- O
Enhanced O
PLMs O
( O
KEPLMs O
) O
( O
Zhang O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2020b O
; O
Wang O
et O
al O
. O
, O
2019b O
) O
further O
beneﬁt O
language O
understanding O
by O
Corresponding O
author O
. O

1The O
code O
and O
pre O
- O
trained O
models O
will O
be O
available O
at O
https://github.com/MatNLP/SMedBERT B-MethodName
. O

ఉᆆ O
⯲ O
( O
sore O
throat O
) O

᯦ශߖ O
⣬ O
⯻ O
∈ O
  O
( O
novel O
coronavirus O
) O
  O

Symptom O
Cause O
of O
disease O
લ੮᝕ḉ O
  O
( O
respiratory O
  O
infection O
) O
  O

Disease O
લ੮㔲ਾᖷ O
  O
( O
respiratory O
  O
syndrome O
) O
  O
Symptom O
- O
Disease O
  O
Cause O
- O
Disease O
  O
Entity O
Type O
Relation O
COVID-19 O
,QSXW7H[W O
   O
䓡։ O
ਇ O
✣ O
θఉᆆ O
⯲ O
θ㞯 O
⌱ O
ᱥ᝕ḉ O
᯦ශߖ O
⣬ O
⯻ O
∈ O
( O
COVID-19 O
) O
Ⲻ O
⯽ O
⣬ O
Ⱦ O
  O
( O
Fever O
, O
sore O
throat O
, O
and O
diarrhea O
  O
are O
symptoms O
of O
novel O
coronavirus O
  O
( O
COVID-19 O
) O
. O
) O
  O

1HLJKERULQJ(QWLW\IURP';<. O
* O
  O
/LQNHG(QWLW\ O

㛰 O
⛄ O
  O
( O
pneumonia O
) O
᝕ḉ〇 O
( O
infectious O
  O
Department O
) O
Symptom O
- O
Symptom O
Medical O
Department O
ਇ O
✝ O
  O
( O
fever O
) O
  O
Cause O
- O
Department O
Figure O
1 O
: O
Example O
of O
neighboring O
entity O
information O
in O
medical O
text O
. O

( O
Best O
viewed O
in O
color O
) O
grounding O
these O
PLMs O
with O
high O
- O
quality O
, O
humancurated O
knowledge O
facts O
, O
which O
are O
difﬁcult O
to O
learn O
from O
raw O
texts O
. O

In O
the O
literatures O
, O
a O
majority O
of O
KEPLMs O
( O
Zhang O
et O
al O
. O
, O
2020a O
; O
Hayashi O
et O
al O
. O
, O
2020 O
; O
Sun O
et O
al O
. O
, O
2020 O
) O
inject O
information O
of O
entities O
corresponding O
to O
mention O
- O
spans O
from O
Knowledge O
Graphs O
( O
KGs O
) O
into O
contextual O
representations O
. O

However O
, O
those O
KEPLMs O
only O
utilize O
linked O
- O
entity O
in O
the O
KGs O
as O
auxiliary O
information O
, O
which O
pay O
little O
attention O
to O
the O
neighboring O
structured O
semantics O
information O
of O
the O
entity O
linked O
with O
text O
mentions O
. O

In O
the O
medical O
context O
, O
there O
exist O
complicated O
domain O
knowledge O
such O
as O
relations O
and O
medical O
facts O
among O
medical O
terms O
( O
Rotmensch O
et O
al O
. O
, O
2017 O
; O
Li O
et O
al O
. O
, O
2020 O
) O
, O
which O
are O
difﬁcult O
to O
model O
using O
previous O
approaches O
. O

To O
address O
this O
issue O
, O
we O
consider O
leveraging O
structured O
semantics O
knowledge O
in O
medical O
KGs O
from O
the O
two O
aspects O
. O

( O
1 O
) O
Rich O
semantic O
information O
from O
neighboring O
structures O
of O
linked O
- O
entities O
, O
such O
as O
entity O
types O
and O
relations O
, O
are O
highly O
useful O
for O
medical O
text O
understanding O
. O

As O
in O
Figure O
1 O
, O
“ O
° O
 O
  O
¶ÅÒ O
” O
( O
novel O
coronavirus O
) O
can O
be O
the O
cause O
of O
many O
diseases O
, O
such O
as O
“ O
º O
” O
( O
pneumonia O
) O
and O
“ O
|8ü”5883(respiratory O
syndrome).2(2 O
) O
Additionally O
, O
we O
leverage O
neighbors O
of O
linked O
- O
entity O
as O
global O
“ O
contexts O
” O
to O
complement O
plain O
- O
text O
contexts O
used O
in O
( O
Mikolov O
et O
al O
. O
, O
2013a O
; O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

The O
structure O
knowledge O
contained O
in O
neighbouring O
entities O
can O
act O
as O
the O
“ O
knowledge O
bridge O
” O
between O
mention O
- O
spans O
, O
facilitating O
the O
interaction O
of O
different O
mention O
representations O
. O

Hence O
, O
PLMs O
can O
learn O
better O
representations O
for O
rare O
medical O
terms O
. O

In O
this O
paper O
, O
we O
introduce O
SMedBERT B-MethodName
, O
a O
KEPLM O
pre O
- O
trained O
over O
large O
- O
scale O
medical O
corpora O
and O
medical O
KGs O
. O

To O
the O
best O
of O
our O
knowledge O
, O
SMedBERT B-MethodName
is O
the O
ﬁrst O
PLM O
with O
structured O
semantics O
knowledge O
injected O
in O
the O
medical O
domain O
. O

Speciﬁcally O
, O
the O
contributions O
of O
SMedBERT B-MethodName
mainly O
include O
two O
modules O
: O
Mention O
- O
neighbor O
Hybrid O
Attention O
: O
We O
fuse O
the O
embeddings O
of O
the O
node O
and O
type O
of O
linkedentity O
neighbors O
into O
contextual O
target O
mention O
representations O
. O

The O
type O
- O
level O
and O
node O
- O
level O
attentions O
help O
to O
learn O
the O
importance O
of O
entity O
types O
and O
the O
neighbors O
of O
linked O
- O
entity O
, O
respectively O
, O
in O
order O
to O
reduce O
the O
knowledge O
noise O
injected O
into O
the O
model O
. O

The O
type O
- O
level O
attention O
transforms O
the O
homogeneous O
node O
- O
level O
attention O
into O
a O
heterogeneous O
learning O
process O
of O
neighboring O
entities O
. O

Mention O
- O
neighbor O
Context O
Modeling O
: O
We O
propose O
two O
novel O
self O
- O
supervised O
learning O
tasks O
for O
promoting O
interaction O
between O
mention O
- O
span O
and O
corresponding O
global O
context O
, O
namely O
masked O
neighbor O
modeling O
and O
masked O
mention O
modeling O
. O

The O
former O
enriches O
the O
representations O
of O
“ O
context O
” O
neighboring O
entities O
based O
on O
the O
well O
trained O
“ O
target O
word O
” O
mention O
- O
span O
, O
while O
the O
latter O
focuses O
on O
gathering O
those O
information O
back O
from O
neighboring O
entities O
to O
the O
masked O
target O
like O
low O
- O
frequency O
mention O
- O
span O
which O
is O
poorly O
represented O
( O
Turian O
et O
al O
. O
, O
2010 O
) O
. O

In O
the O
experiments O
, O
we O
compare O
SMedBERT B-MethodName
against O
various O
strong O
baselines O
, O
including O
mainstream O
KEPLMs O
pre O
- O
trained O
over O
our O
medical O
resources O
. O

The O
underlying O
medical O
NLP O
tasks O
include O
: O
named O
entity O
recognition O
, O
relation O
extraction O
, O
question O
answering O
, O
question O
matching O
and O
natural O
language O
inference O
. O

The O
results O
show O
that O
SMedBERT B-MethodName
consistently O
outperforms O
all O
the O
baselines O
on O
these O
tasks O
. O

2Although O
we O
focus O
on O
Chinese O
medical O
PLMs O
here O
. O

The O
proposed O
method O
can O
be O
easily O
adapted O
to O
other O
languages O
, O
which O
is O
beyond O
the O
scope O
of O
this O
work.2 O
Related O
Work O
PLMs O
in O
the O
Open O
Domain O
. O

PLMs O
have O
gained O
much O
attention O
recently O
, O
proving O
successful O
for O
boosting O
the O
performance O
of O
various O
NLP O
tasks O
( O
Qiu O
et O
al O
. O
, O
2020 O
) O
. O

Early O
works O
on O
PLMs O
focus O
on O
feature O
- O
based O
approaches O
to O
transform O
words O
into O
distributed O
representations O
( O
Collobert O
and O
Weston O
, O
2008 O
; O
Mikolov O
et O
al O
. O
, O
2013b O
; O
Pennington O
et O
al O
. O
, O
2014 O
; O
Peters O
et O
al O
. O
, O
2018 O
) O
. O

BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
( O
as O
well O
as O
its O
robustly O
optimized O
version O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019b O
) O
) O
employs O
bidirectional O
transformer O
encoders O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
and O
self O
- O
supervised O
tasks O
to O
generate O
context O
- O
aware O
token O
representations O
. O

Further O
improvement O
of O
performances O
mostly O
based O
on O
the O
following O
three O
types O
of O
techniques O
, O
including O
self O
- O
supervised O
tasks O
( O
Joshi O
et O
al O
. O
, O
2020 O
) O
, O
transformer O
encoder O
architectures O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
and O
multi O
- O
task O
learning O
( O
Liu O
et O
al O
. O
, O
2019a O
) O
. O

Knowledge O
- O
Enhanced O
PLMs O

. O

As O
existing O
BERTlike O
models O
only O
learn O
knowledge O
from O
plain O
corpora O
, O
various O
works O
have O
investigated O
how O
to O
incorporate O
knowledge O
facts O
to O
enhance O
the O
language O
understanding O
abilities O
of O
PLMs O
. O

KEPLMs O
are O
mainly O
divided O
into O
the O
following O
three O
types O
. O

( O
1 O
) O
Knowledge O
- O
enhanced O
by O
Entity O
Embedding O
: O
ERNIE B-MethodName
- I-MethodName
THU I-MethodName
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
and O
KnowBERT B-MethodName
( O
Peters O
et O
al O
. O
, O
2019 O
) O
inject O
linked O
- O
entity O
as O
heterogeneous O
features O
learned O
by O
KG O
embedding O
algorithms O
such O
as O
TransE B-MethodName
( O
Bordes O
et O
al O
. O
, O
2013 O
) O
. O

( O
2 O
) O
Knowledge O
- O
enhanced O
by O
Entity O
Description O
: O
EBERT B-MethodName
( O
Zhang O
et O
al O
. O
, O
2020a O
) O
and O
KEPLER B-MethodName
( O
Wang O
et O
al O
. O
, O
2019b O
) O
add O
extra O
description O
text O
of O
entities O
to O
enhance O
semantic O
representation O
. O

( O
3 O
) O
Knowledgeenhanced O
by O
Triplet O
Sentence O
: O
K B-MethodName
- I-MethodName
BERT I-MethodName
( O
Liu O
et O
al O
. O
, O
2020b O
) O
and O
CoLAKE B-MethodName
( O
Sun O
et O
al O
. O
, O
2020 O
) O
convert O
triplets O
into O
sentences O
and O
insert O
them O
into O
the O
training O
corpora O
without O
pre O
- O
trained O
embedding O
. O

Previous O
studies O
on O
KG O
embedding O
( O
Nguyen O
et O
al O
. O
, O
2016 O
; O
Schlichtkrull O
et O
al O
. O
, O
2018 O
) O
have O
shown O
that O
utilizing O
the O
surrounding O
facts O
of O
entity O
can O
obtain O
more O
informative O
embedding O
, O
which O
is O
the O
focus O
of O
our O
work O
. O
PLMs O
in O
the O
Medical O
Domain O
. O

PLMs O
in O
the O
medical O
domain O
can O
be O
generally O
divided O
into O
three O
categories O
. O

( O
1 O
) O
BioBERT O
( O
Lee O
et O
al O
. O
, O
2020 O
) O
, O
BlueBERT O
( O
Peng O
et O

al O
. O
, O
2019 O
) O
, O
SCIBERT B-MethodName
( O
Beltagy O
et O
al O
. O
, O
2019 O
) O
and O
ClinicalBert B-MethodName
( O
Huang O
et O
al O
. O
, O
2019 O
) O

apply O
continual O
learning O
on O
medical O
domain O
texts O
, O
such O
as O
PubMed B-DatasetName
abstracts O
, O
PMC O
full O
- O
text O
articles O
and O
MIMIC B-DatasetName
- I-DatasetName
III I-DatasetName
clinical I-DatasetName
notes I-DatasetName
. O

( O
2 O
) O
PubMedBERT5884 B-MethodName
DƵůƚŝͲ,ĞĂĚ O
  O
^ĞůĨͲƚƚĞŶƚŝŽŶ O
& O
ĞĞĚ&ŽƌǁĂƌĚ O
  O
> O
ĂǇĞƌ O
  O
dŽŬĞŶ/ŶƉƵƚ O
Dǆ O
  O
dͲ O
 O
ŶĐŽĚĞƌ O
< O
Ͳ O
 O
ŶĐŽĚĞƌ O
  O
Eǆ O
DƵůƚŝͲ,ĞĂĚ O
  O
^ĞůĨͲƚƚĞŶƚŝŽŶ O
, O
ǇďƌŝĚƚƚĞŶƚŝŽŶ O
  O
EĞƚǁŽƌŬ O
, O
ĞƚĞƌŽŐĞŶĞŽƵƐ/ŶĨŽƌŵĂƚŝŽŶ&ƵƐŝŽŶ O
WƌĞͲƚƌĂŝŶŝŶŐ O
  O
dĂƐŬƐ O

D O
> O
D O
DĞŶƚŝŽŶͲŶĞŝŐŚďŽƌŽŶƚĞǆƚDŽĚĞůŝŶŐ O
  O
;ĂͿ O
^DĞĚ O
 O
ZdƌĐŚŝƚĞĐƚƵƌĞ O

< O
ŶŽǁůĞĚŐĞ/ŶƉƵƚ O
  O
DĞĚŝĐĂů O
< O
' O
   O
ᯠߐ O
⯵ O
∂ O
˄Ks/Ͳϭϵ O

˅ O
EĞŝŐŚďŽƌŝŶŐ O
  O
ŶƚŝƚŝĞƐ O
dǇƉĞ O

EŽĚĞ O
  O
EŽĚĞ O
  O
W O
 O
WZ O
dǇƉĞ O

J O
Ê O
, O
ǇďƌŝĚƚƚ͘Θ/ŶĨŽƌ͘&ƵƐŝŽŶDŽĚƵůĞƐ O
ÊDĞŶƚŝŽŶͲŶĞŝŐŚďŽƌ O
ŽŶƚĞǆƚDŽĚĞůŝŶŐ O
༃ O
DĂƐŬĞĚEĞŝŐŚďŽƌDŽĚĞůŝŶŐ O
  O
U O
UU O
ŬŶŽǁůĞĚŐĞĐŽŶƚĞǆƚ O
ŬŶŽǁůĞĚŐĞĐŽŶƚĞǆƚ O
U΀D^<΁ O
ĸDĂƐŬĞĚDĞŶƚŝŽŶDŽĚĞůŝŶŐ O
  O
W O
> O
DƐ O
ƚŽŬĞŶƐ O
PO O
POPOFigure O
2 O
: O
Model O
overview O
of O
SMedBERT B-MethodName
. O

The O
left O
part O
is O
our O
model O
architecture O
and O
the O
right O
part O
is O
the O
details O
of O
our O
model O
including O
hybrid O
attention O
network O
and O
mention O
- O
neighbor O
context O
modeling O
pre O
- O
training O
tasks O
. O

( O
Gu O
et O
al O
. O
, O
2020 O
) O
learns O
weights O
from O
scratch O
using O
PubMed O
data O
to O
obtain O
an O
in O
- O
domain O
vocabulary O
, O
alleviating O
the O
out O
- O
of O
- O
vocabulary O
( O
OOV O
) O
problem O
. O

This O
training O
paradigm O
needs O
the O
support O
of O
largescale O
domain O
data O
and O
resources O
. O

( O
3 O
) O
Some O
other O
PLMs O
use O
domain O
self O
- O
supervised O
tasks O
for O
pretraining O
. O

For O
example O
, O
MC B-MethodName
- I-MethodName
BERT I-MethodName
( O
Zhang O
et O
al O
. O
, O
2020b O
) O
masks O
Chinese O
medical O
entities O
and O
phrases O
to O
learn O
complex O
structures O
and O
concepts O
. O

DiseaseBERT B-MethodName
( O
He O
et O
al O
. O
, O
2020 O
) O
leverages O
the O
medical O
terms O
and O
its O
category O
as O
the O
labels O
to O
pre O
- O
train O
the O
model O
. O

In O
this O
paper O
, O
we O
utilize O
both O
domain O
corpora O
and O
neighboring O
entity O
triplets O
of O
mentions O
to O
enhance O
the O
learning O
of O
medical O
language O
representations O
. O

3 O
The O
SMedBERT B-MethodName
Model O
3.1 O
Notations O
and O
Model O
Overview O
In O
the O
PLM O
, O
we O
denote O
the O
hidden O
feature O
of O
each O
tokenfw1;:::;wNgasfh1;h2;:::;hNgwhereN O
is O
the O
maximum O
input O
sequence O
length O
and O
the O
total O
number O
of O
pre O
- O
training O
samples O
as O
M. O
LetEbe O
the O
set O
of O
mention O
- O
span O
emin O
the O
training O
corpora O
. O

Furthermore O
, O
the O
medical O
KG O
consists O
of O
the O
entities O
setEand O
the O
relations O
set O
R. O

The O
triplet O
set O
isS O
= O
f(h;r;t O
) O

jh2E;r2R;t2Eg O
, O
whereh O
is O
the O
head O
entity O
with O
relation O
rto O
the O
tail O
entity O
t. O

The O
embeddings O
of O
entities O
and O
relations O
trained O
on O
KG O
by O
TransR O
( O
Lin O
et O
al O
. O
, O
2015 O
) O
are O
represented O
as entand rel O
, O
respectively O
. O

The O
neighboring O
entity O
set O
recalled O
from O
KG O
by O
emis O
denoted O
as O
Nem O
= O
fe1 O
m;e2 O
m;:::;eK O
mgwhereKis O
the O
threshold O
of O
our O
PEPR O
algorithm O
. O

We O
denote O
the O
number O
ofentities O
in O
the O
KG O
as O
Z. O

The O
dimensions O
of O
the O
hidden O
representation O
in O
PLM O
and O
the O
KG O
embeddings O
ared1andd2 O
, O
respectively O
. O

The O
main O
architecture O
of O
the O
our O
model O
is O
shown O
in O
Figure O
2 O
. O
SMedBERT B-MethodName
mainly O
includes O
three O
components O
: O
( O
1 O
) O
Top O
- O
K O
entity O
sorting O
determine O
which O
K O
neighbour O
entities O
to O
use O
for O
each O
mention O
. O

( O
2 O
) O
Mention O
- O
neighbor O
hybrid O
attention O
aims O
to O
infuse O
the O
structured O
semantics O
knowledge O
into O
encoder O
layers O
, O
which O
includes O
type O
attention O
, O
node O
attention O
and O
gated O
position O
infusion O
module O
. O

( O
3 O
) O
Mention O
- O
neighbor O
context O
modeling O
includes O
masked O
neighbor O
modeling O
and O
masked O
mention O
modeling O
aims O
to O
promote O
mentions O
to O
leverage O
and O
interact O
with O
neighbour O
entities O
. O

3.2 O
Top O
- O
K O
Entity O
Sorting O
Previous O
research O
shows O
that O
simple O
neighboring O
entity O
expansion O
may O
induce O
knowledge O
noises O
during O
PLM O
training O
( O
Wang O
et O
al O
. O
, O
2019a O
) O
. O

In O
order O
to O
recall O
the O
most O
important O
neighboring O
entity O
set O
from O
the O
KG O
for O
each O
mention O
, O
we O
extend O
the O
Personalized O
PageRank O
( O
PPR O
) O
( O
Page O
et O
al O
. O
, O
1999 O
) O
algorithm O
to O
ﬁlter O
out O
trivial O
entities.3Recall O
that O
the O
iterative O
process O
in O
PPR O
is O
Vi= O
( O
1  O
 O
) O
AVi 1 O
+ O
 O
P O
whereAis O
the O
normalized O
adjacency O
matrix O
, O
  O
is O
the O
damping O
factor O
, O
Pis O
uniformly O
distributed O
jump O
probability O
vector O
, O
and O
Vis O
the O
iterative O
score O
vector O
for O
each O
entity O
. O

PEPR O
speciﬁcally O
focuses O
on O
learning O
the O
weight O
for O
the O
target O
mention O
span O
in O
each O
iteration O
. O

It O
3We O
name O
our O
algorithm O
to O
be O
Personalized O
Entity O
PageRank O
, O
abbreviated O
as O
PEPR.5885assigns O
the O
span O
ema O
higher O
jump O
probability O
1 O
in O
Pwith O
the O
remaining O
as1 O
Z. O

It O
also O
uses O
the O
entity O
frequency O
to O
initialize O
the O
score O
vector O
V O
: O
Vem=(tem O
Tem2E O
1 O
Mem=2E(1 O
) O
where O
Tis O
the O
sum O
of O
frequencies O
of O
all O
entities O
. O

temis O
the O
frequency O
of O
emin O
the O
corpora O
. O

After O
sorting O
, O
we O
select O
the O
top- O
Kentity O
setNem O
. O

3.3 O
Mention O
- O
neighbor O
Hybrid O
Attention O
Besides O
the O
embeddings O
of O
neighboring O
entities O
, O
SMedBERT B-MethodName
integrates O
the O
type O
information O
of O
medical O
entities O
to O
further O
enhance O
semantic O
representations O
of O
mention O
- O
span O
. O

3.3.1 O
Neighboring O
Entity O
Type O
Attention O
Different O
types O
of O
neighboring O
entities O
may O
have O
different O
impacts O
. O

Given O
a O
speciﬁc O
mention O
- O
span O
em O
, O
we O
compute O
the O
neighboring O
entity O
type O
attention O
. O

Concretely O
, O
we O
calculate O
hidden O
representation O
of O
each O
entity O
type O
 O
ash O
 O
= O
P O
eim2E O
 O
mheim O
. O

E O
  O
mare O
neighboring O
entities O
of O
emwith O
the O
same O
type O
 O
andheim= O
 ent  O
ei O
m O
2Rd2 O
. O

h0 O
em O
= O
LN((fsp(hi;:::;hj)Wbe O
) O
) O
( O
2 O
) O
wherefspis O
the O
self O
- O
attentive O
pooling O
( O
Lin O
et O
al O
. O
, O
2017 O
) O
to O
generate O
the O
mention O
- O
span O
representation O
hem2Rd1and O
the O
( O
hi;hi+1;:::;hj)is O
the O
hidden O
representation O
of O
tokens O
( O
wi;wi+1;:::;wj)in O
mention O
- O
span O
emtrained O
by O
PLMs O
. O

h0 O
em2Rd2 O
is O
obtained O
by O
()non O
- O
linear O
activation O
function O
GELU O
( O
Hendrycks O
and O
Gimpel O
, O
2016 O
) O
and O
the O
learnable O
projection O
matrix O
Wbe2Rd1d2.LN O
is O
the O
LayerNorm O
function O
( O
Ba O
et O
al O
. O
, O
2016 O
) O
. O

Then O
, O
we O
calculate O
the O
each O
type O
attention O
weight O
using O
the O
type O
representation O
h O
 O
2Rd2and O
the O
transformed O
mention O
- O
span O
representation O
h0 O
em O
: O
 O
0 O
 O
= O
tanh  O
h0 O
emWt+h O
 O
Wt0 O
Wa O
( O
3 O
) O
whereWt2Rd2d2,Wt02Rd2d2andWa2 O
Rd21 O
. O

Finally O
, O
the O
neighboring O
entity O
type O
attention O
weights O
 O
are O
obtained O
by O
normalizing O
the O
attention O
score O
 O
0 O
 O
among O
all O
entity O
types O
T. O
3.3.2 O
Neighboring O
Entity O
Node O
Attention O
Apart O
from O
entity O
type O
information O
, O
different O
neighboring O
entities O
also O
have O
different O
inﬂuences O
. O

Speciﬁcally O
, O
we O
devise O
the O
neighboring O
entity O
nodeattention O
to O
capture O
the O
different O
semantic O
inﬂuences O
from O
neighboring O
entities O
to O
the O
target O
mention O
span O
and O
reduce O
the O
effect O
of O
noises O
. O

We O
calculate O
the O
entity O
node O
attention O
using O
the O
mentionspan O
representation O
h0 O
emand O
neighboring O
entities O
representation O
heimwith O
entity O
type O
 O
as O
: O
 O
0 O
emeim=  O
h0 O
emWq  O
heimWkT O
pd2 O
  O
( O
4 O
) O
 O
emeim O
= O
exp O
 O
0 O
emeim O
P O
eim2Nemexp O
 O
0 O
emeim O
( O
5 O
) O
whereWq2Rd2d2andWk2Rd2d2are O
the O
attention O
weight O
matrices O
. O

The O
representations O
of O
all O
neighboring O
entities O
in O
Nemare O
aggregated O
to O
h0 O
em2Rd2 O
: O
bh0 O
em O
= O
X O
eim2Nem O
 O
emeim  O
heimWv+bv O
( O
6 O
) O
h0 O
em O

= O
LN O
bh0 O
em+ O
 O
bh0 O
emWl1+bl1 O
Wl2 O
( O
7 O
) O
whereWv2Rd2d2,Wl12Rd24d2,Wl22 O
R4d2d2.bv2Rd2andbl12R4d2are O
the O
bias O
vectors O
. O

h0 O
emis O
the O
mention O
- O
neighbor O
representation O
from O
hybrid O
attention O
module O
. O

3.3.3 O
Gated O
Position O
Infusion O
Knowledge O
- O
injected O
representations O
may O
divert O
the O
texts O
from O
its O
original O
meanings O
. O

We O
further O
reduce O
knowledge O
noises O
via O
gated O
position O
infusion O
: O
h0 O
emf= h0 O
emkh0 O
em O
Wmf+bmf O
( O
8) O
eh0 O
emf O
= O
LN(h0 O
emfWbp+bbp O
) O
( O
9 O
) O
whereWmf2R2d22d2,Wbp2R2d2d1,bmf2 O
R2d2,bbp2Rd1.h0 O
emf2R2d2is O
the O
span O
- O
level O
infusion O
representation O
. O

“ O
k O
” O
means O
concatenation O
operation.eh0 O
emf2Rd1is O
the O
ﬁnal O
knowledgeinjected O
representation O
for O
mention O
em O
. O

We O
generate O
the O
output O
token O
representation O
hifby4 O
: O
gi= O
tanhh O
hikeh0 O
emfi O
Wug+bug O
( O
10 O
) O
hif=h O
hikgieh0 O
emfi O
Wex+bex O

+ O
hi O
( O
11 O
) O
whereWug O
, O
Wex2R2d1d1.bug;bex2Rd1 O
. O

“ O
 O
” O
means O
element O
- O
wise O
multiplication O
. O

4We O
ﬁnd O
that O
restricting O
the O
knowledge O
infusion O
position O
to O
tokens O
is O
helpful O
to O
improve O
performance.58863.4 O
Mention O
- O
neighbor O
Context O
Modeling O
To O
fully O
exploit O
the O
structured O
semantics O
knowledge O
in O
KG O
, O
we O
further O
introduce O
two O
novel O
selfsupervised O
pre O
- O
training O
tasks O
, O
namely O
Masked O
Neighbor O
Modeling O
( O
MNeM O
) O
and O
Masked O
Mention O
Modeling O
( O
MMeM O
) O
. O

3.4.1 O
Masked O
Neighbor O
Modeling O
Formally O
, O
let O
rbe O
the O
relation O
between O
the O
mentionspanemand O
a O
neighboring O
entity O
ei O
m O
: O
hmf O
= O
LN((fsp(hif;:::;hjf)Wsa))(12 O
) O
wherehmfis O
the O
mention O
- O
span O
hidden O
features O
based O
on O
the O
tokens O
hidden O
representation  O
hif;h(i+1)f;:::;hjf O
.hr= O
 rel(r)2Rd2is O
the O
relationrrepresentation O
and O
Wsa2Rd1d2is O
a O
learnable O
projection O
matrix O
. O

The O
goal O
of O
MNeM O
is O
leveraging O
the O
structured O
semantics O
in O
surrounding O
entities O
while O
reserving O
the O
knowledge O
of O
relations O
between O
entities O
. O

Considering O
the O
object O
functions O
of O
skip O
- O
gram O
with O
negative O
sampling O
( O
SGNS O
) O
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
and O
score O
function O
of O
TransR O
( O
Lin O
et O
al O
. O
, O
2015 O
): O
LS= O
logfs(w;c O
) O
+ O
kEcnPD[logfs(w; cn O
) O
] O
( O
13 O
) O
ftr(h;r;t O
) O

= O
khMr+r tMrk O
( O
14 O
) O
where O
thewinLSis O
the O
target O
word O
of O
context O
c.fs O
is O
the O
compatibility O
function O
measuring O
how O
well O
the O
target O
word O
is O
ﬁtted O
into O
the O
context O
. O

Inspired O
by O
SGNS O
, O
following O
the O
general O
energy O
- O
based O
framework O
( O
LeCun O
et O
al O
. O
, O
2006 O
) O
, O
we O
treat O
mention O
- O
spans O
in O
corpora O
as O
“ O
target O
words O
” O
, O
and O
neighbors O
of O
corresponding O
entities O
in O
KG O
as O
“ O
contexts O
” O
to O
provide O
additional O
global O
contexts O
. O

We O
employ O
the O
Sampled O
- O
Softmax O
( O
Jean O
et O
al O
. O
, O
2015 O
) O
as O
the O
criterionLMNeM O
for O
the O
mention O
- O
span O
em O
: O
X O
Nemlogexp(fs( O
) O
) O
exp(fs( O
) O
) O

+ O
KEenQ(en)[exp(fs(0 O
) O
) O
] O
( O
15 O
) O
wheredenotes O
the O
triplet O
( O
em;r;ei O
m),ei O
m2Nem O
. O

0is O
the O
negative O
triplets O
( O
em;r;en O
) O
, O
andenis O
negative O
entity O
sampled O
with O
Q(ei O
m)detailed O
in O
Appendix O
B. O
To O
keep O
the O
knowledge O
of O
relations O
between O
entities O
, O
we O
deﬁne O
the O
compatibility O
function O
as O
: O
fs  O
em;r;ei O

m O
= O
hmfMr+hr O
jjhmfMr+hrjj(heimMr)T O
jjheimMrjj O
( O
16)whereis O
a O
scale O
factor O
. O

Assuming O
the O
norms O
of O
bothhmfMr+hrandheimMrare O
1,we O
have O
: O
fs  O
em;r;ei O
m O
= O
()ftr(hmf;hr;heim O
) O
= O
0 O
( O
17 O
) O
which O
indicates O
the O
proposed O
fsis O
equivalence O
with O
ftr O
. O

BecausejhenMrjneeds O
to O
be O
calculated O
for O
eachen O
, O
the O
computation O
of O
the O
score O
function O
fs O
is O
costly O
. O

Hence O
, O
we O
transform O
part O
of O
the O
formula O
fsas O
follows O
: O
( O
hmfMr+hr)(henMr)T= O
 O
hmf O
1Mr O
hrMr O
hrT O
hen0T O
= O
 O
hmf O
1 O
MPr O
hen0T O
( O
18 O
) O

In O
this O
way O
, O
we O
eliminate O
computation O
of O
transforming O
eachhen O
. O

Finally O
, O
to O
compensate O
the O
offset O
introduced O
by O
the O
negative O
sampling O
function O
Q(ei O
m O
) O
( O
Jean O
et O
al O
. O
, O
2015 O
) O
, O
we O
complement O
fs(em;r;ei O
m O
) O
as O
: O
 O
hmf O
1 O
MPr O
k O
hmf O
1 O
MPrkheim0 O

kheimk logQ(ei O
m O
) O
( O

19 O
) O
3.4.2 O
Masked O
Mention O
Modeling O
In O
contrast O
to O
MNeM O
, O
MMeM O
transfers O
the O
semantic O
information O
in O
neighboring O
entities O
back O
to O
the O
masked O
mention O
em O
. O

Ym O
= O
LN((fsp(hip;:::;hjp)Wsa O
) O
) O
( O
20 O
) O
whereYmis O
the O
ground O
- O
truth O
representation O
of O
em O
andhip= O
 p(wi)2Rd2. pis O
the O
pre O
- O
trained O
embedding O
of O
BERT O
in O
our O
medical O
corpora O
. O

The O
mention O
- O
span O
representation O
obtained O
by O
our O
model O
ishmf O
. O

For O
a O
sample O
s O
, O
the O
loss O
of O
MMeM O
LMMeM O
is O
calculated O
via O
Mean O
- O
Squared O
Error O
: O
LMMeM O

= O
MsX O
mikhmif Ymik2(21 O
) O
whereMsis O
the O
set O
of O
mentions O
of O
sample O
s. O
3.5 O
Training O
Objective O
In O
SMedBERT B-MethodName
, O
the O
training O
objectives O
mainly O
consist O
of O
three O
parts O
, O
including O
the O
self O
- O
supervised O
loss O
proposed O
in O
previous O
works O
and O
the O
mentionneighbor O
context O
modeling O
loss O
proposed O
in O
our O
work O
. O

Our O
model O
can O
be O
applied O
to O
medical O
text O
pre O
- O
training O
directly O
in O
different O
languages O
as O
long5887as O
high O
- O
quality O
medical O
KGs O
can O
be O
obtained O
. O

The O
total O
loss O
is O
as O
follows O
: O
Ltotal O
= O
LEX+1LMNeM O
+ O
2LMMeM O
( O
22 O
) O
whereLEXis O
the O
sum O
of O
sentence O
- O
order O
prediction O
( O
SOP O
) O
( O
Lan O
et O
al O
. O
, O
2020 O
) O
and O
masked O
language O
modeling.1and2are O
the O
hyperparameters O
. O

4 O
Experiments O
4.1 O
Data O
Source O
Pre O
- O
training O
Data O
. O

The O
pre O
- O
training O
corpora O
after O
pre O
- O
processing O
contains O
5,937,695 O
text O
segments O
with O
3,028,224,412 O
tokens O
( O
4.9 O
GB O
) O
. O

The O
KGs O
embedding O
trained O
by O
TransR O
( O
Lin O
et O
al O
. O
, O
2015 O
) O
on O
two O
trusted O
data O
sources O
, O
including O
the O
SymptomIn O
- O
Chinese O
from O
OpenKG5and O
DXY O
- O
KG6containing O
139,572 O
and O
152,508 O
entities O
, O
respectively O
. O

The O
number O
of O
triplets O
in O
the O
two O
KGs O
are O
1,007,818 O
and O
3,764,711 O
. O

The O
pre O
- O
training O
corpora O
and O
the O
KGs O
are O
further O
described O
in O
Appendix O
A.1 O
. O

Task O
Data O
. O

We O
use O
four O
large O
- O
scale O
datasets O
in O
ChineseBLUE B-DatasetName
( O
Zhang O
et O
al O
. O
, O
2020b O
) O
to O
evaluate O
our O
model O
, O
which O
are O
benchmark O
of O
Chinese O
medical O
NLP O
tasks O
. O

Additionally O
, O
we O
test O
models O
on O
four O
datasets O
from O
real O
application O
scenarios O
provided O
by O
DXY O
company7and O
CHIP8 O
, O
i.e. O
, O
Named B-TaskName
Entity I-TaskName
Recognition I-TaskName
( O
DXY B-DatasetName
- I-DatasetName
NER I-DatasetName
) O
, O
Relation B-TaskName
Extraction I-TaskName
( O
DXY B-DatasetName
- I-DatasetName
RE I-DatasetName
, O
CHIP B-DatasetName
- I-DatasetName
RE I-DatasetName
) O
and O
Question B-TaskName
Answer I-TaskName
( O
WebMedQA B-DatasetName
( O
He O
et O
al O
. O
, O
2019 O
) O
) O
. O

For O
other O
information O
of O
the O
downstream O
datasets O
, O
we O
refer O
readers O
to O
Appendix O
A.2 O
. O

4.2 O
Baselines O
In O
this O
work O
, O
we O
compare O
SMedBERT B-MethodName
with O
general O
PLMs O
, O
domain O
- O
speciﬁc O
PLMs O
and O
KEPLMs O
with O
knowledge O
embedding O
injected O
, O
pre O
- O
trained O
on O
our O
Chinese O
medical O
corpora O
: O
General O
PLMs O
: O

We O
use O
three O
Chinese O
BERT B-MethodName
- O
style O
models O
, O
namely O
BERT B-MethodName
- I-MethodName
base I-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
BERT B-MethodName
- I-MethodName
wwm I-MethodName
( O
Cui O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019b O
) O
. O

All O
the O
weights O
are O
initialized O
from O
( O
Cui O
et O
al O
. O
, O
2020 O
) O
. O

Domain O
- O
speciﬁc O
PLMs O
: O

As O
very O
few O
PLMs O
in O
the O
Chinese O
medical O
domain O
are O
available O
, O
we O
consider O
the O
following O
models O
. O

MC O
- O
BERT O
( O
Zhang O
et O
al O
. O
, O
5http://www.openkg.cn/dataset/ O
symptom O
- O
in O
- O
chinese O
6https://portal.dxy.cn/ O
7https://auth.dxy.cn/accounts/login O
8http://www.cips-chip.org.cn:8088/homeModel O
D1 O
D2 O
D3 O
SGNS O
- O
char O
- O
med O
27.21 O
% O
27.16 O
% O
21.72 O
% O
SGNS O
- O
word O
- O
med O
24.64 O
% O
24.95 O
% O
20.37 O
% O
GLOVE O
- O
char O
- O
med O
27.24 O
% O
27.12 O
% O
21.91 O
% O
GLOVE O
- O
word O
- O
med O
24.41 O
% O
23.89 O
% O
20.56 O
% O
BERT B-MethodName
- I-MethodName
open I-MethodName
29.79 O
% O
29.41 O
% O
21.83 O
% O
BERT B-MethodName
- I-MethodName
wwm I-MethodName
- I-MethodName
open I-MethodName
29.75 O
% O
29.55 O
% O
21.97 O
% O
RoBERTa B-MethodName
- O
open O
30.84 O
% O
30.56 O
% O
21.98 O
% O
MC B-MethodName
- I-MethodName
BERT I-MethodName
30.63 O
% O
30.34 O
% O
22.65 O
% O
BioBERT B-MethodName
- I-MethodName
zh I-MethodName
30.84 O
% O
30.69 O
% O
22.71 O
% O
ERNIE B-MethodName
- I-MethodName
med I-MethodName
30.97 O
% O
30.78 O
% O
22.99 O
% O
KnowBERT B-MethodName
- I-MethodName
med I-MethodName
30.95 O
% O
30.77 O
% O
23.07 O
% O
SMedBERT B-MethodName
31.81 O
% O
32.14 O
% O
24.08 O
% O
Table O
1 O
: O
Results O
of O
unsupervised O
semantic O
similarity O
task O
. O

“ O
med O
” O
refers O
to O
models O
continually O
pre O
- O
trained O
on O
medical O
corpora O
, O
and O
“ O
open O
” O
means O
open O
- O
domain O
corpora O
. O

“ O
char O
’ O
and O
“ O
word O
” O
refer O
to O
the O
token O
granularity O
of O
input O
samples O
. O
2020b O
) O
is O
pre O
- O
trained O
over O
a O
Chinese O
medical O
corpora O
via O
masking O
different O
granularity O
tokens O
. O

We O
also O
pre O
- O
train O
BERT B-MethodName
using O
our O
corpora O
, O
denoted O
as O
BioBERT B-MethodName
- O
zh O
. O
KEPLMs O
: O

We O
employ O
two O
SOTA O
KEPLMs O
continually O
pre O
- O
trained O
on O
our O
medical O
corpora O
as O
our O
baseline O
models O
, O
including O
ERNIE B-MethodName
- I-MethodName
THU I-MethodName
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
and O
KnowBERT B-MethodName
( O
Peters O
et O
al O
. O
, O
2019 O
) O
. O

For O
a O
fair O
comparison O
, O
KEPLMs O
use O
other O
additional O
resources O
rather O
than O
the O
KG O
embedding O
are O
excluded O
( O
See O
Section O
2 O
) O
, O
and O
all O
the O
baseline O
KEPLMs O
are O
injected O
by O
the O
same O
KG O
embedding O
. O

The O
detailed O
parameter O
settings O
and O
training O
procedure O
are O
in O
Appendix O
B. O
4.3 O
Intrinsic O
Evaluation O
To O
evaluate O
the O
semantic O
representation O
ability O
of O
SMedBERT B-MethodName
, O
we O
design O
an O
unsupervised O
semantic O
similarity O
task O
. O

Speciﬁcally O
, O
we O
extract O
all O
entities O
pairs O
with O
equivalence O
relations O
in O
KGs O
as O
positive O
pairs O
. O

For O
each O
positive O
pair O
, O
we O
use O
one O
of O
the O
entity O
as O
query O
entity O
while O
the O
other O
as O
positive O
candidate O
, O
which O
is O
used O
to O
sample O
other O
entities O
as O
negative O
candidates O
. O

We O
denote O
this O
dataset O
as O
D1 O
. O

Besides O
, O
the O
entities O
in O
the O
same O
positive O
pair O
often O
have O
many O
neighbours O
in O
common O
. O

We O
select O
positive O
pairs O
with O
large O
proportions O
of O
common O
neighbours O
as O
D2 O
. O

Additionally O
, O
to O
verify O
the O
ability O
of O
SMedBERT B-MethodName
of O
enhancing O
the O
low O
- O
frequency O
mention O
representation O
, O
we O
extract O
all O
positive O
pairs O
that O
with O
at O
least O
one O
low O
- O
frequency O
mention O
as O
D3 O
. O

There O
are O
totally O
359,358 O
, O
272,320 O
and O
41,583 O
samples O
for O
D1,D2,D3respectively O
. O

We O
describe O
the5888Named O
Entity O
Recognition O
Relation O
Extraction O
Model O
cMedQANER O
DXY O
- O
NER O
Average O
CHIP O
- O
RE O
DXY O
- O
RE O
Average O
Dev O
Test O
Dev O
Test O
Test O
Test O
Dev O
Test O
Test O
BERT O
- O
open O
80.69 O
% O
83.12 O
% O
79.12 O
% O
79.03 O
% O
81.08 O
% O
85.86 O
% O
94.18 O
% O
94.13 O
% O
90.00 O
% O
BERT O
- O
wwm O
- O
open O
80.52 O
% O
83.07 O
% O
79.48 O
% O
79.29 O
% O
81.18 O
% O
86.01 O
% O
94.35 O
% O
94.38 O
% O
90.20 O
% O
RoBERT O
- O
open O
80.92 O
% O
83.29 O
% O
79.27 O
% O
79.33 O
% O
81.31 O
% O
86.19 O
% O
94.64 O
% O
94.66 O
% O
90.43 O
% O
BioBERT O
- O
zh O
80.72 O
% O
83.38 O
% O
79.52 O
% O
79.45 O
% O
81.42 O
% O
86.12 O
% O
94.54 O
% O
94.64 O
% O
90.38 O
% O
MC O
- O
BERT O
81.02 O
% O
83.46 O
% O
79.79 O
% O
79.59 O
% O
81.53 O
% O
86.09 O
% O
94.74 O
% O
94.73 O
% O
90.41 O
% O
KnowBERT O
- O
med O
81.29 O
% O
83.75 O
% O
80.86 O
% O
80.44 O
% O
82.10 O
% O
86.27 O
% O
95.05 O
% O
94.97 O
% O
90.62 O
% O
ERNIE O
- O
med O
81.22 O
% O
83.87 O
% O
80.82 O
% O
80.87 O
% O
82.37 O
% O
86.25 O
% O
94.98 O
% O
94.91 O
% O
90.58 O
% O
SMedBERT B-MethodName
82.23 O
% O
84.75 O
% O
83.06 O
% O
82.94 O
% O
83.85 O
% O
86.95 O
% O
95.73 O
% O
95.89 O
% O
91.42 O
% O
Table O
2 O
: O
Performance O
of O
Named O
Entity O
Recognition O
( O
NER O
) O
and O
Relation O
Extraction O
( O
RE O
) O
tasks O
in O
terms O
of O
F1 O
. O

The O
Development O
data O
of O
CHIP O
- O
RE O
is O
unreleased O
in O
public O
dataset O
. O

Question O
Answering O
Question O
Matching O
Natural O
Lang O
. O

Infer O
. O

Model O
cMedQA O
WebMedQA O
Average O
cMedQQ O
cMedNLI O
Dev O
Test O
Dev O
Test O
Test O
Dev O
Test O
Dev O
Test O
BERT O
- O
open O
72.99 O
% O
73.82 O
% O
77.20 O
% O
79.72 O
% O
76.77 O
% O
86.74 O
% O
86.72 O
% O
95.52 O
% O
95.66 O
% O
BERT O
- O
wwm O
- O
open O
72.03 O
% O
72.96 O
% O
77.06 O
% O
79.68 O
% O
76.32 O
% O
86.98 O
% O
86.82 O
% O
95.53 O
% O
95.78 O
% O
RoBERT O
- O
open O
72.22 O
% O
73.18 O
% O
77.18 O
% O
79.57 O
% O
76.38 O
% O
87.24 O
% O
86.97 O
% O
95.87 O
% O
96.11 O
% O
BioBERT O
- O
zh O
74.32 O
% O
75.12 O
% O
78.04 O
% O
80.45 O
% O
77.79 O
% O
87.30 O
% O
87.06 O
% O
95.89 O
% O
96.04 O
% O
MC O
- O
BERT O
74.40 O
% O
74.46 O
% O
77.85 O
% O
80.54 O
% O
77.50 O
% O
87.17 O
% O
87.01 O
% O
95.81 O
% O
96.06 O
% O
KnowBERT O
- O
med O
74.38 O
% O
75.25 O
% O
78.20 O
% O
80.67 O
% O
77.96 O
% O
87.25 O
% O
87.14 O
% O
95.96 O
% O
96.03 O
% O
ERNIE O
- O
med O
74.37 O
% O
75.22 O
% O
77.93 O
% O
80.56 O
% O
77.89 O
% O
87.34 O
% O
87.20 O
% O
96.02 O
% O
96.25 O
% O
SMedBERT B-MethodName
75.06 O
% O
76.04 O
% O
79.26 O
% O
81.68 O
% O
78.86 O
% O
88.13 O
% O
88.09 O
% O
96.64 O
% O
96.88 O
% O
Table O
3 O
: O
Performance O
of O
Question O
Answering O
( O
QA O
) O
, O
Question O
Matching O
( O
QM O
) O
and O
Natural O
Language O
Inference O
( O
NLI O
) O
tasks O
. O

The O
metric O
of O
the O
QA O
task O
is O
Acc@1 O
and O
those O
of O
QM O
and O
NLI O
are O
F1 O
. O

details O
of O
collecting O
data O
and O
embedding O
words O
in O
Appendix O
C. O

In O
this O
experiments O
, O
we O
compare O
SMedBERT B-MethodName
with O
three O
types O
of O
models O
: O
classical O
word O
embedding O
methods O
( O
SGNS O
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
, O
GLOVE O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
) O
, O
PLMs O
and O
KEPLMs O
. O

We O
compute O
the O
similarity O
between O
the O
representation O
of O
query O
entities O
and O
all O
the O
other O
entities O
, O
retrieving O
the O
most O
similar O
one O
. O

The O
evaluation O
metric O
is O
top-1 B-MetricName
accuracy I-MetricName
( O
Acc@1 B-MetricName
) O
. O

Experiment O
results O
are O
shown O
in O
Table O
1 O
. O

From O
the O
results O
, O
we O
observe O
that O
: O
( O
1 O
) O
SMedBERT B-MethodName
greatly O
outperforms O
all O
baselines O
especially O
on O
the O
dataset O
D2 O
( O
+1.36 O
% O
) O
, O
where O
most O
positive O
pairs O
have O
many O
shared O
neighbours O
, O
demonstrating O
that O
ability O
of O
SMedBERT B-MethodName
to O
utilize O
semantic O
information O
from O
the O
global O
context O
. O

( O
2 O
) O
In O
dataset O
D3 O
, O
SMedBERT B-MethodName
improve O
the O
performance O
signiﬁcantly O
( O
+1.01 O
% O
) O
, O
indicating O
our O
model O
is O
effective O
to O
enhance O
the O
representation O
of O
low O
- O
frequency O
mentions O
. O

4.4 O
Results O
of O
Downstream O
Tasks O
We O
ﬁrst O
evaluate O
our O
model O
in O
NER B-TaskName
and O
RE B-TaskName
tasks O
that O
are O
closely O
related O
to O
entities O
in O
the O
input O
texts O
. O

Table O
2 O
shows O
the O
performances O
on O
medical O
NER B-TaskName
and O
RE B-TaskName
tasks O
. O

In O
NER B-TaskName
and O
RE O
tasks O
, O
we O
can O
observe O
from O
the O
results O
: O
( O
1 O
) O
Compared O
with O
PLMs O
trained O
in O
open O
- O
domain O
corpora O
, O
KEPLMs O
with O
medical O
corpora O
and O
knowledge O
facts O
achieve O
better O
results O
. O

( O
2 O
) O
The O
performance O
of O
SMedBERT B-MethodName
is O
greatly O
improved O
compared O
with O
the O
strongest O
baseline O
in O
two O
NER B-TaskName
datasets O
( O
+0.88 B-MetricValue
% I-MetricValue
, O
+2.07 B-MetricValue
% I-MetricValue
) O
, O
and O
( O
+0.68 B-MetricValue
% I-MetricValue
, O
+0.92 B-MetricValue
% I-MetricValue
) O
on O
RE B-TaskName
tasks O
. O

We O
also O
evaluate O
SMedBERT B-MethodName
on O
QA B-TaskName
, O
QM B-TaskName
and O
NLI B-TaskName
tasks O
and O
the O
performance O
is O
shown O
in O
Table O
3 O
. O

We O
can O
observe O
that O
SMedBERT B-MethodName
improve O
the O
performance O
consistently O
on O
these O
datasets O
( O
+0.90 B-MetricValue
% I-MetricValue
on O
QA B-TaskName
, O
+0.89 B-MetricValue
% I-MetricValue
on O
QM B-TaskName
and O
+0.63 B-MetricValue
% I-MetricValue
on O
NLI B-TaskName
) O
. O

In O
general O
, O
it O
can O
be O
seen O
from O
Table O
2 O
and O
Table O
3 O
that O
injecting O
the O
domain O
knowledge O
especially O
the O
structured O
semantics O
knowledge O
can O
improve O
the O
result O
greatly O
. O

4.5 O
Inﬂuence O
of O
Entity O
Hit O
Ratio O
In O
this O
experiment O
, O
we O
explore O
the O
model O
performance O
in O
NER B-TaskName
and O
RE B-TaskName
tasks O
with O
different O
entity O
hit O
ratios O
, O
which O
control O
the O
proportions O
of O
knowledgeenhanced O
mention O
- O
spans O
in O
the O
samples O
. O

The O
aver-5889 O
Figure O
3 O
: O
Entity O
hit O
ratio O
results O
of O
SMedBERT B-MethodName
and O
ERNIE B-MethodName
in O
NER B-TaskName
and O
RE B-TaskName
tasks O
. O

Figure O
4 O
: O
The O
inﬂuence O
of O
different O
K O
values O
in O
results O
. O

age O
number O
of O
mention O
- O
spans O
in O
samples O
is O
about O
40 O
. O

Figure O
3 O
illustrates O
the O
performance O
of O
SMedBERT B-MethodName
and O
ERNIE B-MethodName
- O
med O
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
. O

From O
the O
result O
, O
we O
can O
observe O
that O
: O
( O
1 O
) O
The O
performance O
improves O
signiﬁcantly O
at O
the O
beginning O
and O
then O
keeps O
stable O
as O
the O
hit O
ratio O
increases O
, O
proving O
the O
heterogeneous O
knowledge O
is O
beneﬁcial O
to O
improve O
the O
ability O
of O
language O
understanding O
and O
indicating O
too O
much O
knowledge O
facts O
are O
unhelpful O
to O
further O
improve O
model O
performance O
due O
to O
the O
knowledge O
noise O
( O
Liu O
et O
al O
. O
, O
2020b O
) O
. O

( O
2 O
) O
Compared O
with O
previous O
approaches O
, O
our O
SMedBERT B-MethodName
model O
improves O
performance O
greatly O
and O
more O
stable O
. O

4.6 O
Inﬂuence O
of O
Neighboring O
Entity O
Number O
We O
further O
evaluate O
the O
model O
performance O
under O
differentKover O
the O
test O
set O
of O
DXY B-DatasetName
- I-DatasetName
NER I-DatasetName
and O
DXY B-DatasetName
- I-DatasetName
RE I-DatasetName
. O

Figure O
4 O
shows O
the O
the O
model O
result O
with O
K O
= O
f5;10;20;30 O
g. O
In O
our O
settings O
, O
the O
SMedBERT B-MethodName
can O
achieve O
the O
best O
performance O
in O
different O
tasks O
around O
K= O
10 O
. O

The O
results O
of O
SMedBERT B-MethodName
show O
that O
the O
model O
performance O
increasing O
ﬁrst O
and O
then O
decreasing O
with O
the O
increasing O
of O
K. O

This O
phenomenon O
also O
indicates O
the O
knowledge O
noise O
problem O
that O
injecting O
too O
much O
knowledge O
of O
neighboring O
entities O
may O
hurt O
the O
performance O
. O

4.7 O
Ablation O
Study O
In O
Table O
4 O
, O
we O
choose O
three O
important O
model O
components O
for O
our O
ablation O
study O
and O
report O
the O
testModel O
D5 O
D6 O
D7 O
D8 O
SMedBERT B-MethodName
84.75 B-MetricValue
% I-MetricValue
82.94 B-MetricValue
% I-MetricValue
86.95 B-MetricValue
% I-MetricValue
95.89 B-MetricValue
% I-MetricValue
ERNIE B-MethodName
- I-MethodName
med I-MethodName
83.87 B-MetricValue
% I-MetricValue
80.87 B-MetricValue
% I-MetricValue
86.25 B-MetricValue
% I-MetricValue
94.91 B-MetricValue
% I-MetricValue
-Type O
Att O
. O
84.25 O
% O
81.99 O
% O
86.61 O
% O
95.29 O
% O
-Hybrid O
Att O
. O
83.71 O
% O
80.85 O
% O
86.46 O
% O
95.20 O
% O
-Know O
. O

Loss O
84.31 O
% O
82.12 O
% O
86.50 O
% O
95.43 O
% O
Table O
4 O
: O
Ablation O
study O
of O
SMedBERT B-MethodName
on O
four O
datasets O
( O
testing O
set O
) O
. O

Due O
to O
the O
space O
limitation O
, O
we O
use O
the O
abbreviations O
“ O
D5 O
” O
, O
“ O
D6 O
” O
, O
“ O
D7 O
” O
, O
and O
“ O
D8 O
” O
to O
represent O
the O
cMedQANER B-DatasetName
, I-DatasetName
DXY B-DatasetName
- I-DatasetName
NER I-DatasetName
, O
CHIP B-DatasetName
- I-DatasetName
RE I-DatasetName
, O
and O
DXYRE B-DatasetName
datasets O
respectively O
. O

set O
performance O
on O
four O
datasets O
of O
NER B-TaskName
and O
RE B-TaskName
tasks O
that O
are O
closely O
related O
to O
entities O
. O

Speciﬁcally O
, O
the O
three O
model O
components O
are O
neighboring O
entity O
type O
attention O
, O
the O
whole O
hybrid O
attention O
module O
, O
and O
mention O
- O
neighbor O
context O
modeling O
respectively O
, O
which O
includes O
two O
masked O
language O
model O
lossLMNeM O
andLMMeM O
. O

From O
the O
result O
, O
we O
can O
observe O
that O
: O
( O
1 O
) O
Without O
any O
of O
the O
three O
mechanisms O
, O
our O
model O
performance O
can O
also O
perform O
competitively O
with O
the O
strong O
baseline O
ERNIE B-MethodName
- I-MethodName
med I-MethodName
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
. O

( O
2 O
) O
Note O
that O
after O
removing O
the O
hybrid O
attention O
module O
, O
the O
performance O
of O
our O
model O
has O
the O
greatest O
decline O
, O
which O
indicates O
that O
injecting O
rich O
heterogeneous O
knowledge O
of O
neighboring O
entities O
is O
effective O
. O

5 O
Conclusion O
In O
this O
work O
, O
we O
address O
medical O
text O
mining O
tasks O
with O
the O
structured O
semantics O
KEPLM O
proposed O
named O
SMedBERT B-MethodName
. O

Accordingly O
, O
we O
inject O
entity O
type O
semantic O
information O
of O
neighboring O
entities O
into O
node O
attention O
mechanism O
via O
heterogeneous O
feature O
learning O
process O
. O

Moreover O
, O
we O
treat O
the O
neighboring O
entity O
structures O
as O
additional O
global O
contexts O
to O
predict O
the O
masked O
candidate O
entities O
based O
on O
mention O
- O
spans O
and O
vice O
versa O
. O

The O
experimental O
results O
show O
the O
signiﬁcant O
improvement O
of O
our O
model O
on O
various O
medical O
NLP O
tasks O
and O
the O
intrinsic O
evaluation O
. O

There O
are O
two O
research O
directions O
that O
can O
be O
further O
explored O
: O
( O
1 O
) O
Injecting O
deeper O
knowledge O
by O
using O
“ O
farther O
neighboring O
” O
entities O
as O
contexts O
; O
( O
2 O
) O
Further O
enhancing O
Chinese O
medical O
long O
- O
tail O
entity O
semantic O
representation O
. O

Acknowledgements O
We O
would O
like O
to O
thank O
anonymous O
reviewers O
for O
their O
valuable O
comments O
. O

This O
work O
is O
supported O
by5890the O
National O
Key O
Research O
and O
Development O
Program O
of O
China O
under O
Grant O
No O
. O
2016YFB1000904 O
, O
and O
Alibaba O
Group O
through O
Alibaba O
Research O
Intern O
Program O
. O

References O
Lei O
Jimmy O
Ba O
, O
Jamie O
Ryan O
Kiros O
, O
and O
Geoffrey O
E. O
Hinton O
. O

2016 O
. O

Layer O
normalization O
. O

CoRR O
, O
abs/1607.06450 O
. O

Iz O
Beltagy O
, O
Kyle O
Lo O
, O
and O
Arman O
Cohan O
. O

2019 O
. O

Scibert O
: O
A O
pretrained O
language O
model O
for O
scientiﬁc O
text O
. O

In O
EMNLP O
, O
pages O
3613–3618 O
. O
Antoine O
Bordes O
, O
Nicolas O
Usunier O
, O
Alberto O
Garc O
´ O
ıaDur´an O
, O
Jason O
Weston O
, O
and O
Oksana O
Yakhnenko O
. O
2013 O
. O

Translating O
embeddings O
for O
modeling O
multirelational O
data O
. O

In O
NIPS O
, O
pages O
2787–2795 O
. O

Ronan O
Collobert O
and O
Jason O
Weston O
. O

2008 O
. O

A O
uniﬁed O
architecture O
for O
natural O
language O
processing O
: O
deep O
neural O
networks O
with O
multitask O
learning O
. O

In O
ICML O
, O
pages O
160–167 O
. O

Yiming O
Cui O
, O
Wanxiang O
Che O
, O
Ting O
Liu O
, O
Bing O
Qin O
, O
Shijin O
Wang O
, O
and O
Guoping O
Hu O
. O
2020 O
. O

Revisiting O
pretrained O
models O
for O
chinese O
natural O
language O
processing O
. O

In O
EMNLP O
, O
pages O
657–668 O
. O

Yiming O
Cui O
, O
Wanxiang O
Che O
, O
Ting O
Liu O
, O
Bing O
Qin O
, O
Ziqing O
Yang O
, O
Shijin O
Wang O
, O
and O
Guoping O
Hu O
. O
2019 O
. O

Pre O
- O
training O
with O
whole O
word O
masking O
for O
chinese O
BERT O
. O

CoRR O
, O
abs/1906.08101 O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT O
: O
pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O

In O
NAACL O
, O
pages O
4171–4186 O
. O

Yu O
Gu O
, O
Robert O
Tinn O
, O
Hao O
Cheng O
, O
Michael O
Lucas O
, O
Naoto O
Usuyama O
, O
Xiaodong O
Liu O
, O
Tristan O
Naumann O
, O
Jianfeng O
Gao O
, O
and O
Hoifung O
Poon O
. O

2020 O
. O

Domain O
- O
speciﬁc O
language O
model O
pretraining O
for O
biomedical O
natural O
language O
processing O
. O

CoRR O
, O
abs/2007.15779 O
. O

Hiroaki O
Hayashi O
, O
Zecong O
Hu O
, O
Chenyan O
Xiong O
, O
and O
Graham O
Neubig O
. O

2020 O
. O

Latent O
relation O
language O
models O
. O

InAAAI O
, O
pages O
7911–7918 O
. O

Junqing O
He O
, O
Mingming O
Fu O
, O
and O
Manshu O
Tu O
. O

2019 O
. O

Applying O
deep O
matching O
networks O
to O
chinese O
medical O
question O
answering O
: O
a O
study O
and O
a O
dataset O
. O

BMC O
Medical O
Informatics O
Decis O
. O

Mak O
. O
, O
19 O
- O
S(2):91–100 O
. O

Yun O
He O
, O
Ziwei O
Zhu O
, O
Yin O
Zhang O
, O
Qin O
Chen O
, O
and O
James O
Caverlee O
. O

2020 O
. O

Infusing O
disease O
knowledge O
into O
BERT O
for O
health O
question O
answering O
, O
medical O
inference O
and O
disease O
name O
recognition O
. O

In O
EMNLP O
, O
pages O
4604–4614 O
. O

Dan O
Hendrycks O
and O
Kevin O
Gimpel O
. O

2016 O
. O

Gaussian O
error O
linear O
units O
( O
gelus O
) O
. O

arXiv:1606.08415 O
.Kexin O

Huang O
, O
Jaan O
Altosaar O
, O
and O
Rajesh O
Ranganath O
. O

2019 O
. O

Clinicalbert O
: O
Modeling O
clinical O
notes O
and O
predicting O
hospital O
readmission O
. O

CoRR O
, O
abs/1904.05342 O
. O

Paul O
Jaccard O
. O

1912 O
. O

The O
distribution O
of O
the O
ﬂora O
in O
the O
alpine O
zone O
. O

New O
Phydvtologist O
, O
11(2):37–50 O
. O

S´ebastien O
Jean O
, O
KyungHyun O
Cho O
, O
Roland O
Memisevic O
, O
and O
Yoshua O
Bengio O
. O
2015 O
. O

On O
using O
very O
large O
target O
vocabulary O
for O
neural O
machine O
translation O
. O

In O
ACL O
, O
pages O
1–10 O
. O

Mandar O
Joshi O
, O
Danqi O
Chen O
, O
Yinhan O
Liu O
, O
Daniel O
S. O
Weld O
, O
Luke O
Zettlemoyer O
, O
and O
Omer O
Levy O
. O

2020 O
. O

Spanbert O
: O
Improving O
pre O
- O
training O
by O
representing O
and O
predicting O
spans O
. O

Trans O
. O

Assoc O
. O

Comput O
. O

Linguistics O
, O
8:64–77 O
. O

Zhenzhong O
Lan O
, O
Mingda O
Chen O
, O
Sebastian O
Goodman O
, O
Kevin O
Gimpel O
, O
Piyush O
Sharma O
, O
and O
Radu O
Soricut O
. O
2020 O
. O

ALBERT O
: O

A O
lite O
BERT O
for O
self O
- O
supervised O
learning O
of O
language O
representations O
. O

In O
ICLR O
. O

Yann O
LeCun O
, O
Sumit O
Chopra O
, O
Raia O
Hadsell O
, O
M O
Ranzato O
, O
and O
F O
Huang O
. O
2006 O
. O

A O
tutorial O
on O
energy O
- O
based O
learning O
. O

Predicting O
structured O
data O
, O
1(0 O
) O
. O

Jinhyuk O
Lee O
, O
Wonjin O
Yoon O
, O
Sungdong O
Kim O
, O
Donghyeon O
Kim O
, O
Sunkyu O
Kim O
, O
Chan O
Ho O

So O
, O
and O
Jaewoo O
Kang O
. O

2020 O
. O

Biobert B-MethodName
: O
a O
pre O
- O
trained O
biomedical O
language O
representation O
model O
for O
biomedical O
text O
mining O
. O

Bioinform O
. O
, O
36(4):1234 O
– O
1240 O
. O

Linfeng O
Li O
, O
Peng O
Wang O
, O
Jun O
Yan O
, O
Yao O
Wang O
, O
Simin O
Li O
, O
Jinpeng O
Jiang O
, O
Zhe O
Sun O
, O
Buzhou O
Tang O
, O
TsungHui O
Chang O
, O
Shenghui O
Wang O
, O
and O
Yuting O
Liu O
. O

2020 O
. O

Real O
- O
world O
data O
medical O
knowledge O
graph O
: O
construction O
and O
applications O
. O

Artif O
. O

Intell O
. O

Medicine O
, O
103:101817 O
. O

Yankai O
Lin O
, O
Zhiyuan O
Liu O
, O
Maosong O
Sun O
, O
Yang O
Liu O
, O
and O
Xuan O
Zhu O
. O
2015 O
. O

Learning O
entity O
and O
relation O
embeddings O
for O
knowledge O
graph O
completion O
. O

In O
AAAI O
, O
pages O
2181–2187 O
. O

Zhouhan O
Lin O
, O
Minwei O
Feng O
, O
C O
´ O
ıcero O
Nogueira O
dos O
Santos O
, O
Mo O
Yu O
, O
Bing O
Xiang O
, O
Bowen O
Zhou O
, O
and O
Yoshua O
Bengio O
. O

2017 O
. O

A O
structured O
self O
- O
attentive O
sentence O
embedding O
. O

In O
ICLR O
. O

Dayiheng O
Liu O
, O
Yeyun O
Gong O
, O
Jie O
Fu O
, O
Yu O
Yan O
, O
Jiusheng O
Chen O
, O
Daxin O
Jiang O
, O
Jiancheng O
Lv O
, O
and O
Nan O
Duan O
. O
2020a O
. O

Rikinet O
: O
Reading O
wikipedia O
pages O
for O
natural O
question O
answering O
. O

In O
ACL O
, O
pages O
6762–6771 O
. O

Weijie O
Liu O
, O
Peng O
Zhou O
, O
Zhe O
Zhao O
, O
Zhiruo O
Wang O
, O
Qi O
Ju O
, O
Haotang O
Deng O
, O
and O
Ping O
Wang O
. O

2020b O
. O

K B-MethodName
- I-MethodName
BERT I-MethodName
: O
enabling O
language O
representation O
with O
knowledge O
graph O
. O

In O
AAAI O
, O
pages O
2901–2908 O
. O

Xiaodong O
Liu O
, O
Pengcheng O
He O
, O
Weizhu O
Chen O
, O
and O
Jianfeng O
Gao O
. O
2019a O
. O

Multi O
- O
task O
deep O
neural O
networks O
for O
natural O
language O
understanding O
. O

In O
ACL O
, O
pages O
4487–4496.5891Yinhan O
Liu O
, O
Myle O
Ott O
, O
Naman O
Goyal O
, O
Jingfei O
Du O
, O
Mandar O
Joshi O
, O
Danqi O
Chen O
, O
Omer O
Levy O
, O
Mike O
Lewis O
, O
Luke O
Zettlemoyer O
, O
and O
Veselin O
Stoyanov O
. O

2019b O
. O
Roberta O
: O

A O
robustly O
optimized O
BERT O
pretraining O
approach O
. O

CoRR O
, O
abs/1907.11692 O
. O

Tom´as O
Mikolov O
, O
Kai O
Chen O
, O
Greg O
Corrado O
, O
and O
Jeffrey O
Dean O
. O

2013a O
. O

Efﬁcient O
estimation O
of O
word O
representations O
in O
vector O
space O
. O

In O
ICLR O
. O

Tom´as O
Mikolov O
, O
Ilya O
Sutskever O
, O
Kai O
Chen O
, O
Gregory O
S. O
Corrado O
, O
and O
Jeffrey O
Dean O
. O

2013b O
. O

Distributed O
representations O
of O
words O
and O
phrases O
and O
their O
compositionality O
. O

In O
NIPS O
, O
pages O
3111–3119 O
. O

Guoshun O
Nan O
, O
Zhijiang O
Guo O
, O
Ivan O
Sekulic O
, O
and O
Wei O
Lu O
. O
2020 O
. O

Reasoning O
with O
latent O
structure O
reﬁnement O
for O
document O
- O
level O
relation O
extraction O
. O

In O
ACL O
, O
pages O
1546–1557 O
. O

Dat O
Quoc O
Nguyen O
, O
Kairit O
Sirts O
, O
Lizhen O
Qu O
, O
and O
Mark O
Johnson O
. O

2016 O
. O

Neighborhood O
mixture O
model O
for O
knowledge O
base O
completion O
. O

In O
CoNLL O
, O
pages O
40 O
– O
50 O
. O

Lawrence O
Page O
, O
Sergey O
Brin O
, O
Rajeev O
Motwani O
, O
and O
Terry O
Winograd O
. O
1999 O
. O

The O
pagerank O
citation O
ranking O
: O
Bringing O
order O
to O
the O
web O
. O

Technical O
Report O
1999 O
- O
66 O
, O
Stanford O
InfoLab O
. O

Yifan O
Peng O
, O
Shankai O
Yan O
, O
and O
Zhiyong O
Lu O
. O

2019 O
. O

Transfer O
learning O
in O
biomedical O
natural O
language O
processing O
: O
An O
evaluation O
of O
BERT O
and O
elmo O
on O
ten O
benchmarking O
datasets O
. O

In O
BioNLP O
, O
pages O
58–65 O
. O

Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christopher O
D. O
Manning O
. O

2014 O
. O

Glove O
: O
Global O
vectors O
for O
word O
representation O
. O

In O
EMNLP O
, O
pages O
1532–1543 O
. O
Matthew O
E. O
Peters O
, O
Mark O
Neumann O
, O
Robert B-MethodName
L. O
Logan O
IV O
, O
Roy O
Schwartz O
, O
Vidur O
Joshi O
, O
Sameer O
Singh O
, O
and O
Noah O
A. O
Smith O
. O

2019 O
. O

Knowledge O
enhanced O
contextual O
word O
representations O
. O

In O
EMNLP O
, O
pages O
43–54 O
. O

Matthew O
E. O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O

2018 O
. O

Deep O
contextualized O
word O
representations O
. O

In O
NAACL O
, O
pages O
2227–2237 O
. O

Xipeng O
Qiu O
, O
Tianxiang O
Sun O
, O
Yige O
Xu O
, O
Yunfan O
Shao O
, O
Ning O
Dai O
, O
and O
Xuanjing O
Huang O
. O
2020 O
. O

Pre O
- O
trained O
models O
for O
natural O
language O
processing O
: O
A O
survey O
. O

CoRR O
, O
abs/2003.08271 O
. O

Maya O
Rotmensch O
, O
Yoni O
Halpern O
, O
Abdulhakim O
Tlimat O
, O
Steven O
Horng O
, O
and O
David O
Sontag O
. O
2017 O
. O

Learning O
a O
health O
knowledge O
graph O
from O
electronic O
medical O
records O
. O

Scientiﬁc O
reports O
, O
7(1):1–11 O
. O

Michael O
Sejr O
Schlichtkrull O
, O
Thomas O
N. O
Kipf O
, O
Peter O
Bloem O
, O
Rianne O
van O
den O
Berg O
, O
Ivan O
Titov O
, O
and O
Max O
Welling O
. O

2018 O
. O

Modeling O
relational O
data O
with O
graph O
convolutional O
networks O
. O

In O
ESWC O
, O
pages O
593–607.Tianxiang O
Sun O
, O
Yunfan O
Shao O
, O
Xipeng O
Qiu O
, O
Qipeng O
Guo O
, O
Yaru O
Hu O
, O
Xuanjing O
Huang O
, O
and O
Zheng O
Zhang O
. O

2020 O
. O

Colake O
: O
Contextualized O
language O
and O
knowledge O
embedding O
. O

In O
COLING O
, O
pages O
3660–3670 O
. O
Joseph O
P. O
Turian O
, O
Lev O
- O
Arie O
Ratinov O
, O
and O
Yoshua O
Bengio O
. O

2010 O
. O

Word O
representations O
: O
A O
simple O
and O
general O
method O
for O
semi O
- O
supervised O
learning O
. O

In O
ACL O
, O
pages O
384–394 O
. O
Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N. O
Gomez O
, O
Lukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
NIPS O
, O
pages O
5998–6008 O
. O

Xiaoyan O
Wang O
, O
Pavan O
Kapanipathi O
, O
Ryan O
Musa O
, O
Mo O
Yu O
, O
Kartik O
Talamadupula O
, O
Ibrahim O
Abdelaziz O
, O
Maria O
Chang O
, O
Achille O
Fokoue O
, O
Bassem O
Makni O
, O
Nicholas O
Mattei O
, O
and O
Michael O
Witbrock O
. O
2019a O
. O

Improving O
natural O
language O
inference O
using O
external O
knowledge O
in O
the O
science O
questions O
domain O
. O

In O
AAAI O
, O
pages O
7208–7215 O
. O

Xiaozhi O
Wang O
, O
Tianyu O
Gao O
, O
Zhaocheng O
Zhu O
, O
Zhiyuan O
Liu O
, O
Juanzi O
Li O
, O
and O
Jian O
Tang O
. O

2019b O
. O

KEPLER O
: O
A O
uniﬁed O
model O
for O
knowledge O
embedding O
and O
pre O
- O
trained O
language O
representation O
. O

CoRR O
, O
abs/1911.06136 O
. O

William O
E O
Winkler O
. O
1990 O
. O

String O
comparator O
metrics O
and O
enhanced O
decision O
rules O
in O
the O
fellegi O
- O
sunter O
model O
of O
record O
linkage O
. O

Liang O
Xu O
, O
Xuanwei O
Zhang O
, O
and O
Qianqian O
Dong O
. O
2020 O
. O

Cluecorpus2020 O
: O

A O
large O
- O
scale O
chinese O
corpus O
for O
pre O
- O
training O
language O
model O
. O

CoRR O
, O
abs/2003.01355 O
. O

Zhilin O
Yang O
, O
Zihang O
Dai O
, O
Yiming O
Yang O
, O
Jaime O
G. O
Carbonell O
, O
Ruslan O
Salakhutdinov O
, O
and O
Quoc O
V O
. O

Le O
. O
2019 O
. O

Xlnet O
: O

Generalized O
autoregressive O
pretraining O
for O
language O
understanding O
. O

In O
NIPS O
, O
pages O
5754 O
– O
5764 O
. O

Denghui O
Zhang O
, O
Zixuan O
Yuan O
, O
Yanchi O
Liu O
, O
Zuohui O
Fu O
, O
Fuzhen O
Zhuang O
, O
Pengyang O
Wang O
, O
Haifeng O
Chen O
, O
and O
Hui O
Xiong O
. O
2020a O
. O

E B-MethodName
- I-MethodName
BERT I-MethodName
: O
A O
phrase O
and O
product O
knowledge O
enhanced O
language O
model O
for O
ecommerce O
. O

CoRR O
, O
abs/2009.02835 O
. O

Ningyu O
Zhang O
, O
Qianghuai O
Jia O
, O
Kangping O
Yin O
, O
Liang O
Dong O
, O
Feng O
Gao O
, O
and O
Nengwei O
Hua O
. O
2020b O
. O

Conceptualized O
representation O
learning O
for O
chinese O
biomedical O
text O
mining O
. O

CoRR O
, O
abs/2008.10813 O
. O

Sheng O
Zhang O
, O
Xin O
Zhang O
, O
Hui O
Wang O
, O
Jiajun O
Cheng O
, O
Pei O
Li O
, O
and O
Zhaoyun O
Ding O
. O

2017 O
. O

Chinese O
medical O
question O
answer O
matching O
using O
end O
- O
to O
- O
end O
characterlevel O
multi O
- O
scale O
cnns O
. O

Applied O
Sciences O
, O
7(8):767 O
. O

Zhengyan O
Zhang O
, O
Xu O
Han O
, O
Zhiyuan O
Liu O
, O
Xin O
Jiang O
, O
Maosong O
Sun O
, O
and O
Qun O
Liu O
. O
2019 O
. O

ERNIE O
: O
enhanced O
language O
representation O
with O
informative O
entities O
. O

In O
ACL O
, O
pages O
1441–1451.5892A O

Data O
Source O
A.1 O
Pre O
- O
training O
Data O
A.1.1 O
Training O
Corpora O
The O
pre O
- O
training O
corpora O
is O
crawled O
from O
DXY O
BBS O
( O
Bulletin O
Board O
System)9 O
, O
which O
is O
a O
very O
popular O
Chinese O
social O
network O
for O
doctors O
, O
medical O
institutions O
, O
life O
scientists O
, O
and O
medical O
practitioners O
. O

The O
BBS O
has O
more O
than O
30 O
channels O
, O
which O
contains O
18 O
forums O
and O
130 O
ﬁne O
- O
grained O
groups O
, O
covering O
most O
of O
the O
medical O
domains O
. O

For O
our O
pre O
- O
training O
purpose O
, O
we O
crawl O
texts O
from O
channels O
about O
clinical O
medicine O
, O
pharmacology O
, O
public O
health O
and O
consulting O
. O

For O
text O
pre O
- O
processing O
, O
we O
mainly O
follow O
the O
methods O
of O
( O
Xu O
et O
al O
. O
, O
2020 O
) O
. O

Additionally O
, O
( O
1 O
) O
we O
remove O
all O
URLs O
, O
HTML O
tags O
, O
e O
- O
mail O
addresses O
, O
and O
all O
tokens O
except O
characters O
, O
digits O
, O
and O
punctuation O
( O
2 O
) O
all O
documents O
shorter O
than O
256 O
are O
discard O
, O
while O
documents O
longer O
than O
512 O
are O
cut O
into O
shorter O
text O
segments O
. O

A.1.2 O
Knowledge O
Graph O
The O
DXY O
knowledge O
graph O
is O
construed O
by O
extracting O
structured O
text O
from O
DXY O
website10 O
, O
which O
includes O
information O
of O
diseases O
, O
drugs O
and O
hospitals O
edited O
by O
certiﬁed O
medical O
experts O
, O
thus O
the O
quality O
of O
the O
KG O
is O
guaranteed O
. O

The O
KG O
is O
mainly O
disease O
- O
centered O
, O
including O
totally O
3,764,711 O
triples O
, O
152.508 O
unique O
entities O
, O
and O
44 O
types O
of O
relations O
. O

The O
details O
of O
Symptom O
- O
InChinese O
from O
OpenKG O
is O
available11 O
. O

We O
ﬁnally O
get O
26 O
types O
of O
entities O
, O
274,163 O
unique O
entities O
, O
56 O
types O
of O
relations O
, O
and O
4,390,726 O
triples O
after O
the O
fusion O
of O
the O
two O
KGs O
. O

A.2 O
Task O
Data O
We O
choose O
the O
four O
large O
- O
scale O
datasets O
in O
ChineseBlue O
tasks O
( O
Zhang O
et O
al O
. O
, O
2020b O
) O
while O
others O
are O
ignored O
due O
to O
the O
limitation O
of O
datasets O
size O
, O
which O
are O
cMedQANER B-DatasetName
, O
cMedQQ B-DatasetName
, O
cMedQNLI B-DatasetName
and O
cMedQA B-DatasetName
. O

WebMedQA B-DatasetName
( O
He O
et O
al O
. O
, O
2019 O
) O
is O
a O
real O
- O
world O
Chinese O
medical O
question O
answering O
dataset O
and O
CHIP O
- O
RE O
dataset O
are O
collected O
from O
online O
health O
consultancy O
websites O
. O

Note O
that O
since O
both O
the O
WebMedQA B-DatasetName
and O
cMedQA B-DatasetName
datasets O
are O
very O
large O
while O
we O
have O
many O
baselines O
to O
be O
compared O
, O
we O
randomly O
sample O
the O
ofﬁcial O
training O
set O
, O
development O
set O
and O
test O
set O
respectively O
9https://www.dxy.cn/bbs/newweb/pc/home O
10https://portal.dxy.cn/ O
11http://openkg.cn/dataset/ O
symptom O
- O
in O
- O
chineseto O
form O
their O
corresponding O
smaller O
version O
for O
experiments O
. O

DXY B-DatasetName
- I-DatasetName
NER I-DatasetName
and O
DXY B-DatasetName
- I-DatasetName
RE I-DatasetName
are O
datasets O
from O
real O
medical O
application O
scenarios O
provided O
by O
a O
prestigious O
Chinese O
medical O
company O
. O

The O
DXY B-DatasetName
- I-DatasetName
NER I-DatasetName
contains O
22 O
unique O
entity O
types O
and O
56 O
relation O
types O
in O
the O
DXY O
- O
RE O
. O

These O
two O
datasets O
are O
collected O
from O
the O
medical O
forum O
of O
DXY O
and O
books O
in O
the O
medical O
domain O
. O

Annotators O
are O
selected O
from O
junior O
and O
senior O
students O
with O
clinical O
medical O
background O
. O

In O
the O
process O
of O
quality O
control O
, O
the O
two O
datasets O
are O
annotated O
twice O
by O
different O
groups O
of O
annotators O
. O

An O
expert O
with O
medical O
background O
performs O
quality O
check O
manually O
again O
when O
annotated O
results O
are O
inconsistent O
, O
whereas O
perform O
sampling O
quality O
check O
when O
results O
are O
consistent O
. O

Table O
5 O
shows O
the O
datasets O
size O
of O
our O
experiments O
. O

B O
Model O
Settings O
and O
Training O
Details O
Hyper O
- O
parameters O
. O

d1=768,d2=200,K=10, O
= O
10,1=2,2=4 O
. O

Model O
Details O
. O

We O
align O
the O
all O
mention O
- O
spans O
to O
the O
entity O
in O
KG O
by O
exact O
match O
for O
comparison O
purpose O
with O
ENIRE B-MethodName
- I-MethodName
THU I-MethodName
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
. O

The O
negative O
sampling O
function O
is O
deﬁned O
asQ(ei O
m O
) O

= O
teim O
Ceim O
, O
whereCeimis O
the O
sum O
of O
frequency O
of O
all O
mentions O
with O
the O
same O
type O
of O
ei O
m. O

The O
Mention O
- O
neighbor O
Hybrid O
Attention O
module O
is O
inserted O
after O
the O
tenth O
transformer O
encoder O
layer O
to O
compare O
with O
KnowBERT B-MethodName
( O
Peters O
et O
al O
. O
, O
2019 O
) O
, O
while O
we O
perform O
the O
Mention O
- O
neighbor O
Context O
Modeling O
based O
on O
the O
output O
of O
BERT B-MethodName
encoder O
. O

We O
use O
all O
the O
base O
- O
version O
PLMs O
in O
the O
experiments O
. O

The O
size O
of O
SMedBERT B-MethodName
is O
474 O
MB O
while O
393 O
MB O
of O
that O
are O
components O
of O
BERT B-MethodName
, O
and O
the O
added O
81 O
MB O
is O
mostly O
of O
the O
KG O
embedding O
. O

Results O
are O
presented O
in O
average O
with O
5 O
random O
runs O
with O
different O
random O
seeds O
and O
the O
same O
hyperparameters O
. O

Training O
Procedure O
. O

We O
strictly O
follow O
the O
originally O
pre O
- O
training O
process O
and O
parameter O
setting O
of O
other O
KEPLMs O
. O

We O
only O
adapt O
their O
publicly O
available O
code O
from O
English O
to O
Chinese O
and O
use O
the O
knowledge O
embedding O
trained O
on O
our O
medical O
KG O
. O

To O
have O
a O
fair O
comparison O
, O
the O
pre O
- O
training O
processing O
of O
SMedBERT B-MethodName
is O
mostly O
set O
based O
on O
ENIRE B-MethodName
- I-MethodName
THU I-MethodName
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
without O
layerspecial O
learning O
rates O
in O
KnowBERT B-MethodName
( O
Peters O
et O
al O
. O
, O
2019 O
) O
. O

We O
only O
pre O
- O
train O
SMedBERT B-MethodName
on O
the O
collected O
medical O
data O
for O
1 O
epoch O
. O

In O
pre O
- O
training5893The O
Dataset O
Size O
in O
Our O
Experiments O
Dataset O
Train O
Dev O
Test O
Task O
Metric O
cMedQANER B-DatasetName
( O
Zhang O
et O
al O
. O
, O
2020b)1,673 O
175 O
215 O
NER O
F1 O
cMedQQ O
( O
Zhang O
et O
al O
. O
, O
2020b)16,071 O
1,793 O
1,935 O
QM O
F1 O
cMedQNLI B-DatasetName
( O
Zhang O
et O
al O
. O
, O
2020b)80,950 O
9,065 O
9,969 O
NLI O
F1 O
cMedQA B-DatasetName
( O
Zhang O
et O
al O
. O
, O

2017)186,771 O
46,600 O
46,600 O
QA O
Acc@1 O
WebMedQA O
( O
He O
et O
al O
. O
, O
2019)252,850 O
31,605 O
31,655 O
QA O
Acc@1 O
CHIP O
- O
RE43,649 O
- O
10,622 O
RE O
F1 O
DXY O
- O
NER O
34,224 O
8,576 O
8,592 O
NER O
F1 O
DXY O
- O
RE O
141,696 O
35,456 O
35,794 O
RE O
F1 O
CHIP O
- O
RE O
dataset O
is O
released O
in O
CHIP O
2020 O
. O

( O
http://cips-chip.org.cn/2020/eval2 O
) O
Table O
5 O
: O
The O
statistical O
data O
and O
metric O
of O
eight O
datasets O
used O
in O
our O
SMedBERT B-MethodName
model O
. O
process O
, O
the O
learning B-MetricName
rate I-MetricName
is O
set O
to O
5e 5and B-MetricValue
batch B-MetricName
size I-MetricName
is O
512 B-MetricValue
with O
the O
max B-MetricName
sequence I-MetricName
length I-MetricName
is O
512 B-MetricValue
. O

For O
ﬁne O
- O
tuning O
, O
we O
ﬁnd O
the O
following O
ranges O
of O
possible O
values O
work O
well O
, O
i.e. O
, O
batch B-MetricName
size I-MetricName
is O
f8,16 B-MetricValue
g O
, O
learning B-MetricName
rate I-MetricName
( O
AdamW O
) O
is O
f2e 5,4e 5,6e 5gand B-MetricValue
the O
number O
of O
epochs B-MetricName
is O
f2,3,4 B-MetricValue
g. O
Pre O
- O
training O
SMedBERT B-MethodName
takes O
about O
36 O
hours O
per O
epoch O
on O
2 O
NVIDIA O
GeForce O
RTX O
3090 O
GPUs O
. O

C O
Data O
and O
Embedding O
of O
Unsupervised O
Semantic O
Similarity O
Since O
the O
KGs O
used O
in O
this O
paper O
is O
a O
directed O
graph O
, O
we O
ﬁrst O
transform O
the O
directed O
” O
I÷sû O
” O
( O
equivalence O
relations O
) O
pairs O
to O
undirected O
pairs O
and O
discard O
the O
duplicated O
pairs O
. O

For O
each O
positive O
pairs O
, O
we O
use O
head O
and O
tail O
as O
query O
respectively O
and O
sample O
the O
negative O
candidates O
based O
on O
the O
other O
. O

Speciﬁcally O
, O
we O
randomly O
select O
19 O
negative O
entities O
with O
the O
same O
type O
and O
has O
a O
JaroWinkle O
similarity O
( O
Winkler O
, O
1990 O
) O
bigger O
0.6 O
with O
the O
ground O
- O
truth O
entity O
. O

We O
select O
from O
all O
samples O
inDataset-1 O
with O
positive O
pairs O
that O
the O
neighbours O
sets O
of O
head O
and O
tail O
entity O
have O
Jaccard O
Index O
( O
Jaccard O
, O
1912 O
) O
no O
less O
than O
0.75 O
and O
at O
least O
3 O
common O
element O
to O
construct O
the O
Dataset-2 O
. O

For O
Dataset-3 O
, O
we O
count O
the O
frequency O
of O
all O
entity O
mentions O
in O
pretraining O
corpora O
, O
and O
treat O
mentions O
with O
frequency O
no O
more O
than O
200 O
as O
low O
- O
frequency O
mentions O
. O

Classic O
Word O
Representation O
Embedding O
: O
We O
train O
the O
character O
- O
level O
and O
word O
- O
level O
embedding O
using O
SGNS O
( O
Mikolov O
et O
al O
. O
, O
2013a O
) O
and O
GLOVE O

( O
Pennington O
et O
al O
. O
, O
2014 O
) O
model O
respectively O
on O
our O
medical O
corpora O
with O
open O
- O
source O
toolkits12 O
. O

We O
average O
the O
character O
embedding O
for O
all O
tokens O
in O
the O
mention O
to O
get O
the O
character O
- O
level O
representation O
. O

However O
, O
since O
some O
mentions O
are O
very O
rare O
in O
the O
corpora O
for O
word O
- O
level O
representation O
, O
we O
use O
the O
character O
- O
level O
representation O
as O
their O
word O
- O
level O
representation O
. O

BERT B-MethodName
- O
like O
Representation O
Embedding O
: O
We O
extract O
the O
token O
hidden O
features O
of O
the O
last O
layer O
and O
average O
the O
representations O
of O
the O
input O
tokens O
except O
[ O
CLS O
] O
and O
[ O
SEP O
] O
tag O
, O
to O
get O
a O
vector O
for O
each O
entity O
. O

Similarity O
Measure O
: O
We O
try O
using O
the O
inverse O
of O
L2 O
- O
distance O
and O
cosine O
similarity O
as O
measurement O
, O
and O
we O
ﬁnd O
that O
cosine O
similarity O
always O
perform O
better O
. O

Hence O
, O
we O
report O
all O
experiment O
results O
under O
the O
cosine O
similarity O
metric O
. O

12SGNS O
: O
https://github.com/JuGyang/ O
word2vec O
- O
SGNS O
. O

Glove O
: O
https://github.com/stanfordnlp/ O
GloVe O

