Proceedings O
of O
the O
60th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
5347 O
- O
5363 O
May O
22 O
- O
27 O
, O
2022 O
c O

 O
2022 O
Association O
for O
Computational O
Linguistics O
KinyaBERT B-MethodName
: O
a O
Morphology O
- O
aware O
Kinyarwanda O
Language O
Model O
Antoine O
Nzeyimana O
University O
of O
Massachusetts O
Amherst O
anthonzeyi@gmail.comAndre O
Niyongabo O
Rubungo O
Polytechnic O
University O
of O
Catalonia O
niyongabor.andre@gmail.com O
Abstract O
Pre O
- O
trained O
language O
models O
such O
as O
BERT B-MethodName
have O
been O
successful O
at O
tackling O
many O
natural O
language O
processing O
tasks O
. O

However O
, O
the O
unsupervised O
sub O
- O
word O
tokenization O
methods O
commonly O
used O
in O
these O
models O
( O
e.g. O
, O
byte O
- O
pair O
encoding O
– O
BPE O
) O
are O
sub O
- O
optimal O
at O
handling O
morphologically O
rich O
languages O
. O

Even O
given O
a O
morphological O
analyzer O
, O
naive O
sequencing O
of O
morphemes O
into O
a O
standard O
BERT B-MethodName
architecture O
is O
inefﬁcient O
at O
capturing O
morphological O
compositionality O
and O
expressing O
word O
- O
relative O
syntactic O
regularities O
. O

We O
address O
these O
challenges O
by O
proposing O
a O
simple O
yet O
effective O
twotier O
BERT B-MethodName
architecture O
that O
leverages O
a O
morphological O
analyzer O
and O
explicitly O
represents O
morphological O
compositionality O
. O

Despite O
the O
success O
of O
BERT B-MethodName
, O
most O
of O
its O
evaluations O
have O
been O
conducted O
on O
high O
- O
resource O
languages O
, O
obscuring O
its O
applicability O
on O
low O
- O
resource O
languages O
. O

We O
evaluate O
our O
proposed O
method O
on O
the O
low O
- O
resource O
morphologically O
rich O
Kinyarwanda O
language O
, O
naming O
the O
proposed O
model O
architecture O
KinyaBERT B-MethodName
. O

A O
robust O
set O
of O
experimental O
results O
reveal O
that O
KinyaBERT B-MethodName
outperforms O
solid O
baselines O
by O
2 O
% O
in O
F1 O
score O
on O
a O
named O
entity O
recognition O
task O
and O
by O
4.3 O
% O
in O
average O
score O
of O
a O
machine O
- O
translated O
GLUE O
benchmark O
. O

KinyaBERT B-MethodName
ﬁne O
- O
tuning O
has O
better O
convergence O
and O
achieves O
more O
robust O
results O
on O
multiple O
tasks O
even O
in O
the O
presence O
of O
translation O
noise.1 O
1 O
Introduction O
Recent O
advances O
in O
natural O
language O
processing O
( O
NLP O
) O
through O
deep O
learning O
have O
been O
largely O
enabled O
by O
vector O
representations O
( O
or O
embeddings O
) O
learned O
through O
language O
model O
pre O
- O
training O
( O
Bengio O
et O
al O
. O
, O
2003 O
; O
Mikolov O
et O
al O
. O
, O
2013 O
; O
Pennington O
et O
al O
. O
, O
2014 O
; O
Bojanowski O
et O

al O
. O
, O
2017 O
; O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

Language O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
are O
pre1Code O
and O
data O
are O
released O
at O
https://github O
. O
com O
/ O
anzeyimana O
/ O
kinyabert O
- O
acl2022trained O
on O
large O
text O
corpora O
and O
then O
ﬁne O
- O
tuned O
on O
downstream O
tasks O
, O
resulting O
in O
better O
performance O
on O
many O
NLP O
tasks O
. O

Despite O
attempts O
to O
make O
multilingual O
BERT B-MethodName
models O
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
, O
research O
has O
shown O
that O
models O
pre O
- O
trained O
on O
high O
quality O
monolingual O
corpora O
outperform O
multilingual O
models O
pre O
- O
trained O
on O
large O
Internet O
data O
( O
Scheible O
et O
al O
. O
, O
2020 O
; O
Virtanen O
et O

al O
. O
, O
2019 O
) O
. O

This O
has O
motivated O
many O
researchers O
to O
pretrain O
BERT B-MethodName
models O
on O
individual O
languages O
rather O
than O
adopting O
the O
“ O
language O
- O
agnostic O
” O
multilingual O
models O
. O

This O
work O
is O
partly O
motivated O
by O
the O
same O
ﬁndings O
, O
but O
also O
proposes O
an O
adaptation O
of O
the O
BERT B-MethodName
architecture O
to O
address O
representational O
challenges O
that O
are O
speciﬁc O
to O
morphologically O
rich O
languages O
such O
as O
Kinyarwanda O
. O

In O
order O
to O
handle O
rare O
words O
and O
reduce O
the O
vocabulary O
size O
, O
BERT B-MethodName
- O
like O
models O
use O
statistical O
sub O
- O
word O
tokenization O
algorithms O
such O
as O
byte O
pair O
encoding O
( O
BPE O
) O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
. O

While O
these O
techniques O
have O
been O
widely O
used O
in O
language O
modeling O
and O
machine O
translation O
, O
they O
are O
not O
optimal O
for O
morphologically O
rich O
languages O
( O
Klein O
and O
Tsarfaty O
, O
2020 O
) O
. O

In O
fact O
, O
sub O
- O
word O
tokenization O
methods O
that O
are O
solely O
based O
on O
surface O
forms O
, O
including O
BPE O
and O
character O
- O
based O
models O
, O
can O
not O
capture O
all O
morphological O
details O
. O

This O
is O
due O
to O
morphological O
alternations O
( O
Muhirwe O
, O
2007 O
) O
and O
non O
- O
concatenative O
morphology O
( O
McCarthy O
, O
1981 O
) O
that O
are O
often O
exhibited O
by O
morphologically O
rich O
languages O
. O

For O
example O
, O
as O
shown O
in O
Table O
1 O
, O
a O
BPE O
model O
trained O
on O
390 O
million O
tokens O
of O
Kinyarwanda O
text O
can O
not O
extract O
the O
true O
sub O
- O
word O
lexical O
units O
( O
i.e. O
morphemes O
) O
for O
the O
given O
words O
. O

This O
work O
addresses O
the O
above O
problem O
by O
proposing O
a O
language O
model O
architecture O
that O
explicitly O
represents O
most O
of O
the O
input O
words O
with O
morphological O
parses O
produced O
by O
a O
morphological O
analyzer O
. O

In O
this O
architecture O
BPE O
is O
only O
used O
to O
handle O
words O
which O
can O
not O
be O
directly O
decomposed O
by O
the O
morphological O
analyzer O
such O
as O
misspellings,5347Word O
Morphemes O
Monolingual O
BPE O
Multilingual O
BPE O
twagezeyo O
‘ O
we O
arrived O
there O
’ O
tu O
. O

a O
. O
ger O
. O

ye O
. O

yo O
twag O
. O

ezeyo O
_ O
twa O
. O

ge O
. O

ze O
. O

yo O
ndabyizeye O
‘ O
I O
hope O
so O
’ O

n O
. O

ra O
. O

bi O
. O

izer O
. O

ye O
ndaby O
. O

izeye O
_ O
ndab O
. O

yiz O
. O

eye O
umwarimu O
‘ O
teacher O
’ O
u O
. O

mu O
. O

arimu O
umwarimu O

_ O
um O
. O

wari O
. O

mu O
Table O
1 O
: O
Comparison O
between O
morphemes O
and O
BPE O
- O
produced O
sub O
- O
word O
tokens O
. O

Stems O
are O
underlined O
. O

proper O
names O
and O
foreign O
language O
words O
. O

Given O
the O
output O
of O
a O
morphological O
analyzer O
, O
a O
second O
challenge O
is O
in O
how O
to O
incorporate O
the O
produced O
morphemes O
into O
the O
model O
. O

One O
naive O
approach O
is O
to O
feed O
the O
produced O
morphemes O
to O
a O
standard O
transformer O
encoder O
as O
a O
single O
monolithic O
sequence O
. O

This O
approach O
is O
used O
by O
Mohseni O
and O
Tebbifakhr O
( O
2019 O
) O
. O

One O
problem O
with O
this O
method O
is O
that O
mixing O
sub O
- O
word O
information O
and O
sentencelevel O
tokens O
in O
a O
single O
sequence O
does O
not O
encourage O
the O
model O
to O
learn O
the O
actual O
morphological O
compositionality O
and O
express O
word O
- O
relative O
syntactic O
regularities O
. O

We O
address O
these O
issues O
by O
proposing O
a O
simple O
yet O
effective O
two O
- O
tier O
transformer O
encoder O
architecture O
. O

The O
ﬁrst O
tier O
encodes O
morphological O
information O
, O
which O
is O
then O
transferred O
to O
the O
second O
tier O
to O
encode O
sentence O
level O
information O
. O

We O
call O
this O
new O
model O
architecture O
KinyaBERT B-MethodName
because O
it O
uses O
BERT B-MethodName
’s O
masked O
language O
model O
objective O
for O
pre O
- O
training O
and O
is O
evaluated O
on O
the O
morphologically O
rich O
Kinyarwanda O
language O
. O

This O
work O
also O
represents O
progress O
in O
low O
resource O
NLP O
. O

Advances O
in O
human O
language O
technology O
are O
most O
often O
evaluated O
on O
the O
main O
languages O
spoken O
by O
major O
economic O
powers O
such O
as O
English O
, O
Chinese O
and O
European O
languages O
. O

This O
has O
exacerbated O
the O
language O
technology O
divide O
between O
the O
highly O
resourced O
languages O
and O
the O
underrepresented O
languages O
. O

It O
also O
hinders O
progress O
in O
NLP O
research O
because O
new O
techniques O
are O
mostly O
evaluated O
on O
the O
mainstream O
languages O
and O
some O
NLP O
advances O
become O
less O
informed O
of O
the O
diversity O
of O
the O
linguistic O
phenomena O
( O
Bender O
, O
2019 O
) O
. O

Specifically O
, O
this O
work O
provides O
the O
following O
research O
contributions O
: O
•A O
simple O
yet O
effective O
two O
- O
tier O
BERT B-MethodName
architecture O
for O
representing O
morphologically O
rich O
languages O
. O

•New O
evaluation O
datasets O
for O
Kinyarwanda O
language O
including O
a O
machine O
- O
translated O
subset O
of O
the O
GLUE B-DatasetName
benchmark O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
and O
a O
news B-DatasetName
categorization I-DatasetName
dataset.•Experimental O
results O
which O
set O
a O
benchmark O
for O
future O
studies O
on O
Kinyarwanda O
language O
understanding O
, O
and O
on O
using O
machinetranslated O
versions O
of O
the O
GLUE B-DatasetName
benchmark O
. O

•Code O
and O
datasets O
are O
made O
publicly O
available O
for O
reproducibility1 O
. O

2 O
Morphology O
- O
aware O
Language O
Model O
Our O
modeling O
objective O
is O
to O
be O
able O
to O
express O
morphological O
compositionality O
in O
a O
Transformerbased O
( O
Vaswani O
et O

al O
. O
, O
2017 O
) O
language O
model O
. O

For O
morphologically O
rich O
languages O
such O
as O
Kinyarwanda O
, O
a O
set O
of O
morphemes O
( O
typically O
a O
stem O
and O
a O
set O
of O
functional O
afﬁxes O
) O
combine O
to O
produce O
a O
word O
with O
a O
given O
surface O
form O
. O

This O
requires O
an O
alternative O
to O
the O
ubiquitous O
BPE O
tokenization O
, O
through O
which O
exact O
sub O
- O
word O
lexical O
units O
( O
i.e. O
morphemes O
) O
are O
used O
. O

For O
this O
purpose O
, O
we O
use O
a O
morphological O
analyzer O
which O
takes O
a O
sentence O
as O
input O
and O
, O
for O
every O
word O
, O
produces O
a O
stem O
, O
zero O
or O
more O
afﬁxes O
and O
assigns O
a O
part O
of O
speech O
( O
POS O
) O

tag O
to O
each O
word O
. O

This O
section O
describes O
how O
this O
morphological O
information O
is O
obtained O
and O
then O
integrated O
in O
a O
two O
- O
tier O
transformer O
architecture O
( O
Figure O
1 O
) O
to O
learn O
morphology O
- O
aware O
input O
representations O
. O

2.1 O
Morphological O
Analysis O
and O
Part O
- O
of O
- O
Speech O
Tagging O
Kinyarwanda O
, O
the O
national O
language O
of O
Rwanda O
, O
is O
one O
of O
the O
major O
Bantu O
languages O
( O
Nurse O
and O
Philippson O
, O
2006 O
) O
spoken O
in O
central O
and O
eastern O
Africa O
. O

Kinyarwanda O
has O
16 O
noun O
classes O
. O

Modiﬁers O
( O
demonstratives O
, O
possessives O
, O
adjectives O
, O
numerals O
) O
carry O
a O
class O
marking O
morpheme O
that O
agrees O
with O
the O
main O
noun O
class O
. O

The O
verbal O
morphology O
( O
Nzeyimana O
, O
2020 O
) O
also O
includes O
subject O
and O
object O
markers O
that O
agree O
with O
the O
class O
of O
the O
subject O
or O
object O
. O

This O
agreement O
therefore O
enables O
users O
of O
the O
language O
to O
approximately O
disambiguate O
referred O
entities O
based O
on O
their O
classes O
. O

We O
leverage O
this O
syntactic O
agreement O
property O
in O
designing O
our O
unsupervised O
POS O
tagger.5348 O
    O
V5 O
    O
tu O

ara O
ha O
mu O
  O
bon O
ye O
NP35 O
  O
John O
  O
Morphological O
Analyser O
  O
John O
twarahamubonye O
biradutangaza O
    O
V9 O
   O
bi O
  O
ra O
  O

tu O
  O
tangar O
  O
y O
aSentence O
/ O
Document O
- O
Level O
Encoder O
  O
Morphology O
Encoder O
Morphology O
Encoder O
Morphology O
Encoder O
John O
bon O
tangar O
  O
( O
We O
were O
surprised O
to O
find O
John O
there O
) O
Figure O
1 O
: O
KinyaBERT B-MethodName
model O
architecture O
: O

Encoding O
of O
the O
sentence O
’ O
John O
twarahamusanze O
biradutangaza O
’ O
( O
We O
were O
surprised O
to O
ﬁnd O
John O
there O
) O
. O

The O
morphological O
analyzer O
produces O
morphemes O
for O
each O
word O
and O
assigns O
a O
POS O
tag O
to O
it O
. O

The O
two O
- O
tier O
transformer O
model O
then O
generates O
contextualized O
embeddings O
( O
blue O
vectors O
at O
the O
top O
) O
. O

The O
redcolored O
embeddings O
correspond O
to O
the O
POS O
tags O
, O
yellow O
is O
for O
the O
stem O
embeddings O
, O
green O
is O
for O
the O
variable O
length O
afﬁxes O
while O
the O
purple O
embeddings O
correspond O
to O
the O
afﬁx O
set O
. O

Our O
morphological O
analyzer O
for O
Kinyarwanda O
was O
built O
following O
ﬁnite O
- O
state O
two O
- O
level O
morphology O
principles O
( O
Koskenniemi O
, O
1983 O
; O
Beesley O
and O
Karttunen O
, O
2000 O
, O
2003 O
) O
. O

For O
every O
inﬂectable O
word O
type O
, O
we O
maintain O
a O
morphotactics O
model O
using O
a O
directed O
acyclic O
graph O
( O
DAG O
) O
that O
represents O
the O
regular O
sequencing O
of O
morphemes O
. O

We O
effectively O
model O
all O
inﬂectable O
word O
types O
in O
Kinyarwanda O
which O
include O
verbals O
, O
nouns O
, O
adjectives O
, O
possessive O
and O
demonstrative O
pronouns O
, O
numerals O
and O
quantiﬁers O
. O

The O
morphological O
analyzer O
also O
includes O
many O
hand O
- O
crafted O
rules O
for O
handling O
morphographemics O
and O
other O
linguistic O
regularities O
of O
the O
Kinyarwanda O
language O
. O

The O
morphological O
analyzer O
was O
independently O
developed O
and O
calibrated O
by O
native O
speakers O
as O
a O
closed O
source O
solution O
before O
the O
current O
work O
on O
language O
modeling O
. O

Similar O
to O
Nzeyimana O
( O
2020 O
) O
, O
we O
use O
a O
classiﬁer O
trained O
on O
a O
stemming O
dataset O
to O
disambiguate O
between O
competing O
outputs O
of O
the O
morphological O
analyzer O
. O

Furthermore O
, O
we O
improve O
the O
disambiguation O
quality O
by O
leveraging O
a O
POS O
tagger O
at O
the O
phrase O
level O
so O
that O
the O
syntactic O
context O
can O
be O
taken O
into O
consideration O
. O

We O
devise O
an O
unsupervised O
part O
of O
speech O
tagging O
algorithm O
which O
we O
explain O
here O
. O

Let O
x= O

( O
x1;x2;x3;:::x O
n)be O
a O
sequence O
of O
tokens O
( O
e.g. O
words O
) O
to O
be O
tagged O
with O
a O
corresponding O
sequence O
of O
tagsy= O
( O
y1;y2;y3;:::yn O
) O
. O

A O
sample O
of O
actual O
POS O
tags O
used O
for O
Kinyarwanda O
is O
given O
in O
Table O
12 O
the O
Appendix O
. O

Using O
Bayes O
’ O
rule O
, O
the O
optimal O
tagsequenceyis O
given O
by O
the O
following O
equation O
: O
y= O
arg O
max O
yP(yjx O
) O
= O
arg O
max O
yP(xjy)P(y O
) O
P(x O
) O
= O
arg O
max O
yP(xjy)P(y)(1 O
) O
A O
standard O
hidden O
Markov O
model O
( O
HMM O
) O
can O
decompose O
the O
result O
of O
Equation O
1 O
using O
ﬁrst O
order O
Markov O
assumption O
and O
independence O
assumptions O
into O
P(xjy O
) O

= O
Qn O
t=1P(xtjyt)and O
P(y O
) O
= O
Qn O
t=1P(ytjyt 1 O
) O
. O

The O
tag O
sequence O
y O
can O
then O
be O
efﬁciently O
decoded O
using O
the O
Viterbi O
algorithm O
( O
Forney O
, O
1973 O
) O
. O

A O
better O
decoding O
strategy O
is O
presented O
below O
. O

Inspired O
by O
Tsuruoka O
and O
Tsujii O
( O
2005 O
) O
, O
we O
devise O
a O
greedy O
heuristic O
for O
decoding O
yusing O
the O
same O
ﬁrst O
order O
Markov O
assumptions O
but O
with O
bidirectional O
decoding O
. O

First O
, O
we O
estimate O
the O
local O
emission O
probabilitiesP(xtjyt)using O
a O
factored O
model O
given O
in O
the O
following O
equation O
: O
P(xtjyt)/~P(xtjyt O
) O
~P(xtjyt O
) O

= O
~Pm(xtjyt)~Pp(xtjyt)~Pa(xtjyt)(2 O
) O

In O
Equation O
2 O
, O
~Pm(xtjyt)corresponds O
to O
the O
probability O
/ O
score O
returned O
by O
a O
morphological O
disambiguation O
classiﬁer O
, O
representing O
the O
uncertainty O
of O
the O
morphology O
of O
xt.~Pp(xtjyt)corresponds O
to O
a O
local O
precedence O
weight O
between O
competing O
POS O
tags O
. O

These O
precedence O
weights O
are O
man-5349ually O
crafted O
through O
qualitative O
evaluation O
( O
See O
Table O
12 O
in O
Appendix O
for O
examples O
) O
. O

~Pa(xtjyt O
) O
quantiﬁes O
the O
local O
neighborhood O
syntactic O
agreement O
between O
Bantu O
class O
markers O
. O

When O
there O
are O
two O
or O
more O
agreeing O
class O
markers O
in O
neighboring O
words O
, O
the O
tagger O
should O
be O
more O
conﬁdent O
of O
the O
agreeing O
parts O
of O
speech O
. O

A O
basic O
agreement O
score O
can O
be O
the O
number O
of O
agreeing O
class O
markers O
within O
a O
window O
of O
seven O
words O
around O
a O
given O
candidate O
xt O
. O

We O
manually O
designed O
a O
more O
elaborate O
set O
of O
agreement O
rules O
and O
their O
weights O
for O
different O
contexts O
. O

Therefore O
, O
the O
actual O
agreement O
score O
~Pa(xtjyt)is O
a O
weighted O
sum O
of O
the O
matched O
agreement O
rules O
. O

Each O
of O
the O
unnormalized O
measures O
~P O
in O
Equation O
2 O
is O
mapped O
to O
the O
[ O
0;1]range O
using O
a O
sigmoid O
function O
(zjzA;zB)given O
in O
Equation O
3 O
, O
wherezis O
the O
score O
of O
the O
measure O
and O
[ O
zA;zB]is O
its O
estimated O
active O
range O
. O
(zjzA;zB O
) O

= O

[ O
1 O
+ O
exp( 8z zA O
zB zA)] 8(3 O
) O
After O
estimating O
the O
local O
emission O
model O
, O
we O
greedily O
decode O
y O
t= O
arg O
maxyt O
~ O
P(ytjx)in O
decreasing O
order O
of O
~P(xtjyt)using O
a O
ﬁrst O
order O
bidirectional O
inference O
of O
~P(ytjx)as O
given O
in O
the O
following O
equation O
: O
~P(ytjx O
) O

= O
8 O
> O
> O
> O
> O
> O
> O
> O
> O
> O
> O
> O
< O
> O
> O
> O
> O
> O
> O
> O
> O
> O
> O
> O
: O
~P(xtjyt)~P(ytjy O
t 1;y O
t+1)~P(y O
t 1jx)~P(y O
t+1jx O
) O
if O
bothy O
t 1andy O
t+1have O
been O
decoded O
; O
~P(xtjyt)~P(ytjy O
t 1)~P(y O
t 1jx O
) O
if O
onlyy O
t 1has O
been O
decoded O
; O
~P(xtjyt)~P(ytjy O
t+1)~P(y O
t+1jx O
) O
if O
onlyy O
t+1has O
been O
decoded O
; O
~P(xtjyt)otherwise O
( O
4 O
) O

The O
ﬁrst O
order O
transition O
measures O
~P(ytjyt 1 O
) O
, O
~P(ytjyt+1)and O
~ O
P(ytjyt 1;yt+1)are O
estimated O
using O
count O
tables O
computed O
over O
the O
entire O
corpus O
by O
aggregating O
local O
emission O
marginals O
~P(yt O
) O
= O
P O
xt O
~ O
P(xt;yt)obtained O
through O
morphological O
analysis O
and O
disambiguation O
. O

2.2 O
Morphology O
Encoding O
The O
overall O
architecture O
of O
our O
model O
is O
depicted O
in O
Figure O
1 O
. O

This O
is O
a O
two O
- O
tier O
transformer O
encoder O
architecture O
made O
of O
a O
token O
- O
level O
morphology O
encoder O
that O
feeds O
into O
a O
sentence O
/ O
document O
- O
level O
encoder O
. O

The O
morphology O
encoder O
is O
made O
of O
a O
small O
transformer O
encoder O
that O
is O
applied O
to O
eachanalyzed O
token O
separately O
in O
order O
to O
extract O
its O
morphological O
features O
. O

The O
extracted O
morphological O
features O
are O
then O
concatenated O
with O
the O
token O
’s O
stem O
embedding O
to O
form O
the O
input O
vector O
fed O
to O
the O
sentence O
/ O
document O
encoder O
. O

The O
sentence O
/ O
document O
encoder O
is O
made O
of O
a O
standard O
transformer O
encoder O
as O
used O
in O
other O
BERT B-MethodName
models O
. O

The O
sentence O
/ O
document O
encoder O
uses O
untied O
position O
encoding O
with O
relative O
bias O
as O
proposed O
in O
Ke O
et O
al O
. O

( O
2020 O
) O
. O

The O
input O
to O
the O
morphology O
encoder O
is O
a O
set O
of O
embedding O
vectors O
, O
three O
vectors O
relating O
to O
the O
part O
of O
speech O
, O
one O
for O
the O
stem O
and O
one O
for O
each O
afﬁx O
when O
available O
. O

The O
transformer O
encoder O
operation O
is O
applied O
to O
these O
embedding O
vectors O
without O
any O
positional O
information O
. O

This O
is O
because O
positional O
information O
at O
the O
morphology O
level O
is O
inherent O
since O
no O
morpheme O
repeats O
and O
each O
morpheme O
always O
occupies O
a O
known O
( O
i.e. O
ﬁxed O
) O
slot O
in O
the O
morphotactics O
model O
. O

The O
extracted O
morphological O
features O
are O
four O
encoder O
output O
vectors O
corresponding O
to O
the O
three O
POS O
embeddings O
and O
one O
stem O
embedding O
. O

Vectors O
corresponding O
to O
the O
afﬁxes O
are O
left O
out O
since O
they O
are O
of O
variable O
length O
and O
the O
role O
of O
the O
afﬁxes O
in O
this O
case O
is O
to O
be O
attended O
to O
by O
the O
stem O
and O
the O
POS O
tag O
so O
that O
morphological O
information O
can O
be O
captured O
. O

The O
four O
morphological O
output O
feature O
vectors O
are O
further O
concatenated O
with O
another O
stem O
embedding O
at O
the O
sentence O
level O
to O
form O
the O
input O
vector O
for O
the O
main O
sentence O
/ O
document O
encoder O
. O

The O
choice O
of O
this O
transformer O
- O
based O
architecture O
for O
morphology O
encoding O
is O
motivated O
by O
two O
factors O
. O

First O
, O
Zaheer O
et O
al O
. O

( O
2020 O
) O
has O
demonstrated O
the O
importance O
of O
having O
“ O
global O
tokens O
” O
such O
as O
[ O
CLS O
] O
token O
in O
BERT B-MethodName
models O
. O

These O
are O
tokens O
that O
attend O
to O
all O
other O
tokens O
in O
the O
modeled O
sequence O
. O

These O
“ O
global O
tokens O
” O
effectively O
encapsulate O
some O
“ O
meaning O
” O
of O
the O
encoded O
sequence O
. O

Second O
, O
the O
POS O
tag O
and O
stem O
represent O
the O
high O
level O
information O
content O
of O
a O
word O
. O

Therefore O
, O
having O
the O
POS O
tag O
and O
stem O
embeddings O
be O
transformed O
into O
morphological O
features O
is O
a O
viable O
option O
. O

The O
POS O
tag O
and O
stem O
embeddings O
thus O
serve O
as O
the O
“ O
global O
tokens O
” O
at O
the O
morphology O
encoder O
level O
since O
they O
attend O
to O
all O
other O
morphemes O
that O
can O
be O
associated O
with O
them O
. O

In O
order O
to O
capture O
subtle O
morphological O
information O
, O
we O
make O
one O
of O
the O
three O
POS O
embeddings O
span O
an O
afﬁx O
set O
vocabulary O
that O
is O
a O
subset O
of O
the O
all O
afﬁxes O
power O
set O
. O

We O
form O
an O
afﬁx O
set O
vocabu-5350laryVathat O
is O
made O
of O
the O
Nmost O
frequent O
afﬁx O
combinations O
in O
the O
corpus O
. O

In O
fact O
, O
the O
morphological O
model O
of O
the O
language O
enforces O
constraints O
on O
which O
afﬁxes O
can O
go O
together O
for O
any O
given O
part O
of O
speech O
, O
resulting O
in O
an O
afﬁx O
set O
vocabulary O
that O
is O
much O
smaller O
than O
the O
power O
set O
of O
all O
afﬁxes O
. O

Even O
with O
limiting O
the O
afﬁx O
set O
vocabulary O
Vato O
a O
ﬁxed O
size O
, O
we O
can O
still O
map O
any O
afﬁx O
combination O
toVaby O
dropping O
zero O
or O
very O
few O
afﬁxes O
from O
the O
combination O
. O

Note O
that O
the O
afﬁx O
set O
embedding O
still O
has O
to O
attend O
to O
all O
morphemes O
at O
the O
morphology O
encoder O
level O
, O
making O
it O
adapt O
to O
the O
whole O
morphological O
context O
. O

The O
afﬁx O
set O
embedding O
is O
depicted O
by O
the O
purple O
units O
in O
Figure O
1 O
and O
a O
sample O
ofVais O
given O
in O
Table O
13 O
in O
the O
Appendix O
. O

2.3 O
Pre O
- O
training O
Objective O
Similar O
to O
other O
BERT B-MethodName
models O
, O
we O
use O
a O
masked O
language O
model O
objective O
. O

Speciﬁcally O
, O
15 O
% O
of O
all O
tokens O
in O
the O
training O
set O
are O
considered O
for O
prediction O
, O
of O
which O
80 O
% O
are O
replaced O
with O
[ O
MASK O
] O
tokens O
, O
10 O
% O
are O
replaced O
with O
random O
tokens O
and O
10 O
% O
are O
left O
unchanged O
. O

When O
prediction O
tokens O
are O
replaced O
with O
[ O
MASK O
] O
or O
random O
tokens O
, O
the O
corresponding O
afﬁxes O
are O
randomly O
omitted O
70 O
% O
of O
the O
time O
or O
left O
in O
place O
for O
30 O
% O
of O
the O
time O
, O
while O
the O
units O
corresponding O
to O
POS O
tags O
and O
afﬁx O
sets O
are O
also O
masked O
. O

The O
pre O
- O
training O
objective O
is O
then O
to O
predict O
stems O
and O
the O
associated O
afﬁxes O
for O
all O
tokens O
considered O
for O
prediction O
using O
a O
two O
- O
layer O
feed O
- O
forward O
module O
on O
top O
of O
the O
encoder O
output O
. O

For O
the O
afﬁx O
prediction O
task O
, O
we O
face O
a O
multilabel O
classiﬁcation O
problem O
where O
for O
each O
prediction O
token O
, O
we O
predict O
a O
variable O
number O
of O
afﬁxes O
. O

In O
our O
experiments O
, O
we O
tried O
two O
methods O
. O

For O
one O
, O
we O
use O
the O
Kullback O
– O
Leibler O
( O
KL O
) O
divergence2loss O
function O
to O
solve O
a O
regression O
task O
of O
predicting O
the O
N O
- O
length O
afﬁx O
distribution O
vector O
. O

For O
this O
case O
, O
we O
use O
a O
target O
afﬁx O
probability O
vector O
at2RNin O
which O
each O
target O
afﬁx O
index O
is O
assigned1 O
mprobability O
and O
0probability O
for O
non O
- O
target O
afﬁxes O
. O

Here O
mis O
the O
number O
of O
afﬁxes O
in O
the O
word O
to O
be O
predicted O
andNis O
the O
total O
number O
of O
all O
afﬁxes O
. O

We O
call O
this O
method O
“ O
Afﬁx O
Distribution O
Regression O
” O
( O
ADR O
) O
and O
model O
variant O
KinyaBERT B-MethodName
ADR O
. O

Alter O

natively O
, O
we O
use O
cross O
entropy O
loss O
and O
just O
predict O
the O
afﬁx O
set O
associated O
with O
the O
prediction O
word O
; O
we O
call O
this O
method O
“ O
Afﬁx O
Set O
Classiﬁcation O
” O
( O
ASC O
) O
and O
the O
model O
variant O
KinyaBERT B-MethodName
ASC O
. O

2https://en.wikipedia.org/wiki/ O
Kullback%E2%80%93Leibler_divergence3 O
Experiments O
In O
order O
to O
evaluate O
the O
proposed O
architecture O
, O
we O
pre O
- O
train O
KinyaBERT B-MethodName
( O
101 O
M O
parameters O
for O
KinyaBERT B-MethodName
ADR I-MethodName
and O
105 O
M O
for O
KinyaBERT B-MethodName
ASC I-MethodName
) O
on O
a O
2.4 O
GB O
of O
Kinyarwanda O
text O
along O
with O
3 O
baseline O
BERT B-MethodName
models O
. O

The O
ﬁrst O
baseline O
is O
a O
BERT B-MethodName
model O
pre O
- O
trained O
on O
the O
same O
Kinyarwanda O
corpus O
and O
with O
the O
same O
position O
encoding O
( O
Ke O
et O
al O
. O
, O
2020 O
) O
, O
same O
batch B-HyperparameterName
size I-HyperparameterName
and O
pre O
- O
training B-HyperparameterName
steps I-HyperparameterName
, O
but O
using O
the O
standard O
BPE O
tokenization O
. O

We O
call O
this O
ﬁrst O
baseline O
model O
BERT B-MethodName
BPE I-MethodName
( O
120 O
M O
parameters O
) O
. O

The O
second O
baseline O
is O
a O
similar O
BERT B-MethodName
model O
pretrained O
on O
the O
same O
Kinyarwanda B-DatasetName
corpus O
but O
tokenized O
by O
a O
morphological O
analyzer O
. O

For O
this O
model O
, O
the O
input O
is O
just O
a O
sequence O
of O
morphemes O
, O
in O
a O
similar O
fashion O
to O
Mohseni O
and O
Tebbifakhr O
( O
2019 O
) O
. O

We O
call O
this O
second O
baseline O
model O
BERT B-MethodName
MORPHO I-MethodName
( O
127 O
M O
parameters O
) O
. O

For O
BERT B-MethodName
MORPHO I-MethodName
, O
we O
found O
that O
predicting O
30 O
% O
of O
the O
tokens O
achieves O
better O
results O
than O
using O
15 O
% O
because O
of O
the O
many O
afﬁxes O
generated O
. O

The O
third O
baseline O
is O
XLM B-MethodName
- I-MethodName
R I-MethodName
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
( O
270 O
M O
parameters O
) O
which O
is O
pretrained O
on O
2.5 O
TB O
of O
multilingual O
text O
. O

We O
evaluate O
the O
above O
models O
by O
comparing O
their O
performance O
on O
downstream O
NLP O
tasks O
. O

Language O
Kinyarwanda O
Publication O
Period O
2011 O
- O
2021 O
Websites O
/ O
Sources O
370 O
Documents O
/ O
Articles O
840 O
K O
Sentences O
16 O
M O
Tokens O
/ O
Words O
390 O
M O
Text O
size O
2.4 O
GB O
Table O
2 O
: O
Summary O
of O
the O
pre O
- O
training O
corpus O
. O

3.1 O
Pre O
- O
training O
details O
KinyaBERT B-MethodName
model O
was O
implemented O
using O
Pytorch O
version O
1.9 O
. O

The O
morphological O
analyzer O
and O
POS O
tagger O
were O
implemented O
in O
a O
shared O
library O
using O
POSIX O
C. O
Morphological O
parsing O
of O
the O
corpus O
was O
performed O
as O
a O
pre O
- O
processing O
step O
, O
taking O
20 O
hours O
to O
segment O
the O
390M O
- O
token O
corpus O
on O
an O
12 O
- O
core O
desktop O
machine O
. O

Pre O
- O
training O
was O
performed O
using O
RTX O
3090 O
and O
RTX O
2080Ti O
desktop O
GPUs O
. O

Each O
KinyaBERT B-MethodName
model O
takes O
on O
average O
22 O
hours O
to O
train O
for O
1000 O
steps O
on O
one O
RTX O
3090 O
GPU O
or O
29 O
hours O
on O
one O
RTX O
2080Ti O
GPU O
. O

Baseline O
models O
( O
BERT B-MethodName
BPE O
and O
BERT B-MethodName
MORPHO O
) O
were O
pre O
- O
trained O
on O
cloud O
tensor O
processing O
units O
( O
TPU O
v3 O
- O
8 O
devices O
each O
with O
128 O
GB O
memory O
) O
us-5351ing O
PyTorch O
/ O
XLA3package O
and O
a O
TPU O
- O
optimized O
fairseq O
toolkit O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
. O

Pre O
- O
training O
on O
TPU O
took O
2.3 O
hours O
per O
1000 O
steps O
. O

The O
baselines O
were O
trained O
on O
TPU O
because O
there O
were O
no O
major O
changes O
needed O
to O
the O
existing O
RoBERTA O
( O
base O
) O
architecture O
implemented O
in O
fairseq O
and O
the O
TPU O
resources O
were O
available O
and O
efﬁcient O
. O

In O
all O
cases O
, O
pre O
- O
training O
batch B-HyperparameterName
size I-HyperparameterName
was O
set O
to O
2560 B-HyperparameterValue
sequences O
, O
with O
maximum O
512 O
tokens O
in O
each O
sequence O
. O

The O
maximum O
learning B-HyperparameterName
rates I-HyperparameterName
was O
set O
to O
410 4which O
is O
achieved O
after O
2000 O
steps O
and O
then O
linearly O
decays O
. O

Our O
main O
results O
and O
ablation O
results O
were O
obtained O
from O
models O
pre O
- O
trained O
for O
32 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
in O
all O
cases O
. O

Other O
pre O
- O
training O
details O
, O
model O
architectural O
dimensions O
and O
other O
hyper O
- O
parameters O
are O
given O
in O
the O
Appendix O
. O

3.2 O
Evaluation O
tasks O
Machine O
translated O
GLUE B-DatasetName
benchmark O
– O
The O
General B-DatasetName
Language I-DatasetName
Understanding I-DatasetName
Evaluation I-DatasetName
( O
GLUE B-DatasetName
) O
benchmark O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
has O
been O
widely O
used O
to O
evaluate O
pre O
- O
trained O
language O
models O
. O

In O
order O
to O
assess O
KinyaBERT B-MethodName
performance O
on O
such O
high O
level O
language O
tasks O
, O
we O
used O
Google O
Translate O
API O
to O
translate O
a O
subset O
of O
the O
GLUE B-DatasetName
benchmark O
( O
MRPC B-DatasetName
, O
QNLI B-DatasetName
, O
RTE B-DatasetName
, O
SST-2 B-DatasetName
, O
STS B-DatasetName
- I-DatasetName
B I-DatasetName
and O
WNLI B-DatasetName
tasks O
) O
into O
Kinyarwanda O
. O

CoLA O
task O
was O
left O
because O
it O
is O
English O
- O
speciﬁc O
. O

MNLI O
and O
QQP O
tasks O
were O
also O
not O
translated O
because O
they O
were O
too O
expensive O
to O
translate O
with O
Google O
’s O
commercial O
API O
. O

While O
machine O
translation O
adds O
more O
noise O
to O
the O
data O
, O
evaluating O
on O
this O
dataset O
is O
still O
relevant O
because O
all O
models O
compared O
have O
to O
cope O
with O
the O
same O
noise O
. O

To O
understand O
this O
translation O
noise O
, O
we O
also O
run O
user O
evaluation O
experiments O
, O
whereby O
four O
volunteers O
proﬁcient O
in O
both O
English O
and O
Kinyarwanda O
evaluated O
a O
random O
sample O
of O
6000 O
translated O
GLUE B-DatasetName
examples O
, O
and O
assigned O
a O
score O
to O
each O
example O
on O
a O
scale O
from O
1 O
to O
4 O
( O
See O
Table O
11 O
in O
Appendix O
) O
. O

These O
scores O
help O
us O
characterize O
the O
noise O
in O
the O
data O
and O
contextualize O
our O
results O
with O
regards O
to O
other O
GLUE O
evaluations O
. O

Results O
on O
these O
GLUE B-DatasetName
tasks O
are O
shown O
in O
Table O
3 O
. O
Named B-TaskName
entity I-TaskName
recognition I-TaskName
( O
NER B-TaskName
) O
– O
We O
use O
the O
Kinyarwanda O
subset O
of O
the O
MasakhaNER B-DatasetName
dataset O
( O
Adelani O
et O
al O
. O
, O
2021 O
) O
for O
NER B-TaskName
task O
. O

This O
is O
a O
high O
quality O
NER B-TaskName
dataset O
annotated O
by O
native O
speakers O
for O
major O
African O
languages O
including O
Kinyarwanda O
. O

The O
task O
requires O
predicting O
four O
entity O
types O
: O
Persons O
( O
PER O
) O
, O
Locations O
( O
LOC O
) O
, O
Or3https://github.com/pytorch/xla/ganizations O
( O
ORG O
) O
, O
and O
date O
and O
time O
( O
DATE O
) O
. O

Results O
on O
this O
NER B-TaskName
task O
are O
presented O
in O
Table O
4 O
. O

News B-TaskName
Categorization I-TaskName
Task O
( O
NEWS B-TaskName
) O
– O

For O
a O
document O
classiﬁcation O
experiment O
, O
we O
collected O
a O
set O
of O
categorized O
news O
articles O
from O
seven O
major O
news O
websites O
that O
regularly O
publish O
in O
Kinyarwanda O
. O

The O
articles O
were O
already O
categorized O
, O
so O
no O
more O
manual O
labeling O
was O
needed O
. O

This O
dataset O
is O
similar O
to O
Niyongabo O
et O

al O
. O
( O
2020 O
) O
, O
but O
in O
our O
case O
, O
we O
limited O
the O
number O
collected O
articles O
per O
category O
to O
3000 O
in O
order O
to O
have O
a O
more O
balanced O
label O
distribution O
( O
See O
Table O
10 O
in O
the O
Appendix O
) O
. O

The O
ﬁnal O
dataset O
contains O
a O
total O
of O
25.7 O
K O
articles O
spanning O
12 O
categories O
and O
has O
been O
split O
into O
training O
, O
validation O
and O
test O
sets O
in O
the O
ratios O
of O
70 O
% O
, O
5 O
% O
and O
25 O
% O
respectively O
. O

Results O
on O
this O
NEWS B-TaskName
task O
are O
presented O
in O
Table O
5 O
. O

For O
each O
evaluation O
task O
, O
we O
use O
a O
two O
- O
layer O
feedforward O
network O
on O
top O
of O
the O
sentence O
encoder O
as O
it O
is O
typically O
done O
in O
other O
BERT B-MethodName
models O
. O

The O
ﬁnetuning O
hyper O
- O
parameters O
are O
presented O
in O
Table O
14 O
in O
the O
Appendix O
. O

3.3 O
Main O
results O
The O
main O
results O
are O
presented O
in O
Table O
3 O
, O
Table O
4 O
, O
and O
Table O
5 O
. O

Each O
result O
is O
the O
average O
of O
10 O
independent O
ﬁne O
- O
tuning O
runs O
. O

Each O
average O
result O
is O
shown O
with O
the O
standard O
deviation O
of O
the O
10 O
runs O
. O

Except O
for O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
all O
other O
models O
are O
pre O
- O
trained O
on O
the O
same O
corpus O
( O
See O
Table O
2 O
) O
for O
32 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
using O
the O
same O
hyper O
- O
parameters O
. O

On O
the O
GLUE B-DatasetName
task O
, O
KinyaBERT B-MethodName
ASC I-MethodName
achieves O
4.3 B-MetricValue
% I-MetricValue
better O
average O
score O
than O
the O
strongest O
baseline O
. O

KinyaBERT B-MethodName
ASC I-MethodName
also O
leads O
to O
more O
robust O
results O
on O
multiple O
tasks O
. O

It O
is O
also O
shown O
that O
having O
just O
a O
morphological O
analyzer O
is O
not O
enough O
: O
BERT B-MethodName
MORPHO I-MethodName
still O
under O
- O
performs O
even O
though O
it O
uses O
morphological O
tokenization O
. O

Multilingual O
XLM B-MethodName
- I-MethodName
R I-MethodName
achieves O
least O
performance O
in O
most O
cases O
, O
possibly O
because O
it O
was O
not O
pre O
- O
trained O
on O
Kinyarwanda O
text O
and O
uses O
inadequate O
tokenization O
. O

On O
the O
NER B-TaskName
task O
, O
KinyaBERT B-MethodName
ADR O
achieves O
best O
performance O
, O
about O
3.2 B-MetricValue
% I-MetricValue
better O
average O
F1 B-MetricName
score O
than O
the O
strongest O
baseline O
. O

One O
of O
the O
architectural O
differences O
between O
KinyaBERT B-MethodName
ADR I-MethodName
and O
KinyaBERT B-MethodName
ASC I-MethodName
is O
that O
KinyaBERT B-MethodName
ADR I-MethodName
uses O
three O
POS O
tag O
embeddings O
while O
KinyaBERT B-MethodName
ASC I-MethodName
uses O
two O
. O

Assuming O
that O
POS O
tagging O
facilitates O
named O
entity O
recognition O
, O
this O
empirical O
result O
suggests O
that O
increasing O
the O
amount O
of O
POS O
tag O
information5352Task O
: O

MRPC O
QNLI O

RTE O
SST-2 O
STS O
- O
B O
WNLI O
# O
Train O
examples O
: O
3.4 O
K O
104.7 O
K O
2.5 O
K O
67.4 O
K O
5.8 O
K O
0.6 O
K O
Translation O
score O
: O
2.7/4.0 O
2.9/4.0 O
3.0/4.0 O
2.7/4.0 O
3.1/4.0 O
2.9/4.0 O
Model O
Validation O
Set O
XLM O
- O
R O
84.2/78.3 O
0:8=1:079.00:358.43:278.70:677.7/77.8 O

0:7=0:655.42:0 O
BERT B-MethodName
BPE O
83.3/76.6 O
0:8=1:481.90:259.21:580.10:475.6/75.7 O

7:8=7:355.41:9 O
BERT B-MethodName
MORPHO O
84.3/77.4 O
0:6=1:181.60:259.21:581.60:576.8/77.0 O

0:8=0:754.22:5 O
KinyaBERT B-MethodName
ADR O
87.1/82.1 O
0:5=0:781.60:161.81:481.80:679.6/79.5 O

0:4=0:354.52:2 O
KinyaBERT B-MethodName
ASC O
86.6/81.3 O
0:5=0:782.30:364.31:482.40:580.0/79.9 O
0:5=0:556.20:8 O
Model O
Test O
Set O
XLM O
- O
R O
82.6/76.0 O
0:6=0:678.10:356.43:276.30:469.5/68.9 O

1:0=1:163.73:9 O
BERT B-MethodName
BPE O
82.8/76.2 O
0:6=0:881.10:355.62:879.10:468.9/67.8 O
1:8=1:763.44:1 O
BERT B-MethodName
MORPHO O
82.7/75.4 O
0:8=1:380.80:456.71:080.70:568.9/67.8 O
1:5=1:365.00:3 O
KinyaBERT B-MethodName
ADR O
84.4/ O
78.70:5=0:681.20:358.11:180.90:573.2/72.0 O
0:4=0:365.10:0 O
KinyaBERT B-MethodName
ASC O
84.6/78.40:2=0:382.20:658.80:781.40:674.5/73.5 O
0:2=0:265.00:2 O
Table O
3 O
: O
Performance O
results O
on O
the O
machine O
translated O
GLUE B-DatasetName
benchmark O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
. O

The O
translation O
score O
is O
the O
sample O
average O
translation O
quality O
score O
assigned O
by O
volunteers O
. O

For O
MRPC B-DatasetName
, O
we O
report O
accuracy B-MetricName
and O
F1 B-MetricName
. O

For O
STS B-DatasetName
- I-DatasetName
B I-DatasetName
, O
we O
report O
Pearson B-MetricName
and O
Spearman B-MetricName
correlations I-MetricName
. O

For O
all O
others O
, O
we O
report O
accuracy B-MetricName
. O

The O
best O
results O
are O
shown O
in O
bold O
while O
equal O
top O
results O
are O
underlined O
. O

Task O
: O
NER O
# O
Train O
examples O
: O
2.1 O
K O
Model O
Validation O
Set O
Test O
Set O
XLM O
- O
R O
80.3 O
1:0 O
71.81:5 O
BERT B-MethodName
BPE O
83.40:9 O

74.80:8 O
BERT B-MethodName
MORPHO O
83.20:9 O

72.80:9 O
KinyaBERT B-MethodName
ADR O
87.10:8 O
77.21:0 O
KinyaBERT B-MethodName
ASC O
86.20:4 O
76.30:5 O
Table O
4 O
: O
Micro B-MetricName
average I-MetricName
F1 I-MetricName
scores O
on O
Kinyarwanda O
NER B-TaskName
task O
( O
Adelani O
et O
al O
. O
, O
2021 O
) O
. O
Task O
: O
NEWS O
# O
Train O
examples O
: O
18.0 O
K O
Model O
Validation O
Set O
Test O
Set O
XLM O
- O
R O
83.8 O
0:3 O
84.00:2 O
BERT B-MethodName
BPE O
87.60:4 O
88.30:3 O
BERT B-MethodName
MORPHO O
86.90:4 O

86.90:3 O
KinyaBERT B-MethodName
ADR O
88.80:3 O
88.00:3 O
KinyaBERT B-MethodName
ASC O
88.40:3 O
88.00:2 O
Table O
5 O
: O
Accuracy B-MetricName
results O
on O
Kinyarwanda O
NEWS B-TaskName
categorization I-TaskName
task O
. O

in O
the O
model O
, O
possibly O
through O
diversiﬁcation O
( O
i.e. O
multiple O
POS O
tag O
embedding O
vectors O
per O
word O
) O
, O
can O
lead O
to O
better O
NER B-TaskName
performance O
. O

The O
NEWS B-TaskName
categorization I-TaskName
task O
resulted O
in O
differing O
performances O
between O
validation O
and O
testsets O
. O

This O
may O
be O
a O
result O
that O
solving O
such O
task O
does O
not O
require O
high O
level O
language O
modeling O
but O
rather O
depends O
on O
spotting O
few O
keywords O
. O

Previous O
research O
on O
a O
similar O
task O
( O
Niyongabo O
et O
al O
. O
, O
2020 O
) O
has O
shown O
that O
simple O
classiﬁers O
based O
on O
TF O
- O
IDF O
features O
sufﬁce O
to O
achieve O
best O
performance O
. O

The O
morphological O
analyzer O
and O
POS O
tagger O
inherently O
have O
some O
level O
of O
noise O
because O
they O
do O
not O
always O
perform O
with O
perfect O
accuracy O
. O

While O
we O
did O
not O
have O
a O
simple O
way O
of O
assessing O
the O
impact O
of O
this O
noise O
in O
this O
work O
, O
we O
can O
logically O
expect O
that O
the O
lower O
the O
noise O
the O
better O
the O
results O
could O
be O
. O

Improving O
the O
morphological O
analyzer O
and O
POS O
tagger O
and O
quantitatively O
evaluating O
its O
accuracy O
is O
part O
of O
future O
work O
. O

Even O
though O
our O
POS O
tagger O
uses O
heuristic O
methods O
and O
was O
evaluated O
mainly O
through O
qualitative O
exploration O
, O
we O
can O
still O
see O
its O
positive O
impact O
on O
the O
pre O
- O
trained O
language O
model O
. O

We O
did O
not O
use O
previous O
work O
on O
Kinyarwanda O
POS O
tagging O
because O
it O
is O
largely O
different O
from O
this O
work O
in O
terms O
of O
scale O
, O
tag O
dictionary O
and O
dataset O
size O
and O
availability O
. O

We O
plot O
the O
learning O
curves O
during O
ﬁne O
- O
tuning O
process O
of O
KinyaBERT B-MethodName
and O
the O
baselines O
. O

The O
results O
in O
Figure O
2 O
indicate O
that O
KinyaBERT B-MethodName
ﬁnetuning O
has O
better O
convergence O
across O
all O
tasks O
. O

Additional O
results O
also O
show O
that O
positional O
attention O
( O
Ke O
et O
al O
. O
, O
2020 O
) O
learned O
by O
KinyaBERT B-MethodName
has O
more O
uniform O
and O
smoother O
relative O
bias O
while O
BERT B-MethodName
BPE I-MethodName
and O
BERT B-MethodName
MORPHO I-MethodName
have O
more O
noisy5353Figure O
2 O
: O
Comparison O
of O
ﬁne O
- O
tuning O
loss O
curves O
between O
KinyaBERT B-MethodName
and O
baselines O
on O
the O
evaluation O
tasks O
. O

KinyaBERT B-MethodName
ASC I-MethodName
achieves O
the O
best O
convergence O
in O
most O
cases O
, O
indicating O
better O
effectiveness O
of O
its O
model O
architecture O
and O
pre O
- O
training O
objective O
. O
relative O
positional O
bias O
( O
See O
Figure O
3 O
in O
Appendix O
) O
. O

This O
is O
possibly O
an O
indication O
that O
KinyaBERT B-MethodName
allows O
learning O
better O
word O
- O
relative O
syntactic O
regularities O
. O

However O
, O
this O
aspect O
needs O
to O
be O
investigated O
more O
systematically O
in O
future O
research O
. O

While O
the O
main O
sentence O
/ O
document O
encoder O
of O
KinyaBERT B-MethodName
is O
equivalent O
to O
a O
standard O
BERT B-MethodName
“ O
BASE O
” O
conﬁguration O
on O
top O
of O
a O
small O
morphology O
encoder O
, O
overall O
, O
the O
model O
actually O
decreases O
the O
number O
of O
parameters O
by O
more O
than O
12 O
% O
through O
embedding O
layer O
savings O
. O

This O
is O
because O
using O
morphological O
representation O
reduces O
the O
vocabulary O
size O
. O

Using O
smaller O
embedding O
vectors O
at O
the O
morphology O
encoder O
level O
also O
signiﬁcantly O
reduces O
the O
overall O
number O
of O
parameters O
. O

Table O
8 O
in O
Appendix O
shows O
the O
vocabulary O
sizes O
and O
parameter O
count O
of O
KinyaBERT B-MethodName
in O
comparison O
to O
the O
baselines O
. O

While O
the O
sizing O
of O
the O
embeddings O
was O
done O
essentially O
to O
match O
BERT B-MethodName
“ O
BASE O
” O
conﬁguration O
, O
future O
studies O
can O
shed O
more O
light O
on O
how O
different O
model O
sizes O
affect O
performance O
. O

3.4 O
Ablation O
study O
We O
conducted O
an O
ablation O
study O
to O
clarify O
some O
of O
the O
design O
choices O
made O
for O
KinyaBERT B-MethodName
architecture O
. O

We O
make O
variations O
along O
two O
axes O
: O
( O
i O
) O
morphology O
input O
and O
( O
ii O
) O
pre O
- O
training O
task O
, O
which O
gave O
us O
four O
variants O
that O
we O
pre O
- O
trained O
for O
32 O
K O
steps O
and O
evaluated O
on O
the O
same O
downstream O
tasks O
. O

•AFS!STEM+ASC O
: O
Morphological O
features O
are O
captured O
by O
two O
POS O
tag O
and O
one O
afﬁx O
set O
vectors O
. O

We O
predict O
both O
the O
stem O
and O
afﬁx O
set O
. O

This O
corresponds O
to O
KinyaBERT B-MethodName
ASC O
presented O
in O
the O
main O
results.•POS!STEM+ADR O
: O
Morphological O
features O
are O
carried O
by O
three O
POS O
tag O
vectors O
and O
we O
predict O
the O
stem O
and O
afﬁx O
probability O
vector O
. O

This O
corresponds O
to O
KinyaBERT B-MethodName
ADR O
. O

•A O
VG!STEM+ADR O
: O
Morphological O
features O
are O
captured O
by O
two O
POS O
tag O
vectors O
and O
the O
pointwise O
average O
of O
afﬁx O
hidden O
vectors O
from O
the O
morphology O
encoder O
. O

We O
predict O
the O
stem O
and O
afﬁx O
probability O
vector O
. O
•STEM!STEM O
: O
We O
omit O
the O
morphology O
encoder O
and O
train O
a O
model O
with O
only O
the O
stem O
parts O
without O
afﬁxes O
and O
only O
predict O
the O
stem O
. O

Ablation O
results O
presented O
in O
Table O
6 O
indicate O
that O
using O
afﬁx O
sets O
for O
both O
morphology O
encoding O
and O
prediction O
gives O
better O
results O
for O
many O
GLUE O
tasks O
. O

The O
under O
- O
performance O
of O
“ O
STEM O
! O
STEM O
” O
on O
high O
resource O
tasks O
( O
QNLI B-DatasetName
and O
SST-2 B-DatasetName
) O
is O
an O
indication O
that O
morphological O
information O
from O
afﬁxes O
is O
important O
. O

However O
, O
the O
utility O
of O
this O
information O
depends O
on O
the O
task O
as O
we O
see O
mixed O
results O
on O
other O
tasks O
. O

Due O
to O
a O
large O
design O
space O
for O
a O
morphologyaware O
language O
model O
, O
there O
are O
still O
a O
number O
of O
other O
design O
choices O
that O
can O
be O
explored O
in O
future O
studies O
. O

One O
may O
vary O
the O
amount O
of O
POS O
tag O
embeddings O
used O
, O
vary O
the O
size O
afﬁx O
set O
vocabulary O
or O
the O
dimension O
of O
the O
morphology O
encoder O
embeddings O
. O

One O
may O
also O
investigate O
the O
potential O
of O
other O
architectures O
for O
the O
morphology O
encoder O
, O
such O
as O
convolutional O
networks O
. O

Our O
early O
attempt O
of O
using O
recurrent O
neural O
networks O
( O
RNNs O
) O
for O
the O
morphology O
encoder O
was O
abandoned O
because O
it O
was O
too O
slow O
to O
train.5354Task O
: O

MRPC O
QNLI O
RTE O
SST-2 O
STS O
- O
B O
WNLI O
NER O
NEWS O
Morphology!Prediction O

Validation O
Set O
AFS!STEM+ASC O
86.6/81.3 O
82.3 O
64.3 O
82.4 O
80.0/79.9 O
56.2 O
86.2 O
88.4 O
POS!STEM+ADR O
87.1/82.1 O
81.6 O
61.8 O
81.8 O
79.6/79.5 O
54.5 O
87.1 O
88.8 O
A O
VG!STEM+ADR O
85.5/80.3 O
81.4 O
63.0 O
82.1 O
79.6/79.5 O
55.8 O
86.6 O
88.3 O
STEM!STEM O
86.4/81.5 O
80.4 O
63.4 O
77.5 O
79.7/79.5 O
50.4 O
86.6 O
88.0 O
Morphology!Prediction O

Test O
Set O
AFS!STEM+ASC O
84.6/78.4 O

82.2 O
58.8 O
81.4 O
74.5/73.5 O
65.0 O
76.3 O
88.0 O
POS!STEM+ADR O
84.4/ O
78.7 O
81.2 O
58.1 O
80.9 O
73.2/72.0 O
65.1 O
77.2 O
88.0 O
A O
VG!STEM+ADR O

84.0/78.2 O

81.7 O
59.4 O
80.7 O
73.6/72.6 O
65.0 O
76.9 O
88.2 O
STEM!STEM O
84.2/78.6 O
80.3 O
59.8 O
77.5 O
73.3/72.0 O
59.6 O
76.4 O
88.4 O
Table O
6 O
: O
Ablation O
results O
: O
each O
result O
is O
an O
average O
of O
10 O
independent O
ﬁne O
- O
tuning O
runs O
. O

Metrics O
, O
dataset O
sizes O
and O
noise O
statistics O
are O
the O
same O
as O
for O
the O
main O
results O
in O
Table O
3 O
, O
Table O
4 O
and O
Table O
5 O
. O
4 O
Related O
Work O
BERT B-MethodName
- O
variant O
pre O
- O
trained O
language O
models O
( O
PLMs O
) O
were O
initially O
pre O
- O
trained O
on O
monolingual O
highresource O
languages O
. O

Multilingual O
PLMs O
that O
include O
both O
high O
- O
resource O
and O
low O
- O
resource O
languages O
have O
also O
been O
introduced O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Conneau O
et O
al O
. O
, O
2020 O
; O

Xue O
et O
al O
. O
, O
2021 O
; O
Chung O
et O

al O
. O
, O
2020 O
) O
. O

However O
, O
it O
has O
been O
found O
that O
these O
multilingual O
models O
are O
biased O
towards O
high O
- O
resource O
languages O
and O
use O
fewer O
low O
quality O
and O
uncleaned O
low O
- O
resource O
data O
( O
Kreutzer O
et O
al O
. O
, O
2022 O
) O
. O

The O
included O
low O
- O
resource O
languages O
are O
also O
very O
limited O
because O
they O
are O
mainly O
sourced O
from O
Wikipedia O
articles O
, O
where O
languages O
with O
few O
articles O
like O
Kinyarwanda O
are O
often O
left O
behind O
( O
Joshi O
et O
al O
. O
, O
2020 O
; O
Nekoto O
et O
al O
. O
, O
2020 O
) O
. O

Joshi O
et O
al O
. O

( O
2020 O
) O
classify O
the O
state O
of O
NLP O
for O
Kinyarwanda O
as O
“ O
Scraping O
- O
By O
” O
, O
meaning O
it O
has O
been O
mostly O
excluded O
from O
previous O
NLP O
research O
, O
and O
require O
the O
creation O
of O
dedicated O
resources O
and O
models O
. O

Kinyarwanda O
has O
been O
studied O
mostly O
in O
descriptive O
linguistics O
( O
Kimenyi O
, O
1976 O
, O
1978a O
, O
b O
, O
1988 O
; O
Jerro O
, O
2016 O
) O
. O

Few O
recent O
NLP O
works O
on O
Kinyarwanda O
include O
Morphological O
Analysis O
( O
Muhirwe O
, O
2009 O
; O
Nzeyimana O
, O
2020 O
) O
, O
Text O
Classiﬁcation O
( O
Niyongabo O
et O
al O
. O
, O
2020 O
) O
, O
Named O
Entity O
Recognition O
( O
Rijhwani O
et O
al O
. O
, O
2020 O
; O

Adelani O
et O

al O
. O
, O
2021 O
; O
Sälevä O
and O
Lignos O
, O
2021 O
) O
, O
POS O
tagging O
( O
Garrette O
and O
Baldridge O
, O
2013 O
; O
Garrette O
et O
al O
. O
, O
2013 O
; O
Duong O
et O

al O
. O
, O
2014 O

; O
Fang O
and O
Cohn O
, O
2016 O
; O
Cardenas O
et O
al O
. O
, O
2019 O
) O
, O
and O
Parsing O
( O
Sun O
et O
al O
. O
, O
2014 O
; O
Mielens O
et O
al O
. O
, O
2015 O
) O
. O

There O
is O
no O
prior O
study O
on O
pre O
- O
trained O
language O
modeling O
for O
Kinyarwanda O
. O

There O
are O
very O
few O
works O
on O
monolingual O
PLMsfor O
African O
languages O
. O

To O
the O
best O
of O
our O
knowledge O
there O
is O
currently O
only O
AfriBERT B-MethodName
( O
Ralethe O
, O
2020 O
) O
that O
has O
been O
pre O
- O
trained O
on O
Afrikaans O
, O
a O
language O
spoken O
in O
South O
Africa O
. O

In O
this O
paper O
, O
we O
aim O
to O
increase O
the O
inclusion O
of O
African O
languages O
in O
NLP O
community O
by O
introducing O
a O
PLM O
for O
Kinyarwanda O
. O

Differently O
to O
the O
previous O
works O
( O
see O
Table O
15 O
in O
Appendix O
) O
which O
solely O
pretrained O
unmodiﬁed O
BERT B-MethodName
models O
, O
we O
propose O
an O
improved O
BERT B-MethodName
architecture O
for O
morphologically O
rich O
languages O
. O

Recently O
, O
there O
has O
been O
a O
research O
push O
to O
improve O
sub O
- O
word O
tokenization O
by O
adopting O
characterbased O
models O
( O
Ma O
et O
al O
. O
, O
2020 O
; O
Clark O
et O
al O
. O
, O
2022 O
) O
. O

While O
these O
methods O
are O
promising O
for O
the O
“ O
language O
- O
agnostic O
” O
case O
, O
they O
are O
still O
solely O
based O
on O
the O
surface O
form O
of O
words O
, O
and O
thus O
have O
the O
same O
limitations O
as O
BPE O
when O
processing O
morphologically O
rich O
languages O
. O

We O
leave O
it O
to O
future O
research O
to O
empirically O
explore O
how O
these O
characterbased O
methods O
compare O
to O
morphology O
- O
aware O
models O
. O

5 O
Conclusion O
This O
work O
demonstrates O
the O
effectiveness O
of O
explicitly O
incorporating O
morphological O
information O
in O
language O
model O
pre O
- O
training O
. O

The O
proposed O
twotier O
Transformer O
architecture O
allows O
the O
model O
to O
represent O
morphological O
compositionality O
. O

Experiments O
conducted O
on O
Kinyarwanda O
, O
a O
low O
resource O
morphologically O
rich O
language O
, O
reveal O
signiﬁcant O
performance O
improvement O
on O
several O
downstream O
NLP O
tasks O
when O
using O
the O
proposed O
architecture O
. O

These O
ﬁndings O
should O
motivate O
more O
research O
into O
morphology O
- O
aware O
language O
models.5355Acknowledgements O
This O
work O
was O
supported O
with O
Cloud O
TPUs O
from O
Google O
’s O
TPU O
Research O
Cloud O
( O
TRC O
) O
program O
and O
Google O
Cloud O
Research O
Credits O
with O
the O
award O
GCP19980904 O
. O

We O
also O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
feedback O
. O

References O
David O
Ifeoluwa O
Adelani O
, O
Jade O
Abbott O
, O
Graham O
Neubig O
, O
Daniel O
D’souza O
, O
Julia O
Kreutzer O
, O
Constantine O
Lignos O
, O
Chester O
Palen O
- O
Michel O
, O
Happy O
Buzaaba O
, O
Shruti O
Rijhwani O
, O
Sebastian O
Ruder O
, O
Stephen O
Mayhew O
, O
Israel O
Abebe O
Azime O
, O
Shamsuddeen O
H. O
Muhammad O
, O
Chris O
Chinenye O
Emezue O
, O
Joyce O
NakatumbaNabende O
, O
Perez O
Ogayo O
, O
Aremu O
Anuoluwapo O
, O
Catherine O
Gitau O
, O
Derguene O
Mbaye O
, O
Jesujoba O
Alabi O
, O
Seid O
Muhie O
Yimam O
, O
Tajuddeen O
Rabiu O
Gwadabe O
, O
Ignatius O
Ezeani O
, O
Rubungo O
Andre O
Niyongabo O
, O
Jonathan O
Mukiibi O
, O
Verrah O
Otiende O
, O
Iroro O
Orife O
, O
Davis O
David O
, O
Samba O
Ngom O
, O
Tosin O
Adewumi O
, O
Paul O
Rayson O
, O
Mofetoluwa O
Adeyemi O
, O
Gerald O
Muriuki O
, O
Emmanuel O
Anebi O
, O
Chiamaka O
Chukwuneke O
, O
Nkiruka O
Odu O
, O
Eric O
Peter O
Wairagala O
, O
Samuel O
Oyerinde O
, O
Clemencia O
Siro O
, O
Tobius O
Saul O
Bateesa O
, O
Temilola O
Oloyede O
, O
Yvonne O
Wambui O
, O
Victor O
Akinode O
, O
Deborah O
Nabagereka O
, O
Maurice O
Katusiime O
, O
Ayodele O
Awokoya O
, O
Mouhamadane O
MBOUP O
, O
Dibora O
Gebreyohannes O
, O
Henok O
Tilaye O
, O
Kelechi O
Nwaike O
, O
Degaga O
Wolde O
, O
Abdoulaye O
Faye O
, O
Blessing O
Sibanda O
, O
Orevaoghene O
Ahia O
, O
Bonaventure O
F. O
P. O
Dossou O
, O
Kelechi O
Ogueji O
, O
Thierno O
Ibrahima O
DIOP O
, O
Abdoulaye O
Diallo O
, O
Adewale O
Akinfaderin O
, O
Tendai O
Marengereke O
, O
and O
Salomey O
Osei O
. O

2021 O
. O

MasakhaNER O
: O
Named O
entity O
recognition O
for O
African O
languages O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
9:1116–1131 O
. O

Fady O
Baly O
, O
Hazem O
Hajj O
, O
et O
al O
. O
2020 O
. O

Arabert O
: O
Transformer O
- O
based O
model O
for O
arabic O
language O
understanding O
. O

In O
Proceedings O
of O
the O
4th O
Workshop O
on O
Open O
- O
Source O
Arabic O
Corpora O
and O
Processing O
Tools O
, O
with O
a O
Shared O
Task O
on O
Offensive O
Language O
Detection O
, O
pages O
9–15 O
. O

Kenneth O
R O
Beesley O
and O
Lauri O
Karttunen O
. O

2000 O
. O

Finitestate O
non O
- O
concatenative O
morphotactics O
. O

In O
Proceedings O
of O
the O
38th O
Annual O
Meeting O
on O
Association O
for O
Computational O
Linguistics O
, O
pages O
191–198 O
. O

Kenneth O
R O
Beesley O
and O
Lauri O
Karttunen O
. O

2003 O
. O

Finitestate O
morphology O
: O
Xerox O
tools O
and O
techniques O
. O

CSLI O
, O
Stanford O
. O

Emily O
M O
Bender O
. O

2019 O
. O

The O
# O
benderrule O
: O
On O
naming O
the O
languages O
we O
study O
and O
why O
it O
matters O
. O

The O
Gradient O
, O
14 O
. O

Yoshua O
Bengio O
, O
Réjean O
Ducharme O
, O
Pascal O
Vincent O
, O
and O
Christian O
Janvin O
. O
2003 O
. O

A O
neural O
probabilistic O
language O
model O
. O

The O
journal O
of O
machine O
learning O
research O
, O
3:1137–1155.Piotr O
Bojanowski O
, O
Edouard O
Grave O
, O
Armand O
Joulin O
, O
and O
Tomas O
Mikolov O
. O

2017 O
. O

Enriching O
word O
vectors O
with O
subword O
information O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
5:135–146 O
. O

José O
Canete O
, O
Gabriel O
Chaperon O
, O
Rodrigo O
Fuentes O
, O
and O
Jorge O
Pérez O
. O

2020 O
. O

Spanish O
pre O
- O
trained O
bert O
model O
and O
evaluation O
data O
. O

PML4DC O
at O
ICLR O
, O
2020 O
. O

Ronald O
Cardenas O
, O
Ying O
Lin O
, O
Heng O
Ji O
, O
and O
Jonathan O
May O
. O

2019 O
. O

A O
grounded O
unsupervised O
universal O
partof O
- O
speech O
tagger O
for O
low O
- O
resource O
languages O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
2428–2439 O
. O

Branden O
Chan O
, O
Stefan O
Schweter O
, O
and O
Timo O
Möller O
. O
2020 O
. O

German O
’s O
next O
language O
model O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
6788–6796 O
, O
Barcelona O
, O
Spain O
( O
Online O
) O
. O

International O
Committee O
on O
Computational O
Linguistics O
. O

Hyung O
Won O
Chung O
, O
Thibault O
Fevry O
, O
Henry O
Tsai O
, O
Melvin O
Johnson O
, O
and O
Sebastian O
Ruder O
. O
2020 O
. O

Rethinking O
embedding O
coupling O
in O
pre O
- O
trained O
language O
models O
. O

In O
International O
Conference O
on O
Learning O
Representations O
. O

Jonathan O
H O
Clark O
, O
Dan O
Garrette O
, O
Iulia O
Turc O
, O
and O
John O
Wieting O
. O
2022 O
. O

Canine O
: O
Pre O
- O
training O
an O
efﬁcient O
tokenization O
- O
free O
encoder O
for O
language O
representation O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
10:73–91 O
. O
Alexis O
Conneau O
, O
Kartikay O
Khandelwal O
, O
Naman O
Goyal O
, O
Vishrav O
Chaudhary O
, O
Guillaume O
Wenzek O
, O
Francisco O
Guzmán O
, O
Edouard O
Grave O
, O
Myle O
Ott O
, O
Luke O
Zettlemoyer O
, O
and O
Veselin O
Stoyanov O
. O

2020 O
. O

Unsupervised O
cross O
- O
lingual O
representation O
learning O
at O
scale O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
8440 O
– O
8451 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Pieter O
Delobelle O
, O
Thomas O
Winters O
, O
and O
Bettina O
Berendt O
. O
2020 O
. O

RobBERT O
: O
a O
Dutch O
RoBERTa O
- O
based O
Language O
Model O
. O

In O
Findings O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
EMNLP O
2020 O
, O
pages O
3255–3265 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

Bert O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
. O

Long O
Duong O
, O
Trevor O
Cohn O
, O
Karin O
Verspoor O
, O
Steven O
Bird O
, O
and O
Paul O
Cook O
. O

2014 O
. O

What O
can O
we O
get O
from O
1000 O
tokens O
? O

a O
case O
study O
of O
multilingual O
pos O
tagging O
for O
resource O
- O
poor O
languages O
. O

In O
Proceedings O
of5356the O
2014 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
886–897 O
. O
Meng O
Fang O
and O
Trevor O
Cohn O
. O
2016 O
. O

Learning O
when O
to O
trust O
distant O
supervision O
: O
An O
application O
to O
lowresource O
pos O
tagging O
using O
cross O
- O
lingual O
projection O
. O

InProceedings O
of O
The O
20th O
SIGNLL O
Conference O
on O
Computational O
Natural O
Language O
Learning O
, O
pages O
178–186 O
. O

G O
David O
Forney O
. O

1973 O
. O

The O
viterbi O
algorithm O
. O

Proceedings O
of O
the O
IEEE O
, O
61(3):268–278 O
. O

Dan O
Garrette O
and O
Jason O
Baldridge O
. O

2013 O
. O

Learning O
a O
part O
- O
of O
- O
speech O
tagger O
from O
two O
hours O
of O
annotation O
. O

In O
Proceedings O
of O
the O
2013 O
conference O
of O
the O
North O
American O
chapter O
of O
the O
association O
for O
computational O
linguistics O
: O
Human O
language O
technologies O
, O
pages O
138–147 O
. O

Dan O
Garrette O
, O
Jason O
Mielens O
, O
and O
Jason O
Baldridge O
. O
2013 O
. O

Real O
- O
world O
semi O
- O
supervised O
learning O
of O
postaggers O
for O
low O
- O
resource O
languages O
. O

In O
Proceedings O
of O
the O
51st O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
583–592 O
. O

Kyle O
Jerro O
. O

2016 O
. O

The O
locative O
applicative O
and O
the O
semantics O
of O
verb O
class O
in O
kinyarwanda O
. O

Diversity O
in O
African O
languages O
, O
page O
289 O
. O

Pratik O
Joshi O
, O
Sebastin O
Santy O
, O
Amar O
Budhiraja O
, O
Kalika O
Bali O
, O
and O
Monojit O
Choudhury O
. O

2020 O
. O

The O
state O
and O
fate O
of O
linguistic O
diversity O
and O
inclusion O
in O
the O
nlp O
world O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
6282–6293 O
. O

Guolin O
Ke O
, O
Di O
He O
, O
and O
Tie O
- O
Yan O
Liu O
. O
2020 O
. O

Rethinking O
positional O
encoding O
in O
language O
pre O
- O
training O
. O

In O
International O
Conference O
on O
Learning O
Representations O
. O

Alexandre O
Kimenyi O
. O

1976 O
. O

Subjectivization O
rules O
in O
kinyarwanda O
. O

In O
Annual O
Meeting O
of O
the O
Berkeley O
Linguistics O
Society O
, O
volume O
2 O
, O
pages O
258–268 O
. O

Alexandre O
Kimenyi O
. O

1978a O
. O

Aspects O
of O
naming O
in O
kinyarwanda O
. O

Anthropological O
linguistics O
, O
20(6):258–271 O
. O

Alexandre O
Kimenyi O
. O

1978b O
. O

A O
relational O
grammar O
of O
kinyarwanda O
. O

University O
of O
California O
, O
Publications O
in O
Linguistics O
Berkeley O
, O
Cal O
, O
91:1–248 O
. O

Alexandre O
Kimenyi O
. O

1988 O
. O

Passiveness O
in O
kinyarwanda O
. O

In O
Passive O
and O
Voice O
, O
page O
355 O
. O

John O
Benjamins O
. O

Stav O
Klein O
and O
Reut O
Tsarfaty O
. O

2020 O
. O

Getting O
the O
# O
# O
life O
out O
of O
living O
: O
How O
adequate O
are O
word O
- O
pieces O
for O
modelling O
complex O
morphology O
? O

In O
Proceedings O
of O
the O
17th O
SIGMORPHON O
Workshop O
on O
Computational O
Research O
in O
Phonetics O
, O
Phonology O
, O
and O
Morphology O
, O
pages O
204–209 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Kimmo O
Koskenniemi O
. O

1983 O
. O

Two O
- O
level O
model O
for O
morphological O
analysis O
. O

In O
IJCAI O
, O
volume O
83 O
, O
pages O
683–685 O
. O

Fajri O
Koto O
, O
Afshin O
Rahimi O
, O
Jey O
Han O
Lau O
, O
and O
Timothy O
Baldwin O
. O

2020 O
. O

Indolem O
and O
indobert O
: O
A O
benchmark O
dataset O
and O
pre O
- O
trained O
language O
model O
for O
indonesian O
nlp O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
757–770 O
. O
John O
Koutsikakis O
, O
Ilias O
Chalkidis O
, O
Prodromos O
Malakasiotis O
, O
and O
Ion O
Androutsopoulos O
. O

2020 O
. O

Greek O
- O
bert O
: O
The O
greeks O
visiting O
sesame O
street O
. O

In O
11th O
Hellenic O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
pages O
110 O
– O
117 O
. O

Julia O
Kreutzer O
, O
Isaac O
Caswell O
, O
Lisa O
Wang O
, O
Ahsan O
Wahab O
, O
Daan O
van O
Esch O
, O
Nasanbayar O
Ulzii O
- O
Orshikh O
, O
Allahsera O
Tapo O
, O
Nishant O
Subramani O
, O
Artem O
Sokolov O
, O
Claytone O
Sikasote O
, O
et O
al O
. O
2022 O
. O

Quality O
at O
a O
glance O
: O
An O
audit O
of O
web O
- O
crawled O
multilingual O
datasets O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
10:50–72 O
. O

Y O
Kuratov O
and O
M O
Arkhipov O
. O

2019 O
. O

Adaptation O
of O
deep O
bidirectional O
multilingual O
transformers O
for O
russian O
language O
. O

In O
Komp’juternaja O
Lingvistika O
i O
Intellektual’nye O
Tehnologii O
, O
pages O
333–339 O
. O
Hang O
Le O
, O
Loïc O
Vial O
, O
Jibril O
Frej O
, O
Vincent O
Segonne O
, O
Maximin O
Coavoux O
, O
Benjamin O
Lecouteux O
, O
Alexandre O
Allauzen O
, O
Benoit O
Crabbe O
, O
Laurent O
Besacier O
, O
and O
Didier O
Schwab O
. O

2020 O
. O

Flaubert O
: O

Unsupervised O
language O
model O
pre O
- O
training O
for O
french O
. O

In O
LREC O
. O

Wentao O
Ma O
, O
Yiming O
Cui O
, O
Chenglei O
Si O
, O
Ting O
Liu O
, O
Shijin O
Wang O
, O
and O
Guoping O
Hu O
. O
2020 O
. O

CharBERT O
: O
Character O
- O
aware O
pre O
- O
trained O
language O
model O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
39–50 O
, O
Barcelona O
, O
Spain O
( O
Online O
) O
. O

International O
Committee O
on O
Computational O
Linguistics O
. O

Louis O
Martin O
, O
Benjamin O
Muller O
, O
Pedro O
Javier O
Ortiz O
Suárez O
, O
Yoann O
Dupont O
, O
Laurent O
Romary O
, O
Éric O
de O
la O
Clergerie O
, O
Djamé O
Seddah O
, O
and O
Benoît O
Sagot O
. O
2020 O
. O

CamemBERT O
: O
a O
tasty O
French O
language O
model O
. O

InProceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
7203–7219 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Mihai O
Masala O
, O
Stefan O
Ruseti O
, O
and O
Mihai O
Dascalu O
. O

2020 O
. O

Robert O
– O
a O
romanian O
bert O
model O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
6626–6637 O
. O

John O
J O
McCarthy O
. O

1981 O
. O

A O
prosodic O
theory O
of O
nonconcatenative O
morphology O
. O

Linguistic O
inquiry O
, O
12(3):373–418 O
. O

Jason O
Mielens O
, O
Liang O
Sun O
, O
and O
Jason O
Baldridge O
. O
2015 O
. O

Parse O
imputation O
for O
dependency O
annotations O
. O

In5357Proceedings O
of O
the O
53rd O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
7th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1385 O
– O
1394 O
. O

Tomas O
Mikolov O
, O
Ilya O
Sutskever O
, O
Kai O
Chen O
, O
Greg O
S O

Corrado O
, O
and O
Jeff O
Dean O
. O

2013 O
. O

Distributed O
representations O
of O
words O
and O
phrases O
and O
their O
compositionality O
. O

Advances O
in O
neural O
information O
processing O
systems O
, O
26 O
. O

Mahdi O
Mohseni O
and O
Amirhossein O
Tebbifakhr O
. O

2019 O
. O

MorphoBERT O
: O
a O
Persian O
NER O
system O
with O
BERT B-MethodName
and O
morphological O
analysis O
. O

In O
Proceedings O
of O
The O
First O
International O
Workshop O
on O
NLP O
Solutions O
for O
Under O
Resourced O
Languages O
( O
NSURL O
2019 O
) O
colocated O
with O
ICNLSP O
2019 O
- O
Short O
Papers O
, O
pages O
23 O
– O
30 O
, O
Trento O
, O
Italy O
. O

Association O
for O
Computational O
Linguistics O
. O

Jackson O
Muhirwe O
. O

2007 O
. O

Computational O
analysis O
of O
kinyarwanda O
morphology O
: O

The O
morphological O
alternations O
. O

International O
Journal O
of O
computing O
and O
ICT O
Research O
, O
1(1):85–92 O
. O

Jackson O
Muhirwe O
. O

2009 O
. O

Morphological O
analysis O
of O
tone O
marked O
kinyarwanda O
text O
. O

In O
International O
Workshop O
on O
Finite O
- O
State O
Methods O
and O
Natural O
Language O
Processing O
, O
pages O
48–55 O
. O

Springer O
. O

Wilhelmina O
Nekoto O
, O
Vukosi O
Marivate O
, O
Tshinondiwa O
Matsila O
, O
Timi O
Fasubaa O
, O
Taiwo O
Fagbohungbe O
, O
Solomon O
Oluwole O
Akinola O
, O
Shamsuddeen O
Muhammad O
, O
Salomon O
Kabongo O
Kabenamualu O
, O
Salomey O
Osei O
, O
Freshia O
Sackey O
, O
Rubungo O
Andre O
Niyongabo O
, O
Ricky O
Macharm O
, O
Perez O
Ogayo O
, O
Orevaoghene O
Ahia O
, O
Musie O
Meressa O
Berhe O
, O
Mofetoluwa O
Adeyemi O
, O
Masabata O
Mokgesi O
- O
Selinga O
, O
Lawrence O
Okegbemi O
, O
Laura O
Martinus O
, O
Kolawole O
Tajudeen O
, O
Kevin O
Degila O
, O
Kelechi O
Ogueji O
, O
Kathleen O
Siminyu O
, O
Julia O
Kreutzer O
, O
Jason O
Webster O
, O
Jamiil O
Toure O
Ali O
, O
Jade O
Abbott O
, O
Iroro O
Orife O
, O
Ignatius O
Ezeani O
, O
Idris O
Abdulkadir O
Dangana O
, O
Herman O
Kamper O
, O
Hady O
Elsahar O
, O
Goodness O
Duru O
, O
Ghollah O
Kioko O
, O
Murhabazi O
Espoir O
, O
Elan O
van O
Biljon O
, O
Daniel O
Whitenack O
, O
Christopher O
Onyefuluchi O
, O
Chris O
Chinenye O
Emezue O
, O
Bonaventure O
F. O
P. O
Dossou O
, O
Blessing O
Sibanda O
, O
Blessing O
Bassey O
, O
Ayodele O
Olabiyi O
, O
Arshath O
Ramkilowan O
, O
Alp O
Öktem O
, O
Adewale O
Akinfaderin O
, O
and O
Abdallah O
Bashir O
. O

2020 O
. O

Participatory O
research O
for O
low O
- O
resourced O
machine O
translation O
: O
A O
case O
study O
in O
African O
languages O
. O

InFindings O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
EMNLP O
2020 O
, O
pages O
2144–2160 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Dat O
Quoc O
Nguyen O
and O
Anh O
Tuan O
Nguyen O
. O

2020 O
. O

PhoBERT O
: O
Pre O
- O
trained O
language O
models O
for O
Vietnamese O
. O

In O
Findings O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
EMNLP O
2020 O
, O
pages O
1037–1042 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Rubungo O
Andre O
Niyongabo O
, O
Qu O
Hong O
, O
Julia O
Kreutzer O
, O
and O
Li O
Huang O
. O

2020 O
. O

Kinnews O
and O
kirnews O
: O
Benchmarking O
cross O
- O
lingual O
text O
classiﬁcation O
forkinyarwanda O
and O
kirundi O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
5507–5521 O
. O

Derek O
Nurse O
and O
Gérard O
Philippson O
. O

2006 O
. O

The O
bantu O
languages O
. O

Routledge O
. O

Antoine O
Nzeyimana O
. O

2020 O
. O

Morphological O
disambiguation O
from O
stemming O
data O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
4649–4660 O
, O
Barcelona O
, O
Spain O
( O
Online O
) O
. O

International O
Committee O
on O
Computational O
Linguistics O
. O

Myle O
Ott O
, O
Sergey O
Edunov O
, O
Alexei O
Baevski O
, O
Angela O
Fan O
, O
Sam O
Gross O
, O
Nathan O
Ng O
, O
David O
Grangier O
, O
and O
Michael O
Auli O
. O

2019 O
. O

fairseq O
: O
A O
fast O
, O
extensible O
toolkit O
for O
sequence O
modeling O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Demonstrations O
) O
, O
pages O
48–53 O
, O
Minneapolis O
, O
Minnesota O
. O

Association O
for O
Computational O
Linguistics O
. O

Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christopher O
D O
Manning O
. O

2014 O
. O

Glove O
: O
Global O
vectors O
for O
word O
representation O
. O

In O
Proceedings O
of O
the O
2014 O
conference O
on O
empirical O
methods O
in O
natural O
language O
processing O
( O
EMNLP O
) O
, O
pages O
1532–1543 O
. O

Matthew O
E. O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O

2018 O
. O

Deep O
contextualized O
word O
representations O
. O

In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
Papers O
) O
, O
pages O
2227–2237 O
, O
New O
Orleans O
, O
Louisiana O
. O

Association O
for O
Computational O
Linguistics O
. O

Sello O
Ralethe O
. O

2020 O
. O

Adaptation O
of O
deep O
bidirectional O
transformers O
for O
afrikaans O
language O
. O

In O
Proceedings O
of O
The O
12th O
Language O
Resources O
and O
Evaluation O
Conference O
, O
pages O
2475–2478 O
. O

Shruti O
Rijhwani O
, O
Shuyan O
Zhou O
, O
Graham O
Neubig O
, O
and O
Jaime O
G O
Carbonell O
. O

2020 O
. O

Soft O
gazetteers O
for O
lowresource O
named O
entity O
recognition O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
8118–8123 O
. O
Piotr O
Rybak O
, O
Robert O
Mroczkowski O
, O
Janusz O
Tracz O
, O
and O
Ireneusz O
Gawlik O
. O

2020 O
. O

Klej O
: O
Comprehensive O
benchmark O
for O
polish O
language O
understanding O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
1191 O
– O
1201 O
. O

Jonne O
Sälevä O
and O
Constantine O
Lignos O
. O

2021 O
. O

Mining O
wikidata O
for O
name O
resources O
for O
african O
languages O
. O

arXiv O
preprint O
arXiv:2104.00558 O
. O

Raphael O
Scheible O
, O
Fabian O
Thomczyk O
, O
Patric O
Tippmann O
, O
Victor O
Jaravine O
, O
and O
Martin O
Boeker O
. O

2020 O
. O

Gottbert O
: O
a O
pure O
german O
language O
model O
. O

arXiv O
preprint O
arXiv:2012.02110 O
.5358Rico O

Sennrich O
, O
Barry O
Haddow O
, O
and O
Alexandra O
Birch O
. O
2016 O
. O

Neural O
machine O
translation O
of O
rare O
words O
with O
subword O
units O
. O

In O
Proceedings O
of O
the O
54th O

Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1715 O
– O
1725 O
, O
Berlin O
, O
Germany O
. O

Association O
for O
Computational O
Linguistics O
. O

Fábio O
Souza O
, O
Rodrigo O
Nogueira O
, O
and O
Roberto O
Lotufo O
. O
2020 O
. O

Bertimbau O
: O
Pretrained O
bert O
models O
for O
brazilian O
portuguese O
. O

In O
Brazilian O
Conference O
on O
Intelligent O
Systems O
, O
pages O
403–417 O
. O

Springer O
. O

Liang O
Sun O
, O
Jason O
Mielens O
, O
and O
Jason O
Baldridge O
. O

2014 O
. O

Parsing O
low O
- O
resource O
languages O
using O
gibbs O
sampling O
for O
pcfgs O
with O
latent O
annotations O
. O

In O
Proceedings O
of O
the O
2014 O
conference O
on O
empirical O
methods O
in O
natural O
language O
processing O
( O
EMNLP O
) O
, O
pages O
290 O
– O
300 O
. O

Yoshimasa O
Tsuruoka O
and O
Jun’ichi O
Tsujii O
. O

2005 O
. O

Bidirectional O
inference O
with O
the O
easiest-ﬁrst O
strategy O
for O
tagging O
sequence O
data O
. O

In O
Proceedings O
of O
Human O
Language O
Technology O
Conference O
and O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
467–474 O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
Lukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
NIPS O
. O
Antti O
Virtanen O
, O
Jenna O
Kanerva O
, O
Rami O
Ilo O
, O
Jouni O
Luoma O
, O
Juhani O
Luotolahti O
, O
Tapio O
Salakoski O
, O
Filip O
Ginter O
, O
and O
Sampo O
Pyysalo O
. O

2019 O
. O

Multilingual O
is O
not O
enough O
: O
Bert O
for O
ﬁnnish O
. O

arXiv O
preprint O
arXiv:1912.07076 O
. O

Alex O
Wang O
, O
Amanpreet O
Singh O
, O
Julian O
Michael O
, O
Felix O
Hill O
, O
Omer O
Levy O
, O
and O
Samuel O
R O
Bowman O
. O

2019 O
. O

Glue B-DatasetName
: O
A O
multi O
- O
task O
benchmark O
and O
analysis O
platform O
for O
natural O
language O
understanding O
. O

In O
7th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2019 O
. O

Linting O
Xue O
, O
Noah O
Constant O
, O
Adam O
Roberts O
, O
Mihir O
Kale O
, O
Rami O
Al O
- O
Rfou O
, O
Aditya O
Siddhant O
, O
Aditya O
Barua O
, O
and O
Colin O
Raffel O
. O
2021 O
. O

mT5 O
: O

A O
massively O
multilingual O
pre O
- O
trained O
text O
- O
to O
- O
text O
transformer O
. O

In O
Proceedings O
of O
the O
2021 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
483–498 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Manzil O
Zaheer O
, O
Guru O
Guruganesh O
, O
Kumar O
Avinava O
Dubey O
, O
Joshua O
Ainslie O
, O
Chris O
Alberti O
, O
Santiago O
Ontanon O
, O
Philip O
Pham O
, O
Anirudh O
Ravula O
, O
Qifan O
Wang O
, O
Li O
Yang O
, O
et O
al O
. O
2020 O
. O

Big O
bird O
: O

Transformers O
for O
longer O
sequences O
. O

Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
33:17283–17297.5359Appendix O
A O
Data O
Tables O
, O
Hyper O
- O
parameters O
& O
Additional O
results O
Module O
Values O
Morphology O
Encoder O
: O
Number B-HyperparameterName
of I-HyperparameterName
Layers I-HyperparameterName
4 B-HyperparameterValue
Attention B-HyperparameterName
heads I-HyperparameterName
4 B-HyperparameterValue
Hidden B-HyperparameterName
Size I-HyperparameterName
128 B-HyperparameterValue
Attention B-HyperparameterName
head I-HyperparameterName
size I-HyperparameterName
32 B-HyperparameterValue
FFN B-HyperparameterName
inner I-HyperparameterName
hidden I-HyperparameterName
size I-HyperparameterName
512 B-HyperparameterValue
Morphological B-HyperparameterName
embedding I-HyperparameterName
size I-HyperparameterName
128 B-HyperparameterValue
Sentence O
/ O
Document O
Encoder O
: O
Number B-HyperparameterName
of I-HyperparameterName
Layers I-HyperparameterName
12 B-HyperparameterValue
Attention B-HyperparameterName
heads I-HyperparameterName
12 B-HyperparameterValue
Hidden B-HyperparameterName
Size I-HyperparameterName
768 B-HyperparameterValue
Attention B-HyperparameterName
head I-HyperparameterName
size I-HyperparameterName
64 B-HyperparameterValue
FFN B-HyperparameterName
inner I-HyperparameterName
hidden I-HyperparameterName
size I-HyperparameterName
3072 B-HyperparameterValue
Stem B-HyperparameterName
embedding I-HyperparameterName
size I-HyperparameterName
256 B-HyperparameterValue
Table O
7 O
: O
KinyaBERT B-MethodName
Architectural O
dimensions O
. O

Model O
( O
# O
Params O
) O
Vocab O
. O

Size O
XLM O
- O
R O
( O
270 O
M O
): O
Sentence O
- O
Piece O
tokens O
250 O
K O
BERT B-MethodName
BPE O
( O
120 O
M O
): O
BPE O
Tokens O
43 O
K O
BERT B-MethodName
MORPHO O
( O
127 O
M O
): O
Morphemes O
& O
BPE O
Tokens O
51 O
K O
KinyaBERT B-MethodName
ADR O
( O
101 O
M O
): O
Stems O
& O
BPE O
Tokens O
34 O
K O
Afﬁxes O

0.3 O
K O
POS O

Tags O
0.2 O
K O
KinyaBERT B-MethodName
ASC O
( O
105 O
M O
): O
Stems O
& O
BPE O
Tokens O
34 O
K O
Afﬁx O
sets O
34 O
K O
Afﬁxes O
0.3 O
K O
POS O

Tags O
0.2 O
K O
Table O
8 O
: O
V O
ocabulary O
sizes O
for O
embedding O
layers O
. O

Hyper O
- O
parameter O
Values O
Dropout B-HyperparameterName
0.1 B-HyperparameterValue
Attention B-HyperparameterName
Dropout I-HyperparameterName

0.1 B-HyperparameterValue
Warmup B-HyperparameterName
Steps I-HyperparameterName
2 B-HyperparameterValue
K I-HyperparameterValue
Max B-HyperparameterName
Steps I-HyperparameterName
200 B-HyperparameterValue
K I-HyperparameterValue
Weight B-HyperparameterName
Decay I-HyperparameterName
0.01 B-HyperparameterValue
Learning B-HyperparameterName
Rate I-HyperparameterName
Decay I-HyperparameterName
Linear B-HyperparameterValue
Peak B-HyperparameterName
Learning I-HyperparameterName
Rate I-HyperparameterName
4e-4 B-HyperparameterValue
Batch B-HyperparameterName
Size I-HyperparameterName
2560 B-HyperparameterValue
Optimizer B-HyperparameterName
LAMB B-HyperparameterValue
Adam B-HyperparameterName
1e-6 B-HyperparameterValue
Adam B-HyperparameterName
 I-HyperparameterName
1 I-HyperparameterName
0.90 B-HyperparameterValue
Adam B-HyperparameterName
 I-HyperparameterName
2 I-HyperparameterName
0.98 B-HyperparameterValue
Gradient B-HyperparameterName
Clipping I-HyperparameterName
0 B-HyperparameterValue
Table O
9 O
: O
Pre O
- O
training O
hyper O
- O
parameters O
Category O
# O
Articles O
entertainment O
3000 O
sports O
3000 O
security O
3000 O
economy O
3000 O
health O
3000 O
politics O
3000 O
religion O
2020 O
development O
1813 O
technology O
1105 O
culture O
994 O
relationships O
940 O
people O
852 O
Total O
25724 O
Table O
10 O
: O
NEWS O
categorization O
dataset O
label O
distribution O
. O

Score O
Translation O
quality O
1 O
Invalid O
or O
meaningless O
translation O
2 O
Invalid O
but O
not O
totally O
wrong O
3 O
Almost O
valid O
, O
but O
not O
totally O
correct O
4 O
Valid O
and O
correct O
translation O
Table O
11 O
: O
Machine O
- O
translated O
GLUE B-DatasetName
benchmark O
scoring O
prompt O
levels.5360POS O
Tag O
~Ppweight O
Description O
Example O
V#000 O
1.8 O
Inﬁnitive O
Verb O
kuvuga O
‘ O
to O
say O
’ O
V#001 O
1 O
Gerund O
or O
verbal O
noun O
uwavuze O
‘ O
the O
one O
who O
said O
’ O
V#002 O
1.5 O
Imperative O
verb O
vuga O
‘ O
say O
’ O
V#004 O
1.5 O
Continuous O
present O
verb O
aracyavuga O
‘ O
she O
is O
still O
saying O
’ O
V#005 O
1.5 O
Past O
tense O
verb O
yaravuze O

‘ O
she O
said O
’ O
V#006 O
1.5 O
Future O
tense O
verb O
azavuga O
‘ O
she O
will O
say O
’ O
V#010 O
1.5 O
Verb O
without O
tense O
mark O
avuga O
‘ O
saying O
’ O
N#011 O
1 O
Noun O
without O
augmment O
( O
wa)muntu O
‘ O
person O
’ O
N#012 O
2 O
Noun O
with O
augment O
umuntu O
‘ O
a O
person O
’ O
DE#013 O
2 O
Demonstrative O
ng- O
nguyu O
‘ O
this O
is O
her O
’ O
DE#020 O
3 O
Personal O
demonstrative O
wowe O
‘ O
you O
’ O
DE#021 O
2 O
Demonstrative O
with O
augment O
uwo O
‘ O
this O
( O
person O
) O
’ O
PO#025 O
2 O
Possessive O
+ O
augment O
+ O
owner O
uwawe O
‘ O
yours O
’ O
QA#026 O
0.5 O
Qualiﬁcative O
adjective O
+ O
augment O
+ O
bu O
ubuto O
‘ O
littleness O
’ O
QA#027 O
1 O
Qualiﬁcative O
adjective O
+ O
augment O
-bu O
umuto O
‘ O
the O
little O
one O
’ O
QA#028 O
2.5 O
Qualiﬁcative O
adjective O
-augment O
muto O
‘ O
little O
’ O
QA#029 O
3 O
Qualiﬁcative O
adjective O
-augment O
+ O
reduplication O
mutomuto O
‘ O
( O
kind O
of O
) O
little O
’ O
NU#030 O
2.5 O
Numeral O
babiri O
‘ O
two O
( O
people O
) O
’ O
OT#033 O
2.5 O
Quoting O
-ti O
bati O
: O
‘ O
they O
said O
: O
’ O
NP#035 O
2 O
Proper O
names O
Yohana O
‘ O
John O
’ O
DI#036 O
3 O
Digits O
84 O
AD#037 O
2.5 O
Adverb O
bucece O
‘ O
silently O
’ O
VC#038 O
2.5 O
Conjunctive O
adverbs O
hanyuma O
‘ O
and O
then O
’ O
CO#039 O
2.5 O
Commanding O
expressions O
cyono O
‘ O
please O
’ O
CA#040 O
2.5 O
Calling O
expressions O
yewe O
‘ O
you O
’ O
QU#044 O
3 O
Questioning O
adverb O

he O
he O
‘ O

where O
’ O
SP#054 O
2.5 O
Spatial O
hakurya O
‘ O
over O
there O
’ O
TE#055 O
2.5 O
Temporal O
kare O
‘ O
early O
’ O
RL#056 O
3 O
Relatives O
masenge O
‘ O
my O
aunt O
’ O
PR#057 O
3 O
Prepositions O
ku O
‘ O
on O
’ O
OR#064 O
2.5 O
Orientations O
amajyaruguru O
‘ O
north O
’ O
AJ#065 O
2.5 O
Adjectives O
rusange O
‘ O
common O
’ O
NN#066 O
2.5 O
Nominal O
loanwords O
kopi O
‘ O
copy O
’ O
HR#067 O
3 O
Hours O
( O
saa O
) O
mbiri O
‘ O
eight O
o’clock O
’ O
DT#068 O
2.5 O
Date O
taliki O
‘ O
date O
’ O
EN#069 O
3 O
Common O
English O
terms O
live O
, O
like O
, O
share O
IJ#070 O
2.5 O
Interjections O
dorere O
‘ O
see O
! O
’ O

CJ#071 O
3 O
Conjunctions O
ko O
‘ O
that O
’ O
CP#078 O
3 O
Copula O
ni O

‘ O
it O
is O
’ O
RE#079 O
3 O
Responses O
yego O
‘ O
yes O
’ O
UN#083 O
3 O
Measuring O
units O
metero O
‘ O
meter O
’ O
MO#084 O
4 O
Months O
Mutarama O
‘ O
January O
’ O
PT#085 O
3 O
Punctuations O
. O

Table O
12 O
: O
Examples O
of O
POS O
tags O
used O
in O
KinyaBERT B-MethodName
along O
with O
precedence O
weights O
~Pp(xtjyt)in O
Equation O
2.5361Afﬁx O
Set O
Example O
Surface O
form O
V:2 O
: O
ku O
- O
V:18 O
: O
a O
ku O
- O
gend O
- O
a O
kugenda O
‘ O
to O
walk O
’ O
N:0 O
: O
u O
- O
N:1 O
: O
mu O
u O
- O
mu O
- O
ntu O
umuntu O
‘ O
a O
person O
’ O
PO:1 O
: O
i O
i O
- O
a O
- O
cu O
yacu O
‘ O
our O
’ O
N:0 O
: O
i O
- O
N:1 O
: O
n O
i O
- O
n O
- O
kiko O
inkiko O
‘ O
courts O
’ O
PO:1 O
: O
u O
u O
- O
a O
- O
bo O
wabo O
‘ O
their O
’ O
V:2 O
: O
a O
- O
V:4 O
: O
a O
- O
V:18 O
: O
ye O
a O
- O
a O
- O
bon O
- O
ye O
yabonye O
‘ O
she O
saw O
’ O
DE:1 O
: O
u O
- O
DE:2 O
: O
u O
u O
- O
u O
- O
o O
uwo O
‘ O
that O
’ O
V:2 O
: O
u O
- O
V:4 O
: O
a O
- O
V:17 O
: O
w O
- O
V:18 O
: O
ye O
u O
- O
a O
- O
vug O
- O
w O
- O
ye O
wavuzwe O
‘ O
who O
was O
talked O
about O
’ O
QA:1 O
: O
ki O
- O
QA:3 O
: O
ki O
- O
QA:4 O
: O
re O
ki O
- O
re O
- O
ki O
- O
re O
kirekire O
‘ O
tall O
’ O
Table O
13 O
: O
Examples O
of O
afﬁx O
sets O
used O
by O
KinyaBERT B-MethodName
ASC O
; O
there O
are O
34 O
K O
sets O
in O
total O
. O

Hyperparameter O
MRPC O
QNLI O

RTE O
SST-2 O
STS O
- O
B O
WNLI O
NER O
NEWS O
Peak O
Learning O
Rate O

1e-5 O

1e-5 O
2e-5 O

1e-5 O
2e-5 O

1e-5 O
5e-5 O
1e-5 O
Batch O
Size O
16 O
32 O
16 O
32 O
16 O
16 O
32 O
32 O

Learning O
Rate O
Decay O
Linear O

Linear O

Linear O

Linear O
Linear O
Linear O
Linear O
Linear O
Weight O
Decay O
0.1 O
0.1 O
0.1 O
0.1 O
0.1 O
0.1 O
0.1 O
0.1 O
Max O
Epochs O
15 O
15 O
15 O
15 O
15 O
15 O
30 O
15 O
Warmup O
Steps O
proportion O
6 O
% O
6 O
% O
6 O
% O
6 O
% O
6 O
% O
6 O
% O
6 O
% O
6 O
% O
Optimizer O
AdamW O

AdamW O
AdamW O
AdamW O
AdamW O
AdamW O
AdamW O
AdamW O
Table O
14 O
: O
Downstream O
task O
ﬁne O
- O
tuning O
hyper O
- O
parameters O
. O

Paper O
LanguagePre O
- O
training O
Positional O
Input O
Tasks O
Embedding O
Representation O
Mohseni O
and O
Tebbifakhr O
( O
2019 O
) O
Persian O
MLM+NSP O
Absolute O
Morphemes O
Kuratov O
and O
Arkhipov O
( O
2019 O
) O
Russian O
MLM+NSP O
Absolute O
BPE O
Masala O
et O
al O
. O

( O
2020 O
) O

Romanian O
MLM+NSP O
Absolute O
BPE O
Baly O
et O
al O
. O

( O
2020 O
) O

Arabic O
WWM+NSP O
Absolute O
BPE O
Koto O
et O

al O
. O

( O
2020 O
) O

Indonesian O
MLM+NSP O
Absolute O
BPE O
Chan O
et O

al O
. O

( O
2020 O
) O

German O
WWM O
Absolute O
BPE O
Delobelle O

et O
al O
. O

( O
2020 O
) O

Dutch O
MLM O
Absolute O
BPE O
Nguyen O
and O
Tuan O
Nguyen O
( O
2020 O
) O

Vietnamese O
MLM O
Absolute O
BPE O
Canete O
et O

al O
. O

( O
2020 O
) O

Spanish O
WWM O
Absolute O
BPE O
Rybak O
et O
al O
. O

( O
2020 O
) O

Polish O
MLM O
Absolute O
BPE O
Martin O
et O
al O
. O

( O
2020 O
) O

French O
MLM O
Absolute O
BPE O
Le O
et O
al O
. O

( O
2020 O
) O

French O
MLM O
Absolute O
BPE O
Koutsikakis O

et O

al O
. O

( O
2020 O
) O

Greek O
MLM+NSP O
Absolute O
BPE O
Souza O
et O
al O
. O

( O
2020 O
) O

Portuguese O
MLM O
Absolute O
BPE O
Ralethe O
( O
2020 O
) O
Afrikaans O
MLM+NSP O
Absolute O
BPE O
This O
work O
Kinyarwanda O
MLM O
: O
STEM+AFFIXES O
TUPE O
- O
R O
Morphemes+BPE O
Table O
15 O
: O
Comparison O
between O
KinyaBERT B-MethodName
and O
other O
monolingual O
BERT B-MethodName
- O
variant O
PLMs O
. O

We O
only O
compare O
with O
previous O
works O
that O
have O
been O
published O
in O
either O
journals O
or O
conferences O
as O
of O
August O
2021 O
. O

We O
excluded O
some O
extremely O
high O
- O
resource O
languages O
such O
as O
English O
and O
Chinese O
. O

MLM O
: O
Masked O
language O
model O
; O
NSP O
: O
Next O
Sentence O
Prediction O
; O
WWM O
: O
Whole O
Word O
Masked.5362BERT O
BPE O
; O
Average O
non O
- O
adjacent O
diagonal O
STDEV O
= O
0.81 O
for O
ji jj2[2;10 O
] O
BERT B-MethodName
MORPHO O
; O
Average O
non O
- O
adjacent O
diagonal O
STDEV O
= O
0.80 O
for O
ji jj2[2;10 O
] O
KinyaBERT B-MethodName
ADR O
; O
Average O
non O
- O
adjacent O
diagonal O
STDEV O
= O
0.75 O
for O
ji jj2[2;10 O
] O
KinyaBERT B-MethodName
ASC O
; O
Average O
non O
- O
adjacent O
diagonal O
STDEV O
= O
0.75 O
for O
ji jj2[2;10 O
] O
Figure O
3 O
: O
Visualization O
of O
the O
positional O
attention O
bias O
( O
normalized O
) O
of O
the O
12 O
attention O
heads O
. O

Each O
( O
i;j)attention O
bias O
( O
Ke O
et O
al O
. O
, O
2020 O
) O
indicates O
the O
positional O
correlations O
between O
the O
ithandjthwords O
/ O
tokens O
in O
a O
sentence.5363 O

