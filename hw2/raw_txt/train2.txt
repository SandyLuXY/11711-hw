Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
133–138 O
July O
5 O
- O
10 O
, O
2020 O
. O

c O

 O
2020 O
Association O
for O
Computational O
Linguistics133A O
Complete O
Shift O
- O
Reduce O
Chinese O
Discourse O
Parser O
with O
Robust O
Dynamic O
Oracle O
Shyh O
- O
Shiun O
Hung,1Hen O
- O
Hsen O
Huang,2,3and O
Hsin O
- O
Hsi O
Chen1,3 O
1Department O
of O
Computer O
Science O
and O
Information O
Engineering O
, O
National O
Taiwan O
University O
, O
Taiwan O
2Department O
of O
Computer O
Science O
, O
National O
Chengchi O
University O
, O
Taiwan O
3MOST O
Joint O
Research O
Center O
for O
AI O
Technology O
and O
All O
Vista O
Healthcare O
, O
Taiwan O
shhung@nlg.csie.ntu.edu.tw O
, O
hhhuang@nccu.edu.tw O
, O
hhchen@ntu.edu.tw O
Abstract O
This O
work O
proposes O
a O
standalone O
, O
complete O
Chinese O
discourse O
parser O
for O
practical O
applications O
. O

We O
approach O
Chinese O
discourse B-TaskName
parsing I-TaskName
from O
a O
variety O
of O
aspects O
and O
improve O
the O
shift O
- O
reduce O
parser O
not O
only O
by O
integrating O
the O
pre O
- O
trained O
text O
encoder O
, O
but O
also O
by O
employing O
novel O
training O
strategies O
. O

We O
revise O
the O
dynamic O
- O
oracle O
procedure O
for O
training O
the O
shift O
- O
reduce O
parser O
, O
and O
apply O
unsupervised O
data O
augmentation O
to O
enhance O
rhetorical B-TaskName
relation I-TaskName
recognition I-TaskName
. O

Experimental O
results O
show O
that O
our O
Chinese O
discourse O
parser O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
. O

1 O
Introduction O
Discourse B-TaskName
parsing I-TaskName
is O
one O
of O
the O
fundamental O
tasks O
in O
natural O
language O
processing O
( O
NLP O
) O
. O

Typical O
types O
of O
discourse B-TaskName
parsing I-TaskName
include O
hierarchical B-TaskName
discourse I-TaskName
parsing I-TaskName
and O
shallow O
discourse O
parsing O
. O

The O
former O
is O
aimed O
at O
ﬁnding O
the O
relationships O
among O
a O
series O
of O
neighboring O
elementary O
discourse O
units O
( O
EDUs O
) O
and O
further O
building O
up O
a O
hierarchical O
tree O
structure O
( O
Mann O
and O
Thompson O
, O
1988 O
) O
. O

Instead O
of O
establishing O
a O
tree O
structure O
, O
the O
latter O
ﬁnds O
the O
across O
- O
paragraph O
relations O
between O
all O
text O
units O
in O
a O
paragraph O
or O
a O
document O
. O

Based O
on O
Rhetorical O
Structure O
Theory O
Discourse O
Treebank O
( O
RST O
- O
DT O
) O
( O
Carlson O
et O
al O
. O
, O
2001a O
) O
, O
hierarchical B-TaskName
discourse I-TaskName
parsing I-TaskName
in O
English O
has O
been O
well O
- O
studied O
. O

This O
paper O
focuses O
on O
hierarchical B-TaskName
discourse I-TaskName
parsing I-TaskName
in O
Chinese O
. O

Previous O
work O
on O
hierarchical O
Chinese O
discourse B-TaskName
parsing I-TaskName
is O
mostly O
based O
on O
the O
RST O
- O
style O
Chinese O
Discourse O
Treebank O
( O
Li O
et O
al O
. O
, O
2014 O
) O
. O

To O
distinguish O
from O
the O
other O
Chinese O
Discourse O
Treebank O
( O
Zhou O
and O
Xue O
, O
2012 O
) O
, O
which O
is O
annotated O
with O
the O
PDTB O
- O
style O
for O
shallow B-TaskName
discourse I-TaskName
parsing I-TaskName
, O
we O
use O
the O
term O
CDTB-14 B-DatasetName
to O
refer O
to O
the O
RST O
- O
style O
one O
and O
the O
term O
CDTB-12 B-DatasetName
to O
refer O
to O
the O
PDTB O
- O
style O
one O
. O

Kong O
and O
Zhou O
( O
2017)propose O
a O
pipeline O
framework O
and O
generate O
the O
discourse O
parsing O
tree O
in O
a O
bottom O
- O
up O
way O
. O

Lin O
et O

al O
. O
( O
2018 O
) O
propose O
an O
end O
- O
to O
- O
end O
system O
based O
on O
a O
recursive O
neural O
network O
( O
RvNN O
) O
to O
construct O
the O
parsing O
tree O
with O
a O
CKY O
- O
like O
algorithm O
. O

Sun O
and O
Kong O
( O
2018 O
) O
use O
transition O
- O
based O
method O
with O
the O
stack O
augmented O
parser O
- O
interpreter O
neural O
network O
( O
SPINN O
) O
( O
Bowman O
et O
al O
. O
, O
2016 O
) O
as O
the O
backbone O
model O
, O
helping O
their O
model O
make O
a O
better O
prediction O
with O
the O
previous O
information O
. O

In O
this O
work O
, O
we O
attempt O
to O
construct O
a O
complete O
Chinese O
discourse O
parser O
, O
which O
supports O
all O
the O
four O
sub O
- O
tasks O
in O
hierarchical B-TaskName
discourse I-TaskName
parsing I-TaskName
, O
including O
EDU O
segmentation O
, O
tree O
structure O
construction O
, O
nuclearity O
labeling O
, O
and O
rhetorical B-TaskName
relation I-TaskName
recognition I-TaskName
. O

Given O
a O
paragraph O
, O
our O
parser O
extracts O
all O
EDUs O
, O
builds O
the O
tree O
structure O
, O
identiﬁes O
the O
nucleuses O
, O
and O
recognizes O
the O
rhetorical O
relations O
of O
all O
internal O
nodes O
. O

We O
propose O
a O
revised O
dynamic O
- O
oracle O
procedure O
( O
Yu O
et O
al O
. O
, O
2018 O
) O
for O
training O
the O
shift O
- O
reduce O
parser O
. O

Because O
of O
the O
limited O
training O
instances O
in O
CDTB-14 B-DatasetName
, O
we O
also O
address O
the O
data O
sparsity O
issue O
by O
introducing O
unsupervised O
data O
augmentation O
( O
Xie O
et O
al O
. O
, O
2019 O
) O
. O

Experimental O
results O
show O
that O
our O
methodology O
is O
effective O
, O
and O
our O
model O
outperforms O
all O
the O
previous O
models O
. O

The O
contributions O
of O
this O
work O
are O
three O
- O
fold O
shown O
as O
follows O
. O

1.We O
explore O
the O
task O
of O
Chinese O
discourse B-TaskName
parsing I-TaskName
with O
a O
variety O
of O
strategies O
, O
and O
our O
parser O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
. O

Our O
robust O
dynamic O
- O
oracle O
procedure O
can O
be O
applied O
to O
other O
shift O
- O
reduce O
parsers O
. O
2.Our O

complete O
Chinese O
discourse O
parser O
handles O
a O
raw O
paragraph O
/ O
document O
directly O
and O
performs O
all O
the O
subtasks O
in O
hierarchical B-TaskName
discourse I-TaskName
parsing I-TaskName
. O

No O
pre O
- O
processing O
procedures O
such O
as O
Chinese O
word O
segmentation O
, O
POStagging O
, O
and O
syntactic O
parsing O
are O
required.1343.We O
release O
the O
pre O
- O
trained O
, O
standalone O
, O
readyto O
- O
use O
parser O
as O
a O
resource O
for O
the O
research O
community.1 O
2 O
Methodology O
Figure O
1 O
gives O
an O
overview O
of O
our O
parser O
. O

Five O
stages O
are O
performed O
to O
transform O
a O
raw O
document O
into O
a O
parse O
tree O
: O
EDU O
segmentation O
, O
tree O
structure O
construction O
, O
rhetorical O
relation O
and O
nuclearity O
classiﬁcation O
, O
binary O
tree O
conversion O
, O
and O
beam O
search O
. O

2.1 O
Elementary O
Discourse O
Unit O
Segmentation O
Typically O
, O
EDU O
segmentation O
is O
a O
sequence O
labeling O
task O
( O
Wang O
et O
al O
. O
, O
2018 O
; O
Peters O
et O
al O
. O
, O
2018 O
) O
. O

We O
propose O
a O
model O
for O
labeling O
each O
Chinese O
character O
in O
a O
raw O
document O
. O

The O
Begin O
- O
Inside O
scheme O
is O
employed O
that O
the O
word O
beginning O
with O
a O
new O
EDU O
will O
be O
labeled O
as O
B O
, O
and O
the O
rest O
of O
the O
words O
will O
be O
labeled O
as O
I. O
Our O
model O
is O
based O
on O
the O
pretrained O
text O
encoder O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O

More O
speciﬁcally O
, O
we O
adopt O
the O
version O
BERT O
- O
base O
, O
Chinese O
since O
this O
is O
the O
only O
pre O
- O
trained O
BERT O
dedicated O
to O
Chinese O
so O
far O
. O

As O
the O
BERT O
for O
Chinese O
is O
character O
- O
based O
, O
we O
feed O
each O
Chinese O
character O
into O
a O
BERT O
layer O
to O
obtain O
its O
contextual O
embedding O
. O

Then O
, O
we O
ﬁne O
tune O
the O
representation O
with O
an O
additional O
dense O
layer O
and O
measure O
the O
probability O
of O
each O
label O
of O
each O
character O
with O
a O
softmax O
layer O
. O

The O
model O
is O
further O
trained O
as O
conditional O
random O
ﬁelds O
( O
CRFs O
) O
( O
Lafferty O
et O
al O
. O
, O
2001 O
) O
for O
ﬁnding O
the O
global O
optimal O
label O
sequence O
. O

2.2 O
Tree O
Construction O
We O
propose O
a O
shift O
- O
reduce O
parser O
for O
building O
the O
structure O
of O
the O
discourse O
parse O
tree O
. O

A O
shift O
- O
reduce O
parser O
maintains O
a O
stack O
and O
a O
queue O
for O
representing O
a O
state O
during O
parsing O
, O
and O
an O
action O
classiﬁer O
is O
trained O
to O
predict O
the O
action O
( O
i.e. O
, O
shift O
or O
reduce O
) O
for O
making O
a O
transition O
from O
the O
given O
state O
to O
the O
next O
state O
. O

In O
the O
initial O
state O
, O
the O
stack O
is O
empty O
, O
and O
the O
queue O
contains O
all O
the O
EDUs O
in O
a O
raw O
document O
. O

In O
the O
ﬁnal O
state O
, O
the O
queue O
is O
empty O
, O
and O
the O
stack O
contains O
only O
one O
element O
, O
i.e. O
, O
the O
discourse O
parse O
tree O
of O
the O
whole O
paragraph O
. O

To O
decide O
whether O
to O
shift O
or O
to O
reduce O
, O
we O
propose O
an O
action O
classiﬁer O
by O
considering O
the O
information O
of O
the O
top O
two O
elements O
of O
the O
stack O
s1 O
ands2(i.e O
. O
, O
the O
two O
most O
recent O
discourse O
units O
) O
and O
the O
ﬁrst O
element O
of O
the O
queue O
q(i.e O
. O
, O
the O
next O
1https://github.com/jeffrey9977/ O
Chinese O
- O
Discourse O
- O
Parser O
- O
ACL2020 O
Raw O
documentClassifierSense O
, O
Center O
Reduce O
EDUs O
SegmenterBI O
III O
IBII O

I O
IIIII O
IIIIIBI O
IIIIIIIIIIIIIIIIIBIIIIIIII O
IIB O
I O
IIIIIIIIIIIIII O
  O

Converter O
stack O
queueShiftFigure O
1 O
: O
Overview O
of O
our O
Chinese O
discourse O
parser O
. O
EDU O
) O
. O

The O
textual O
form O
of O
each O
of O
these O
three O
discourse O
units O
will O
be O
fed O
into O
the O
BERT O
encoder O
for O
representing O
as O
Enc(s1),Enc(s2 O
) O
, O
andEnc(q O
) O
. O

Next O
, O
we O
concatenate O
the O
max O
pooling O
of O
Enc(s1 O
) O
, O
Enc(s2 O
) O
, O
andEnc(q)and O
feed O
the O
resulting O
vector O
into O
a O
dense O
layer O
to O
predict O
the O
next O
action O
. O

Since O
shift O
- O
reduce O
is O
a O
greedy O
algorithm O
, O
it O
can O
hardly O
recover O
from O
an O
error O
state O
. O

The O
shift O
- O
reduce O
parser O
is O
typically O
trained O
with O
the O
teacher O
mode O
, O
where O
only O
correct O
states O
are O
given O
, O
and O
the O
resulting O
parser O
may O
perform O
poor O
when O
it O
reaches O
unfamiliar O
states O
. O

For O
this O
reason O
, O
we O
propose O
a O
revised O
dynamic O
- O
oracle O
procedure O
( O
Yu O
et O
al O
. O
, O
2018 O
) O
for O
training O
our O
discourse O
parser O
. O

One O
drawback O
of O
the O
original O
dynamic O
oracle O
is O
that O
some O
golden O
training O
examples O
may O
be O
neglected O
. O

Because O
CDTB-14 B-DatasetName
has O
relatively O
few O
action O
steps O
to O
build O
a O
tree O
, O
the O
probability O
of O
falling O
into O
a O
wrong O
state O
is O
much O
small O
compared O
to O
that O
of O
RST O
- O
DT O
. O

In O
our O
revision O
, O
we O
want O
to O
guarantee O
all O
correct O
states O
have O
been O
trained O
. O

As O
shown O
in O
Algorithm O
1 O
, O
the O
document O
will O
be O
gone O
through O
twice O
when O
training O
a O
document O
example O
. O

We O
ﬁrst O
follow O
the O
golden O
actions O
, O
and O
choose O
action O
predicted O
by O
the O
model O
with O
a O
probability O
 O
at O
the O
second O
time O
. O

We O
refer O
to O
them O
as O
teacher O
mode O
and O
student O
mode O
, O
respectively O
. O

Note O
that O
we O
follow O
the O
suggestion O
of O
Yu O
et O

al O
. O
( O
2018 O
) O
to O
set O
 O
to O
0.7.135Algorithm O
1 O
Training O
Procedure O
for O
Our O
Shift O
- O
Reduce O
Discourse O
Parser O
. O

1 O
: O
S;Q O
empty O
stack O
, O
elementary O
discourse O
units O
2 O
: O
whileQis O
not O
empty_Shas O
more O
than O
1unitdo O
.Teacher O
mode O
3 O
: O
predicted;golden O
  O
ACTION O
CLASSIFIER O
( O
S O
: O
top O
1();S O
: O
top O
2();Q O
: O
front O
( O
) O
) O
, O
GOLDEN O
ACTION O
4 O
: O
COMPUTE O
LOSSANDUPDATE O
( O
predicted;golden O
) O
5 O
: O
PERFORM O
ACTION O
( O
golden O
) O
6 O
: O
S;Q O
empty O
stack O
, O
elementary O
discourse O
units O
7 O
: O
whileQis O
not O
empty_Shas O
more O
than O
1unitdo O
.Student O
mode O
8 O
: O

predicted;golden O
  O
ACTION O
CLASSIFIER O
( O
S O
: O
top O
1();S O
: O
top O
2();Q O
: O
front O
( O
) O
) O
, O
GOLDEN O
ACTION O
9 O
: O
COMPUTE O
LOSSANDUPDATE O
( O
predicted;golden O
) O
10 O
: O
ifrand O
( O
) O
> O
 O
then O
PERFORM O
ACTION O
( O
golden O
) O
elsePERFORM O
ACTION O
( O
predicted O
) O
2.3 O
Rhetorical B-TaskName
Relation I-TaskName
Recognition I-TaskName

If O
two O
discourse O
units O
are O
decided O
to O
be O
merged O
during O
the O
tree O
construction O
stage O
, O
a O
new O
internal O
node O
will O
be O
generated O
and O
the O
relationship O
of O
the O
two O
discourse O
units O
will O
be O
determined O
. O

Predicting O
the O
relation O
between O
two O
textual O
arguments O
is O
a O
typical O
classiﬁcation O
task O
in O
NLP O
. O

We O
propose O
a O
BERT O
- O
based O
classiﬁer O
, O
which O
predicts O
the O
relation O
of O
two O
arguments O
separated O
by O
the O
symbol O

[ O
SEP O
] O
, O
with O
additional O
dense O
layers O
as O
the O
output O
. O

In O
CDTB-14 B-DatasetName
, O
the O
“ O
coordination O
” O
relation O
accounts O
for O
59.6 O
% O
of O
the O
training O
data O
, O
while O
minor O
relations O
suffer O
from O
data O
sparseness O
. O

To O
address O
this O
issue O
, O
we O
introduce O
unsupervised O
data O
augmentation O
( O
UDA O
) O
( O
Xie O
et O
al O
. O
, O
2019 O
) O
to O
enhance O
the O
performance O
. O

We O
adopt O
the O
discourse O
pairs O
in O
CDTB-12 B-DatasetName
as O
the O
material O
for O
UDA O
. O

Note O
that O
other O
unlabeled O
text O
pairs O
can O
also O
be O
used O
for O
UDA O
. O

We O
chose O
those O
from O
CDTB-12 B-DatasetName
simply O
because O
the O
format O
is O
convenient O
for O
us O
to O
use O
. O

The O
original O
loss O
is O
shown O
as O
Eq O
. O

1 O
. O

Given O
a O
span O
of O
text O
x O
, O
our O
main O
model O
P()predicts O
the O
rhetorical O
relation O
yc O
. O

Eq O
. O
2 O
shows O
the O
additional O
consistency O
loss O
to O
enforce O
the O
smoothness O
of O
our O
main O
model O
, O
and O
^xstands O
for O
the O
augmented O
unlabeled O
sentence O
pair O
. O

LandUstand O
for O
labeled O
data O
and O
unlabeled O
data O
, O
respectively O
. O

As O
shown O
in O
Eq O
. O

3 O
, O
we O
train O
both O
objectives O
at O
the O
same O
time O
with O
a O
weight O
to O
adjust O
the O
effect O
of O
UDA O
. O
H= 1 O
NNX O
x2LMX O
c=1yclog O
( O
P(ycjx O
) O
) O
( O
1 O
) O
DKL= 1 O
NNX O
x2UP(yjx O
) O
logP(yjx O
) O

P(yj^x) O
( O
2 O
) O
L( O
) O

= O
H+DKL O
( O
3 O
) O

The O
UDA O
procedure O
ﬁrst O
generates O
the O
augmented O
unlabeled O
sentence O
pairs O
. O

Various O
ap O
- O
proaches O
to O
paraphrasing O
can O
be O
employed O
. O

In O
this O
work O
, O
we O
utilize O
the O
back O
- O
translation O
strategy O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
, O
where O
we O
translate O
the O
Chinese O
sentence O
pair O
to O
English O
and O
then O
translate O
back O
to O
Chinese O
. O

This O
is O
equivalent O
to O
add O
noises O
to O
the O
original O
inputs O
. O

As O
the O
original O
and O
the O
backtranslated O
sentence O
pairs O
express O
the O
same O
meaning O
, O
our O
model O
is O
expected O
to O
predict O
the O
same O
label O
for O
both O
pairs O
. O

By O
minimizing O
the O
consistency O
loss O
, O
our O
model O
can O
behave O
consistently O
no O
matter O
whether O
an O
original O
instance O
or O
its O
paraphrases O
are O
given O
. O

In O
this O
way O
, O
the O
model O
can O
be O
more O
generalized O
and O
robust O
. O

Besides O
, O
when O
our O
model O
is O
able O
to O
predict O
the O
same O
label O
for O
both O
sentence O
pairs O
, O
it O
means O
that O
our O
model O
has O
also O
learned O
their O
label O
. O

2.4 O
Nuclearity O
Labeling O
Nuclearity O
labeling O
is O
aimed O
at O
determining O
the O
nucleus O
from O
a O
sentence O
pair O
. O

The O
nuclearity O
of O
two O
sentences O
has O
a O
correlation O
with O
their O
relationship O
, O
thus O
we O
jointly O
train O
the O
rhetorical O
relation O
and O
the O
nuclearity O
classiﬁers O
, O
where O
the O
loss O
for O
back O
- O
propagation O
is O
the O
sum O
of O
the O
losses O
of O
both O
classiﬁers O
. O

Similar O
to O
the O
imbalance O
issue O
of O
rhetorical B-TaskName
relation I-TaskName
recognition I-TaskName
, O
the O
’ O
Equal O
’ O
class O
accounts O
for O
51 O
% O
of O
training O
data O
. O

We O
also O
employ O
UDA O
for O
performance O
enhancement O
. O

2.5 O
Binary O
Tree O
Conversion O
For O
simplicity O
, O
our O
shift O
- O
reduce O
parser O
constructs O
a O
binary O
tree O
. O

However O
, O
the O
parse O
trees O
annotated O
in O
CDTB-14 B-DatasetName
are O
not O
always O
binary O
. O

In O
the O
training O
and O
the O
test O
sets O
, O
8.9 O
% O
and O
10 O
% O
of O
the O
internal O
nodes O
have O
more O
than O
two O
children O
, O
respectively O
. O

Most O
of O
the O
previous O
works O
do O
not O
handle O
the O
binary O
tree O
conversion O
, O
and O
some O
of O
the O
work O
further O
convert O
the O
golden O
trees O
into O
binary O
trees O
to O
calculate O
their O
scores O
, O
resulting O
in O
less O
accurate O
evaluation O
. O

In O
the136training O
stage O
, O
we O
convert O
the O
multiway O
trees O
to O
their O
corresponding O
left O
- O
heavy O
binary O
trees O
( O
Morey O
et O
al O
. O
, O
2018 O
) O
. O

In O
the O
testing O
stage O
, O
we O
convert O
the O
binary O
tree O
constructed O
by O
our O
parser O
to O
the O
corresponding O
multiway O
tree O
. O

For O
example O
, O
a O
three O
- O
way O
node O
, O
A!XYZ O
, O
will O
be O
converted O
to O
A!A0Z O
andA0!XY O
. O

The O
conversion O
is O
deterministic O
and O
bidirectional O
, O
so O
it O
is O
free O
from O
ambiguity O
. O

2.6 O
Beam O
Search O
To O
decode O
a O
transition O
sequence O
during O
the O
testing O
stage O
, O
the O
standard O
method O
is O
to O
choose O
the O
action O
that O
has O
the O
maximum O
probability O
of O
the O
current O
time O
step O
as O
the O
input O
for O
the O
next O
time O
step O
. O

However O
, O
this O
greedy O
approach O
might O
fail O
to O
ﬁnd O
the O
sequence O
that O
has O
the O
maximum O
overall O
probability O
only O
because O
one O
of O
the O
action O
probability O
is O
small O
in O
that O
sequence O
. O

Beam O
search O
( O
Wiseman O
and O
Rush O
, O
2016 O
) O
is O
a O
heuristic O
search O
algorithm O
that O
explores O
a O
graph O
by O
maintaining O
the O
top O
kresults O
at O
every O
time O
step O
. O

This O
approach O
helps O
keep O
a O
number O
of O
potential O
candidates O
from O
discarding O
. O

Note O
that O
the O
greedy O
approach O
is O
equivalent O
to O
beam O
search O
with O
a O
beam O
width O
k= O
1 O
. O
When O
performing O
the O
shift O
- O
reduce O
parsing O
, O
two O
kinds O
of O
states O
have O
only O
one O
action O
to O
choose O
: O
( O
1 O
) O
less O
than O
two O
elements O
in O
the O
stack O
, O
and O
( O
2 O
) O
no O
element O
in O
the O
queue O
. O

Under O
the O
above O
two O
conditions O
, O
the O
probability O
of O
the O
selected O
action O
will O
be O
1 O
, O
making O
our O
model O
to O
be O
overly O
biased O
on O
those O
sequences O
having O
many O
non O
- O
optional O
stages O
. O

For O
this O
reason O
, O
we O
apply O
an O
alternative O
way O
to O
compute O
the O
sequence O
probability O
during O
beam O
search O
. O

Our O
modiﬁed O
beam O
search O
is O
still O
fulﬁlled O
by O
maintaining O
the O
topksequences O
, O
but O
the O
score O
of O
a O
sequence O
is O
calculated O
by O
the O
average O
probabilities O
of O
the O
selected O
actions O
that O
have O
more O
than O
one O
choice O
. O

3 O
Experiments O
3.1 O
Experimental O
Settings O
Following O
the O
setting O
of O
Kong O
and O
Zhou O
( O
2017 O
) O
, O
we O
divide O
CDTB-14 B-DatasetName
into O
the O
training O
set O
, O
including O
450 O
articles O
( O
2,125 O
paragraphs O
) O
, O
and O
test O
set O
, O
including O
50 O
articles O
( O
217 O
paragraphs O
) O
. O

We O
keep O
10 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
training O
data O
for O
validation O
. O

PARSEV O
AL O
( O
Carlson O
et O
al O
. O
, O
2001b O
) O
is O
used O
for O
evaluation O
. O

3.2 O
Experimental O
Results O
Table O
1 O
shows O
the O
performances O
of O
our O
parser O
in O
micro B-MetricName
- I-MetricName
averaged I-MetricName
F I-MetricName
- I-MetricName
score I-MetricName
, O
compared O
with O
previous O
work O
Zhou O
( O
Kong O
and O
Zhou O
, O
2017 O
) O
and O
Lin(LinModel O
EDU O

+ O
T O

+ O
R O
+ O
N O
All O
Zhou O
Given52.3 O
33.8 O
23.9 O
23.2 O
Lin O
64.6 O
42.7 O
38.5 O
35.0 O
BERT B-MethodName
- I-MethodName
CKY I-MethodName
76.5 O
50.8 O
48.5 O
43.1 O
Ours O
82.8 O
57.6 O
56.0 O
50.5 O
Zhou O
93.8 O
46.4 O
28.8 O
23.1 O
22.0 O
Lin O
87.2 O
49.5 O
32.6 O
28.8 O
26.8 O
BERT O
- O
CKY O
92.4 O
68.9 O
43.3 O
42.0 O
37.0 O

al O
. O
, O
2018 O
) O
. O

We O
also O
implement O
BERT B-MethodName
- I-MethodName
CKY I-MethodName
, O
a O
CKY O
parser O
by O
using O
BERT O
for O
representation O
, O
as O
an O
additional O
baseline O
model O
. O

The O
evaluation O
is O
based O
on O
multiway O
trees O
. O

Both O
the O
performances O
with O
and O
without O
golden O
EDUs O
are O
measured O
. O

The O
results O
show O
that O
BERT O
is O
highly O
competitive O
and O
has O
the O
ability O
to O
catch O
the O
potential O
relations O
between O
discourse O
units O
since O
LinandBERT O
- O
CKY O
basically O
use O
the O
same O
approach O
while O
the O
latter O
model O
uses O
BERT O
as O
the O
text O
encoder O
. O

Our O
parser O
outperforms O
all O
the O
baseline O
models O
and O
achieves O
a O
signiﬁcant O
improvement O
without O
the O
golden O
EDUs O
given O
. O

Note O
that O
BERT B-MethodName
- I-MethodName
CKY I-MethodName
is O
based O
on O
Lin O
et O
al O
. O

( O
2018 O
) O
, O
which O
has O
its O
own O
EDU O
segmentation O
module O
different O
from O
ours O
, O
hence O
the O
EDU O
score O
is O
different O
. O

We O
examine O
the O
performance O
of O
three O
different O
training O
techniques O
for O
shift O
- O
reduce O
parsing O
. O

As O
mentioned O
in O
Section O
2.2 O
, O
Normal O
stands O
for O
action O
classiﬁer O
trained O
with O
gold O
standard O
actions O
, O
Dynamic O
stands O
for O
Dynamic O
Oracle O
introduced O
by O
Yu O
et O
al O
. O

( O
2018 O
) O
, O
and O
Ours O
stands O
for O
our O
revised O
dynamic O
- O
oracle O
procedure O
where O
the O
model O
is O
trained O
with O
both O
gold O
standard O
actions O
and O
dynamic O
oracle O
actions O
. O

Compared O
to O
Normal O
, O
experimental O
results O
show O
no O
improvement O
made O
by O
the O
original O
dynamic O
oracle O
, O
while O
our O
revised O
dynamic O
oracle O
outperforms O
the O
other O
two O
strategies O
. O

Our O
strategy O
does O
not O
ignore O
the O
golden O
action O
in O
every O
correct O
state O
and O
also O
has O
the O
chance O
to O
explore O
error O
states O
. O

nary O
trees O
in O
macro B-MetricName
- I-MetricName
averaged I-MetricName
F I-MetricName
- I-MetricName
score I-MetricName
. O

The O
results O
are O
shown O
in O
Table O
2 O
. O

Sun O
and O
Kong O
( O
2018 O
) O
do O
not O
address O
all O
subtasks O
in O
Chinese B-TaskName
discourse I-TaskName
parsing I-TaskName
, O
and O
our O
model O
outperforms O
SUN O
in O
every O
subtask O
. O

Occurrences O
of O
these O
relations O
are O
59.6 O
% O
, O
17.1 O
% O
, O
1.6 O
% O
, O
and O
21.7 O
% O
, O
respectively O
. O

3.3 O
Discussions O
To O
examine O
the O
effectiveness O
of O
UDA O
, O
Table O
3 O
shows O
the O
performances B-MetricName
of I-MetricName
rhetorical I-MetricName
relation I-MetricName
recognition I-MetricName
with O
and O
without O
UDA O
. O

Experimental O
results O
show O
that O
application O
of O
UDA O
successfully O
enhances O
the O
recall O
scores O
of O
the O
three O
minor O
classes O
with O
a O
little O
trade O
- O
off O
in O
the O
recall O
score O
of O
the O
dominant O
class O
, O
Coordination O
. O

In O
addition O
, O
the O
F B-MetricName
- I-MetricName
scores I-MetricName
of O
all O
the O
four O
relations O
are O
increased O
. O

In O
other O
words O
, O
applying O
UDA O
deals O
with O
the O
data O
imbalance O
issue O
and O
improves O
the O
overall O
performance O
. O

Applying O
UDA O
to O
nuclearity O
classiﬁcation O
also O
has O
a O
similar O
improvement O
as O
Table O
3 O
. O

Theoretically O
, O
beam O
search O
with O
a O
larger O
beam O
width O
helps O
ﬁnd O
a O
better O
solution O
. O

Table O
4 O
, O
however O
, O
our O
parser O
is O
worse O
when O
a O
larger O
beam O
width O
is O
used O
, O
which O
means O
the O
sequence O
having O
higher O
overall O
score O
does O
not O
ensure O
the O
better O
decoding O
result O
. O

Our O
experiment O
only O
shows O
the O
beam O
widths O
up O
to O
ﬁve O
because O
the O
scores O
of O
worse O
sequences O
are O
already O
higher O
than O
that O
of O
the O
correct O
sequence O
in O
some O
cases O
. O

That O
is O
, O
the O
larger O
beam O
widths O
seem O
to O
be O
unnecessary O
. O

The O
reason O
may O
be O
that O
beam O
search O
is O
not O
really O
suitable O
for O
the O
shift O
- O
reduce O
paradigm O
. O

For O
example O
, O
a O
sequence O
might O
fall O
into O
a O
seriously O
bad O
stage O
but O
the O
rest O
of O
actions O
can O
be O
easily O
determined O
so O
that O
the O
sequence O
will O
get O
a O
high O
overall O
probability O
. O

This O
assumption O
also O
implies O
that O
unlike O
beam O
search O
applied O
on O
sequence O
to O
sequence O
model O
, O
we O
can O
not O
judge O
a O
transition O
sequence O
is O
good O
or O
bad O
by O
solely O
considering O
its O
overall O
score O
. O

In O
addition O
, O
for O
longer O
textual O
units O
such O
as O
paragraph O
, O
human O
readers O
and O
writers O
may O
not O
follow O
the O
assumption O
of O
overall O
optimization O
. O

Instead O
, O
human O
beings O
read O
and O
write O
sequentially O
, O
similar O
to O
the O
greedy O
nature O
. O

We O
also O
evaluate O
our O
approach O
in O
English B-TaskName
discourse I-TaskName
parsing I-TaskName
. O

The O
famous O
dataset O
, O
RST O
- O
DT O
, O
is O
used O
. O

Our O
model O
achieves O
F B-MetricName
- I-MetricName
scores I-MetricName
of O
85.0 B-MetricValue
% I-MetricValue
, O
58.8 B-MetricValue
% I-MetricValue
, O
69.9 B-MetricValue
% I-MetricValue
, O
and O
56.7 B-MetricValue
% I-MetricValue
in O
tree O
construction O
, O
rhetorical B-TaskName
relation I-TaskName
recognition I-TaskName
, O
nuclearity O
labeling O
, O
and O
all O
subtasks O
, O
respectively O
. O

The O
overall O
performance O
is O
similar O
to O
that O
of O
the O
state O
- O
of O
- O
the O
- O
art O
model O
( O
Yu O
et O
al O
. O
, O
2018 O
) O
. O

4 O
Conclusion O
This O
work O
proposes O
a O
standalone O
, O
complete O
Chinese O
discourse O
parser O
. O

We O
integrate O
BERT O
, O
UDA O
, O
and O
a O
revised O
training O
procedure O
for O
constructing O
a O
robust O
shift O
- O
reduce O
parser O
. O

Our O
model O
is O
compared O
with O
a O
number O
of O
previous O
models O
, O
and O
experimental O
results O
show O
that O
our O
model O
achieves O
the O
stateof O
- O
the O
- O
art O
performance O
and O
is O
highly O
competitive O
with O
different O
setups O
. O

We O
will O
explore O
cross O
- O
lingual O
transfer O
learning O
for O
supporting O
more O
languages O
. O

Acknowledgements O
This O
research O
was O
partially O
supported O
by O
Ministry O
of O
Science O
and O
Technology O
, O
Taiwan O
, O
under O
grants O
MOST-106 O
- O
2923 O
- O
E-002 O
- O
012 O
- O
MY3 O
, O
MOST-1092634 O
- O
F-002 O
- O
040- O
, O
MOST-109 O
- O
2634 O
- O
F-002 O
- O
034- O
, O
MOST-108 O
- O
2218 O
- O
E-009 O
- O
051- O
, O
and O
by O
Academia O
Sinica O
, O
Taiwan O
, O
under O
grant O
AS O
- O
TP-107 O
- O
M05.138 O
