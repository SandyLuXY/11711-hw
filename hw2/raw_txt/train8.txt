Proceedings O
of O
the O
60th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
921 O
- O
931 O
May O
22 O
- O
27 O
, O
2022 O
c O

 O
2022 O
Association O
for O
Computational O
Linguistics O
RoCBert B-MethodName
: O
Robust B-MethodName
Chinese I-MethodName
Bert I-MethodName
with O
Multimodal O
Contrastive O
Pretraining O
Hui O
Su1 O
, O
Weiwei O
Shi1 O
, O
Xiaoyu O
Shen2 O
Xiao O
Zhou1 O
, O
Tuo O
Ji1 O
, O
Jiarui O
Fang1 O
, O
and O
Jie O
Zhou1 O
1Pattern O
Recognition O
Center O
, O
Wechat O
AI O
, O
Tencent O
Inc O
, O
China O
2Saarland O
Informatics O
Campus O
aaronsu@tencent.com O
Abstract O
Large O
- O
scale O
pretrained O
language O
models O
have O
achieved O
SOTA O
results O
on O
NLP O
tasks O
. O

However O
, O
they O
have O
been O
shown O
vulnerable O
to O
adversarial O
attacks O
especially O
for O
logographic O
languages O
like O
Chinese O
. O

In O
this O
work O
, O
we O
propose O
ROCBERT B-MethodName
: O
a O
pretrained O
Chinese O
Bert O
that O
is O
robust O
to O
various O
forms O
of O
adversarial O
attacks O
like O
word O
perturbation O
, O
synonyms O
, O
typos O
, O
etc O
. O

It O
is O
pretrained O
with O
the O
contrastive O
learning O
objective O
which O
maximizes O
the O
label O
consistency O
under O
different O
synthesized O
adversarial O
examples O
. O

The O
model O
takes O
as O
input O
multimodal O
information O
including O
the O
semantic O
, O
phonetic O
and O
visual O
features O
. O

We O
show O
all O
these O
features O
are O
important O
to O
the O
model O
robustness O
since O
the O
attack O
can O
be O
performed O
in O
all O
the O
three O
forms O
. O

Across O
5 O
Chinese O
NLU B-TaskName
tasks O
, O
R O
OCBERT O
outperforms O
strong O
baselines O
under O
three O
blackbox O
adversarial O
algorithms O
without O
sacriﬁcing O
the O
performance O
on O
clean O
testset O
. O

It O
also O
performs O
the O
best O
in O
the O
toxic B-TaskName
content I-TaskName
detection I-TaskName
task O
under O
human O
- O
made O
attacks O
. O

1 O
Introduction O
Large O
- O
scale O
pretrained O
models O
, O
by O
ﬁnetuning O
on O
sufﬁcient O
annotated O
data O
, O
have O
been O
able O
to O
approach O
or O
even O
surpass O
human O
performance O
on O
many O
benchmark O
testsets O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Radford O
et O
al O
. O
; O
Devlin O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
; O
Brown O
et O
al O
. O
, O
2020 O
) O
. O

However O
, O
even O
pretrained O
with O
huge O
amounts O
of O
text O
, O
the O
models O
are O
still O
vulnerable O
under O
adversarial O
attacks O
like O
synonyms O
, O
word O
deletion O
/ O
swapping O
, O
misspelling O
, O
etc O
( O
Li O
et O
al O
. O
, O
2019a O
; O

Jin O
et O
al O
. O
, O
2020 O
; O
Sun O
et O
al O
. O
, O
2020a O
; O
Eger O
and O
Benz O
, O
2020 O
) O
. O

These O
adversarial O
examples O
occur O
frequently O
in O
the O
real O
- O
world O
scenario O
and O
can O
be O
made O
either O
naturally O
( O
e.g. O
, O
typos O
) O
or O
maliciously O
( O
e.g. O
, O
to O
avoid O
auto O
detection O
of O
toxic O
content)1 O
. O

Equal O
contribution O
. O

1The O
concept O
of O
adversarial O
examples O
is O
quite O
wide O
. O

In O
this O
paper O
, O
we O
focus O
on O
adversarial O
examples O
that O
do O
NOT O
change O
the O
original O
semantics O
( O
Mozes O
et O
al O
. O
, O
2021)Attacker O
Text O
phonetic O
克(kè)比的精神值得永远学习 O
visual O
科此的精神值得永远学习 O
character O
split O
禾斗匕匕的精神值得永远学习 O
synonym O
我(wˇo)科(k¯e)的精神值得永远学习 O
synonym O
+ O
phonetic O
蜗(w¯o)壳(ké)的精神值得永远学习 O
to O
pinyin O
kebi O
的精神值得永远学习 O
to O
pinyin O
+ O
unicode O
keb1 O
的精神值得学习永远 O
swap O
科比的精神值得永远习学 O
insertion O
科比的精神九值得永远学习 O
deletion O
科比的神值得永远学习 O
Original O
: O
科(k¯e)比(bˇı)的精神值得永远学习 O
Translation O
: O
Kobe O
’s O
spirit O
is O
worth O
studying O
forever O
. O
Table O
1 O
: O
Examples O
of O
various O
attackers O
. O

Contents O
in O
the O
brackets O
are O
corresponding O
pinyins O
of O
Chinese O
characters O
. O

The O
lack O
of O
robustness O
with O
them O
can O
easily O
lead O
to O
large O
performance O
drop O
when O
testing O
in O
the O
noisy O
real O
- O
world O
trafﬁc O
. O

The O
issue O
is O
particularly O
outstanding O
for O
logographic O
languages O
like O
Chinese O
since O
the O
attack O
can O
be O
either O
with O
the O
glyph O
character O
, O
pinyin O
( O
the O
romanized O
phonetic O
representations O
) O
or O
a O
combination O
of O
them O
( O
Wang O
et O
al O
. O
, O
2020 O
; O

Li O
et O
al O
. O
, O
2020d O
; O
Zhang O
et O
al O
. O
, O
2020 O
; O
Nuo O
et O
al O
. O
, O
2020 O
) O
. O

We O
show O
some O
examples O
in O
Table O
1 O
. O

The O
word O
“ O
科 O
比(Kobe O
) O
” O
can O
be O
replaced O
with O
synonyms O
, O
phonetically O
or O
visually O
similar O
words O
. O

The O
attacker O
can O
also O
replace O
the O
character O
with O
its O
pinyin O
then O
continue O
the O
attack O
in O
the O
alphabet O
- O
level O
( O
“ O
keb1 O
” O
in O
the O
table O
) O
. O

The O
isolation O
of O
semantics O
and O
phonetics O
, O
and O
the O
rich O
set O
of O
glyph O
characters O
in O
written O
Chinese O
makes O
the O
attacking O
forms O
much O
more O
diverse O
than O
in O
alphabetic O
languages O
like O
English O
. O

Current O
research O
works O
usually O
adopt O
two O
ways O
to O
defend O
adversarial O
attacks O
: O
( O
1 O
) O
Run O
spell O
checking O
to O
correct O
the O
written O
errors O
before O
feeding O
to O
the O
prediction O
model O
( O
Pruthi O
et O
al O
. O
, O
2019 O
; O
Li O
et O

al O
. O
, O
2020b O
; O
Mozes O
et O
al O
. O
, O
2021 O
) O
, O
and O
( O
2 O
) O
Adversarial O
training O
, O
which O
adds O
adversarial O
example O
to O
the O
training O
data O
( O
Zang O
et O
al O
. O
, O
2020 O
; O
Li O
et O
al O
. O
, O
2020a O
; O
Liu O
et O
al O
. O
, O
2020 O
) O
. O

For O
the O
former O
, O
Chinese O
spell O
checking O
itself O
is O
even O
a O
more O
difﬁcult O
task O
because O
it O
requires O
the O
model O
to O
accurately O
recover O
the O
original O
text O
. O

Any O
tiny O
errors O
of O
the O
spell O
checking O
can921lead O
to O
unpredicted O
model O
behaviors O
. O

For O
the O
latter O
, O
it O
is O
hard O
for O
the O
model O
to O
adapt O
to O
all O
adversarial O
variants O
only O
in O
the O
ﬁnetuning O
stage O
, O
especially O
when O
the O
training O
data O
is O
sparse O
( O
Meng O
et O
al O
. O
, O
2021 O
) O
. O

To O
address O
the O
above O
challenges O
, O
we O
propose O
ROCBERT B-MethodName
, O
aRobust B-MethodName
Chinese I-MethodName
BERT I-MethodName
pretrained O
with O
the O
contrastive O
learning O
objective O
by O
maximizing O
the O
label O
consistency O
under O
various O
adversarial O
examples O
. O

The O
adversarial O
examples O
are O
synthesized O
from O
an O
algorithm O
that O
encapsulates O
common O
types O
of O
attacks O
. O

We O
also O
consider O
combinatorial O
attacks O
where O
multiple O
types O
of O
attacks O
can O
be O
added O
on O
top O
of O
each O
other O
, O
which O
has O
never O
been O
considered O
in O
previous O
research O
. O

To O
defend O
attacks O
in O
all O
levels O
, O
we O
incorporate O
multimodal O
information O
into O
the O
encoder O
. O

The O
phonetic O
and O
visual O
features O
are O
inserted O
into O
one O
self O
- O
attention O
layer O
then O
dynamically O
fused O
in O
later O
layers O
. O

Across O
5 O
standard O
NLU B-TaskName
tasks O
and O
one O
toxic B-TaskName
content I-TaskName
detection I-TaskName
task O
, O
we O
show O
the O
pretrained O
model O
achieves O
new O
SOTAs O
under O
various O
adversarial O
attackers O
. O

In O
short O
, O
our O
contribution O
are O
( O
1 O
) O
We O
propose O
pretraining O
a O
robust O
Chinese O
Bert B-MethodName
with O
adversarial O
contrastive O
learning O
, O
such O
that O
the O
model O
can O
perform O
well O
on O
not O
only O
clean O
testbeds O
, O
but O
also O
adversarial O
examples O
. O

( O
2 O
) O
The O
model O
is O
pretrained O
with O
synthesized O
adversarial O
examples O
covering O
combinations O
of O
semantic O
, O
phonetic O
and O
visual O
attacks O
. O

It O
takes O
as O
input O
multimodal O
features O
to O
handle O
all O
levels O
of O
possible O
attacks O
. O

( O
3 O
) O
The O
pretrained O
model O
outperforms O
strong O
baselines O
across O
5 O
NLU B-TaskName
tasks O
and O
1 O
toxic B-TaskName
content I-TaskName
detection I-TaskName
task O
under O
various O
adversarial O
attackers O
. O

( O
4 O
) O
We O
perform O
an O
extensive O
ablation O
studies O
for O
pretraining O
options O
and O
have O
a O
wide O
comparison O
with O
popular O
defending O
methods O
, O
which O
we O
hope O
will O
beneﬁt O
future O
research O
. O

2 O
Related O
Work O
Adversarial O
attack O
There O
have O
been O
a O
lot O
of O
works O
showing O
the O
vulnerability O
of O
NLP O
models O
under O
adversarial O
examples O
( O
Li O
et O
al O
. O
, O
2020c O
; O
Garg O
and O
Ramakrishnan O
, O
2020 O
; O
Zang O
et O
al O
. O
, O
2020 O
) O
, O
which O
are O
understandable O
by O
humans O
yet O
lead O
to O
significant O
model O
prediction O
drops O
. O

There O
are O
usually O
two O
types O
of O
attacks O
: O
( O
1 O
) O
semantic O
equivalent O
replacement O
, O
which O
can O
be O
synthesized O
by O
replacing O
words O
based O
on O
vector O
similarity O
( O
Jin O
et O
al O
. O
, O
2020 O
; O
Wang O
et O

al O
. O
, O
2020 O
) O
, O
WordNet O
synonyms O
( O
Zang O
et O
al O
. O
, O
2020 O
) O
, O
masked O
prediction O
from O
pretrained O
models O
( O
Li O
et O

al O
. O
, O
2020c O
; O
Garg O
and O
Ramakrishnan O
, O
2020 O
; O
Li O
et O
al O
. O
, O
2020d O
) O
, O
etc O
. O
( O
2 O
) O
noise O
injection O
, O
whichcan O
be O
synthesized O
by O
adding O
/ O
deleting O
/ O
swapping O
words O
( O
Li O
et O
al O
. O
, O
2019a O
; O
Gil O
et O
al O
. O
, O
2019 O
; O
Sun O
et O
al O
. O
, O
2020a O
) O
, O
replacing O
words O
with O
phonetically O
or O
visually O
similar O
ones O
( O
Eger O
et O
al O
. O
, O
2019 O
; O
Eger O
and O
Benz O
, O
2020 O
) O
. O

For O
logographic O
languages O
like O
Chinese O
, O
the O
noise O
can O
be O
much O
more O
complex O
as O
it O
can O
be O
injected O
on O
both O
the O
glyph O
characters O
or O
romanized O
pinyins O
( O
Zhang O
et O
al O
. O
, O
2020 O
; O

Nuo O
et O
al O
. O
, O
2020 O
) O
. O

Adversarial O
defense O

The O
most O
common O
way O
of O
adversarial O
defense O
is O
adversarial O
training O
, O
which O
simply O
appends O
synthesized O
adversarial O
examples O
into O
the O
training O
data O
( O
Zang O
et O
al O
. O
, O
2020 O
; O
Li O
et O

al O
. O
, O
2020a O
) O

. O

Nonetheless O
, O
it O
relies O
only O
on O
the O
limited O
labeled O
training O
data O
. O

In O
contrast O
, O
the O
proposed O
ROCBERTis O
pretrained O
on O
billions O
of O
text O
and O
can O
better O
adapted O
to O
diverse O
adversarial O
variants O
. O

Another O
popular O
way O
is O
to O
ﬁrst O
remove O
the O
noise O
with O
off O
- O
the O
- O
shelf O
spell O
checkers O
, O
then O
feed O
the O
corrected O
text O
into O
the O
model O
( O
Li O
et O
al O
. O
, O
2020b O
) O
. O

However O
, O
Chinese O
spell O
checking O
requires O
fully O
recovering O
the O
correct O
text O
and O
current O
model O
performances O
are O
far O
from O
satisfactory O
( O
Liu O
et O
al O
. O
, O
2021 O
; O
Xu O
et O

al O
. O
, O
2021 O
; O
Wang O
et O

al O
. O
, O
2021a O
) O
. O

Any O
tiny O
error O
in O
the O
spell O
checking O
process O
can O
lead O
to O
unpredicted O
model O
behaviors O
. O

It O
also O
incurs O
signiﬁcant O
latency O
to O
model O
prediction O
. O

ROCBERTdoes O
not O
add O
additional O
latency O
and O
can O
perform O
well O
even O
if O
fully O
recovery O
is O
difﬁcult O
due O
to O
its O
consistency O
- O
maximization O
pretraining O
objective O
. O

There O
have O
also O
been O
works O
on O
pretraining O
more O
robust O
models O
through O
virtual O
adversarial O
training O
and O
noise O
regularization O
( O
Yoo O
and O
Qi O
, O
2021 O
; O
Wang O
et O
al O
. O
, O
2021b O
; O
Meng O
et O
al O
. O
, O
2021 O
) O
, O
but O
they O
perform O
poorly O
on O
man O
- O
made O
attacks O
. O

3 O
Adversarial O
Example O
Synthesis O
3.1 O
Attacking O
Chinese O
Characters O
As O
we O
focus O
on O
Chinese O
in O
this O
paper O
and O
Chinese O
characters O
are O
much O
more O
diverse O
than O
in O
alphabetical O
languages O
, O
we O
design O
the O
following O
5 O
Chinesespeciﬁc O
attacking O
algorithms O
ﬁrst O
. O

phonetic O
: O
Replace O
a O
Chinese O
character O
with O
a O
random O
homonym O
( O
ignoring O
diacritics O
) O
. O

For O
polyphones O
, O
we O
consider O
the O
2 O
most O
common O
pinyins2 O
. O

Visual O
: O
Replace O
Chinese O
characters O
with O
their O
visually O
similar O
characters O
( O
with O
the O
similarity O
table O
in O
the O
Kanji O
Database O
Project)3 O
. O

Character O
Split O
: O
Split O
one O
character O
into O
two O
parts O
with O
every O
part O
still O
being O
( O
or O
visually O
similar O
to O
) O
a O
valid O
Chinese O
character O
. O

We O
follow O
the O
Chinese O
2https://unicode.org/charts/unihan.html O
3http://kanji-database.sourceforge.net/922Figure O
1 O
: O
Adversarial O
example O
synthesis O
process O
. O

splitting O
dictionary4 O
, O
which O
contains O
17,803 O
splitting O
ways O
for O
Chinese O
characters O
in O
total O
. O
Synonym O
: O
Segment O
Chinese O
characters O
into O
words O
with O
the O
jieba O
tokenizer5 O
, O
then O
randomly O
replace O
the O
word O
with O
one O
of O
its O
synonyms O
. O

Two O
words O
are O
treated O
as O
synonym O
if O
they O
share O
a O
similarity O
score O
of O
over O
0.756 O
. O

We O
only O
replace O
adjectives O
or O
nouns O
as O
we O
ﬁnd O
other O
words O
can O
be O
hardly O
replaced O
without O
changing O
the O
semantics O
. O

Character O
to O
Pinyin O
: O
Replaces O
the O
character O
into O
its O
pinyin O
representation O
( O
without O
diacritics O
) O
. O

3.2 O
Attacking O
Other O
Characters O
Apart O
from O
Chinese O
characters O
, O
there O
are O
often O
other O
characters O
like O
the O
pinyin O
, O
numbers O
, O
punctuations O
and O
foreign O
words O
in O
the O
Chinese O
corpus O
. O

The O
following O
4 O
types O
of O
attacks O
apply O
to O
not O
only O
Chinese O
characters O
, O
but O
also O
all O
other O
characters O
. O

Unicode O
: O
Randomly O
sample O
one O
of O
the O
visually O
similar O
unicodes O
as O
a O
replacement7 O
. O

Random O
Insertion O
: O
Sample O
one O
character O
from O
the O
vocabulary O
set O
, O
then O
randomly O
insert O
the O
character O
to O
the O
left O
or O
right O
of O
the O
current O
character O
. O

Swap O
: O

Swap O
the O
character O
with O
its O
neighbor O
. O

Deletion O
: O

Delete O
the O
character O
directly O
. O

Examples O
of O
all O
types O
of O
attacks O
are O
in O
Table O
1 O
. O
3.3 O
Synthesis O
Process O
The O
synthesis O
process O
of O
adversarial O
examples O
is O
as O
follow O
: O
Given O
one O
sentence O
, O
we O
ﬁrst O
select O
several O
4https://github.com/kfcd/chaizi O
5https://github.com/fxsjy/jieba O
6https://github.com/chatopera/Synonyms O
7http://www.unicode.org/Public/security/revision03/confusablesSummary.txtcharacters O
to O
attack O
. O

For O
each O
selected O
character O
, O
we O
then O
combine O
the O
above O
mentioned O
characterlevel O
attacking O
algorithms8to O
get O
its O
attacked O
form O
. O

Attack O
Ratio O
: O

The O
attack O
ratio O

 O
decides O
how O
many O
characters O
we O
will O
attack O
. O

Let O
ncbe O
the O
number O
of O
characters O
in O
the O
sentence O
, O
we O
deﬁne O

 O
as O
: O

 O
= O
min(max O
( O
int();1);nc O
) O
N(max(1;0:15nc);1)(1 O
) O
where O
the O
intfunction O
rounds O
into O
the O
closest O
integer O
. O

The O
intuition O
is O
that O
we O
want O
to O
attack O
15 O
% O
of O
the O
characters O
on O
average9 O
. O

If O
the O
sentence O
is O
short O
, O
we O
will O
make O
sure O
to O
attack O
at O
least O
one O
character O
. O

We O
insert O
normal O
Gaussian O
noise O
on O
top O
of O
the O
average O
ratio O
to O
add O
some O
randomness O
. O

Character O
Selection O
: O
There O
have O
been O
many O
research O
works O
showing O
that O
attacking O
informative O
words O
is O
more O
effective O
than O
random O
words O
( O
Li O
et O
al O
. O
, O
2019a O
; O
Sun O
et O
al O
. O
, O
2020a O
) O
. O

Therefore O
, O
we O
decide O
the O
chance O
of O
one O
character O
cibeing O
selected O
based O
on O
its O
informativeness O
in O
the O
sentence O
. O

Let O
w(ci)denote O
the O
word O
cibelongs O
to O
, O
the O
informative O
score O
for O
ciis O
counted O
as O
the O
difference O
of O
the O
language O
model O
loss O
after O
deleting O
w(ci)(denoted O
asL(Ow(ci O
) O
) O

( O
Li O
et O
al O
. O
, O
2016)10 O
. O

The O
chance O
that O
ciwill O
be O
selected O
to O
be O
attacked O
is O
: O
p(ci O
) O
= O
eL(Ow(ci O
) O
) O
jw(ci)jPnw O
j=1eL(Owj)(2 O
) O

wherenwis O
the O
number O
of O
words O
in O
the O
sentence O
. O

jw(ci)jmeans O
the O
number O
of O
characters O
in O
w(ci O
) O
such O
that O
characters O
in O
the O
same O
word O
have O
equal O
chances O
to O
be O
selected O
. O

Attack O
Combination O
: O
There O
can O
be O
combinations O
of O
attacks O
for O
one O
character O
. O

For O
example O
, O
we O
can O
transfer O
one O
Chinese O
character O
into O
its O
pinyin O
then O
continue O
to O
attack O
it O
in O
the O
alphabet O
level O
( O
“ O
to O
pinyin O
+ O
unicode O
” O
in O
Table O
1 O
) O
. O

We O
deﬁne O
it O
as O
a O
sequential O
process O
where O
a O
new O
attack O
can O
be O
added O
on O
top O
at O
each O
step O
. O

Speciﬁcally O
, O
the O
new O
character O
~cafter O
all O
the O
attack O
combinations O
applied O
to O
cis O
: O
~c O
= O
AS(c)A2A1(c O
) O
p(S(c O
) O
= O
k O
) O
= O
q(1 q)k 1(3 O
) O
8For O
synonym O
replacement O
which O
applies O
in O
the O
word O
level O
, O
we O
apply O
it O
on O
the O
word O
that O
the O
selected O
character O
belongs O
to O
. O

9The O
ratio O
is O
chosen O
by O
manual O
annotation O
. O

15 O
% O
is O
the O
highest O
ratio O
we O
can O
attack O
without O
hurting O
human O
reading O
. O

10We O
use O
ChineseGPT O
( O
Zhang O
et O
al O
. O
, O
2021 O
) O
as O
the O
language O
model O
, O
so O
“ O
word O
” O
here O
means O
the O
subword O
token O
deﬁned O
in O
the O
vocabulary O
of O
ChineseGPT.923whermeans O
applying O
a O
new O
attacking O
algorithm O
Ato O
the O
output O
of O
the O
last O
step O
. O

At O
each O
step O
i O
, O
the O
attacking O
algorithm O
Aiis O
randomly O
selected O
from O
all O
algorithms O
that O
are O
applicable O
to O
the O
output O
from O
stepi 1.S(c)is O
the O
number O
of O
attacking O
steps O
applied O
to O
c O
, O
which O
follows O
an O
exponentially O
decay O
function O
. O

We O
set O
q= O
0:7empirically O
. O

The O
full O
process O
of O
adversarial O
example O
synthesis O
is O
illustreated O
in O
Figure O
1 O
. O
4 O
Multimodal O
Contrastive O
Pretraining O
With O
the O
above O
- O
mentioned O
algorithm O
to O
sample O
adversarial O
examples O
, O
we O
can O
pretrain O
the O
model O
with O
the O
multimodal O
contrastive O
learning O
objective O
. O

4.1 O
Multimodal O
Features O
We O
follow O
the O
standard O
Bert B-MethodName
architecture O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
as O
our O
backbone O
, O
based O
on O
which O
we O
integrate O
phonetic O
and O
visual O
features O
for O
input O
text O
. O

Feature O
Representation O
: O
For O
every O
character O
c O
in O
our O
vocabulary O
, O
apart O
from O
the O
standard O
semantic O
embeddingSe(c O
) O
, O
we O
include O
two O
more O
vectors O
Ph(c)andVi(c)to O
encode O
its O
phonetic O
and O
visual O
features O
respectively O
. O

If O
cis O
not O
a O
Chinese O
character O
, O
it O
has O
its O
own O
phonetic O
vector O
. O

Otherwise O
, O
Ph(c O
) O

= O
P O
k2pinyin O
( O
c)Ph(k)wherepinyin O
( O
c)is O
its O
pinyin O
sequence O
. O
Vi(c)is O

extracted O
from O
its O
3232image O
I(c O
) O
. O

The O
image O
is O
in O
simsun O
( O
宋体 O
) O
for O
Chinese O
characters O
and O
arial O
for O
others O
, O
the O
default O
fonts O
for O
most O
online O
text O
. O

Vi(c)is O

deﬁned O
as O
: O
Vi(c O
) O

= O
LayerNorm O
( O
MTResNet O
18(I(c O
) O
) O
) O

( O
4 O
) O
M O
is O
a O
learnable O
matrix O
and O
we O
utilize O
Resnet18 O
( O
He O
et O
al O
. O
, O
2016 O
) O
to O
map O
I(c)into O
a O
onedimentional O
vector O
( O
freezed O
during O
training O
) O
. O

Visual O
Representation O
Pretrain O
: O
To O
get O
an O
reasonable O
initialization O
, O
we O
add O
another O
pretraining O
stage O
only O
for O
the O
visual O
representation O
. O

Phonetic O
representations O
are O
randomly O
initialized11.Min O
Eq O
4 O
is O
pretrained O
with O
the O
same O
contrastive O
loss O
as O
in O
Eq O
5 O
. O

The O
positive O
sample O
for O
the O
charactercis O
its O
visually O
adversarial O
form O
~c O
= O
A(c O
) O
. O

A O
 O
U(visual O
, O
character O
split O
, O
unicode O
) O
, O
which O
means O
uniform O
sampling O
from O
the O
three O
visual O
attacking O
algorithms O
mentioned O
in O
§ O
3 O
. O

If O
cis O
split O
into O
two O
characters O
c1andc2 O
, O
we O
sum O
the O
visual O
representation O
of O
the O
two O
split O
characters O
Vi(~c O
) O
= O
Vi(c1 O
) O

+ O
Vi(c2 O
) O
. O

The O
negative O
samples O
11We O
show O
in O
Section O
5.3 O
that O
pretraining O
is O
necessary O
for O
visual O
features O
not O
but O
for O
phonetic O
features.are O
all O
other O
characters O
in O
the O
same O
batch O
. O

After O
training O
, O
visually O
similar O
characters O
will O
be O
close O
in O
their O
representation O
space O
. O

Feature O
Integration O
: O
A O
straightforward O
way O
to O
integrate O
these O
multimodal O
features O
is O
to O
fuse O
them O
before O
fed O
to O
the O
encoder O
( O
Sun O
et O
al O
. O
, O
2021 O
; O

Liu O
et O
al O
. O
, O
2021 O
) O
. O

However O
, O
three O
features O
will O
be O
given O
equal O
weights O
and O
the O
model O
can O
not O
dynamically O
attend O
to O
only O
useful O
features O
. O

Another O
way O
is O
a O
twostep O
encoding O
which O
ﬁrst O
decides O
the O
weight O
, O
then O
encode O
with O
selective O
attention O
( O
Xu O
et O
al O
. O
, O
2021 O
) O
, O
but O
it O
will O
signiﬁcantly O
slow O
down O
the O
system O
. O

We O
propose O
a O
lightweight O
fusion O
method O
layer O
- O
insert O
, O
which O
insert O
multimodal O
features O
in O
only O
one O
encoder O
layer O
. O

Let O
Hk(i)denote O
the O
representation O
of O
theith O
word O
in O
the O
kth O
layer O
, O
we O
insert O
by O
: O
W1 O
= O
KT O
1Hk(i)Hk(i)V1 O
W2 O
= O
KT O
2Hk(i)Ph(i)V2 O
W3 O
= O
KT O
3Hk(i)Vi(i)V3 O
Hk(i O
) O
= O
W1Hk(i O
) O
+ O
W2Ph(i O
) O

+ O
W3Vi(i O
) O

W1+W2+W3 O
wherePh(i)andVi(i)are O
the O
phonetic O
and O
visual O
representations O
and O
Kj O
= O
Vjare O
learnable O
matrices O
. O

Intuitively O
we O
can O
use O
the O
layer O
0tok 1to O

decide O
the O
weights O
of O
three O
multimodal O
representations O
and O
use O
the O
rest O
layers O
for O
sentence O
representation O
learning O
. O

It O
allows O
dynamic O
fusion O
according O
to O
sentence O
context O
yet O
adds O
marginal O
complexity O
. O

4.2 O
Model O
Loss O
The O
model O
loss O
has O
two O
components O
: O
the O
contrastive O
learning O
loss O
and O
the O
standard O
masked O
language O
model O
( O
MLM O
) O
loss O
. O

Contrastive O
Learning O
: O
The O
idea O
of O
contrastive O
learning O
( O
Chen O
et O
al O
. O
, O
2020 O
; O
Kim O
et O
al O
. O
, O
2021 O
) O
is O
that O
the O
representation O
space O
should O
be O
made O
closer O
for O
similar O
( O
positive O
) O
samples O
and O
farther O
for O
dissimilar O
( O
negative O
) O
samples O
. O

For O
each O
sentence O
, O
we O
treat O
its O
adversarial O
form O
( O
obtained O
from O
the O
algorithm O
in O
§ O
3 O
) O
as O
positive O
and O
all O
the O
other O
sentences O
in O
the O
same O
batch O
as O
negative O
. O

Given O
a O
batch O
with O
N O
sentences O
, O
the O
loss O
to O
the O
ith O
sentencesiis O
: O
Lc(i O
) O

= O
 logesim(si;~si)= O
  O
PN O
j=1esim(si;sj)= O
 O
; O
( O
5 O
) O
where O
 O
is O
a O
temperature O
hyperparameter O
and O
~siis O
the O
adversarial O
example O
synthesized O
from O
si O
. O

We O
set O
 O
= O
0:01based O
on O
our O
pilot O
experiments O
and924deﬁnesim(si;~si)ash O
> O
i O
~ O
hi O
khikk O
~ O
hik O
, O
which O
is O
the O
cosine O
similarity O
in O
their O
representation O
space O
hiand O
~ O
hi O
. O

Mix O
with O
MLM O
: O
We O
mix O
the O
contrastive O
learning O
loss O
with O
the O
standard O
masked O
language O
model O
( O
MLM O
) O
loss O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
to O
enable O
both O
sentence O
and O
word O
level O
representation O
learning O
. O

We O
use O
a O
character O
- O
based O
tokenizer O
because O
( O
1 O
) O
Chinese O
characters O
as O
themselves O
stand O
for O
individual O
semantic O
units O
( O
Li O
et O
al O
. O
, O
2019b O
) O
and O
( O
2 O
) O
char O
- O
based O
models O
are O
much O
more O
robust O
under O
noisy O
and O
adversarial O
scenarios O
( O
El O
Boukkouri O
et O
al O
. O
, O
2020 O
) O
. O

For O
Chinese O
characters O
, O
we O
use O
two O
masking O
strategies O
– O
Whole O
Word O
Masking O
( O
WWM O
) O
and O
Char O
Masking O
( O
CM O
) O
because O
a O
large O
number O
of O
words O
in O
Chinese O
consist O
of O
multiple O
characters O
( O
Cui O
et O
al O
. O
, O
2019 O
; O
Sun O
et O
al O
. O
, O
2021 O
) O
. O

The O
contrastive O
learning O
loss O
and O
the O
MLM O
loss O
are O
equally O
weighted O
. O

5 O
Experiments O
5.1 O

Experiment O
Setup O
Model O
Details O
We O
use O
a O
vocabulary O
size O
of O
16224 O
, O
out O
of O
which O
14642 O
are O
Chinese O
characters O
. O

We O
provide O
two O
versions O
of O
ROCBERT B-MethodName
: O
base O
and O
large O
. O

The O
base O
version O
has O
12 B-HyperparameterValue
layers B-HyperparameterName
/ O
heads O
with O
768 B-HyperparameterValue
hidden B-HyperparameterName
neurons I-HyperparameterName
. O

It O
is O
trained O
for O
600k B-HyperparameterValue
steps B-HyperparameterName
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
4k B-HyperparameterValue
, O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-4 B-HyperparameterValue
and O
warmup B-HyperparameterName
rate I-HyperparameterName
of O
25k B-HyperparameterValue
steps O
. O

The O
large O
version O
has O
48 B-HyperparameterValue
layers B-HyperparameterName
and O
24 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
with O
1024 B-HyperparameterValue
hidden B-HyperparameterName
neurons I-HyperparameterName
. O

It O
is O
trained O
for O
500 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
3e-4 B-HyperparameterValue
, O
warmup B-HyperparameterName
of O
70 B-HyperparameterValue
K I-HyperparameterValue
steps O
and O
batch B-HyperparameterName
size I-HyperparameterName
of O
8k B-HyperparameterValue
. O

Pretraining O
Details O
Following O
the O
common O
practice O
, O
we O
pretrain O
our O
model O
on O
2 O
TB O
text O
extracted O
from O
a O
mixture O
of O
THUCTC12 B-DatasetName
, O
Chinese B-DatasetName
Wikipedia I-DatasetName
and O
Common B-DatasetName
Crawl I-DatasetName
. O

Models O
are O
trained O
on O
64 O
NVIDIA O
V100 O
( O
32 O
GB O
) O
GPUs O
with O
FP16 O
and O
ZERO O
- O
stage-1 O
optimization O
( O
Rasley O
et O
al O
. O
, O
2020 O
) O
. O

To O
make O
better O
use O
of O
the O
GPU O
, O
we O
train O
our O
model O
with O
PatricStar13which O
applies O
a O
dynamic O
memory O
scheduling O
with O
a O
chunk O
- O
based O
memory O
management O
module O
( O
Fang O
et O
al O
. O
, O
2021 O
) O
. O

The O
memory O
management O
ofﬂoads O
everything O
but O
the O
current O
computing O
part O
of O
the O
model O
to O
CPUs O
. O

This O
results O
in O
training O
a O
much O
larger O
model O
within O
the O
same O
hardware O
environment O
. O

The O
chunk O
- O
based O
memory O
management O
takes O
advantage O
of O
the O
linear O
structure O
of O
the O
transformer O
- O
based O
model O
, O
so O
that O
it O
will O
inherently O
prefetch O
the O
upcoming O
layers O
to O
GPUs O
. O

12https://github.com/thunlp/THUCTC O
13https://github.com/Tencent/PatrickStarBaseline O
Models O
We O
compare O
our O
model O
with O
SOTA O
pretrained O
Chinese O
models O
: O
( O
1 O
) O
MBertChinese B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
( O
2 O
) O
Bert B-MethodName
- I-MethodName
wwm I-MethodName
( O
Cui O
et O
al O
. O
, O
2019 O
) O
, O
( O
3 O
) O
MacBert B-MethodName
( O
Cui O
et O
al O
. O
, O
2020 O
) O
, O
( O
4 O
) O
Ernie B-MethodName
- I-MethodName
gram I-MethodName
( O
Sun O
et O
al O
. O
, O
2019 O
, O
2020b O
) O
and O
( O
5 O
) O
ChineseBert B-MethodName
( O
Sun O
et O
al O
. O
, O
2021 O
) O
. O

BERT B-MethodName
- I-MethodName
wwm I-MethodName
continues O
pretraining O
from O
MBert B-MethodName
- I-MethodName
Chinese I-MethodName
with O
the O
Whole O
Word O
Masking O
pretraining O
strategy O
. O

MacBERT B-MethodName
applies O
the O
MLM O
- O
As O
- O
Correlation O
( O
MAC O
) O
pretraining O
strategy O
as O
well O
as O
the O
sentence O
- O
order O
prediction O
( O
SOP O
) O
task O
. O

ERNIE B-MethodName
- I-MethodName
gram I-MethodName
adopts O
various O
masking O
strategies O
including O
token O
- O
level O
, O
phraselevel O
and O
entity O
- O
level O
masking O
to O
pretrain O
BERT B-MethodName
on O
largescale O
heterogeneous O
data O
. O

Chinese B-MethodName
- I-MethodName
Bert I-MethodName
is O
pretrained O
with O
the O
glyph O
and O
phonetic O
features O
. O

Tasks O
We O
test O
our O
model O
on O
5 O
standard O
Chinese O
NLU B-TaskName
tasks O
and O
one O
toxic B-TaskName
detection I-TaskName
tasks O
. O

The O
5 O
NLU B-TaskName
tasks O
are O
: O
( O
1 O
) O
ChnSentiCorp B-DatasetName
, O
Chinese B-TaskName
sentiment I-TaskName
classiﬁcation I-TaskName
with O
2k O
training O
data14 O
, O
( O
2 O
) O

TNEWS B-DatasetName
: O
news B-TaskName
title I-TaskName
classiﬁcation I-TaskName
with O
50k O
training O
data O
, O
( O
3 O
) O
AFQMC B-DatasetName
: O
question B-TaskName
matching I-TaskName
with O
34k O
training O
data O
, O
( O
4 O
) O
CSL B-DatasetName
, O
keyword B-TaskName
recognition I-TaskName
from O
paper O
abstracts O
with O
20k O
training O
data O
ChnSentiCorp B-DatasetName
: O
2k O
( O
Xu O
et O

al O
. O
, O
2020 O
) O
and O
( O
5 O
) O
CMNLI B-DatasetName
, O
Chinese B-TaskName
Multi I-TaskName
- I-TaskName
Genre I-TaskName
NLI I-TaskName
with O
390k O
data O
( O
Conneau O
et O
al O
. O
, O
2018 O
) O
. O

Toxic B-TaskName
detection I-TaskName
can O
server O
as O
a O
task O
with O
“ O
human O
- O
made O
" O
attacks O
in O
contrast O
with O
the O
synthesized O
ones O
. O

It O
is O
collected O
from O
user O
interactions O
( O
written O
) O
with O
a O
popular O
online O
conversational O
platform O
, O
where O
users O
sometimes O
use O
various O
manmade O
attacks O
to O
avoid O
automatic O
system O
ﬁltering O
of O
junk O
ads O
, O
porn O
and O
abusive O
information O
. O

We O
manually O
annotate O
50k O
user O
inputs O
and O
identify O
2k O
toxic O
contents O
( O
positive O
) O
, O
out O
of O
which O
90 O
% O
are O
in O
adversarial O
forms O
. O

We O
randomly O
sample O
2k O
negative O
text O
then O
split O
the O
whole O
into O
train O
/ O
dev O
/ O
test O
with O
8:1:1 O
. O

Attacker O
We O
test O
the O
model O
performance O
under O
three O
different O
attackers O
( O
all O
untargeted O
as O
we O
do O
not O
need O
restrictions O
to O
the O
target O
class O
): O
( O
1 O
) O
ADV O
, O
our O
own O
attacking O
algorithm O
, O
( O
2 O
) O
TextFooler O
( O
Jin O
et O
al O
. O
, O
2020 O
) O
, O
a O
black O
- O
box O
algorithm O
replacing O
important O
words O
with O
semantically O
similar O
ones O
and O
( O
3 O
) O
Argot(Zhang O
et O
al O
. O
, O
2020 O
) O
, O
a O
black O
- O
box O
attacking O
algorithm O
considering O
Chinese O
- O
speciﬁc O
features O
. O

We O
set O
the O
maximum O
attacking O
ratio O
for O
all O
the O
three O
algorithms O
as O
20 O
% O
. O

TextFooler O
is O
originally O
designed O
for O
English O
, O
we O
reimplement O
it O
with O
corresponding O
pretrained O
Chinese O
- O
version O
models O
. O

14We O
use O
the O
small O
version O
of O
training O
data O
to O
test O
the O
fewshot O
capability O
of O
models.925Model O
Clean O
ADV O
TextFooler O
Argot O
Base O
MBert O
91.16 O
58.57 O
62.29 O
46.65 O
Bert O
- O
wwm O
91.27 O
59.28 O
63.22 O
44.52 O
MacBert O
91.33 O
59.72 O
63.18 O
44.34 O
Ernie O
- O
gram O
90.76 O
57.81 O
60.20 O
42.71 O
ChineseBert O
91.01 O
60.07 O
65.73 O
47.78 O
RoCBert O
91.45 O
81.62 O
83.11 O
68.40 O
Large O
MacBert O
92.05 O
55.92 O
45.75 O
41.83 O
RoCBert O
92.58 O
83.17 O
85.74 O
69.40 O
Table O
2 O
: O
Performance O
on O
ChnSentiCorp O
5.2 O
Experiment O
Results O
Chinese O
NLU B-TaskName
Results O
We O
show O
the O
results O
on O
5 O
Chinese O
NLU B-TaskName
tasks O
in O
tables O
2 O
to O
6 O
. O

For O
every O
task O
, O
we O
report O
the O
model O
accuracy B-MetricName
measured O
in O
the O
clean O
testset O
and O
the O
adversarial O
testsets O
under O
3 O
adversarial O
algorithms O
ADV O
, O
TextFooler O
andArgot O
. O

We O
report O
the O
performance O
of O
all O
base O
- O
version O
models O
for O
a O
fair O
comparison O
. O

We O
select O
the O
best O
- O
performed O
base O
- O
version O
model O
to O
test O
its O
large O
- O
version O
performance O
and O
compare O
it O
with O
ROCBERT B-MethodName
. O

As O
can O
be O
seen O
, O
our O
attacking O
algorithm O
ADV O
do O
not O
affect O
much O
on O
TNEWS B-DatasetName
, O
AFQMC B-DatasetName
and O
CSL B-DatasetName
because O
they O
rely O
more O
on O
the O
global O
sentence O
structure O
instead O
of O
individual O
words O
. O

On O
tasks O
like O
sentiment B-TaskName
classiﬁcation I-TaskName
and O
NLI B-TaskName
, O
single O
words O
contribute O
mostly O
to O
the O
model O
decision O
and O
therefore O
the O
attacking O
can O
lead O
to O
signiﬁcant O
performance O
drop O
. O

Argot O
and O
TextFooler O
lead O
to O
more O
drop O
compared O
with O
ADV O
because O
they O
explicitly O
select O
words O
that O
affect O
the O
model O
decisions O
most O
while O
ADV O
selects O
words O
to O
attack O
based O
on O
the O
general O
language O
model O
scores O
. O

Argot O
is O
more O
effective O
than O
TextFooler O
because O
it O
tailors O
its O
character O
replacement O
to O
consider O
Chinese O
- O
speciﬁc O
features O
. O

Overall O
ROCBERT B-MethodName
outperforms O
other O
models O
over O
all O
attacking O
algorithms O
on O
all O
the O
5 O
tasks O
. O

Even O
in O
the O
clean O
dataset O
, O
it O
performs O
the O
best O
on O
4 O
out O
of O
the O
5 O
tasks O
. O

ChineseBert B-MethodName
performs O
the O
second O
under O
various O
attacks O
because O
it O
also O
considers O
multimodal O
features O
during O
its O
pretraining O
same O
as O
ROCBERT B-MethodName
, O
which O
further O
conﬁrms O
the O
importance O
of O
using O
mulimodal O
features O
in O
Chinese O
language O
pretraining O
. O

Toxic B-TaskName
Content I-TaskName
Detection I-TaskName
Results O

We O
train O
all O
models O
in O
the O
toxic B-TaskName
content I-TaskName
detection I-TaskName
task O
. O

As O
can O
be O
seen O
in O
Table O
7 O
, O
ROCBERT B-MethodName
outperforms O
all O
other O
models O
over O
4 O
metrics O
. O

This O
conﬁrms O
the O
its O
effectiveness O
at O
capturing O
the O
true O
semantics O
regardless O
of O
its O
adversarial O
form O
. O

The O
differenceModel O
Clean O
ADV O
TextFooler O
Argot O
Base O
MBert O
56.84 O
53.76 O
42.05 O
40.18 O
Bert O
- O
wwm O
57.44 O
54.12 O
45.25 O
40.76 O
MacBert O
57.53 O
54.41 O
45.10 O
41.94 O
Ernie O
- O
gram O
57.30 O
52.58 O
43.02 O
41.16 O
ChineseBert O
57.65 O
55.74 O
51.01 O
50.27 O
RoCBert O
58.64 O
57.14 O
52.05 O
52.21 O
Large O
ChineseBert O
59.65 O
55.92 O
50.75 O
51.83 O
RoCBert O
59.98 O
59.17 O
54.74 O
54.46 O
Table O
3 O
: O
Performance O
on O
TNEWS O
Model O
Clean O
ADV O
TextFooler O
Argot O
Base O
MBert O
74.07 O
72.04 O
57.69 O
51.24 O
Bert O
- O
wwm O
75.07 O
72.40 O
57.58 O
51.05 O
MacBert O
74.79 O
72.08 O
57.37 O
50.78 O
Ernie O
- O
gram O
75.42 O
71.07 O
56.81 O
50.34 O
ChineseBert O
73.77 O
72.59 O
57.92 O
52.41 O
RoCBert O
75.48 O
74.11 O
62.95 O
62.16 O
Large O
Ernie O
- O
gram O
76.35 O
70.92 O
58.04 O
50.64 O
RoCBert O
77.48 O
76.43 O
65.85 O
64.97 O
Table O
4 O
: O
Performance O
on O
AFQMC O
among O
models O
is O
smaller O
because O
they O
have O
all O
been O
ﬁnetuned O
on O
this O
task O
. O

All O
models O
can O
get O
adapted O
to O
different O
forms O
of O
attacks O
in O
the O
training O
process O
while O
the O
tables O
2 O
to O
6 O
are O
testing O
the O
zeroshot O
generalization O
to O
unknown O
attacks O
. O

T O
A O
CL O
CI O
CP0:60:70:8 O
ADVAccuracy O
T O

A O
CL O
CI O
CP0:50:60:70:8 O
TextfoolerT O
A O
CL O
CI O
CP0:40:50:6 O
Argotbest O
- O
other O
+ O
spell O
- O
checker O
RoCBert O
A O
CL O

CI O
CP O
T0:60:70:8 O
ADVAccuracy O
A O
CL O
CI O
CP O
T0:60:8 O
TextfoolerA O
CL O
CI O
CP O
T0:40:50:60:7 O
Argotbest O
- O
other O
+ O
advtrain O
RoCBert O
+ O
advtrain O
Figure O
2 O
: O
Defending O
Method O
Comparison O
on O
CI(CMNLI O
) O
CP(ChnSentiCorp O
) O
, O
T(TNEWS O
) O
, O
A(AFQMC O
) O
and O
CL(CSL O
) O
. O

Defending O
Method O
Comparison O
We O
further O
compare O
ROCBERTwith O
two O
other O
popular O
ways O
of O
defending O
adversarial O
attack O
: O
( O
1 O
) O
run O
a O
spellchecker O
before O
fed O
to O
the O
model O
and O
( O
2 O
) O
adversarial O
training O
( O
advtrain O
) O
which O
augments O
training O
data O
with O
adversarial O
examples O
. O

We O
add O
these O
two O
de-926Model O
Clean O
ADV O
TextFooler O
Argot O
Base O
MBert O
81.83 O
78.28 O
61.06 O
52.40 O
Bert O
- O
wwm O
81.50 O
79.08 O
61.68 O
53.41 O
MacBert O
81.97 O
78.34 O
61.75 O
52.35 O
Ernie O
- O
gram O
82.70 O
79.53 O
63.54 O
53.66 O
ChineseBert O
81.77 O
78.69 O
61.27 O
53.79 O
RoCBert O
83.83 O
82.56 O
69.29 O
63.07 O
Large O
Ernie O
- O
gram O
83.05 O
79.42 O
61.85 O
57.43 O
RoCBert O
85.28 O
83.59 O
70.13 O
66.38 O
Table O
5 O
: O
Performance O
on O
CSL O
Model O
Clean O
ADV O
TextFooler O
Argot O
Base O
MBert O
80.53 O
69.57 O
50.21 O
45.52 O
Bert O
- O
wwm O
80.79 O
68.54 O
50.46 O
44.26 O
MacBert O
81.01 O
69.94 O
49.86 O
42.07 O
Ernie O
- O
gram O
82.22 O
68.83 O
50.77 O
44.69 O
ChineseBert O
81.42 O
72.27 O
52.85 O
47.15 O
RoCBert O
81.27 O
74.14 O
59.95 O
55.17 O
Large O
Ernie O
- O
gram O
82.36 O
70.11 O
52.45 O
45.82 O
RoCBert O
82.38 O
76.83 O
60.26 O
56.64 O
Table O
6 O
: O
Performance O
on O
CMNLI O
fending O
methods O
on O
top O
of O
the O
best O
- O
performed O
base O
model O
( O
on O
clean O
testsets O
) O
in O
different O
tasks O
: O
ChineseBert B-MethodName
for O
TNEWS B-DatasetName
, O
Ernie B-MethodName
- I-MethodName
gram I-MethodName
for O
AFQMC B-DatasetName
, O
CSL B-DatasetName
and O
CMNLI B-DatasetName
, O
MacBert B-MethodName
for O
ChnSentiCorp B-DatasetName
. O

We O
apply O
the O
spell O
- O
checker O
in O
Cheng O
et O
al O
. O

( O
2020 O
) O
. O

The O
results O
are O
visualized O
in O
Figure O
2 O
. O

We O
can O
see O
that O
spell O
checking O
improves O
the O
performance O
only O
marginally O
and O
sometimes O
even O
hurt O
the O
performance O
( O
best O
- O
other O
under O
ADV O
in O
CI O
) O
. O

The O
reason O
could O
be O
that O
the O
spell O
checker O
performs O
poorly O
for O
out O
- O
of O
- O
domain O
adversarial O
examples O
. O

The O
errors O
could O
be O
propagated O
and O
further O
reduce O
the O
performance O
. O

advtrain O
can O
signiﬁcantly O
beneﬁt O
the O
performance O
, O
but O
note O
that O
it O
explicitly O
“ O
peeps O
" O
at O
the O
adversarial O
algorithm O
applied O
in O
the O
testset O
while O
ROCBERTis O
not O
aware O
of O
the O
testing O
adversarial O
algorithm O
. O

Nevertheless O
, O
it O
is O
still O
comparable O
and O
in O
some O
cases O
even O
outperforms O
advtrain O
. O

By O
combiningROCBERTand O
advtrain O
, O
the O
model O
robustness O
can O
be O
further O
improved O
. O

5.3 O
Ablation O
Study O
We O
perform O
a O
set O
of O
ablation O
studies O
to O
understand O
the O
choice O
of O
different O
components O
in O
ROCBERT B-MethodName
. O

All O
models O
in O
this O
section O
are O
pretrained O
with O
the O
same O
base O
architecture O
and O
hyperparameters O
for O
one O
epoch O
on O
1 O
M O
sampled O
training O
text O
then O
tested O
in O
TNEWS B-DatasetName
. O

The O
results O
are O
shown O
in O
Table O
8.Model O

Acc O
Precision O
Recall O
F1 O
Base O
MBert O
85.11 O
87.12 O
81.35 O
84.13 O
Bert O
- O
wwm O
85.70 O
87.30 O
81.37 O
84.23 O
MacBert O
85.26 O
87.24 O
81.35 O
84.19 O
Ernie O
- O
gram O
85.94 O
87.43 O
81.38 O
84.29 O
ChineseBert O
85.52 O
87.29 O
81.36 O
84.22 O
RoCBert O
87.10 O
89.26 O
83.14 O
86.42 O
Large O
Ernie O
- O
gram O
87.30 O
88.96 O
82.57 O
85.64 O
RoCBert O
88.49 O
90.36 O
84.25 O
87.20 O
Table O
7 O
: O
Performance O
on O
Toxic O
Detection O
Setting O
Clean O
ADV O
Textfooler O

Argot O
Best O
55.38 O
52.23 O
47.72 O
44.59 O
Model O
Loss O
MLM O
54.63 O
48.58 O
38.63 O
33.75 O
Contrastive O
54.97 O
50.73 O
41.80 O
39.25 O
Tokenization O
bpe O
55.40 O
48.64 O
38.19 O
35.67 O
char O
- O
cnn O
53.23 O
49.45 O
44.37 O
41.44 O
Multimodal O
- O
vis O
- O
pretrain O
53.29 O
51.18 O
44.42 O
40.08 O
- O
vis O
53.35 O
51.20 O
45.45 O
41.86 O
- O
pho O
54.71 O
51.18 O
46.02 O
42.08 O
+ O
pho O
- O
pretrain O
54.96 O
51.95 O
47.03 O
43.56 O
Architecture O
Sum O
54.63 O
52.06 O
46.57 O
43.27 O
Concatenate O
55.13 O
52.14 O
46.69 O
43.84 O
Two O
- O
step O
55.09 O
51.81 O
45.39 O
42.67 O
Insert O
Layer O
Layer O
0 O
55.10 O
52.02 O
47.46 O
44.27 O
Layer O
4 O
54.63 O
51.95 O
47.35 O
44.33 O
Layer O
7 O
54.43 O
51.76 O
46.83 O
44.08 O
Layer O
10 O
54.25 O
50.98 O
46.20 O
43.56 O
Table O
8 O
: O
Ablation O
studies O
on O
TNEWS O
with O
different O
settings O
. O

Best O
indicates O
the O
best O
setting O
used O
in O
R O
OCBERT O
. O

Loss O
To O
study O
the O
effects O
of O
the O
loss O
function O
used O
in O
the O
pretraining O
stage O
. O

We O
tried O
two O
other O
settings O
: O
( O
1)contrastive O
only O
, O
where O
the O
model O
is O
pretrained O
only O
with O
the O
contrastive O
learning O
loss O
in O
Eq O
5 O
and O
( O
2)MLM O
- O
only O
, O
where O
the O
model O
is O
pretrained O
only O
with O
the O
MLM O
objective O
as O
in O
standard O
Bert O
. O

We O
can O
see O
that O
both O
options O
lowers O
down O
the O
model O
performance O
. O

By O
combining O
both O
loss O
, O
the O
model O
can O
be O
robust O
under O
adversarial O
attacks O
without O
affecting O
the O
performance O
in O
clean O
data O
. O

Tokenization O
It O
has O
been O
widely O
demonstrated O
that O
char O
- O
based O
tokenization O
is O
preferred O
for O
Chinese O
characters O
( O
Li O
et O
al O
. O
, O
2019b O
) O
, O
but O
it O
is O
rather O
unclear O
how O
we O
should O
model O
pinyins O
and O
nonChinese O
words O
. O

We O
try O
different O
tokenization O
methods O
for O
non O
- O
Chinese O
characters O
: O
( O
1 O
) O
bpe(Sennrich O
et O
al O
. O
, O
2016 O
) O
. O

We O
set O
the O
vocabulary O
as O
20k O
and O
train O
the O
split O
on O
the O
training O
data O
( O
after O
convert-9275 O
% O
10 O
% O
15 O
% O
20 O
% O
25%0:890:90:91 O
CleanAccuracy O
5 O
% O
10 O
% O
15 O
% O
20 O
% O
25%0:60:650:70:75 O
ADV O
5 O
% O
10 O
% O
15 O
% O
20 O
% O
25%0:650:70:750:8 O
ArgotAccuracy O
5 O
% O
10 O
% O
15 O
% O
20 O
% O
25%0:450:50:550:60:65 O
TextfoolerRoCBert O
w/o O
noise O
w/o O
CS O
SimCSE O
Figure O
3 O
: O
Ablation O
study O
with O
varying O
attacking O
ratio O
, O
w/o O
Gaussian O
noise O
, O
w/o O
character O
selection O
( O
CS O
) O
and O
SimCSE O
. O

ing O
all O
Chinese O
characters O
into O
pinyin O
) O
. O

( O
2 O
) O
charcnn(Zhang O
et O
al O
. O
, O
2015 O
) O
, O
which O
process O
each O
character O
individually O
but O
get O
the O
pinyin O
embedding O
with O
a O
char O
- O
cnn O
. O

The O
best O
setting O
in O
ROCBERT O
used O
char O
- O
sum O
which O
processes O
each O
character O
individually O
and O
set O
the O
pinyin O
embedding O
as O
the O
sum O
of O
its O
character O
embeddings O
. O

We O
can O
see O
that O
bpe O
hurt O
the O
performance O
. O

This O
might O
be O
because O
the O
bpe O
split O
is O
trained O
on O
clean O
data O
only O
. O

For O
adversarial O
examples O
, O
the O
letters O
in O
pinyins O
can O
be O
easily O
perturbed O
and O
break O
its O
vocabulary O
. O

Char O
- O
based O
tokenization O
is O
more O
robust O
under O
adversarial O
attacks O
. O

Char O
- O
cnn O
does O
not O
lead O
to O
improvement O
here O
, O
probably O
because O
there O
are O
a O
limited O
combination O
of O
letters O
in O
Chinese O
pinyins O
( O
400 O
) O
, O
each O
pinyin O
can O
usually O
be O
uniquely O
identiﬁed O
by O
its O
bag O
of O
characters O
without O
the O
need O
of O
order O
information O
. O

Multimodal O
feature O
We O
tried O
removing O
the O
visual O
feature O
pretraining O
as O
mentioned O
in O
§ O
4 O
and O
observe O
the O
performance O
drop O
( O
-vis O
- O
pretrain O
) O
. O

It O
is O
even O
worse O
than O
removing O
the O
visual O
feature O
completely O
( O
-vis O
) O
, O
suggesting O
the O
pretraining O
for O
visual O
features O
is O
essential O
, O
without O
which O
the O
model O
can O
be O
hard O
to O
learn O
meaningful O
visual O
features O
. O

The O
phonetic O
feature O
is O
less O
crucial O
than O
visual O
features O
but O
also O
brings O
positive O
improvement O
. O

By O
adding O
a O
pretraining O
stage O
for O
the O
phonetic O
features O
too O
( O
+ O
pho O
- O
pretrain O
) O
, O
the O
improvement O
is O
very O
marginal O
. O

As O
the O
phonetic O
features O
are O
also O
based O
on O
character O
embeddings O
, O
it O
might O
be O
easier O
for O
the O
model O
to O
automatically O
learn O
the O
phonetic O
features O
compared O
with O
the O
visual O
features O
. O

Multimodal O
integration O
We O
compare O
our O
proposed O
layer O
- O
insert O
with O
three O
other O
ways O
of O
integrating O
multimodal O
features O
: O
( O
1 O
) O
sum O
( O
Liu O
et O
al O
. O
, O
2021 O
) O
, O
which O
sums O
the O
multimodal O
embeddings O
, O
( O
2)concatenation O
, O
which O
concatenate(Sun O
et O

al O
. O
, O
2021 O
) O
, O
which O
concatenate O
the O
multimodal O
embeddings O
then O
fuse O
with O
an O
MLP O
layer O
, O
( O
3 O
) O
two O
- O
step O
( O
Xu O
et O
al O
. O
, O
2021 O
) O
, O
which O
ﬁrst O
determine O
the O
weight O
of O
different O
embeddings O
then O
fuse O
to O
the O
encoder O
. O

We O
can O
see O
that O
ROCBERT B-MethodName
performs O
best O
with O
only O
marginal O
computational O
overhead O
by O
updating O
the O
encoder O
representation O
in O
one O
layer O
. O

Insert O
Layer O
We O
further O
analyze O
the O
effects O
of O
the O
insertion O
layer O
. O

Our O
best O
setting O
inserts O
the O
multimodal O
features O
in O
layer O
1 O
for O
the O
base O
model O
and O
layer O
3 O
for O
the O
large O
model O
. O

From O
Table O
8 O
, O
we O
can O
see O
that O
when O
inserting O
them O
in O
the O
upper O
layer O
4,7 O
and O
10 O
, O
the O
performance O
gradually O
drops O
, O
suggesting O
an O
earlier O
insert O
is O
helpful O
for O
the O
model O
to O
incorporate O
these O
features O
in O
- O
depth O
. O

However O
, O
inserting O
them O
in O
layer O
0 O
is O
also O
worse O
since O
the O
model O
can O
only O
learn O
weight O
among O
multimodal O
features O
solely O
from O
bag O
of O
words O
. O

Attacking O
Algorithm O
We O
change O
the O
settings O
in O
our O
attacking O
algorithm O
to O
see O
the O
effects O
in O
Figure O
3 O
. O

We O
can O
see O
the O
attacking O
ratio O
can O
neither O
be O
too O
small O
nor O
too O
large O
. O

15 O
% O
is O
a O
sweet O
spot O
for O
pretraining O
. O

The O
Gaussian O
noise O
added O
in O
Eq O
1 O
also O
brings O
positive O
effects O
consistently O
, O
suggesting O
we O
should O
not O
use O
a O
ﬁxed O
attacking O
ratio O
in O
the O
pretraining O
stage O
. O

The O
character O
selection O
is O
also O
crucial O
and O
removing O
it O
signiﬁcantly O
reduces O
the O
performance O
. O

To O
show O
whether O
it O
is O
necessary O
to O
adopt O
our O
attacking O
algorithm O
with O
complex O
combinations O
of O
attacking O
forms O
. O

We O
further O
compare O
with O
pretraining O
the O
model O
with O
SimCSE O
( O
Gao O
et O
al O
. O
, O
2021 O
) O
, O
an O
algorithm O
which O
uses O
drop O
out O
as O
the O
noise O
instead O
of O
our O
adversarial O
examples O
. O

We O
can O
see O
that O
SimCSE O
is O
rarely O
helpful O
under O
different O
attacks O
. O

This O
suggests O
it O
is O
important O
to O
deﬁne O
rulebased O
attacking O
algorithms O
to O
better O
ﬁt O
the O
realworld O
attacks O
. O

General O
drop O
- O
out O
regularizations O
can O
not O
adapt O
well O
to O
complex O
real O
- O
world O
attacks O
. O

6 O
Conclusion O
We O
present O
ROCBERT B-MethodName
: O
the O
ﬁrst O
pretrained O
Chinese O
language O
model O
that O
is O
robust O
under O
various O
forms O
of O
adversarial O
attacks O
. O

It O
is O
pretrained O
with O
the O
multimodal O
contrastive O
learning O
objective O
and O
achieves O
the O
best O
performance O
on O
5 O
Chinese O
NLU B-TaskName
tasks O
un-928der O
three O
different O
attacking O
algorithms O
without O
negative O
effects O
on O
clean O
testsets O
. O

It O
also O
signiﬁcantly O
outperforms O
the O
others O
in O
the O
toxic O
content O
detection O
task O
. O

Extensive O
ablation O
studies O
are O
provided O
to O
beneﬁt O
future O
research O
. O

References O
Tom O
B O
Brown O
, O
Benjamin O
Mann O
, O
Nick O
Ryder O
, O
Melanie O
Subbiah O
, O
Jared O
Kaplan O
, O
Prafulla O
Dhariwal O
, O
Arvind O
Neelakantan O
, O
Pranav O
Shyam O
, O
Girish O
Sastry O
, O
Amanda O
Askell O
, O
et O
al O
. O
2020 O
. O

Language O
models O
are O
few O
- O
shot O
learners O
. O

arXiv O
preprint O
arXiv:2005.14165 O
. O

Ting O
Chen O
, O
Simon O
Kornblith O
, O
Mohammad O
Norouzi O
, O
and O
Geoffrey O
Hinton O
. O

2020 O
. O

A O
simple O
framework O
for O
contrastive O
learning O
of O
visual O
representations O
. O

In O
International O
conference O
on O
machine O
learning O
, O
pages O
1597–1607 O
. O

PMLR O
. O

Xingyi O
Cheng O
, O
Weidi O
Xu O
, O
Kunlong O
Chen O
, O
Shaohua O
Jiang O
, O
Feng O
Wang O
, O
Taifeng O
Wang O
, O
Wei O
Chu O
, O
and O
Yuan O
Qi O
. O
2020 O
. O

Spellgcn O
: O
Incorporating O
phonological O
and O
visual O
similarities O
into O
language O
models O
for O
chinese O
spelling O
check O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
871–881 O
. O

Alexis O
Conneau O
, O
Ruty O
Rinott O
, O
Guillaume O
Lample O
, O
Adina O
Williams O
, O
Samuel O
Bowman O
, O
Holger O
Schwenk O
, O
and O
Veselin O
Stoyanov O
. O

2018 O
. O

Xnli O
: O
Evaluating O
crosslingual O
sentence O
representations O
. O

In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2475–2485 O
. O
Yiming O
Cui O
, O
Wanxiang O
Che O
, O
Ting O
Liu O
, O
Bing O
Qin O
, O
Shijin O
Wang O
, O
and O
Guoping O
Hu O
. O
2020 O
. O

Revisiting O
pretrained O
models O
for O
chinese O
natural O
language O
processing O
. O

In O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
: O
Findings O
, O
pages O
657–668 O
. O

Yiming O
Cui O
, O
Wanxiang O
Che O
, O
Ting O
Liu O
, O
Bing O
Qin O
, O
Ziqing O
Yang O
, O
Shijin O
Wang O
, O
and O
Guoping O
Hu O
. O
2019 O
. O

Pre O
- O
training O
with O
whole O
word O
masking O
for O
chinese O
bert O
. O

arXiv O
preprint O
arXiv:1906.08101 O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

Bert B-MethodName
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
. O

Steffen O
Eger O
and O
Yannik O
Benz O
. O
2020 O
. O

From O
hero O
to O
zn’eroe O
: O
A O
benchmark O
of O
low O
- O
level O
adversarial O
attacks O
. O

arXiv O
preprint O
arXiv:2010.05648 O
. O

Steffen O
Eger O
, O
Gözde O
Gül O
¸ O
Sahin O
, O
Andreas O
Rücklé O
, O
JiUng O
Lee O
, O
Claudia O
Schulz O
, O
Mohsen O
Mesgar O
, O
Krishnkant O
Swarnkar O
, O
Edwin O
Simpson O
, O
and O
IrynaGurevych O
. O

2019 O
. O

Text O
processing O
like O
humans O
do O
: O
Visually O
attacking O
and O
shielding O
nlp O
systems O
. O

arXiv O
preprint O
arXiv:1903.11508 O
. O

Hicham O
El O
Boukkouri O
, O
Olivier O
Ferret O
, O
Thomas O
Lavergne O
, O
Hiroshi O
Noji O
, O
Pierre O
Zweigenbaum O
, O
and O
Jun’ichi O
Tsujii O
. O

2020 O
. O

Characterbert B-MethodName
: O
Reconciling O
elmo O
and O
bert O
for O
word O
- O
level O
open O
- O
vocabulary O
representations O
from O
characters O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
6903–6915 O
. O
Jiarui O
Fang O
, O
Yang O
Yu O
, O
Zilin O
Zhu O
, O
Shenggui O
Li O
, O
Yang O
You O
, O
and O
Jie O
Zhou O
. O

2021 O
. O

Patrickstar O
: O
Parallel O
training O
of O
pre O
- O
trained O
models O
via O
a O
chunk O
- O
based O
memory O
management O
. O

arXiv O
preprint O
arXiv:2108.05818 O
. O

Tianyu O
Gao O
, O
Xingcheng O
Yao O
, O
and O
Danqi O
Chen O
. O
2021 O
. O
Simcse O
: O
Simple O
contrastive O
learning O
of O
sentence O
embeddings O
. O

arXiv O
preprint O
arXiv:2104.08821 O
. O

Siddhant O
Garg O
and O
Goutham O
Ramakrishnan O
. O

2020 O
. O

Bae O
: O
Bert O
- O
based O
adversarial O
examples O
for O
text O
classiﬁcation O
. O

arXiv O
preprint O
arXiv:2004.01970 O
. O

Yotam O
Gil O
, O
Yoav O
Chai O
, O
Or O
Gorodissky O
, O
and O
Jonathan O
Berant O
. O

2019 O
. O

White O
- O
to O
- O
black O
: O
Efﬁcient O
distillation O
of O
black O
- O
box O
adversarial O
attacks O
. O

arXiv O
preprint O
arXiv:1904.02405 O
. O

Kaiming O
He O
, O
Xiangyu O
Zhang O
, O
Shaoqing O
Ren O
, O
and O
Jian O
Sun O
. O

2016 O
. O

Deep O
residual O
learning O
for O
image O
recognition O
. O

In O
Proceedings O
of O
the O
IEEE O
conference O
on O
computer O
vision O
and O
pattern O
recognition O
, O
pages O
770 O
– O
778 O
. O

Di O
Jin O
, O
Zhijing O
Jin O
, O
Joey O
Tianyi O
Zhou O
, O
and O
Peter O
Szolovits O
. O
2020 O
. O

Is O
bert O
really O
robust O
? O

a O
strong O
baseline O
for O
natural O
language O
attack O
on O
text O
classiﬁcation O
and O
entailment O
. O

In O
Proceedings O
of O
the O
AAAI O
conference O
on O
artiﬁcial O
intelligence O
, O
volume O
34 O
, O
pages O
8018–8025 O
. O

Taeuk O
Kim O
, O
Kang O
Min O
Yoo O
, O
and O
Sang O
- O
goo O
Lee O
. O
2021 O
. O

Self O
- O
guided O
contrastive O
learning O
for O
bert O
sentence O
representations O
. O

arXiv O
preprint O
arXiv:2106.07345 O
. O

Dianqi O
Li O
, O
Yizhe O
Zhang O
, O
Hao O
Peng O
, O
Liqun O
Chen O
, O
Chris O
Brockett O
, O
Ming O
- O
Ting O
Sun O
, O
and O
Bill O
Dolan O
. O

2020a O
. O

Contextualized O
perturbation O
for O
textual O
adversarial O
attack O
. O

arXiv O
preprint O
arXiv:2009.07502 O
. O

Jinfeng O
Li O
, O
Tianyu O
Du O
, O
Shouling O
Ji O
, O
Rong O
Zhang O
, O
Quan O
Lu O
, O
Min O
Yang O
, O
and O
Ting O
Wang O
. O

2020b O
. O

Textshield O
: O
Robust O
text O
classiﬁcation O
based O
on O
multimodal O
embedding O
and O
neural O
machine O
translation O
. O

In O
29th O
fUSENIXgSecurity O
Symposium O
( O
fUSENIXgSecurity O
20 O
) O
, O
pages O
1381–1398 O
. O

Jinfeng O
Li O
, O
Shouling O
Ji O
, O
Tianyu O
Du O
, O
Bo O
Li O
, O
and O
Ting O
Wang O
. O

2019a O
. O

Textbugger O
: O
Generating O
adversarial O
text O
against O
real O
- O
world O
applications O
. O

arXiv O
preprint O
arXiv:1812.05271 O
. O

Jiwei O
Li O
, O
Will O
Monroe O
, O
and O
Dan O
Jurafsky O
. O

2016 O
. O

Understanding O
neural O
networks O
through O
representation O
erasure O
. O

arXiv O
preprint O
arXiv:1612.08220 O
.929Linyang O

Li O
, O
Ruotian O
Ma O
, O
Qipeng O
Guo O
, O
Xiangyang O
Xue O
, O
and O
Xipeng O
Qiu O
. O
2020c O
. O

Bert O
- O
attack O
: O
Adversarial O
attack O
against O
bert O
using O
bert O
. O

arXiv O
preprint O
arXiv:2004.09984 O
. O

Linyang O
Li O
, O
Yunfan O
Shao O
, O
Demin O
Song O
, O
Xipeng O
Qiu O
, O
and O
Xuanjing O
Huang O
. O

2020d O
. O

Generating O
adversarial O
examples O
in O
chinese O
texts O
using O
sentence O
- O
pieces O
. O

arXiv O
preprint O
arXiv:2012.14769 O
. O

Xiaoya O
Li O
, O
Yuxian O
Meng O
, O
Xiaofei O
Sun O
, O
Qinghong O
Han O
, O
Arianna O
Yuan O
, O
and O
Jiwei O
Li O
. O

2019b O
. O

Is O
word O
segmentation O
necessary O
for O
deep O
learning O
of O
chinese O
representations O
? O

In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
3242–3252 O
. O

Hui O
Liu O
, O
Yongzheng O
Zhang O
, O
Yipeng O
Wang O
, O
Zheng O
Lin O
, O
and O
Yige O
Chen O
. O

2020 O
. O

Joint O
character O
- O
level O
word O
embedding O
and O
adversarial O
stability O
training O
to O
defend O
adversarial O
text O
. O

In O
Proceedings O
of O
the O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
volume O
34 O
, O
pages O
8384–8391 O
. O

Shulin O
Liu O
, O
Tao O
Yang O
, O
Tianchi O
Yue O
, O
Feng O
Zhang O
, O
and O
Di O
Wang O
. O
2021 O
. O

Plome O
: O

Pre O
- O
training O
with O
misspelled O
knowledge O
for O
chinese O
spelling O
correction O
. O

InProceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
2991–3000 O
. O

Yinhan O
Liu O
, O
Myle O
Ott O
, O
Naman O
Goyal O
, O
Jingfei O
Du O
, O
Mandar O
Joshi O
, O
Danqi O
Chen O
, O
Omer O
Levy O
, O
Mike O
Lewis O
, O
Luke O
Zettlemoyer O
, O
and O
Veselin O
Stoyanov O
. O

2019 O
. O

Roberta O
: O
A O
robustly O
optimized O
bert O
pretraining O
approach O
. O

Zhao O
Meng O
, O
Yihan O
Dong O
, O
Mrinmaya O
Sachan O
, O
and O
Roger O
Wattenhofer O
. O
2021 O
. O

Self O
- O
supervised O
contrastive O
learning O
with O
adversarial O
perturbations O
for O
robust O
pretrained O
language O
models O
. O

arXiv O
preprint O
arXiv:2107.07610 O
. O

Maximilian O
Mozes O
, O
Pontus O
Stenetorp O
, O
Bennett O
Kleinberg O
, O
and O
Lewis O
Grifﬁn O
. O

2021 O
. O

Frequency O
- O
guided O
word O
substitutions O
for O
detecting O
textual O
adversarial O
examples O
. O

In O
Proceedings O
of O
the O
16th O
Conference O
of O
the O
European O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Main O
Volume O
, O
pages O
171–186 O
. O

Cheng O
Nuo O
, O
Guo O
- O
Qin O
Chang O
, O
Haichang O
Gao O
, O
Ge O
Pei O
, O
and O
Yang O
Zhang O
. O

2020 O
. O

Wordchange O
: O

Adversarial O
examples O
generation O
approach O
for O
chinese O
text O
classiﬁcation O
. O

IEEE O
Access O
, O
8:79561–79572 O
. O

Matthew O
E O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O

2018 O
. O

Deep O
contextualized O
word O
representations O
. O

In O
Proceedings O
of O
NAACL O
- O
HLT O
, O
pages O
2227–2237 O
. O

Danish O
Pruthi O
, O
Bhuwan O
Dhingra O
, O
and O
Zachary O
C O
Lipton O
. O
2019 O
. O

Combating O
adversarial O
misspellings O
with O
robust O
word O
recognition O
. O

In O
Proceedings O
of O
the57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
5582–5591 O
. O

Alec O
Radford O
, O
Karthik O
Narasimhan O
, O
Tim O
Salimans O
, O
and O
Ilya O
Sutskever O
. O

Improving O
language O
understanding O
by O
generative O
pre O
- O
training O
. O

Jeff O
Rasley O
, O
Samyam O
Rajbhandari O
, O
Olatunji O
Ruwase O
, O
and O
Yuxiong O
He O
. O
2020 O
. O

Deepspeed O
: O
System O
optimizations O
enable O
training O
deep O
learning O
models O
with O
over O
100 O
billion O
parameters O
. O

In O
Proceedings O
of O
the O
26th O
ACM O
SIGKDD O
International O
Conference O
on O
Knowledge O
Discovery O
& O
Data O
Mining O
, O
pages O
3505 O
– O
3506 O
. O

Rico O
Sennrich O
, O
Barry O
Haddow O
, O
and O
Alexandra O
Birch O
. O
2016 O
. O

Neural O
machine O
translation O
of O
rare O
words O
with O
subword O
units O
. O

In O
Proceedings O
of O
the O
54th O

Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1715 O
– O
1725 O
. O

Lichao O
Sun O
, O
Kazuma O
Hashimoto O
, O
Wenpeng O
Yin O
, O
Akari O
Asai O
, O
Jia O
Li O
, O
Philip O
Yu O
, O
and O
Caiming O
Xiong O
. O
2020a O
. O

Adv O
- O
bert O
: O
Bert O
is O
not O
robust O
on O
misspellings O
! O

generating O
nature O
adversarial O
samples O
on O
bert O
. O

arXiv O
preprint O
arXiv:2003.04985 O
. O

Yu O
Sun O
, O
Shuohuan O
Wang O
, O
Yukun O
Li O
, O
Shikun O
Feng O
, O
Xuyi O
Chen O
, O
Han O
Zhang O
, O
Xin O
Tian O
, O
Danxiang O
Zhu O
, O
Hao O
Tian O
, O
and O
Hua O
Wu O
. O
2019 O
. O

Ernie B-MethodName
: O
Enhanced O
representation O
through O
knowledge O
integration O
. O

arXiv O
preprint O
arXiv:1904.09223 O
. O

Yu O
Sun O
, O
Shuohuan O
Wang O
, O
Yukun O
Li O
, O
Shikun O
Feng O
, O
Hao O
Tian O
, O
Hua O
Wu O
, O
and O
Haifeng O
Wang O
. O

2020b O
. O

Ernie B-MethodName
2.0 I-MethodName
: O
A O
continual O
pre O
- O
training O
framework O
for O
language O
understanding O
. O

In O
Proceedings O
of O
the O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
volume O
34 O
, O
pages O
8968 O
– O
8975 O
. O

Zijun O
Sun O
, O
Xiaoya O
Li O
, O
Xiaofei O
Sun O
, O
Yuxian O
Meng O
, O
Xiang O
Ao O
, O
Qing O
He O
, O
Fei O
Wu O
, O
and O
Jiwei O
Li O
. O
2021 O
. O

Chinesebert B-MethodName
: O
Chinese O
pretraining O
enhanced O
by O
glyph O
and O
pinyin O
information O
. O

arXiv O
preprint O
arXiv:2106.16038 O
. O

Baoxin O
Wang O
, O
Wanxiang O
Che O
, O
Dayong O
Wu O
, O
Shijin O
Wang O
, O
Guoping O
Hu O
, O
and O
Ting O
Liu O
. O

2021a O
. O

Dynamic O
connected O
networks O
for O
chinese O
spelling O
check O
. O

In O
Findings O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
ACL O
- O
IJCNLP O
2021 O
, O
pages O
2437–2446 O
. O

Boxin O
Wang O
, O
Boyuan O
Pan O
, O
Xin O
Li O
, O
and O
Bo O
Li O
. O
2020 O
. O

Towards O
evaluating O
the O
robustness O
of O
chinese O
bert O
classiﬁers O
. O

arXiv O
preprint O
arXiv:2004.03742 O
. O

Dong O
Wang O
, O
Ning O
Ding O
, O
Piji O
Li O
, O
and O
Hai O
- O
Tao O
Zheng O
. O
2021b O
. O

Cline O
: O
Contrastive O
learning O
with O
semantic O
negative O
examples O
for O
natural O
language O
understanding O
. O

arXiv O
preprint O
arXiv:2107.00440 O
. O

Heng O
- O
Da O
Xu O
, O
Zhongli O
Li O
, O
Qingyu O
Zhou O
, O
Chao O
Li O
, O
Zizhen O
Wang O
, O
Yunbo O
Cao O
, O
Heyan O
Huang O
, O
and O
XianLing O
Mao O
. O
2021 O
. O

Read O
, O
listen O
, O
and O
see O
: O
Leveraging O
multimodal O
information O
helps O
chinese O
spell O
checking O
. O

arXiv O
preprint O
arXiv:2105.12306 O
.930Liang O

Xu O
, O
Hai O
Hu O
, O
Xuanwei O
Zhang O
, O
Lu O
Li O
, O
Chenjie O
Cao O
, O
Yudong O
Li O
, O
Yechen O
Xu O
, O
Kai O
Sun O
, O
Dian O
Yu O
, O
Cong O
Yu O
, O
et O
al O
. O
2020 O
. O

Clue O
: O
A O
chinese O
language O
understanding O
evaluation O
benchmark O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
4762–4772 O
. O

Jin O
Yong O
Yoo O
and O
Yanjun O
Qi O
. O
2021 O
. O

Towards O
improving O
adversarial O
training O
of O
nlp O
models O
. O

arXiv O
preprint O
arXiv:2109.00544 O
. O

Yuan O
Zang O
, O
Fanchao O
Qi O
, O
Chenghao O
Yang O
, O
Zhiyuan O
Liu O
, O
Meng O
Zhang O
, O
Qun O
Liu O
, O
and O
Maosong O
Sun O
. O
2020 O
. O

Word O
- O
level O
textual O
adversarial O
attacking O
as O
combinatorial O
optimization O
. O

arXiv O
preprint O
arXiv:1910.12196 O
. O

Xiang O
Zhang O
, O
Junbo O
Zhao O
, O
and O
Yann O
LeCun O
. O
2015 O
. O

Character O
- O
level O
convolutional O
networks O
for O
text O
classiﬁcation O
. O

Advances O
in O
neural O
information O
processing O
systems O
, O
28:649–657 O
. O

Zhengyan O
Zhang O
, O
Xu O
Han O
, O
Hao O
Zhou O
, O
Pei O
Ke O
, O
Yuxian O
Gu O
, O
Deming O
Ye O
, O
Yujia O
Qin O
, O
Yusheng O
Su O
, O
Haozhe O
Ji O
, O
Jian O
Guan O
, O
et O
al O
. O
2021 O
. O

Cpm B-MethodName
: O
A O
large O
- O
scale O
generative O
chinese O
pre O
- O
trained O
language O
model O
. O

AI O
Open O
, O
2:93–99 O
. O

Zihan O
Zhang O
, O
Mingxuan O
Liu O
, O
Chao O
Zhang O
, O
Yiming O
Zhang O
, O
Zhou O
Li O
, O
Qi O
Li O
, O
Haixin O
Duan O
, O
and O
Donghong O
Sun O
. O
2020 O
. O

Argot O
: O
Generating O
adversarial O
readable O
chinese O
texts.931 O

