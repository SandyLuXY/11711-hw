Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
619–624 O
July O
5 O
- O
10 O
, O
2020 O
. O

c O

 O
2020 O
Association O
for O
Computational O
Linguistics619Learning O
to O
Tag O
OOV O
Tokens O
by O
Integrating O
Contextual O
Representation O
and O
Background O
Knowledge O
Keqing O
He O
, O
Yuanmeng O
Yan O
, O
Weiran O
Xu O
Pattern O
Recognition O
& O
Intelligent O
System O
Laboratory O
School O
of O
Information O
and O
Communication O
Engineering O
Beijing O
University O
of O
Posts O
and O
Telecommunications O
, O
Beijing O
, O
China O
fkqin O
, O
yanyuanmeng O
, O
xuweiran O
g@bupt.edu.cn O
Abstract O
Neural O
- O
based O
context B-MethodName
- I-MethodName
aware I-MethodName
models O
for O
slot B-TaskName
tagging I-TaskName
have O
achieved O
state O
- O
of O
- O
the O
- O
art O
performance O
. O

However O
, O
the O
presence O
of O
OOV(outof O
- O
vocab O
) O
words O
signiﬁcantly O
degrades O
the O
performance O
of O
neural O
- O
based O
models O
, O
especially O
in O
a O
few O
- O
shot O
scenario O
. O

In O
this O
paper O
, O
we O
propose O
a O
novel O
knowledge O
- O
enhanced O
slot B-TaskName
tagging I-TaskName
model O
to O
integrate O
contextual O
representation O
of O
input O
text O
and O
the O
large O
- O
scale O
lexical O
background O
knowledge O
. O

Besides O
, O
we O
use O
multilevel O
graph O
attention O
to O
explicitly O
model O
lexical O
relations O
. O

The O
experiments O
show O
that O
our O
proposed O
knowledge O
integration O
mechanism O
achieves O
consistent O
improvements O
across O
settings O
with O
different O
sizes O
of O
training O
data O
on O
two O
public O
benchmark O
datasets O
. O

1 O
Introduction O
Slot B-TaskName
tagging I-TaskName
is O
a O
critical O
component O
of O
spoken O
language O
understanding(SLU O
) O
in O
dialogue O
systems O
. O

It O
aims O
at O
parsing O
semantic O
concepts O
from O
user O
utterances O
. O

For O
instance O
, O
given O
the O
utterance O
” O
I O
’d O
also O
like O
to O
have O
lunch O
during O
my O
ﬂight O
” O
from O
the O
ATIS B-DatasetName
dataset O
, O
a O
slot B-TaskName
tagging I-TaskName
model O
might O
identify O
lunch O
as O
ameal O
description O
type O
. O

Given O
sufﬁcient O
training O
data O
, O
recent O
neural O
- O
based O
models O
( O
Mesnil O
et O
al O
. O
, O
2014 O
; O
Liu O
and O
Lane O
, O
2015 O
, O
2016 O
; O

Goo O
et O
al O
. O
, O
2018 O
; O
Haihong O
et O

al O
. O
, O
2019 O
; O
He O
et O
al O
. O
, O
2020 O
) O
have O
achieved O
remarkably O
good O
results O
. O

However O
, O
these O
works O
often O
suffer O
from O
poor O
slot B-TaskName
tagging I-TaskName
accuracy O
when O
rare O
words O
or O
OOV O
( O
out O
- O
of O
- O
vocab O
) O
words O
exist O
. O

( O
Ray O
et O
al O
. O
, O
2018 O
) O
has O
veriﬁed O
the O
presence O
of O
OOV O
words O
further O
degrades O
the O
performance O
of O
neural O
- O
based O
models O
, O
especially O
in O
a O
few O
- O
shot O
scenario O
where O
training O
data O
can O
not O
provide O
adequate O
contextual O
semantics O
. O

Previous O
context O
- O
aware O
models O
merely O
focus O
on O
how O
to O
capture O
deep O
contextual O
semantics O
to O
aid O
Weiran O
Xu O
is O
the O
corresponding O
author O
. O

playlist O
broadcastmusic O
genre O
classical O
  O
music O
popular O
jazz O
scat O
  O
singinghyponym O
s O
sister O
term O
train O
setcan O
  O
you O
  O
append O
  O
some O
   O
classical O
   O
music O
   O
to O
  O
my O
  O
playlist O
    O
O O
            O

O O
            O

O O
            O
O O
     O
B O
-music_type O
   O

I O
-music_type O
   O

O O
      O

O O
       O

O O
find O
  O
and O
  O
add O
   O
some O
    O
scat O
  O
singing O
     O
to O
   O
my O
  O
broadcast O
    O

O O
          O

O O
           O

O O
          O

O O
                                                    O

O O
         O

O O
          O
Otest O
setsemantic O
   O
synset O
O O
          O

O O
( O
context O
-aware O
model)B O
- O
music_type O
   O

I O
-music_type O
( O
knowledge O
integration)Figure O
1 O
: O
An O
example O
of O
slot B-TaskName
tagging I-TaskName
in O
the O
few O
- O
shot O
scenario O
where O
scat O
singing O
is O
unseen O
in O
the O
training O
set O
. O

The O
prior O
context O
- O
aware O
model O
fails O
to O
recognize O
its O
correct O
type O
because O
of O
low O
- O
coverage O
contextual O
information O
. O

After O
integrating O
background O
knowledge O
from O
WordNet O
, O
it O
succeeds O
to O
reason O
the O
correct O
type O
via O
lexical O
relations O
. O

in O
recognizing O
slot O
entities O
, O
while O
neglecting O
ontology O
behind O
the O
words O
or O
large O
- O
scale O
background O
knowledge O
. O

Explicit O
lexical O
relations O
are O
vital O
to O
recognizing O
unseen O
words O
when O
there O
is O
not O
adequate O
training O
data O
, O
that O
is O
, O
few O
- O
shot O
scenarios O
. O

Fig O
1 O
gives O
a O
motivating O
example O
of O
slot B-TaskName
tagging I-TaskName
to O
explain O
the O
phenomenon O
. O

This O
example O
suggests O
slot B-TaskName
tagging I-TaskName
requires O
not O
only O
understanding O
the O
complex O
linguistic O
context O
constraints O
but O
also O
reasoning O
explicit O
lexical O
relations O
via O
large O
- O
scale O
background O
knowledge O
graphs O
. O

Previous O
state O
- O
of O
- O
the O
- O
art O
context O
- O
aware O
models O
( O
Goo O
et O
al O
. O
, O
2018 O
; O
Haihong O
et O
al O
. O
, O
2019 O
) O
only O
learn O
contextual O
information O
based O
on O
a O
multi O
- O
layer O
BiLSTM O
encoder O
and O
self O
- O
attention O
layer O
. O

( O
Dugas O
and O
Nichols O
, O
2016 O
; O
Williams O
, O
2019 O
; O
Shah O
et O
al O
. O
, O
2019 O
) O
use O
handcrafted B-MethodName
lexicons I-MethodName
( O
also O
known O
as O
gazettes O
or O
dictionaries O
) O
, O
which O
are O
typically O
collections O
of O
phrases O
semantically O
related O
, O
to O
improve O
slot B-TaskName
tagging I-TaskName
. O

One O
major O
limitation O
is O
that O
lexicons O
collected O
by O
domain O
experts O
are O
relatively O
small O
on O
the O
scale O
and O
fail O
to O
model O
complicated O
relations620between O
words O
, O
such O
as O
relation O
hierarchy O
. O

In O
this O
paper O
, O
we O
propose O
a O
novel O
knowledge B-MethodName
enhanced I-MethodName
method O
for O
slot B-TaskName
tagging I-TaskName
by O
integrating O
contextual O
representation O
of O
input O
text O
and O
the O
largescale O
lexical O
background O
knowledge O
, O
enabling O
the O
model O
to O
reason O
explicit O
lexical O
relations O
. O

We O
aim O
to O
leverage O
both O
linguistic O
regularities O
covered O
by O
deep O
LMs O
and O
high O
- O
quality O
knowledge O
derived O
from O
curated O
KBs O
. O

Consequently O
, O
our O
model O
could O
infer O
rare O
and O
unseen O
words O
in O
the O
test O
dataset O
by O
incorporating O
contextual O
semantics O
learned O
from O
the O
training O
dataset O
and O
lexical O
relations O
from O
ontology O
. O

As O
depicted O
in O
Fig O
2 O
, O
given O
an O
input O
sequence O
, O
we O
ﬁrst O
retrieve O
potentially O
relevant O
KB O
entities O
and O
encode O
them O
into O
distributed O
representations O
that O
describe O
global O
graph O
- O
structured O
information O
. O

Then O
we O
employ O
a O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
encoder O
layer O
to O
capture O
context O
- O
aware O
representations O
of O
the O
sequence O
and O
attend O
to O
the O
KB O
embeddings O
using O
multi O
- O
level O
graph O
attention O
. O

Finally O
, O
we O
integrate O
BERT B-MethodName
embeddings O
and O
the O
desired O
KB O
embeddings O
to O
predict O
the O
slot O
type O
. O

Our O
main O
contributions O
are O
three O
- O
fold O
: O
( O
1 O
) O
We O
investigate O
and O
demonstrate O
the O
feasibility O
of O
applying O
lexical O
ontology O
to O
facilitate O
recognizing O
OOV O
words O
in O
the O
few O
- O
shot O
scenario O
. O

To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
ﬁrst O
to O
consider O
the O
large O
- O
scale O
background O
knowledge O
for O
enhancing O
context O
- O
aware O
slot B-TaskName
tagging I-TaskName
models O
. O

( O
2 O
) O
We O
propose O
a O
knowledge B-MethodName
integration I-MethodName
mechanism I-MethodName
and O
use O
multi O
- O
level O
graph O
attention O
to O
model O
explicit O
lexical O
relations O
. O

( O
3 O
) O
Plenty O
of O
experiments O
on O
two O
benchmark O
datasets O
show O
that O
our O
proposed O
method O
achieves O
consistently O
better O
performance O
than O
various O
state O
- O
of O
- O
theart O
context O
- O
aware O
methods O
. O

2 O
Our O
Approach O
In O
this O
work O
, O
we O
consider O
the O
slot B-TaskName
tagging I-TaskName
task O
in O
the O
few O
- O
shot O
scenario O
, O
especially O
for O
OOV O
tokens O
. O

Given O
a O
sequence O
with O
n O
tokens O
X O
= O
fxign O
i=1 O
, O
our O
goal O
is O
to O
predict O
a O
corresponding O
tagging O
sequenceY O
= O
fyign O
i=1 O
. O

This O
section O
ﬁrst O
explains O
our O
BERT B-MethodName
- O
based O
model O
and O
then O
introduces O
the O
proposed O
knowledge O
integration O
mechanism O
for O
inducing O
background O
commonsense O
. O

The O
overall O
model O
architecture O
is O
illustrated O
in O
Fig O
2 O
. O
2.1 O
BERT B-MethodName
- O
Based O
Model O
for O
Slot B-TaskName
Tagging I-TaskName
The O
model O
architecture O
of O
BERT B-MethodName
is O
a O
multi O
- O
layer O
bidirectional O
Transformer O
encoder O
. O

The O
input O
representation O
is O
a O
concatenation O
of O
WordPiece O
emKnowledge O
Integration O
  O
Layer O

x1 O
x2 O

xnh1 O
h2 O
hny1 O
y2 O
yn O
hic1c2 O
cm O
sentinelC1(xi O
) O
C2(xi)fi O
... O
… O
… O
  O

BiLSTM O
Matching O
LayerCRF O
LayerFigure O
2 O
: O
The O
overall O
architecture O
of O
the O
proposed O
slot B-TaskName
tagging I-TaskName
model O
. O
beddings O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
, O
positional O
embeddings O
, O
and O
the O
segment O
embeddings O
. O

Inspired O
by O
previous O
RNN B-MethodName
- I-MethodName
based I-MethodName
works O
( O
Mesnil O
et O
al O
. O
, O
2014 O
; O
Liu O
and O
Lane O
, O
2016 O
) O
, O
we O
extend O
BERT B-MethodName
to O
a O
slot B-TaskName
tagging I-TaskName
model O
. O

We O
ﬁrst O
feed O
the O
input O
sequence O
X O
= O
fxign O
i=1to O
a O
pre O
- O
trained O
BERT B-MethodName
encoding O
layer O
and O
then O
get O
ﬁnal O
hidden O
states O
H= O
( O
h1;:::;h O
n O
) O
. O

To O
make O
this O
procedure O
compatible O
with O
the O
original O
BERT B-MethodName
tokenization O
, O
we O
feed O
each O
input O
word O
into O
a O
WordPiece O
tokenizer O
and O
use O
the O
hidden O
state O
corresponding O
to O
the O
ﬁrst O
sub O
- O
word O
as O
input O
to O
the O
softmax O
classiﬁer O
. O

yi= O
softmax O
( O
Whi+b);i21:::n O
( O
1 O
) O
wherehi2Rd1is O
the O
hidden O
state O
corresponding O
to O
the O
ﬁrst O
sub O
- O
word O
of O
the O
i O
- O
th O
input O
word O
xiand O
yiis O
the O
slot O
label O
. O

2.2 O
Knowledge O
Integration O
Mechanism O
The O
knowledge O
integration O
mechanism O
aims O
at O
enhancing O
the O
deep O
contextual O
representation O
of O
input O
text O
via O
leveraging O
the O
large O
- O
scale O
lexical O
background O
knowledge O
, O
Wordnet O
( O
Miller O
, O
1995 O
) O
, O
to O
recognize O
unseen O
tokens O
in O
the O
training O
set O
. O

Essentially O
, O
it O
applies O
multi O
- O
level O
graph O
attention O
to O
KB O
embeddings O
with O
the O
BERT B-MethodName
representations O
from O
the O
previous O
layer O
to O
enhance O
the O
contextual O
BERT B-MethodName
embeddings O
with O
human O
- O
curated O
background O
knowledge O
. O

We O
ﬁrst O
introduce O
the O
KB O
embedding O
and O
retrieval O
process O
. O

In O
this O
paper O
, O
we O
use O
the O
lexical O
KB O
, O
WordNet O
, O
stored O
as O
( O
subject O
, O
relation O
, O
object O
) O
triples O
, O
where O
each O
triple O
indicates O
a O
speciﬁc O
relation O
between O
word O
synsets O
, O
e.g. O
, O
( O
state O
, O
hypernym-621of O
, O
california O
) O
. O

Each O
synset O
expresses O
a O
distinct O
concept O
, O
organized O
by O
a O
human O
- O
curated O
tree O
hierarchy O
. O

KB O
Embeddings O
We O
represent O
KB O
concepts O
as O
continuous O
vectors O
in O
this O
paper O
. O

The O
goal O
is O
that O
the O
KB O
tuples O
( O
s;r;o O
) O
can O
be O
measured O
in O
the O
dense O
vector O
space O
based O
on O
the O
embeddings O
. O

We O
adopt O
the O
BILINEAR O
model O
( O
Yang O
et O
al O
. O
, O
2014 O
) O
which O
measures O
the O
relevance O
via O
a O
bilinear O
function O
: O
f(s;r;o O
) O

= O
sTMro O
, O
where O
s;o2Rd2are O
the O
vector O
embeddings O
for O
s;orespectively O
and O
and O
Mris O
a O
relation O
- O
speciﬁc O
embedding O
matrix O
. O

Then O
we O
train O
the O
embeddings O
using O
the O
max O
- O
margin O
ranking O
objective O
: O
X O
q=(s;r;o O
) O
2TX O
q0=(s;r;o0)2T0max O
0;1 Sq+Sq0 O
	  O
( O
2 O
) O
whereTdenotes O
the O
set O
of O
triples O
in O
the O
KB O
and O
T0 O
denotes O
the O
negative O
triples O
that O
are O
not O
observed O
in O
the O
KB O
. O

Finally O
we O
can O
acquire O
vector O
representations O
for O
concepts O
of O
the O
KB O
. O

Because O
we O
mainly O
focus O
on O
the O
slot B-TaskName
tagging I-TaskName
task O
, O
and O
the O
datasets O
are O
relatively O
small O
for O
joint O
learning O
KB O
embeddings O
. O

Furthermore O
, O
the O
KB O
contains O
many O
triplets O
not O
present O
in O
the O
ATIS B-DatasetName
and O
Snips B-DatasetName
dataset O
. O

Therefore O
we O
pre O
- O
train O
the O
KB O
vectors O
and O
keep O
them O
ﬁxed O
while O
training O
the O
whole O
model O
to O
reduce O
the O
complexity O
. O

KB O
Concepts O
Retrieval O
We O
need O
to O
retrieve O
all O
the O
concepts O
or O
synsets O
relevant O
to O
the O
input O
word O
xi O
from O
the O
KB O
. O

Different O
from O
( O
Yang O
and O
Mitchell O
, O
2017 O
; O
Yang O
et O
al O
. O
, O
2019 O
) O
, O
for O
a O
word O
xi O
, O
we O
ﬁrst O
return O
its O
synsets O
as O
the O
ﬁrst O
- O
level O
candidate O
set O
C1(xi)of O
KB O
concepts O
. O

Then O
we O
construct O
the O
second O
- O
level O
candidate O
set O
C2(xi)by O
retrieving O
all O
the O
direct O
hyponyms O
of O
each O
synset O
in O
C1(xi O
) O
, O
as O
shown O
in O
the O
right O
part O
of O
Fig O
2 O
. O
Multi O
- O
Level O
Graph O
Attention O
After O
obtaining O
the O
two O
- O
level O
concept O
candidate O
sets O
, O
we O
apply O
the O
BERT B-MethodName
embedding O
hiof O
input O
token O
xito O
attending O
over O
the O
multi O
- O
level O
memory O
. O

The O
ﬁrst O
- O
level O
attention O
, O
 O
, O
is O
calculated O
by O
a O
bilinear O
operation O
between O
hiand O
each O
synset O
cjin O
the O
ﬁrst O
level O
set O
C1(xi O
): O
 O
ij O
/ O
exp(cT O
jW1hi O
) O
( O
3 O
) O

Then O
we O
add O
an O
additional O
sentinel O
vector O
c(Yang O
and O
Mitchell O
, O
2017 O
) O
and O
accumulate O
all O
the O
embeddings O
as O
follows O
: O
s1 O
i O
= O
X O
j O
 O
ijcj+ O

 O

 O
iis O
similar O
to O
 O
ijandP O
j O
 O
ij+ O

 O
i= O
1 O
. O

Heres1 O
iis O
regarded O
as O
a O
one O
- O
hop O
knowledge O
state O
vector O
for O
it O
only O
represents O
its O
directly O
linked O
synsets O
. O

Therefore O
, O
we O
perform O
the O
second O
- O
level O
graph O
attention O
to O
encode O
the O
hyponyms O
of O
its O
direct O
synsets O
to O
enrich O
the O
information O
of O
original O
synsets O
. O

Intuitively O
the O
second O
- O
level O
attention O
over O
the O
hyponyms O
can O
be O
viewed O
as O
a O
relational O
reasoning O
process O
. O

Because O
once O
a O
synset O
belongs O
to O
an O
entity O
type O
, O
its O
hyponyms O
always O
conform O
to O
the O
same O
type O
. O

Likewise O
, O
the O
second O
- O
level O
attention O
overC2(xi)is O
calculated O
: O
 O
ijk O
/ O
exp(cT O
jkW2hi O
) O
( O
5 O
) O
where O
cjis O
thej O
- O
th O
synset O
linked O
to O
token O
xiand O
cjkthek O
- O
th O
hyponym O
of O
cj O
. O

So O
we O
can O
obtain O
the O
multi O
- O
hop O
knowledge O
state O
vector O
s2 O
i O
: O
s2 O
i O
= O
X O
jX O
k O
 O
ij O
 O
ijkcjk O
( O
6 O
) O
Then O
we O
concat O
multi O
- O
level O
knowledge O
- O
aware O
vectors1 O
i;s2 O
i O
, O
and O
original O
BERT B-MethodName
representation O
hi O
, O
and O
output O
fi= O

[ O
s1 O
i;s2 O
i;hi O
] O
. O

We O
also O
add O
a O
BiLSTM O
matching O
layer O
which O
takes O
as O
input O
the O
knowledge O
- O
enriched O
representationsfi O
. O

Then O
we O
forward O
the O
hidden O
states O
to O
a O
CRF O
layer O
and O
predict O
the O
ﬁnal O
results O
. O

The O
training O
objective O
is O
the O
sum O
of O
log O
- O
likelihood O
of O
all O
the O
words O
. O

3 O
Experiments O
3.1 O
Setup O
Datasets O
To O
evaluate O
our O
approach O
, O
we O
conduct O
experiments O
on O
two O
public O
benchmark O
datasets O
, O
ATIS B-DatasetName
( O
T¨ur O
et O
al O
. O
, O
2010 O
) O
and O
Snips B-DatasetName
( O
Coucke O
et O
al O
. O
, O
2018 O
) O
. O

ATIS B-DatasetName
contains O
4,478 O
utterances O
in O
the O
training O
set O
and O
893 O
utterances O
in O
the O
test O
set O
, O
while O
Snips B-DatasetName
contains O
13,084 O
and O
700 O
utterances O
, O
respectively O
. O

% O
represents O
how O
much O
training O
data O
we O
randomly O
choose O
from O
the O
original O
training O
set O
. O

We O
report O
the O
F1 B-MetricName
scores O
on O
the O
same O
test O
sets O
. O

Samples O
in O
Snips B-DatasetName
are O
from O
different O
topics O
, O
such O
as O
getting O
weather O
and O
booking O
a O
restaurant O
, O
resulting O
in O
a O
larger O
vocabulary O
. O

By O
contrast O
, O
samples O
in O
ATIS B-DatasetName
are O
all O
about O
ﬂight O
information O
with O
similar O
vocabularies O
across O
them O
. O

Therefore O
, O
Snips B-DatasetName
is O
much O
more O
complicated O
, O
mainly O
due O
to O
data O
diversity O
and O
the O
large O
vocabulary O
. O

The O
full O
statistics O
are O
shown O
in O
the O
Table O
1 O
. O

To O
simulate O
the O
few O
- O
shot O
scenarios O
, O
we O
downsample O
the O
original O
training O
sets O
of O
ATIS B-DatasetName
and O
Snips B-DatasetName
to O
different O
extents O
while O
keeping O
valid O
and O
test O
sets O
ﬁxed O
. O

We O
aim O
to O
evaluate O
the O
effectiveness O
of O
integrating O
external O
KB O
under O
the O
settings O
of O
varied O
sizes O
of O
training O
data O
available O
. O

Evaluation O
We O
evaluate O
the O
performance O
of O
slot B-TaskName
tagging I-TaskName
using O
the O
F1 B-MetricName
score O
metric O
. O

In O
the O
experiments O
, O
we O
use O
the O
English O
uncased O
BERT B-MethodName
- O
base O
model O
, O
which O
has O
12 B-HyperparameterValue
layers B-HyperparameterName
, O
768 B-HyperparameterValue
hidden B-HyperparameterName
states I-HyperparameterName
, O
and O
12 B-HyperparameterValue
heads B-HyperparameterName
. O

The O
hidden B-HyperparameterName
size I-HyperparameterName
for O
the O
BiLSTM O
layer O
is O
set O
to O
128 B-HyperparameterValue
. O

Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
is O
used O
for O
optimization O
with O
an O
initial O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-5 B-HyperparameterValue
. O

The O
dropout B-HyperparameterName
probability I-HyperparameterName
is O
0.1 B-HyperparameterValue
, O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
64 B-HyperparameterValue
. O

We O
ﬁnetune O
all O
hyperparameters O
on O
the O
valid O
set O
. O

3.2 O
Baselines O
Attention B-MethodName
- I-MethodName
Based I-MethodName
( O
Liu O
and O
Lane O
, O
2016 O
) O
uses O
an O
RNN O
layer O
and O
a O
self O
- O
attention O
layer O
to O
encode O
the O
input O
text O
. O

Slot B-MethodName
- I-MethodName
Gated I-MethodName
( O
Goo O
et O
al O
. O
, O
2018 O
) O
, O
which O
has O
two O
variants O
, O
Full O
Atten O
andIntent O
Atten O
, O
applies O
the O
information O
of O
intent O
detection O
task O
to O
enhance O
slot B-TaskName
tagging I-TaskName
. O

SF B-MethodName
- I-MethodName
ID I-MethodName
Network O
( O
Haihong O
et O
al O
. O
, O
2019 O
) O

designs O
a O
multiple O
iteration O
mechanism O
to O
construct O
bi O
- O
directional O
interrelated O
connections O
between O
slot B-TaskName
tagging I-TaskName
and O
intent O
detection O
. O

Most O
of O
the O
previous O
methods O
consider O
improving O
the O
performance O
of O
slot B-TaskName
tagging I-TaskName
by O
joint O
learning O
with O
intent O
detection O
. O

However O
, O
the O
effectiveness O
of O
background O
knowledge O
for O
slot B-TaskName
tagging I-TaskName
is O
still O
unexplored O
. O

Con O
- O
sequently O
, O
our O
proposed O
approach O
intends O
to O
integrate O
the O
large O
- O
scale O
lexical O
background O
knowledge O
, O
WordNet O
, O
to O
enhance O
the O
deep O
contextual O
representation O
of O
input O
text O
. O

We O
hope O
to O
further O
improve O
the O
performance O
of O
slot B-TaskName
tagging I-TaskName
, O
especially O
in O
the O
fewshot O
scenario O
where O
there O
is O
no O
plenty O
of O
training O
data O
available.1 O
3.3 O
Overall O
Results O
We O
display O
the O
experiment O
results O
in O
Table O
2 O
, O
where O
we O
choose O
two O
model O
architectures O
RNN B-MethodName
and O
BERT B-MethodName
as O
the O
encoding O
layer O
. O

Table O
2 O
shows O
that O
our O
proposed O
knowledge O
integration O
mechanism O
signiﬁcantly O
outperforms O
the O
baselines O
for O
both O
datasets O
, O
demonstrating O
that O
explicitly O
integrating O
the O
largescale O
background O
knowledge O
and O
contextual O
representation O
can O
beneﬁt O
slot B-TaskName
tagging I-TaskName
effectively O
. O

Moreover O
, O
the O
improvement O
of O
0.72 B-MetricValue
% I-MetricValue
over O
strong O
baseline O
BERT B-MethodName
on O
Snips B-DatasetName
is O
considerably O
higher O
than O
0.27 B-MetricValue
% I-MetricValue
on O
ATIS B-DatasetName
. O

Considering O
the O
distinct O
complexity O
of O
the O
two O
datasets O
, O
the O
probable O
reason O
is O
that O
a O
simpler O
slot B-TaskName
tagging I-TaskName
task O
, O
such O
as O
ATIS B-DatasetName
, O
does O
not O
require O
much O
background O
knowledge O
to O
achieve O
good O
results O
. O

Because O
the O
vocabulary O
of O
ATIS B-DatasetName
is O
extremely O
smaller O
than O
that O
of O
Snips B-DatasetName
, O
therefore O
the O
context O
- O
aware O
models O
are O
capable O
of O
providing O
enough O
cues O
for O
recognizing O
rare O
or O
OOV O
words O
. O

Hence O
, O
our O
method O
makes O
a O
notable O
difference O
in O
a O
scenario O
where O
samples O
are O
linguistically O
diverse O
, O
and O
large O
vocab O
exists O
. O

The O
results O
also O
demonstrate O
that O
incorporating O
external O
knowledge O
will O
not O
bring O
in O
much O
noise O
since O
we O
use O
a O
knowledge O
sentinel O
for O
the O
better O
tradeoff O
between O
the O
impact O
of O
background O
knowledge O
and O
information O
from O
the O
context O
. O

On O
the O
other O
hand O
, O
the O
main O
results O
of O
the O
1We O
do O
not O
choose O
( O
Williams O
, O
2019 O
) O
as O
a O
baseline O
since O
it O
only O
performs O
experiments O
on O
private O
industrial O
datasets O
and O
does O
not O
open O
source O
. O

We O
can O
hardly O
ﬁgure O
out O
the O
details O
of O
manually O
collecting O
lexicons O
from O
the O
dataset.623 O
0.01 O
0.02 O
0.05 O
0.10 O
0.50 O
1.00 O
Train O
set O
size0.00.51.01.52.02.53.0Relative O
F1 B-MetricName
improvement O
( O
% O
) O

1.41 O
1.06 O
0.82 O
0.54 O
0.330.282.89 O
2.32 O
1.74 O
1.37 O
1.24 O
0.76ATIS O
SNIPSFigure O
3 O
: O
Relative O
F1 B-MetricName
improvement O
over O
BERT B-MethodName
baseline O
under O
the O
different O
sizes O
of O
training O
data O
. O

RNN B-MethodName
- I-MethodName
based I-MethodName
models O
are O
95.17 B-MetricValue
( O
+0.46 B-MetricValue
) O
on O
ATIS B-DatasetName
and O
89.30 B-MetricValue
( O
+1.51 B-MetricValue
) O
on O
Snips B-DatasetName
, O
where O
the O
scores O
in O
the O
brackets O
are O
the O
absolute O
improvements O
arisen O
by O
KB O
. O

Compared O
to O
the O
BERT B-MethodName
- O
based O
models O
, O
95.98 B-MetricValue
( O
+0.27 B-MetricValue
) O
on O
ATIS B-DatasetName
and O
95.17 B-MetricValue
( O
+0.72 B-MetricValue
) O
on O
Snips B-DatasetName
, O
the O
RNN B-MethodName
- I-MethodName
based I-MethodName
model O
achieves O
more O
signiﬁcant O
improvements O
in O
BERT B-MethodName
- O
based O
models O
. O

We O
believe O
BERT B-MethodName
can O
effectively O
transfer O
prior O
linguistic O
context O
constraints O
, O
so O
that O
background O
knowledge O
beneﬁts O
RNN B-MethodName
- I-MethodName
based I-MethodName
models O
more O
. O

BERT B-MethodName
does O
improve O
the O
model O
’s O
ability O
to O
solve O
the O
OOV O
problem O
since O
it O
has O
learned O
linguistic O
knowledge O
from O
the O
large O
corpus O
. O

However O
, O
our O
method O
focuses O
more O
on O
the O
effect O
of O
using O
human O
- O
curated O
structured O
background O
knowledge O
and O
further O
enhances O
BERT B-MethodName
in O
a O
distinct O
way O
. O

4 O
Qualitative O
Analysis O
4.1 O
Effect O
of O
Training O
Data O
Size O
Fig O
3 O
shows O
the O
relative O
improvement O
percentages O
on O
ATIS B-DatasetName
and O
Snips B-DatasetName
using O
different O
sizes O
of O
training O
data O
. O

Results O
substantiate O
knowledge O
integration O
better O
facilitates O
few O
- O
shot O
slot B-TaskName
tagging I-TaskName
. O

This O
is O
because O
traditional O
context O
- O
aware O
models O
can O
not O
learn O
enough O
contextual O
semantics O
well O
while O
only O
given O
several O
samples O
. O

Explicit O
lexical O
relations O
become O
essentially O
necessary O
when O
there O
is O
not O
adequate O
training O
data O
, O
especially O
for O
rare O
words O
or O
OOV O
words O
. O

Background O
KB O
enables O
the O
model O
to O
reason O
explicit O
lexical O
relations O
and O
helps O
recognize O
rare O
and O
unseen O
words O
. O

Meanwhile O
, O
incorporating O
background O
knowledge O
can O
also O
enhance O
the O
original O
representation O
of O
BERT B-MethodName
, O
which O
can O
provide O
direct O
lexical O
relations O
. O

4.2 O
Ablation O
Study O
To O
study O
the O
effect O
of O
each O
component O
of O
our O
method O
, O
we O
conduct O
ablation O
analysis O
under O
the O
10 O
% O
training O
data O
setting O
( O
Table O
3 O
) O
. O

We O
can O
see O
that O
knowledge O
integration O
is O
crucial O
to O
the O
improvements O
. O

Besides O
, O
the O
ﬁrst O
- O
level O
graph O
attention O
acquires O
better O
performance O
gain O
than O
the O
secondlevel O
attention O
. O

We O
assume O
that O
directly O
linked O
synsets O
are O
more O
signiﬁcant O
than O
the O
hyponyms O
. O

The O
matching O
layer O
and O
CRF O
also O
play O
a O
role O
. O

The O
reason O
why O
the O
RNN O
matching O
layer O
matters O
is O
partly O
to O
build O
explicit O
interactions O
between O
knowledge O
vectors O
and O
context O
vectors O
. O

5 O
Conclusion O
We O
present O
a O
novel O
knowledge O
integration O
mechanism O
of O
incorporating O
background O
KB O
and O
deep O
contextual O
representations O
to O
facilitate O
the O
few O
- O
shot O
slot B-TaskName
tagging I-TaskName
task O
. O

Experiments O
conﬁrm O
the O
effectiveness O
of O
modeling O
explicit O
lexical O
relations O
, O
which O
has O
not O
yet O
been O
explored O
by O
previous O
works O
. O

Moreover O
, O
we O
ﬁnd O
that O
our O
method O
delivers O
more O
beneﬁts O
to O
data O
scarcity O
scenarios O
. O

We O
hope O
to O
provide O
new O
guidance O
for O
the O
future O
slot B-TaskName
tagging I-TaskName
work O
. O

Acknowledgments O

The O
authors O
would O
like O
to O
thank O
the O
reviewers O
for O
their O
valuable O
comments O
. O

This O
work O
was O
partially O
supported O
by O
National O
Key O
R&D O
Program O
of O
China O
No O
. O
2019YFF0303300 O
and O
Subject O
II O

No O
. O
2019YFF0303302 O
, O
DOCOMO O
Beijing O
Communications O
Laboratories O
Co. O
, O
Ltd O
, O
MoE O
- O
CMCC O
” O
Artifical O
Intelligence O
” O
Project O
No O
. O
MCM20190701 O
. O
