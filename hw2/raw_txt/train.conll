Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
133–138 O
July O
5 O
- O
10 O
, O
2020 O
. O

c O

 O
2020 O
Association O
for O
Computational O
Linguistics133A O
Complete O
Shift O
- O
Reduce O
Chinese O
Discourse O
Parser O
with O
Robust O
Dynamic O
Oracle O
Shyh O
- O
Shiun O
Hung,1Hen O
- O
Hsen O
Huang,2,3and O
Hsin O
- O
Hsi O
Chen1,3 O
1Department O
of O
Computer O
Science O
and O
Information O
Engineering O
, O
National O
Taiwan O
University O
, O
Taiwan O
2Department O
of O
Computer O
Science O
, O
National O
Chengchi O
University O
, O
Taiwan O
3MOST O
Joint O
Research O
Center O
for O
AI O
Technology O
and O
All O
Vista O
Healthcare O
, O
Taiwan O
shhung@nlg.csie.ntu.edu.tw O
, O
hhhuang@nccu.edu.tw O
, O
hhchen@ntu.edu.tw O
Abstract O
This O
work O
proposes O
a O
standalone O
, O
complete O
Chinese O
discourse O
parser O
for O
practical O
applications O
. O

We O
approach O
Chinese O
discourse B-TaskName
parsing I-TaskName
from O
a O
variety O
of O
aspects O
and O
improve O
the O
shift O
- O
reduce O
parser O
not O
only O
by O
integrating O
the O
pre O
- O
trained O
text O
encoder O
, O
but O
also O
by O
employing O
novel O
training O
strategies O
. O

We O
revise O
the O
dynamic O
- O
oracle O
procedure O
for O
training O
the O
shift O
- O
reduce O
parser O
, O
and O
apply O
unsupervised O
data O
augmentation O
to O
enhance O
rhetorical B-TaskName
relation I-TaskName
recognition I-TaskName
. O

Experimental O
results O
show O
that O
our O
Chinese O
discourse O
parser O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
. O

1 O
Introduction O
Discourse B-TaskName
parsing I-TaskName
is O
one O
of O
the O
fundamental O
tasks O
in O
natural O
language O
processing O
( O
NLP O
) O
. O

Typical O
types O
of O
discourse B-TaskName
parsing I-TaskName
include O
hierarchical B-TaskName
discourse I-TaskName
parsing I-TaskName
and O
shallow O
discourse O
parsing O
. O

The O
former O
is O
aimed O
at O
ﬁnding O
the O
relationships O
among O
a O
series O
of O
neighboring O
elementary O
discourse O
units O
( O
EDUs O
) O
and O
further O
building O
up O
a O
hierarchical O
tree O
structure O
( O
Mann O
and O
Thompson O
, O
1988 O
) O
. O

Instead O
of O
establishing O
a O
tree O
structure O
, O
the O
latter O
ﬁnds O
the O
across O
- O
paragraph O
relations O
between O
all O
text O
units O
in O
a O
paragraph O
or O
a O
document O
. O

Based O
on O
Rhetorical O
Structure O
Theory O
Discourse O
Treebank O
( O
RST O
- O
DT O
) O
( O
Carlson O
et O
al O
. O
, O
2001a O
) O
, O
hierarchical B-TaskName
discourse I-TaskName
parsing I-TaskName
in O
English O
has O
been O
well O
- O
studied O
. O

This O
paper O
focuses O
on O
hierarchical B-TaskName
discourse I-TaskName
parsing I-TaskName
in O
Chinese O
. O

Previous O
work O
on O
hierarchical O
Chinese O
discourse B-TaskName
parsing I-TaskName
is O
mostly O
based O
on O
the O
RST O
- O
style O
Chinese O
Discourse O
Treebank O
( O
Li O
et O
al O
. O
, O
2014 O
) O
. O

To O
distinguish O
from O
the O
other O
Chinese O
Discourse O
Treebank O
( O
Zhou O
and O
Xue O
, O
2012 O
) O
, O
which O
is O
annotated O
with O
the O
PDTB O
- O
style O
for O
shallow B-TaskName
discourse I-TaskName
parsing I-TaskName
, O
we O
use O
the O
term O
CDTB-14 B-DatasetName
to O
refer O
to O
the O
RST O
- O
style O
one O
and O
the O
term O
CDTB-12 B-DatasetName
to O
refer O
to O
the O
PDTB O
- O
style O
one O
. O

Kong O
and O
Zhou O
( O
2017)propose O
a O
pipeline O
framework O
and O
generate O
the O
discourse O
parsing O
tree O
in O
a O
bottom O
- O
up O
way O
. O

Lin O
et O

al O
. O
( O
2018 O
) O
propose O
an O
end O
- O
to O
- O
end O
system O
based O
on O
a O
recursive O
neural O
network O
( O
RvNN O
) O
to O
construct O
the O
parsing O
tree O
with O
a O
CKY O
- O
like O
algorithm O
. O

Sun O
and O
Kong O
( O
2018 O
) O
use O
transition O
- O
based O
method O
with O
the O
stack O
augmented O
parser O
- O
interpreter O
neural O
network O
( O
SPINN O
) O
( O
Bowman O
et O
al O
. O
, O
2016 O
) O
as O
the O
backbone O
model O
, O
helping O
their O
model O
make O
a O
better O
prediction O
with O
the O
previous O
information O
. O

In O
this O
work O
, O
we O
attempt O
to O
construct O
a O
complete O
Chinese O
discourse O
parser O
, O
which O
supports O
all O
the O
four O
sub O
- O
tasks O
in O
hierarchical B-TaskName
discourse I-TaskName
parsing I-TaskName
, O
including O
EDU O
segmentation O
, O
tree O
structure O
construction O
, O
nuclearity O
labeling O
, O
and O
rhetorical B-TaskName
relation I-TaskName
recognition I-TaskName
. O

Given O
a O
paragraph O
, O
our O
parser O
extracts O
all O
EDUs O
, O
builds O
the O
tree O
structure O
, O
identiﬁes O
the O
nucleuses O
, O
and O
recognizes O
the O
rhetorical O
relations O
of O
all O
internal O
nodes O
. O

We O
propose O
a O
revised O
dynamic O
- O
oracle O
procedure O
( O
Yu O
et O
al O
. O
, O
2018 O
) O
for O
training O
the O
shift O
- O
reduce O
parser O
. O

Because O
of O
the O
limited O
training O
instances O
in O
CDTB-14 B-DatasetName
, O
we O
also O
address O
the O
data O
sparsity O
issue O
by O
introducing O
unsupervised O
data O
augmentation O
( O
Xie O
et O
al O
. O
, O
2019 O
) O
. O

Experimental O
results O
show O
that O
our O
methodology O
is O
effective O
, O
and O
our O
model O
outperforms O
all O
the O
previous O
models O
. O

The O
contributions O
of O
this O
work O
are O
three O
- O
fold O
shown O
as O
follows O
. O

1.We O
explore O
the O
task O
of O
Chinese O
discourse B-TaskName
parsing I-TaskName
with O
a O
variety O
of O
strategies O
, O
and O
our O
parser O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
performance O
. O

Our O
robust O
dynamic O
- O
oracle O
procedure O
can O
be O
applied O
to O
other O
shift O
- O
reduce O
parsers O
. O
2.Our O

complete O
Chinese O
discourse O
parser O
handles O
a O
raw O
paragraph O
/ O
document O
directly O
and O
performs O
all O
the O
subtasks O
in O
hierarchical B-TaskName
discourse I-TaskName
parsing I-TaskName
. O

No O
pre O
- O
processing O
procedures O
such O
as O
Chinese O
word O
segmentation O
, O
POStagging O
, O
and O
syntactic O
parsing O
are O
required.1343.We O
release O
the O
pre O
- O
trained O
, O
standalone O
, O
readyto O
- O
use O
parser O
as O
a O
resource O
for O
the O
research O
community.1 O
2 O
Methodology O
Figure O
1 O
gives O
an O
overview O
of O
our O
parser O
. O

Five O
stages O
are O
performed O
to O
transform O
a O
raw O
document O
into O
a O
parse O
tree O
: O
EDU O
segmentation O
, O
tree O
structure O
construction O
, O
rhetorical O
relation O
and O
nuclearity O
classiﬁcation O
, O
binary O
tree O
conversion O
, O
and O
beam O
search O
. O

2.1 O
Elementary O
Discourse O
Unit O
Segmentation O
Typically O
, O
EDU O
segmentation O
is O
a O
sequence O
labeling O
task O
( O
Wang O
et O
al O
. O
, O
2018 O
; O
Peters O
et O
al O
. O
, O
2018 O
) O
. O

We O
propose O
a O
model O
for O
labeling O
each O
Chinese O
character O
in O
a O
raw O
document O
. O

The O
Begin O
- O
Inside O
scheme O
is O
employed O
that O
the O
word O
beginning O
with O
a O
new O
EDU O
will O
be O
labeled O
as O
B O
, O
and O
the O
rest O
of O
the O
words O
will O
be O
labeled O
as O
I. O
Our O
model O
is O
based O
on O
the O
pretrained O
text O
encoder O
BERT O
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
. O

More O
speciﬁcally O
, O
we O
adopt O
the O
version O
BERT O
- O
base O
, O
Chinese O
since O
this O
is O
the O
only O
pre O
- O
trained O
BERT O
dedicated O
to O
Chinese O
so O
far O
. O

As O
the O
BERT O
for O
Chinese O
is O
character O
- O
based O
, O
we O
feed O
each O
Chinese O
character O
into O
a O
BERT O
layer O
to O
obtain O
its O
contextual O
embedding O
. O

Then O
, O
we O
ﬁne O
tune O
the O
representation O
with O
an O
additional O
dense O
layer O
and O
measure O
the O
probability O
of O
each O
label O
of O
each O
character O
with O
a O
softmax O
layer O
. O

The O
model O
is O
further O
trained O
as O
conditional O
random O
ﬁelds O
( O
CRFs O
) O
( O
Lafferty O
et O
al O
. O
, O
2001 O
) O
for O
ﬁnding O
the O
global O
optimal O
label O
sequence O
. O

2.2 O
Tree O
Construction O
We O
propose O
a O
shift O
- O
reduce O
parser O
for O
building O
the O
structure O
of O
the O
discourse O
parse O
tree O
. O

A O
shift O
- O
reduce O
parser O
maintains O
a O
stack O
and O
a O
queue O
for O
representing O
a O
state O
during O
parsing O
, O
and O
an O
action O
classiﬁer O
is O
trained O
to O
predict O
the O
action O
( O
i.e. O
, O
shift O
or O
reduce O
) O
for O
making O
a O
transition O
from O
the O
given O
state O
to O
the O
next O
state O
. O

In O
the O
initial O
state O
, O
the O
stack O
is O
empty O
, O
and O
the O
queue O
contains O
all O
the O
EDUs O
in O
a O
raw O
document O
. O

In O
the O
ﬁnal O
state O
, O
the O
queue O
is O
empty O
, O
and O
the O
stack O
contains O
only O
one O
element O
, O
i.e. O
, O
the O
discourse O
parse O
tree O
of O
the O
whole O
paragraph O
. O

To O
decide O
whether O
to O
shift O
or O
to O
reduce O
, O
we O
propose O
an O
action O
classiﬁer O
by O
considering O
the O
information O
of O
the O
top O
two O
elements O
of O
the O
stack O
s1 O
ands2(i.e O
. O
, O
the O
two O
most O
recent O
discourse O
units O
) O
and O
the O
ﬁrst O
element O
of O
the O
queue O
q(i.e O
. O
, O
the O
next O
1https://github.com/jeffrey9977/ O
Chinese O
- O
Discourse O
- O
Parser O
- O
ACL2020 O
Raw O
documentClassifierSense O
, O
Center O
Reduce O
EDUs O
SegmenterBI O
III O
IBII O

I O
IIIII O
IIIIIBI O
IIIIIIIIIIIIIIIIIBIIIIIIII O
IIB O
I O
IIIIIIIIIIIIII O
  O

Converter O
stack O
queueShiftFigure O
1 O
: O
Overview O
of O
our O
Chinese O
discourse O
parser O
. O
EDU O
) O
. O

The O
textual O
form O
of O
each O
of O
these O
three O
discourse O
units O
will O
be O
fed O
into O
the O
BERT O
encoder O
for O
representing O
as O
Enc(s1),Enc(s2 O
) O
, O
andEnc(q O
) O
. O

Next O
, O
we O
concatenate O
the O
max O
pooling O
of O
Enc(s1 O
) O
, O
Enc(s2 O
) O
, O
andEnc(q)and O
feed O
the O
resulting O
vector O
into O
a O
dense O
layer O
to O
predict O
the O
next O
action O
. O

Since O
shift O
- O
reduce O
is O
a O
greedy O
algorithm O
, O
it O
can O
hardly O
recover O
from O
an O
error O
state O
. O

The O
shift O
- O
reduce O
parser O
is O
typically O
trained O
with O
the O
teacher O
mode O
, O
where O
only O
correct O
states O
are O
given O
, O
and O
the O
resulting O
parser O
may O
perform O
poor O
when O
it O
reaches O
unfamiliar O
states O
. O

For O
this O
reason O
, O
we O
propose O
a O
revised O
dynamic O
- O
oracle O
procedure O
( O
Yu O
et O
al O
. O
, O
2018 O
) O
for O
training O
our O
discourse O
parser O
. O

One O
drawback O
of O
the O
original O
dynamic O
oracle O
is O
that O
some O
golden O
training O
examples O
may O
be O
neglected O
. O

Because O
CDTB-14 B-DatasetName
has O
relatively O
few O
action O
steps O
to O
build O
a O
tree O
, O
the O
probability O
of O
falling O
into O
a O
wrong O
state O
is O
much O
small O
compared O
to O
that O
of O
RST O
- O
DT O
. O

In O
our O
revision O
, O
we O
want O
to O
guarantee O
all O
correct O
states O
have O
been O
trained O
. O

As O
shown O
in O
Algorithm O
1 O
, O
the O
document O
will O
be O
gone O
through O
twice O
when O
training O
a O
document O
example O
. O

We O
ﬁrst O
follow O
the O
golden O
actions O
, O
and O
choose O
action O
predicted O
by O
the O
model O
with O
a O
probability O
 O
at O
the O
second O
time O
. O

We O
refer O
to O
them O
as O
teacher O
mode O
and O
student O
mode O
, O
respectively O
. O

Note O
that O
we O
follow O
the O
suggestion O
of O
Yu O
et O

al O
. O
( O
2018 O
) O
to O
set O
 O
to O
0.7.135Algorithm O
1 O
Training O
Procedure O
for O
Our O
Shift O
- O
Reduce O
Discourse O
Parser O
. O

1 O
: O
S;Q O
empty O
stack O
, O
elementary O
discourse O
units O
2 O
: O
whileQis O
not O
empty_Shas O
more O
than O
1unitdo O
.Teacher O
mode O
3 O
: O
predicted;golden O
  O
ACTION O
CLASSIFIER O
( O
S O
: O
top O
1();S O
: O
top O
2();Q O
: O
front O
( O
) O
) O
, O
GOLDEN O
ACTION O
4 O
: O
COMPUTE O
LOSSANDUPDATE O
( O
predicted;golden O
) O
5 O
: O
PERFORM O
ACTION O
( O
golden O
) O
6 O
: O
S;Q O
empty O
stack O
, O
elementary O
discourse O
units O
7 O
: O
whileQis O
not O
empty_Shas O
more O
than O
1unitdo O
.Student O
mode O
8 O
: O

predicted;golden O
  O
ACTION O
CLASSIFIER O
( O
S O
: O
top O
1();S O
: O
top O
2();Q O
: O
front O
( O
) O
) O
, O
GOLDEN O
ACTION O
9 O
: O
COMPUTE O
LOSSANDUPDATE O
( O
predicted;golden O
) O
10 O
: O
ifrand O
( O
) O
> O
 O
then O
PERFORM O
ACTION O
( O
golden O
) O
elsePERFORM O
ACTION O
( O
predicted O
) O
2.3 O
Rhetorical B-TaskName
Relation I-TaskName
Recognition I-TaskName

If O
two O
discourse O
units O
are O
decided O
to O
be O
merged O
during O
the O
tree O
construction O
stage O
, O
a O
new O
internal O
node O
will O
be O
generated O
and O
the O
relationship O
of O
the O
two O
discourse O
units O
will O
be O
determined O
. O

Predicting O
the O
relation O
between O
two O
textual O
arguments O
is O
a O
typical O
classiﬁcation O
task O
in O
NLP O
. O

We O
propose O
a O
BERT O
- O
based O
classiﬁer O
, O
which O
predicts O
the O
relation O
of O
two O
arguments O
separated O
by O
the O
symbol O

[ O
SEP O
] O
, O
with O
additional O
dense O
layers O
as O
the O
output O
. O

In O
CDTB-14 B-DatasetName
, O
the O
“ O
coordination O
” O
relation O
accounts O
for O
59.6 O
% O
of O
the O
training O
data O
, O
while O
minor O
relations O
suffer O
from O
data O
sparseness O
. O

To O
address O
this O
issue O
, O
we O
introduce O
unsupervised O
data O
augmentation O
( O
UDA O
) O
( O
Xie O
et O
al O
. O
, O
2019 O
) O
to O
enhance O
the O
performance O
. O

We O
adopt O
the O
discourse O
pairs O
in O
CDTB-12 B-DatasetName
as O
the O
material O
for O
UDA O
. O

Note O
that O
other O
unlabeled O
text O
pairs O
can O
also O
be O
used O
for O
UDA O
. O

We O
chose O
those O
from O
CDTB-12 B-DatasetName
simply O
because O
the O
format O
is O
convenient O
for O
us O
to O
use O
. O

The O
original O
loss O
is O
shown O
as O
Eq O
. O

1 O
. O

Given O
a O
span O
of O
text O
x O
, O
our O
main O
model O
P()predicts O
the O
rhetorical O
relation O
yc O
. O

Eq O
. O
2 O
shows O
the O
additional O
consistency O
loss O
to O
enforce O
the O
smoothness O
of O
our O
main O
model O
, O
and O
^xstands O
for O
the O
augmented O
unlabeled O
sentence O
pair O
. O

LandUstand O
for O
labeled O
data O
and O
unlabeled O
data O
, O
respectively O
. O

As O
shown O
in O
Eq O
. O

3 O
, O
we O
train O
both O
objectives O
at O
the O
same O
time O
with O
a O
weight O
to O
adjust O
the O
effect O
of O
UDA O
. O
H= 1 O
NNX O
x2LMX O
c=1yclog O
( O
P(ycjx O
) O
) O
( O
1 O
) O
DKL= 1 O
NNX O
x2UP(yjx O
) O
logP(yjx O
) O

P(yj^x) O
( O
2 O
) O
L( O
) O

= O
H+DKL O
( O
3 O
) O

The O
UDA O
procedure O
ﬁrst O
generates O
the O
augmented O
unlabeled O
sentence O
pairs O
. O

Various O
ap O
- O
proaches O
to O
paraphrasing O
can O
be O
employed O
. O

In O
this O
work O
, O
we O
utilize O
the O
back O
- O
translation O
strategy O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
, O
where O
we O
translate O
the O
Chinese O
sentence O
pair O
to O
English O
and O
then O
translate O
back O
to O
Chinese O
. O

This O
is O
equivalent O
to O
add O
noises O
to O
the O
original O
inputs O
. O

As O
the O
original O
and O
the O
backtranslated O
sentence O
pairs O
express O
the O
same O
meaning O
, O
our O
model O
is O
expected O
to O
predict O
the O
same O
label O
for O
both O
pairs O
. O

By O
minimizing O
the O
consistency O
loss O
, O
our O
model O
can O
behave O
consistently O
no O
matter O
whether O
an O
original O
instance O
or O
its O
paraphrases O
are O
given O
. O

In O
this O
way O
, O
the O
model O
can O
be O
more O
generalized O
and O
robust O
. O

Besides O
, O
when O
our O
model O
is O
able O
to O
predict O
the O
same O
label O
for O
both O
sentence O
pairs O
, O
it O
means O
that O
our O
model O
has O
also O
learned O
their O
label O
. O

2.4 O
Nuclearity O
Labeling O
Nuclearity O
labeling O
is O
aimed O
at O
determining O
the O
nucleus O
from O
a O
sentence O
pair O
. O

The O
nuclearity O
of O
two O
sentences O
has O
a O
correlation O
with O
their O
relationship O
, O
thus O
we O
jointly O
train O
the O
rhetorical O
relation O
and O
the O
nuclearity O
classiﬁers O
, O
where O
the O
loss O
for O
back O
- O
propagation O
is O
the O
sum O
of O
the O
losses O
of O
both O
classiﬁers O
. O

Similar O
to O
the O
imbalance O
issue O
of O
rhetorical B-TaskName
relation I-TaskName
recognition I-TaskName
, O
the O
’ O
Equal O
’ O
class O
accounts O
for O
51 O
% O
of O
training O
data O
. O

We O
also O
employ O
UDA O
for O
performance O
enhancement O
. O

2.5 O
Binary O
Tree O
Conversion O
For O
simplicity O
, O
our O
shift O
- O
reduce O
parser O
constructs O
a O
binary O
tree O
. O

However O
, O
the O
parse O
trees O
annotated O
in O
CDTB-14 B-DatasetName
are O
not O
always O
binary O
. O

In O
the O
training O
and O
the O
test O
sets O
, O
8.9 O
% O
and O
10 O
% O
of O
the O
internal O
nodes O
have O
more O
than O
two O
children O
, O
respectively O
. O

Most O
of O
the O
previous O
works O
do O
not O
handle O
the O
binary O
tree O
conversion O
, O
and O
some O
of O
the O
work O
further O
convert O
the O
golden O
trees O
into O
binary O
trees O
to O
calculate O
their O
scores O
, O
resulting O
in O
less O
accurate O
evaluation O
. O

In O
the136training O
stage O
, O
we O
convert O
the O
multiway O
trees O
to O
their O
corresponding O
left O
- O
heavy O
binary O
trees O
( O
Morey O
et O
al O
. O
, O
2018 O
) O
. O

In O
the O
testing O
stage O
, O
we O
convert O
the O
binary O
tree O
constructed O
by O
our O
parser O
to O
the O
corresponding O
multiway O
tree O
. O

For O
example O
, O
a O
three O
- O
way O
node O
, O
A!XYZ O
, O
will O
be O
converted O
to O
A!A0Z O
andA0!XY O
. O

The O
conversion O
is O
deterministic O
and O
bidirectional O
, O
so O
it O
is O
free O
from O
ambiguity O
. O

2.6 O
Beam O
Search O
To O
decode O
a O
transition O
sequence O
during O
the O
testing O
stage O
, O
the O
standard O
method O
is O
to O
choose O
the O
action O
that O
has O
the O
maximum O
probability O
of O
the O
current O
time O
step O
as O
the O
input O
for O
the O
next O
time O
step O
. O

However O
, O
this O
greedy O
approach O
might O
fail O
to O
ﬁnd O
the O
sequence O
that O
has O
the O
maximum O
overall O
probability O
only O
because O
one O
of O
the O
action O
probability O
is O
small O
in O
that O
sequence O
. O

Beam O
search O
( O
Wiseman O
and O
Rush O
, O
2016 O
) O
is O
a O
heuristic O
search O
algorithm O
that O
explores O
a O
graph O
by O
maintaining O
the O
top O
kresults O
at O
every O
time O
step O
. O

This O
approach O
helps O
keep O
a O
number O
of O
potential O
candidates O
from O
discarding O
. O

Note O
that O
the O
greedy O
approach O
is O
equivalent O
to O
beam O
search O
with O
a O
beam O
width O
k= O
1 O
. O
When O
performing O
the O
shift O
- O
reduce O
parsing O
, O
two O
kinds O
of O
states O
have O
only O
one O
action O
to O
choose O
: O
( O
1 O
) O
less O
than O
two O
elements O
in O
the O
stack O
, O
and O
( O
2 O
) O
no O
element O
in O
the O
queue O
. O

Under O
the O
above O
two O
conditions O
, O
the O
probability O
of O
the O
selected O
action O
will O
be O
1 O
, O
making O
our O
model O
to O
be O
overly O
biased O
on O
those O
sequences O
having O
many O
non O
- O
optional O
stages O
. O

For O
this O
reason O
, O
we O
apply O
an O
alternative O
way O
to O
compute O
the O
sequence O
probability O
during O
beam O
search O
. O

Our O
modiﬁed O
beam O
search O
is O
still O
fulﬁlled O
by O
maintaining O
the O
topksequences O
, O
but O
the O
score O
of O
a O
sequence O
is O
calculated O
by O
the O
average O
probabilities O
of O
the O
selected O
actions O
that O
have O
more O
than O
one O
choice O
. O

3 O
Experiments O
3.1 O
Experimental O
Settings O
Following O
the O
setting O
of O
Kong O
and O
Zhou O
( O
2017 O
) O
, O
we O
divide O
CDTB-14 B-DatasetName
into O
the O
training O
set O
, O
including O
450 O
articles O
( O
2,125 O
paragraphs O
) O
, O
and O
test O
set O
, O
including O
50 O
articles O
( O
217 O
paragraphs O
) O
. O

We O
keep O
10 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
training O
data O
for O
validation O
. O

PARSEV O
AL O
( O
Carlson O
et O
al O
. O
, O
2001b O
) O
is O
used O
for O
evaluation O
. O

3.2 O
Experimental O
Results O
Table O
1 O
shows O
the O
performances O
of O
our O
parser O
in O
micro B-MetricName
- I-MetricName
averaged I-MetricName
F I-MetricName
- I-MetricName
score I-MetricName
, O
compared O
with O
previous O
work O
Zhou O
( O
Kong O
and O
Zhou O
, O
2017 O
) O
and O
Lin(LinModel O
EDU O

+ O
T O

+ O
R O
+ O
N O
All O
Zhou O
Given52.3 O
33.8 O
23.9 O
23.2 O
Lin O
64.6 O
42.7 O
38.5 O
35.0 O
BERT B-MethodName
- I-MethodName
CKY I-MethodName
76.5 O
50.8 O
48.5 O
43.1 O
Ours O
82.8 O
57.6 O
56.0 O
50.5 O
Zhou O
93.8 O
46.4 O
28.8 O
23.1 O
22.0 O
Lin O
87.2 O
49.5 O
32.6 O
28.8 O
26.8 O
BERT O
- O
CKY O
92.4 O
68.9 O
43.3 O
42.0 O
37.0 O

al O
. O
, O
2018 O
) O
. O

We O
also O
implement O
BERT B-MethodName
- I-MethodName
CKY I-MethodName
, O
a O
CKY O
parser O
by O
using O
BERT O
for O
representation O
, O
as O
an O
additional O
baseline O
model O
. O

The O
evaluation O
is O
based O
on O
multiway O
trees O
. O

Both O
the O
performances O
with O
and O
without O
golden O
EDUs O
are O
measured O
. O

The O
results O
show O
that O
BERT O
is O
highly O
competitive O
and O
has O
the O
ability O
to O
catch O
the O
potential O
relations O
between O
discourse O
units O
since O
LinandBERT O
- O
CKY O
basically O
use O
the O
same O
approach O
while O
the O
latter O
model O
uses O
BERT O
as O
the O
text O
encoder O
. O

Our O
parser O
outperforms O
all O
the O
baseline O
models O
and O
achieves O
a O
signiﬁcant O
improvement O
without O
the O
golden O
EDUs O
given O
. O

Note O
that O
BERT B-MethodName
- I-MethodName
CKY I-MethodName
is O
based O
on O
Lin O
et O
al O
. O

( O
2018 O
) O
, O
which O
has O
its O
own O
EDU O
segmentation O
module O
different O
from O
ours O
, O
hence O
the O
EDU O
score O
is O
different O
. O

We O
examine O
the O
performance O
of O
three O
different O
training O
techniques O
for O
shift O
- O
reduce O
parsing O
. O

As O
mentioned O
in O
Section O
2.2 O
, O
Normal O
stands O
for O
action O
classiﬁer O
trained O
with O
gold O
standard O
actions O
, O
Dynamic O
stands O
for O
Dynamic O
Oracle O
introduced O
by O
Yu O
et O
al O
. O

( O
2018 O
) O
, O
and O
Ours O
stands O
for O
our O
revised O
dynamic O
- O
oracle O
procedure O
where O
the O
model O
is O
trained O
with O
both O
gold O
standard O
actions O
and O
dynamic O
oracle O
actions O
. O

Compared O
to O
Normal O
, O
experimental O
results O
show O
no O
improvement O
made O
by O
the O
original O
dynamic O
oracle O
, O
while O
our O
revised O
dynamic O
oracle O
outperforms O
the O
other O
two O
strategies O
. O

Our O
strategy O
does O
not O
ignore O
the O
golden O
action O
in O
every O
correct O
state O
and O
also O
has O
the O
chance O
to O
explore O
error O
states O
. O

nary O
trees O
in O
macro B-MetricName
- I-MetricName
averaged I-MetricName
F I-MetricName
- I-MetricName
score I-MetricName
. O

The O
results O
are O
shown O
in O
Table O
2 O
. O

Sun O
and O
Kong O
( O
2018 O
) O
do O
not O
address O
all O
subtasks O
in O
Chinese B-TaskName
discourse I-TaskName
parsing I-TaskName
, O
and O
our O
model O
outperforms O
SUN O
in O
every O
subtask O
. O

Occurrences O
of O
these O
relations O
are O
59.6 O
% O
, O
17.1 O
% O
, O
1.6 O
% O
, O
and O
21.7 O
% O
, O
respectively O
. O

3.3 O
Discussions O
To O
examine O
the O
effectiveness O
of O
UDA O
, O
Table O
3 O
shows O
the O
performances B-MetricName
of I-MetricName
rhetorical I-MetricName
relation I-MetricName
recognition I-MetricName
with O
and O
without O
UDA O
. O

Experimental O
results O
show O
that O
application O
of O
UDA O
successfully O
enhances O
the O
recall O
scores O
of O
the O
three O
minor O
classes O
with O
a O
little O
trade O
- O
off O
in O
the O
recall O
score O
of O
the O
dominant O
class O
, O
Coordination O
. O

In O
addition O
, O
the O
F B-MetricName
- I-MetricName
scores I-MetricName
of O
all O
the O
four O
relations O
are O
increased O
. O

In O
other O
words O
, O
applying O
UDA O
deals O
with O
the O
data O
imbalance O
issue O
and O
improves O
the O
overall O
performance O
. O

Applying O
UDA O
to O
nuclearity O
classiﬁcation O
also O
has O
a O
similar O
improvement O
as O
Table O
3 O
. O

Theoretically O
, O
beam O
search O
with O
a O
larger O
beam O
width O
helps O
ﬁnd O
a O
better O
solution O
. O

Table O
4 O
, O
however O
, O
our O
parser O
is O
worse O
when O
a O
larger O
beam O
width O
is O
used O
, O
which O
means O
the O
sequence O
having O
higher O
overall O
score O
does O
not O
ensure O
the O
better O
decoding O
result O
. O

Our O
experiment O
only O
shows O
the O
beam O
widths O
up O
to O
ﬁve O
because O
the O
scores O
of O
worse O
sequences O
are O
already O
higher O
than O
that O
of O
the O
correct O
sequence O
in O
some O
cases O
. O

That O
is O
, O
the O
larger O
beam O
widths O
seem O
to O
be O
unnecessary O
. O

The O
reason O
may O
be O
that O
beam O
search O
is O
not O
really O
suitable O
for O
the O
shift O
- O
reduce O
paradigm O
. O

For O
example O
, O
a O
sequence O
might O
fall O
into O
a O
seriously O
bad O
stage O
but O
the O
rest O
of O
actions O
can O
be O
easily O
determined O
so O
that O
the O
sequence O
will O
get O
a O
high O
overall O
probability O
. O

This O
assumption O
also O
implies O
that O
unlike O
beam O
search O
applied O
on O
sequence O
to O
sequence O
model O
, O
we O
can O
not O
judge O
a O
transition O
sequence O
is O
good O
or O
bad O
by O
solely O
considering O
its O
overall O
score O
. O

In O
addition O
, O
for O
longer O
textual O
units O
such O
as O
paragraph O
, O
human O
readers O
and O
writers O
may O
not O
follow O
the O
assumption O
of O
overall O
optimization O
. O

Instead O
, O
human O
beings O
read O
and O
write O
sequentially O
, O
similar O
to O
the O
greedy O
nature O
. O

We O
also O
evaluate O
our O
approach O
in O
English B-TaskName
discourse I-TaskName
parsing I-TaskName
. O

The O
famous O
dataset O
, O
RST O
- O
DT O
, O
is O
used O
. O

Our O
model O
achieves O
F B-MetricName
- I-MetricName
scores I-MetricName
of O
85.0 B-MetricValue
% I-MetricValue
, O
58.8 B-MetricValue
% I-MetricValue
, O
69.9 B-MetricValue
% I-MetricValue
, O
and O
56.7 B-MetricValue
% I-MetricValue
in O
tree O
construction O
, O
rhetorical B-TaskName
relation I-TaskName
recognition I-TaskName
, O
nuclearity O
labeling O
, O
and O
all O
subtasks O
, O
respectively O
. O

The O
overall O
performance O
is O
similar O
to O
that O
of O
the O
state O
- O
of O
- O
the O
- O
art O
model O
( O
Yu O
et O
al O
. O
, O
2018 O
) O
. O

4 O
Conclusion O
This O
work O
proposes O
a O
standalone O
, O
complete O
Chinese O
discourse O
parser O
. O

We O
integrate O
BERT O
, O
UDA O
, O
and O
a O
revised O
training O
procedure O
for O
constructing O
a O
robust O
shift O
- O
reduce O
parser O
. O

Our O
model O
is O
compared O
with O
a O
number O
of O
previous O
models O
, O
and O
experimental O
results O
show O
that O
our O
model O
achieves O
the O
stateof O
- O
the O
- O
art O
performance O
and O
is O
highly O
competitive O
with O
different O
setups O
. O

We O
will O
explore O
cross O
- O
lingual O
transfer O
learning O
for O
supporting O
more O
languages O
. O

Acknowledgements O
This O
research O
was O
partially O
supported O
by O
Ministry O
of O
Science O
and O
Technology O
, O
Taiwan O
, O
under O
grants O
MOST-106 O
- O
2923 O
- O
E-002 O
- O
012 O
- O
MY3 O
, O
MOST-1092634 O
- O
F-002 O
- O
040- O
, O
MOST-109 O
- O
2634 O
- O
F-002 O
- O
034- O
, O
MOST-108 O
- O
2218 O
- O
E-009 O
- O
051- O
, O
and O
by O
Academia O
Sinica O
, O
Taiwan O
, O
under O
grant O
AS O
- O
TP-107 O
- O
M05.138 O

Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
7–18 O
July O
5 O
- O
10 O
, O
2020 O
. O

c O

 O
2020 O
Association O
for O
Computational O
Linguistics O
Predicting B-TaskName
Depression I-TaskName
in I-TaskName
Screening I-TaskName
Interviews I-TaskName
from I-TaskName
Latent I-TaskName
Categorization I-TaskName
of I-TaskName
Interview I-TaskName
Prompts I-TaskName
Alex O
Rinaldi O
Department O
of O
Computer O
Science O
UC O
Santa O
Cruz O
arinaldi@ucsc.eduJean O
E. O
Fox O
Tree O
Department O
of O
Psychology O
UC O
Santa O
Cruz O
foxtree@ucsc.eduSnigdha O
Chaturvedi O
Department O
of O
Computer O
Science O
University O
of O
North O
Carolina O
at O
Chapel O
Hill O
snigdha@cs.unc.edu O
Abstract O
Despite O
the O
pervasiveness O
of O
clinical O
depression O
in O
modern O
society O
, O
professional O
help O
remains O
highly O
stigmatized O
, O
inaccessible O
, O
and O
expensive O
. O

Accurately O
diagnosing B-TaskName
depression I-TaskName
is O
difﬁcult O
– O
requiring O
time O
- O
intensive O
interviews O
, O
assessments O
, O
and O
analysis O
. O

Hence O
, O
automated O
methods O
that O
can O
assess O
linguistic O
patterns O
in O
these O
interviews O
could O
help O
psychiatric O
professionals O
make O
faster O
, O
more O
informed O
decisions O
about O
diagnosis O
. O

We O
propose O
JLPC B-MethodName
, O
a O
method O
that O
analyzes O
interview O
transcripts O
to O
identify O
depression O
while O
jointly O
categorizing O
interview O
prompts O
into O
latent O
categories O
. O

This O
latent O
categorization O
allows O
the O
model O
to O
identify O
high O
- O
level O
conversational O
contexts O
that O
inﬂuence O
patterns O
of O
language O
in O
depressed O
individuals O
. O

We O
show O
that O
the O
proposed O
model O
not O
only O
outperforms O
competitive O
baselines O
, O
but O
that O
its O
latent O
prompt O
categories O
provide O
psycholinguistic O
insights O
about O
depression O
. O

1 O
Introduction O
Depression O
is O
a O
dangerous O
disease O
that O
effects O
many O
. O

A O
2017 O
study O
by O
Weinberger O
et O
al O
. O

( O
2018 O
) O
ﬁnds O
that O
one O
in O
ﬁve O
US O
adults O
experienced O
depression O
symptoms O
in O
their O
lifetime O
. O

Weinberger O
et O
al O
. O
also O
identify O
depression O
as O
a O
signiﬁcant O
risk O
factor O
for O
suicidal O
behavior O
. O

Unfortunately O
, O
professional O
help O
for O
depression O
is O
not O
only O
stigmatized O
, O
but O
also O
expensive O
, O
timeconsuming O
and O
inaccessible O
to O
a O
large O
population O
. O

Lakhan O
et O
al O
. O

( O
2010 O
) O
explain O
that O
there O
are O
no O
laboratory O
tests O
for O
diagnosing O
psychiatric O
disorders O
; O
instead O
these O
disorders O
must O
be O
identiﬁed O
through O
screening O
interviews O
of O
potential O
patients O
that O
require O
time O
- O
intensive O
analysis O
by O
medical O
experts O
. O

This O
has O
motivated O
developing O
automated O
depression B-TaskName
detection I-TaskName
systems O
that O
can O
provide O
conﬁdential O
, O
inexpensive O
and O
timely O
preliminary O
triaging O
that O
can O
help O
individuals O
in O
seeking O
help O
frommedical O
experts O
. O

Such O
systems O
can O
help O
psychiatric O
professionals O
by O
analyzing O
interviewees O
for O
predictive O
behavioral O
indicators O
that O
could O
serve O
as O
additional O
evidence O
( O
DeVault O
et O
al O
. O
, O
2014 O
) O
. O

Language O
is O
a O
well O
- O
studied O
behavioral O
indicator O
for O
depression O
. O

Psycholinguistic O
studies O
by O
Segrin O
( O
1990 O
) O
, O
Rude O
et O
al O
. O
( O
2004 O
) O
, O
and O
Andreasen O
( O
1976 O
) O
identify O
patterns O
of O
language O
in O
depressed O
individuals O
, O
such O
as O
focus O
on O
self O
and O
detachment O
from O
community O
. O

To O
capitalize O
on O
this O
source O
of O
information O
, O
recent O
work O
has O
proposed O
deep O
learning O
models O
that O
leverage O
linguistic O
features O
to O
identify O
depressed O
individuals O
( O
Mallol O
- O
Ragolta O
et O
al O
. O
, O
2019 O
) O
. O

Such O
deep O
learning O
models O
achieve O
high O
performance O
by O
uncovering O
complex O
, O
unobservable O
patterns O
in O
data O
at O
the O
cost O
of O
transparency O
. O

However O
, O
in O
the O
sensitive O
problem O
domain O
of O
diagnosing O
psychiatric O
disorders O
, O
a O
model O
should O
offer O
insight O
about O
its O
functionality O
in O
order O
for O
it O
to O
be O
useful O
as O
a O
clinical O
support O
tool O
. O

One O
way O
for O
a O
model O
to O
do O
this O
is O
utilizing O
the O
structure O
of O
the O
input O
( O
interview O
transcript O
) O
to O
identify O
patterns O
of O
conversational O
contexts O
that O
can O
help O
professionals O
in O
understanding O
how O
the O
model O
behaves O
in O
different O
contexts O
. O

A O
typical O
interview O
is O
structured O
as O
pairs O
of O
prompts O
and O
responses O
such O
that O
participant O
responses O
follow O
interviewer O
prompts O
( O
such O
as O
“ O
How O
have O
you O
been O
feeling O
lately O
? O
” O
) O
. O

Intuitively O
, O
each O
interviewer O
prompt O
serves O
as O
a O
context O
that O
informs O
how O
its O
response O
should O
be O
analyzed O
. O

For O
example O
, O
a O
short O
response O
like O
“ O
yeah O
” O
could O
communicate O
agreement O
in O
response O
to O
a O
question O
such O
as O
“ O
Are O
you O
happy O
you O
did O
that O
? O
” O
, O
but O
the O
same O
response O
could O
signal O
taciturnity O
or O
withdrawal O
( O
indicators O
of O
depression O
) O
in O
response O
to O
an O
encouraging O
prompt O
like O
“ O
Nice O
! O
” O
. O

To O
enable O
such O
contextdependent O
analysis O
, O
the O
model O
should O
be O
able O
to O
group O
prompts O
based O
on O
the O
types O
of O
conversa-8tional O
context O
they O
provide O
. O

To O
accomplish O
this O
, O
we O
propose O
a O
neural O
Joint B-MethodName
Latent I-MethodName
Prompt I-MethodName
Categorization I-MethodName
( O
JLPC B-MethodName
) O
model O
that O
infers O
latent O
prompt O
categories O
. O

Depending O
on O
a O
prompt O
’s O
category O
, O
the O
model O
has O
the O
ﬂexibility O
to O
focus O
on O
different O
signals O
for O
depression O
in O
the O
corresponding O
response O
. O

This O
prompt O
categorization O
is O
learned O
jointly O
with O
the O
end O
task O
of O
depression B-TaskName
prediction I-TaskName
. O

Beyond O
improving O
prediction O
accuracy B-MetricName
, O
the O
latent O
prompt O
categorization O
makes O
the O
proposed O
model O
more O
transparent O
and O
offers O
insight O
for O
expert O
analysis O
. O

To O
demonstrate O
this O
, O
we O
analyze O
learned O
prompt O
categories O
based O
on O
existing O
psycholinguistic O
research O
. O

We O
also O
test O
existing O
hypotheses O
about O
depressed O
language O
with O
respect O
to O
these O
prompt O
categories O
. O

This O
not O
only O
offers O
a O
window O
into O
the O
model O
’s O
working O
, O
but O
also O
can O
be O
used O
to O
design O
better O
clinical O
support O
tools O
that O
analyze O
linguistic O
cues O
in O
light O
of O
the O
interviewer O
prompt O
context O
. O

Our O
key O
contributions O
are O
: O
We O
propose O
an O
end O
- O
to O
- O
end O
, O
data O
- O
driven O
model O
for O
predicting B-TaskName
depression I-TaskName
from I-TaskName
interview I-TaskName
transcripts I-TaskName
that O
leverages O
the O
contextual O
information O
provided O
by O
interviewer O
prompts O
Our O
model O
jointly O
learns O
latent O
categorizations O
of O
prompts O
to O
aid O
prediction O
We O
conduct O
robust O
experiments O
to O
show O
that O
our O
model O
outperforms O
competitive O
baselines O
We O
analyze O
the O
model O
’s O
behavior O
against O
existing O
psycholinguistic O
theory O
surrounding O
depressed O
language O
to O
demonstrate O
the O
interpretability O
of O
our O
model O
2 O
Joint O
Latent O
Prompt O
Categorization O
We O
propose O
a O
Joint B-MethodName
Latent I-MethodName
Prompt I-MethodName
Categorization I-MethodName
( O
JLPC B-MethodName
) O
model O
that O
jointly O
learns O
to O
predict B-TaskName
depression I-TaskName
from I-TaskName
interview I-TaskName
transcripts I-TaskName
while O
grouping O
interview O
prompts O
into O
latent O
categories.1 O
. O

The O
general O
problem O
of O
classifying O
interview O
text O
is O
deﬁned O
as O
follows O
: O
let O
Xdenote O
the O
set O
ofNinterview O
transcripts O
. O

Each O
interview O
Xiis O
a O
sequence O
of O
jconversational O
turns O
consisting O
of O
interviewer O
’s O
prompts O
and O
participant O
’s O
responses O
: O

Xi O
= O
f(Pij O
; O
Rij)forj O
= O
f1:::Mig O
, O
whereMiis O
the O
number O
of O
turns O
in O
Xi O
, O
Pijis O
thejthprompt O
in O
theithinterview O
, O
and O
Rijis O
the O
participant O
’s O
re1Code O
and O
instructions O
for O
reproducing O
our O
results O
are O
available O
at O
https://github.com/alexwgr/ O
LatentPromptReleasesponse O
to O
that O
prompt O
. O

Together O
, O
( O
Pij O
; O
Rij)form O
thejthturn O
inithinterview O
. O

Each O
interview O
Xi O
is O
labeled O
with O
a O
ground O
- O
truth O
class O
Yi2f1;::Cg O
, O
where O
C B-HyperparameterName
is O
the O
number O
of O
possible O
labels O
. O

In O
our O
case O
, O
there O
are O
two O
possible O
labels O
: O
depressed O
or O
not O
depressed O
. O

Our O
model O
, O
shown O
in O
Figure O
1 O
, O
takes O
as O
input O
an O
interview O
Xiand O
outputs O
the O
predicted O
label O
^Yi O
. O

Our O
approach O
assumes O
that O
prompts O
and O
responses O
are O
represented O
as O
embeddings O
Pij2RE O
andRij2RErespectively O
. O

We O
hypothesize O
that O
prompts O
can O
be O
grouped O
into O
latent O
categories O
( O
Kin O
number O
) O
such O
that O
corresponding O
responses O
will O
exhibit O
unique O
, O
useful O
patterns O
. O

To O
perform O
a O
soft O
assignment O
of O
prompts O
to O
categories O
, O
for O
each O
prompt O
, O
our O
model O
computes O
a O
category O
membership O
vector O
hij=[h1 O
ij;;hK O
ij O
] O
. O

It O
represents O
the O
probability O
distribution O
for O
the O
jthprompt O
of O
the O
ithinterview O
over O
each O
of O
Klatent O
categories O
. O

hij O
is O
computed O
as O
a O
function O
 O
ofPijand O
trainable O
parametersCI(illustrated O
as O
the O
Category O
Inference O
layer O
in O
Figure O
1 O
): O
hij= O
 O
( O
Pij;CI O
) O
( O
1 O
) O
Based O
on O
these O
category O
memberships O
for O
each O
prompt O
, O
the O
model O
then O
analyzes O
the O
corresponding O
responses O
so O
that O
unique O
patterns O
can O
be O
learned O
for O
each O
category O
. O

Speciﬁcally O
, O
we O
form O
K B-HyperparameterName
category O
- O
aware O
response O
aggregations O
. O

Each O
of O
these O
aggregations O
, O
Rk O
i2RE O
, O
is O
a O
category O
- O
aware O
representation O
of O
all O
responses O
of O
the O
ithinterview O
with O
respect O
to O
the O
kthcategory O
. O

Rk O
i=1 O
Zk O
iMiX O
j=1hk O
ijRij O
( O
2 O
) O

Zk O
i O
= O
MiX O
j=1hk O
ij O
( O
3 O
) O
where O
, O
hk O
ijis O
thekthscalar O
component O
of O
the O
latent O
category O
distribution O
vector O
hijandZk O
iis O
a O
normalizer O
added O
to O
prevent O
varying O
signal O
strength O
, O
which O
interferes O
with O
training O
. O

We O
then O
compute O
the O
output O
class O
probability O
vector O
yias O
a O
function O
  O
of O
the O
response O
aggregations O
[ O
R1 O
i;;RK O
i]and O
trainable O
parameters O
D O
( O
illustrated O
as O
the O
Decision O
Layer O
in O
Figure O
1 O
) O
. O

yi= O
( O
R1 O
i;;RK O
i;D O
) O
( O
4 O
) O

The O
predicted O
label O
^Yiis O
selected O
as O
the O
class O
with O
the O
highest O
probability O
based O
on O
yi.9 O
Figure O
1 O
: O
The O
architecture O
of O
our O
JLPC B-MethodName
model O
with O
K B-HyperparameterName
= O
3 B-HyperparameterValue
. O

For O
each O
prompt O
Pijin O
interview O
i O
, O
the O
Category O
Inference O
layer O
computes O
a O
latent O
category O
membership O
vector O
, O
hij O
. O

These O
are O
used O
as O
weights O
to O
form O
K B-HyperparameterName
separate O
Category O
- O
Aware O
Response O
Aggregations O
, O
which O
in O
turn O
are O
used O
by O
the O
Decision O
Layer O
to O
predict O
the O
output O
. O

2.1 O
The O
Category O
Inference O
Layer O
We O
compute O
the O
latent O
category O
membership O
for O
all O
prompts O
in O
interview O
iusing O
a O
feed O
- O
forward O
layer O
with O
K B-HyperparameterName
outputs O
and O
softmax O
activation O
: O
 O
( O
Pij;CI O
) O
= O
(rowj(PiWCI+BCI))(5 O
) O
As O
shown O
in O
Equation O
1 O
, O
 O
( O
Pij;CI)produces O
the O
desired O
category O
membership O
vector O
hijover O
latent O
categories O
for O
the O
jthprompt O
of O
the O
ithinterview O
. O

Pi2RMEis O
deﬁned O
as O
[ O
Pi1;;PiM]T O
, O
where O
M B-HyperparameterName
is O
the O
maximum O
conversation O
length O
in O
XiandPim=0Efor O
allMi O
< O
mM. O
PiWCI+BCIcomputes O
a O
matrix O
where O
row O
j O
is O
a O
vector O
of O
energies O
for O
the O
latent O
category O
distribution O
for O
prompt O
j O
, O
anddenotes O
the O
softmax O
function O
. O

WCI2REKandBCI2RK O
are O
the O
trainable O
parameters O
for O
this O
layer O
: O
CI= O
fWCI;BCIg O
. O

2.2 O
The O
Decision O
Layer O
The O
Decision O
Layer O
models O
the O
probabilities O
for O
each O
output O
class O
( O
depressed O
andnot O
- O
depressed O
) O
using O
a O
feed O
- O
forward O
layer O
over O
the O
concatenation O
Riof O
response O
aggregations O
[ O
R1 O
i;;RK O
i O
] O
. O

This O
allows O
each O
response O
aggregation O
Rk O
ito O
contribute O
to O
the O
ﬁnal O
classiﬁcation O
through O
a O
separate O
set O
of O
trainable O
parameters O
. O
  O

( O
R1 O
i;;RK O
i;D O
) O

= O
(RT O
iWD+BD)(6 O
) O

As O
shown O
in O
Equation O
4 O
, O
  O
( O
R1 O
i;;RK O
i;D O
) O
produces O
the O
output O
class O
probability O
vector O
yi O
. O

WD2R(EK)CandBD2RCare O
the O
trainable O
parameters O
for O
the O
decision O
layer O
: O
D= O
fWD;BDg O
. O

We O
then O
compute O
the O
cross O
entropy O
loss O
L(Y;^Y)between O
ground O
truth O
labels O
and O
yi O
. O

2.3 O
Entropy O
regularization O
The O
model O
’s O
learning O
goal O
as O
described O
above O
only O
allows O
the O
output O
prediction O
error O
to O
guide O
the O
separation O
of O
prompts O
into O
useful O
categories O
. O

However O
, O
in O
order O
to O
encourage O
the O
model O
to O
learn O
distinct O
categories O
, O
we O
employ O
entropy O
regularization O
( O
Grandvalet O
and O
Bengio O
, O
2005 O
) O
by O
penalizing O
overlap O
in O
the O
latent O
category O
distributions O
for O
prompts O
. O

That O
is O
, O
we O
compute O
the O
following O
entropy O
term O
using O
components O
of O
the O
category O
membership O
vector O
hijfrom O
Equation O
1 O
: O
E(Xi O
) O
= O
1 O
uiNX O
i=1MiX O
j=1Ej(Xi O
) O
( O
7 O
) O
where O
, O
Ej(Xi O
) O
= O
 KX O
k=1hk O
ijlnhk O
ij O
( O
8) O
ui O
= O
NX O
i=1Mi O
( O
9 O
) O

Finally O
, O
the O
model O
’s O
overall O
learning O
goal O
minimizes O
entropy O
regularized O
cross O
entropy O
loss O
: O
argmin O
L(Y;^Y O
) O
+ O
E(Xi)10where, O
is B-HyperparameterName
a O
hyper O
- O
parameter O
that O
controls O
the O
strength O
of O
the O
entropy O
regularization O
term O
. O

2.4 O
Leveraging O
Prompt O
Representations O
in O
the O
Decision O
Layer O
While O
prompt O
representations O
are O
used O
to O
compute O
latent O
category O
assignments O
, O
the O
model O
described O
so O
far O
( O
JLPC B-MethodName
) O
can O
not O
directly O
leverage O
prompt O
features O
in O
the O
ﬁnal O
classiﬁcation O
. O

To O
provide O
this O
capability O
, O
we O
deﬁne O
two O
additional O
model O
variants O
with O
pre O
- O
aggregation O
and O
post O
- O
aggregation O
prompt O
injection O
: O
JLPCPre B-MethodName
and O
JLPCPost B-MethodName
, O
respectively O
. O

JLPCPre B-MethodName
is O
similar O
to O
the O
JLPC B-MethodName
model O
, O
except O
that O
it O
aggregates O
both O
prompt O
and O
response O
representations O
based O
on O
prompt O
categories O
. O

In O
other O
words O
, O
the O
aggregated O
response O
representation O
, O
Rk O
i O
in O
Equation O
2 O
, O
is O
computed O
as O
: O
Rk O

i=1 O
Zk O
iMiX O
j=1hk O
ij[Pij;Rij O
] O
JLPCPost B-MethodName
is O
also O
similar O
to O
JLPC B-MethodName
except O
that O
it O
includes O
the O
average O
of O
prompt O
representations O
as O
additional O
input O
to O
the O
decision O
layer O
. O

That O
is O
, O
Equation O
6 O
is O
modiﬁed O
to O
the O
following O
: O
  O
( O
R1 O
i;;RK O
i;D O
) O
= O
([Pi;Ri]TWD+BD O
) O
( O
10 O
) O

Piis O
the O
uniformly O
- O
weighted O
average O
of O
prompt O
representations O
in O
Xi O
. O
3 O
Dataset O
We O
evaluate O
our O
model O
on O
the O
Distress B-DatasetName
Analysis I-DatasetName
Interview I-DatasetName
Corpus I-DatasetName
( O
DAIC B-DatasetName
) O
( O
Gratch O
et O
al O
. O
, O
2014 O
) O
. O

DAIC B-DatasetName
consists O
of O
text O
transcripts O
of O
interviews O
designed O
to O
emulate O
a O
clinical O
assessment O
for O
depression O
. O

The O
interviews O
are O
conducted O
between O
human O
participants O
and O
a O
human O
- O
controlled O
digital O
avatar O
. O

Each O
interview O
is O
labeled O
with O
a O
binary O
depression O
rating O
based O
on O
a O
score O
threshold O
for O
the O
9th O
revision O
of O
the O
Patient O
Health O
Questionnaire O
( O
PHQ-9 O
) O
. O

In O
total O
, O
there O
are O
170interviews O
, O
with O
49participants O
identiﬁed O
as O
depressed O
. O

To O
achieve O
stable O
and O
robust O
results O
given O
the O
small O
size O
of O
the O
DAIC B-DatasetName
dataset O
, O
we O
report O
performance O
over O
10 B-HyperparameterValue
separate O
splits O
of O
the O
dataset O
into O
training O
, O
validation O
, O
and O
test O
sets O
. O

For O
each O
split O
, O
70 B-HyperparameterValue
% I-HyperparameterValue
is O
used O
as O
training O
data O
, O
and O
20 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
training O
data O
is O
set O
aside O
as O
validation O
data.3.1 O
Preprocessing O
and O
Representation O
DAIC B-DatasetName
interview O
transcripts O
are O
split O
into O
utterances O
based O
on O
pauses O
in O
speech O
and O
speaker O
change O
, O
so O
we O
concatenate O
adjacent O
utterances O
by O
the O
same O
speaker O
to O
achieve O
a O
prompt O
- O
response O
structure O
. O

We O
experiment O
with O
two O
types O
of O
continuous O
representations O
for O
prompts O
and O
responses O
: O
averaged O
word O
embeddings O
from O
the O
pretrained O
GloVe B-MethodName
model O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
, O
and O
sentence O
embeddings O
from O
the O
pretrained O
BERT B-MethodName
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

Further O
details O
are O
given O
in O
Appendix O
A.1 O
. O

Reported O
results O
use O
GloVe B-MethodName
embeddings O
because O
they O
led O
to O
better O
validation O
scores O
. O

3.2 O
Exclusion O
of O
Predictive O
Prompts O
Our O
preliminary O
experiments O
showed O
that O
it O
is O
possible O
to O
achieve O
better O
- O
than O
- O
random O
performance O
on O
the O
depression O
identiﬁcation O
task O
using O
only O
the O
set O
of O
prompts O
( O
excluding O
the O
responses O
) O
. O

This O
is O
possibly O
because O
the O
interviewer O
identiﬁed O
some O
individuals O
as O
potentially O
depressed O
during O
the O
interview O
, O
resulting O
in O
predictive O
follow O
- O
up O
prompts O
( O
for O
example O
, O
“ O
How O
long O
ago O
were O
you O
diagnosed O
? O
” O
) O
. O

To O
address O
this O
, O
we O
iteratively O
remove O
predictive O
prompts O
until O
the O
development O
performance O
using O
prompts O
alone O
is O
not O
signiﬁcantly O
better O
than O
random O
( O
see O
Appendix O
A.3 O
) O
. O

This O
is O
to O
ensure O
our O
experiments O
evaluate O
the O
content O
of O
prompts O
and O
responses O
rather O
than O
ﬁtting O
to O
any O
bias O
in O
question O
selection O
by O
the O
DAIC B-DatasetName
corpus O
interviewers O
, O
and O
so O
are O
generalizable O
to O
other O
interview O
scenarios O
, O
including O
future O
fully O
- O
automated O
ones O
. O

4 O
Experiments O
We O
now O
describe O
our O
experiments O
and O
analysis O
. O

4.1 O
Baselines O
Our O
experiments O
use O
the O
following O
baselines O
: O
The O
RO B-MethodName
baseline O
only O
has O
access O
to O
responses O
. O

It O
applies O
a O
dense O
layer O
to O
the O
average O
of O
response O
representations O
for O
an O
interview O
. O

The O
PO B-MethodName
baseline O
only O
has O
access O
to O
prompts O
, O
following O
the O
same O
architecture O
as O
RO B-MethodName
. O
The O
PR B-MethodName
baseline O
has O
access O
to O
both O
prompts O
and O
responses O
. O

It O
applies O
a O
dense O
layer O
to O
the O
average O
of O
prompt O
and O
response O
concatenations O
. O

Table O
1 O
: O
Mean B-MetricName
F1 I-MetricName
scores O
for O
the O
positive O
( O
depressed O
) O
and O
negative O
( O
not O
depressed O
) O
across O
the O
10 B-HyperparameterValue
test O
sets O
. O

Standard O
deviation O
is O
reported O
in O
parentheses O
. O

Two O
of O
the O
proposed O
models O
, O
JLPC B-MethodName
and O
JLPCPost B-MethodName
, O
improve O
over O
baselines O
including O
the O
BERT B-MethodName
ﬁne O
- O
tuned O
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
with O
the O
JLPCPost B-MethodName
achieving O
a O
statistically O
signiﬁcant O
improvement O
( O
p<0:05 O
) O
. O

 O
BERT B-MethodName
refers O
to O
the O
BERT B-MethodName
model O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
ﬁne O
- O
tuned O
on O
our O
dataset O
( O
see O
Appendix O
A.2 O
) O
. O

4.2 O
Training O
details O
All O
models O
are O
trained O
using O
the O
Adam O
optimizer O
. O

We O
use O
mean O
validation O
performance O
to O
select O
hyper O
- O
parameter O
values O
: O
number B-HyperparameterName
of I-HyperparameterName
epochs I-HyperparameterName
= O
1300 B-HyperparameterValue
, O
learning B-HyperparameterName
rate I-HyperparameterName
= O
510 4 B-HyperparameterValue
, O
number B-HyperparameterName
of I-HyperparameterName
prompt I-HyperparameterName
categories I-HyperparameterName
K B-HyperparameterName
= O
11 B-HyperparameterValue
and O
entropy B-HyperparameterName
regularization I-HyperparameterName
strength I-HyperparameterName
= O
0.1 B-HyperparameterValue
. O
4.3 O
Quantitative O
Results O
We O
computed O
the O
F1 B-HyperparameterName
scores O
of O
the O
positive O
( O
depressed O
) O
and O
negative O
( O
not O
- O
depressed O
) O
classes O
averaged O
over O
the O
10test O
sets O
. O

Given O
the O
class O
imbalance O
in O
the O
DAIC B-DatasetName
dataset O
, O
we O
compare O
models O
using O
F1 B-MetricName
score O
for O
the O
depressed O
class O
. O

As O
an O
additional O
baseline O
, O
we O
also O
implemented O
methods O
from O
Mallol O
- O
Ragolta O
et O
al O
. O

( O
2019 O
) O
but O
do O
not O
report O
their O
performance O
since O
their O
model O
performs O
very O
poorly O
( O
close O
to O
random O
) O
when O
we O
consider O
averaged O
performance O
over O
10 B-HyperparameterValue
test O
sets O
. O

This O
is O
likely O
because O
of O
the O
large O
number O
of O
parameters O
required O
by O
the O
hierarchical O
attention O
model O
. O

Table O
1 O
summarizes O
our O
results O
. O

The O
belowrandom O
performance O
of O
the O
PO B-MethodName
baseline O
is O
expected O
, O
since O
the O
prompts O
indicative O
of O
depression O
were O
removed O
as O
described O
in O
Section O
3.2 O
. O

This O
indicates O
the O
remaining O
prompts O
, O
by O
themselves O
, O
are O
not O
sufﬁcient O
to O
accurately O
classify O
interviews O
. O

The O
RO B-MethodName
model O
performs O
better O
, O
indicating O
the O
response O
information O
is O
more O
useful O
. O

The O
PR B-MethodName
baseline O
improves O
over O
the O
RO B-MethodName
baseline O
indicating O
that O
Figure O
2 O
: O
Ablation O
study O
on O
validation O
set O
demonstrating O
the O
importance O
of O
prompt O
categorization O
and O
entropy O
regularization O
for O
our O
model O
. O

the O
combination O
of O
prompt O
and O
response O
information O
is O
informative O
. O

The O
BERT B-MethodName
model O
, O
which O
also O
has O
access O
to O
prompts O
and O
responses O
, O
shows O
a O
reasonable O
improvement O
over O
all O
baselines O
. O

JLPC B-MethodName
and O
JLPCPost B-MethodName
outperform O
the O
baselines O
, O
with O
JLPCPost B-MethodName
achieving O
a O
statistically O
signiﬁcant O
improvement O
over O
both O
the O
PR B-MethodName
and O
BERT B-MethodName
baselines O
( O
p<0:05).2This O
indicates O
the O
utility O
of O
our O
prompt O
- O
category O
aware O
analysis O
of O
the O
interviews O
. O

4.4 O
Ablation O
study O
We O
analyzed O
how O
the O
prompt O
categorization O
and O
entropy O
regularization O
contribute O
to O
our O
model O
’s O
validation O
performance O
. O

The O
contributions O
of O
each O
component O
are O
visualized O
in O
Figure O
2 O
. O

Our O
analysis O
shows O
that O
while O
both O
components O
are O
important O
, O
latent O
prompt O
categorization O
yields O
the O
highest O
contribution O
to O
the O
model O
’s O
performance O
. O

4.5 O
Analyzing O
Prompt O
Categories O
Beyond O
improving O
classiﬁcation O
performance O
, O
the O
latent O
categorization O
of O
prompts O
yields O
insight O
about O
conversational O
contexts O
relevant O
for O
analyzing O
language O
patterns O
in O
depressed O
individuals O
. O

To O
explore O
the O
learned O
categories O
, O
we O
isolate O
interviews O
from O
the O
complete O
corpus O
that O
are O
correctly O
labeled O
by O
our O
best O
- O
performing O
model O
. O

We O
say O
that O
the O
model O
“ O
assigns O
” O
an O
interview O
prompt O
to O
a O
given O
category O
if O
the O
prompt O
’s O
membership O
for O
that O
category O
( O
Equation O
1 O
) O
is O
stronger O
than O
for O
other O
categories O
. O

We O
now O
describe O
the O
various O
prompts O
assigned O
to O
different O
categories.3 O
Firstly O
, O
all O
prompts O
that O
are O
questions O
like O
“ O
Tell O
me O
more O
about O
that O
” O
, O
“ O
When O
was O
the O
last O
time O
you O
had O
an O
argument O
? O
” O
, O
etc O
. O
are O
grouped O
together O
into O
2Statistical O
signiﬁcance O
is O
calculated O
from O
the O
test O
prediction O
using O
two O
- O
sided O
T O
- O
test O
for O
independent O
samples O
of O
scores O
3To O
verify O
consistency O
of O
prompt O
categorization O
, O
we O
rerun O
the O
model O
with O
multiple O
initialization O
and O
they O
all O
yielded O
the O
same O
general O
trends O
as O
described O
in O
the O
paper.12a O
single O
category O
, O
which O
we O
refer O
to O
as O
the O
Starters O
category O
. O

Previous O
work O
has O
identiﬁed O
usefulness O
of O
such O
questions O
as O
conversation O
starters O
since O
they O
assist O
in O
creating O
a O
sense O
of O
closeness O
( O
Mcallister O
et O
al O
. O
, O
2004 O
; O
Heritage O
and O
Robinson O
, O
2006 O
) O
. O

Secondly O
, O
there O
are O
several O
categories O
reserved O
exclusively O
for O
certain O
backchannels O
. O

Backchannels O
are O
short O
utterances O
that O
punctuate O
longer O
turns O
by O
another O
conversational O
participant O
( O
Yngve O
, O
1970 O
; O
Goodwin O
, O
1986 O
; O
Bavelas O
et O

al O
. O
, O
2000 O
) O
. O

Speciﬁcally O
, O
the O
model O
assigns O
the O
backchannels O
“ O
mhm O
, O
” O
“ O
mm O
, O
” O
“ O
nice O
, O
” O
and O
“ O
awesome O
” O
each O
to O
separate O
categories O
. O

Research O
shows O
that O
it O
is O
indeed O
useful O
to O
consider O
the O
effects O
different O
types O
of O
backchannels O
separately O
. O

For O
example O
, O
Bavelas O
et O
al O
. O

( O
2000 O
) O
propose O
a O
distinction O
between O
speciﬁc O
backchannels O
( O
such O
as O
“ O
nice O
” O
and O
“ O
awesome O
” O
) O
and O
generic O
backchannels O
( O
such O
as O
“ O
mm O
” O
and O
“ O
mhm O
” O
) O
, O
and O
Tolins O
and O
Fox O
Tree O
( O
2014 O
) O
demonstrated O
that O
each O
backchannel O
type O
serves O
a O
different O
purpose O
in O
conversation O
. O

Thirdly O
, O
apart O
from O
starters O
and O
backchannels O
, O
the O
model O
isolates O
one O
speciﬁc O
prompt O
- O
“ O
Have O
you O
been O
diagnosed O
with O
depression?”4into O
a O
separate O
category O
. O

Clearly O
, O
this O
is O
an O
important O
prompt O
and O
it O
is O
encouraging O
to O
see O
that O
the O
model O
isolates O
it O
as O
useful O
. O

Interestingly O
, O
the O
model O
assigns O
the O
backchannel O
“ O
aw O
” O
to O
the O
same O
category O
as O
“ O
Have O
you O
been O
diagnosed O
with O
depression O
? O
” O
suggesting O
that O
responses O
to O
both O
prompts O
yield O
similar O
signals O
for O
depression O
. O

Lastly O
, O
the O
remaining O
ﬁve O
categories O
are O
empty O
- O
no O
prompt O
in O
the O
corpus O
has O
maximum O
salience O
with O
any O
of O
them O
. O

A O
likely O
explanation O
for O
this O
observation O
stems O
from O
the O
choice O
of O
normalizing O
factorZk O
iin O
Equation O
3 O
: O
it O
causes O
Rk O
ito O
regress O
to O
the O
unweighted O
average O
of O
response O
embeddings O
when O
all O
prompts O
in O
an O
interview O
have O
low O
salience O
with O
category O
k. O
Repeated O
empty O
categories O
then O
function O
as O
an O
“ O
ensemble O
model O
” O
for O
the O
average O
response O
embeddings O
, O
potentially O
improving O
predictive O
performance O
. O

4.6 O
Category O
- O
based O
Analysis O
of O
Responses O
The O
prompt O
categories O
inferred O
by O
our O
JLPCPost B-MethodName
model O
enable O
us O
to O
take O
a O
data O
- O
driven O
approach O
to O
investigating O
the O
following O
category O
- O
speciﬁc O
psycholinguistic O
hypotheses O
about O
depression O
: O
4Note O
that O
this O
prompt O
was O
not O
removed O
in O
Section O
3.2 O
since O
by O
itself O
, O
the O
prompt O
’s O
presence O
is O
not O
predictive O
of O
depression O
( O
without O
considering O
the O
response).Starters O
Backchannels O
D O
ND O
D O
ND O
RL O
23.2 O
27.2 O
19.9 O
15.1 O
DMF O
( O
10 2)6.55 O
7.31 O
7.98 O
8.55 O
Table O
2 O
: O
Indicators O
for O
social O
skills O
: O
mean O
response O
length O
( O
RL O
) O
and O
discourse O
marker/ﬁller O
rates O
( O
DMF O
) O
for O
responses O
to O
prompts O
in O
starters O
and O
backchannel(collectively O
representing O
“ O
mhm O
” O
, O
“ O
mm O
” O
, O
“ O
nice O
” O
, O
and O
“ O
awesome O
” O
) O
categories O
, O
for O
depressed O
( O
D O
) O
and O
notdepressed O
( O
ND O
) O
participants O
. O

Statistically O
signiﬁcant O
differences O
are O
underlined O
( O
p O
< O
0:05 O
) O
. O

Both O
measures O
are O
signiﬁcantly O
lower O
for O
the O
depressed O
class O
for O
responses O
to O
starters O
, O
but O
not O
to O
backchannels O
. O

H1Depression O
correlates O
with O
social O
skill O
deﬁcits O
( O
Segrin O
, O
1990 O
) O

H2Depressed O
language O
is O
vague O
and O
qualiﬁed O
( O
Andreasen O
, O
1976 O
) O

H3Depressed O
language O
is O
self O
- O
focused O
and O
detached O
from O
community O
( O
Rude O
et O
al O
. O
, O
2004 O
) O

For O
hypothesis O
H1 O
, O
we O
evaluate O
measures O
of O
social O
skill O
in O
responses O
to O
different O
categories O
of O
prompts O
. O

While O
research O
in O
psychology O
uses O
several O
visual O
, O
linguistic O
and O
paralinguistic O
indicators O
of O
social O
skills O
, O
in O
this O
paper O
we O
focus O
on O
two O
indicators O
that O
are O
measurable O
in O
our O
data O
: O
average O
response O
length O
in O
tokens O
and O
the O
rate O
of O
spoken O
- O
language O
ﬁllers O
anddiscourse O
markers O
us O

age.5The O
ﬁrst O
measure O
- O
response O
length O
- O
can O
be O
seen O
as O
a O
basic O
measure O
of O
taciturnity O
. O

The O
second O
measure O
- O
usage O
of O
ﬁllers O
anddiscourse O
markers O
- O
can O
be O
used O
as O
proxy O
for O
conversational O
skills O
, O
since O
speakers O
use O
these O
terms O
to O
manage O
conversations O
( O
Fox O
Tree O
, O
2010 O
) O
. O

Christenfeld O
( O
1995 O
) O
and O
Lake O
et O
al O
. O

( O
2011 O
) O
also O
ﬁnd O
that O
discourse O
marker O
usage O
correlates O
with O
social O
skill O
. O

Following O
is O
the O
list O
of O
ﬁllers O
anddiscourse O
markers O
: O
“ O
um O
” O
, O
“ O
uh O
” O
, O
“ O
you O
know O
” O
, O
“ O
well O
” O
, O
“ O
oh O
” O
, O
“ O
so O
” O
, O
“ O
I O
mean O
” O
, O
and O
“ O
like O
” O
. O

Table O
2 O
shows O
the O
values O
of O
these O
measures O
for O
social O
skill O
for O
responses O
to O
backchannels O
and O
starters O
categories O
. O

We O
found O
that O
both O
measures O
were O
signiﬁcantly O
lower O
for O
responses O
to O
starters O
- O
category O
prompts O
for O
depressed O
participants O
as O
opposed O
to O
not O
- O
depressed O
participants O
( O
p O
< O
0:05 O
) O
. O

However O
, O
the O
measures O
showed O
no O
signiﬁcant O
difference O
between O
depressed O
and O
notdepressed O
individuals O
for O
responses O
to O
categories O
5We O
compute O
this O
measure O
as O
the O
ratio O
of O
discourse O
marker O
and O
ﬁller O
occurrences O
to O
number O
of O
tokens O
, O
averaged O
over O
responses.13representing O
backchannels O
( O
“ O
mhm O
, O
” O
“ O
mm O
, O
” O
“ O
awesome O
, O
” O
and O
“ O
nice O
” O
) O
. O

Note O
that O
a O
conversation O
usually O
begins O
with O
prompts O
from O
the O
starters O
category O
and O
thereafter O
backchannels O
are O
used O
to O
encourage O
the O
speaker O
to O
continue O
speaking O
( O
Goodwin O
, O
1986 O
) O
. O

Given O
this O
, O
our O
results O
suggest O
that O
depressed O
individuals O
in O
the O
given O
population O
indeed O
initially O
demonstrate O
poorer O
social O
skills O
than O
notdepressed O
individuals O
, O
but O
the O
effect O
levels O
off O
as O
the O
interviewer O
encourages O
them O
to O
keep O
speaking O
using O
backchannels O
. O

Given O
this O
, O
our O
results O
suggest O
that O
depressed O
individuals O
in O
the O
given O
population O
indeed O
initially O
demonstrate O
poorer O
social O
skills O
than O
not O
depressed O
individuals O
, O
but O
the O
effect O
stops O
being O
visible O
as O
the O
conversation O
continues O
, O
either O
because O
the O
depressed O
individuals O
become O
more O
comfortable O
talking O
or O
because O
the O
interviewers O
’ O
encouragement O
through O
backchannels O
elicits O
more O
contributions O
. O

Hypotheses O
H2 O
and O
H3 O
- O
regarding O
qualiﬁed O
language O
and O
self O
- O
focus O
, O
respectively O
- O
involve O
semantic O
qualities O
of O
depressed O
language O
. O

To O
explore O
these O
hypotheses O
, O
we O
use O
a O
reverse O
engineering O
approach O
to O
determine O
salient O
words O
for O
depression O
in O
responses O
to O
each O
prompt O
category O
. O

We O
describe O
this O
reverse O
engineering O
approach O
as O
follows O
: O
since O
the O
aggregated O
representation O
of O
an O
individual O
’s O
responses O
in O
a O
category O
( O
Rk O
i O
computed O
in O
Equation O
2 O
) O
resides O
in O
the O
same O
vector O
space O
as O
individual O
word O
embeddings O
, O
we O
can O
identify O
words O
in O
our O
corpus O
that O
produce O
the O
strongest O
( O
positive O
) O
signal O
for O
depression O
in O
various O
categories.6We O
refer O
to O
these O
as O
signal O
words O
. O

Signal O
words O
are O
ranked O
not O
by O
their O
frequency O
in O
the O
dataset O
, O
but O
by O
their O
predictive O
potential O
the O
strength O
of O
association O
between O
the O
word O
’s O
semantic O
representation O
and O
a O
given O
category O
. O

We O
evaluate O
hypotheses O
H2 O
and O
H3 O
by O
observing O
semantic O
similarities O
between O
these O
signal O
words O
and O
the O
language O
themes O
identiﬁed O
by O
the O
hypotheses O
. O

Selections O
from O
the O
top O
10 O
signal O
words O
for O
depression O
associated O
with O
categories O
corresponding O
to O
starters O
, O
speciﬁc O
backchannels O
, O
and O
generic O
backchannels O
are O
shown O
in O
Figure O
3 O
. O

Figure O
3 O
shows O
hypothesis O
H2 O
is O
supported O
by O
6A O
word O
’s O
signal O
strength O
is O
computed O
for O
a O
given O
category O
kby O
taking O
the O
dot O
product O
of O
the O
word O
’s O
embedding O
with O
the O
weights O
in O
the O
decision O
layer O
corresponding O
to O
cat O

egory O
k. O

Large O
positive O
numbers O
correspond O
to O
positive O
predictions O
and O
vice O
versa O
. O

Since O
the O
Decision O
Layer O
is O
a O
dot O
product O
with O
all O
response O
aggregations O
, O
it O
is O
intuitive O
to O
compute O
prediction O
strength O
for O
a O
group O
of O
categories O
by O
adding O
together O
prediction O
strengths O
from O
individual O
groups O
. O

Figure O
3 O
: O
Signal O
words O
associated O
with O
language O
in O
depressed O
individuals O
. O

Columns O
represent O
various O
types O
of O
prompts O
( O
Starters O
, O
Generic O
Backchannels O
andSpeciﬁc O
Backchannels O
) O
. O

The O
bottom O
half O
shows O
ranked O
lists O
of O
signal O
words O
from O
the O
responses O
. O

Blue O
words O
are O
strongly O
indicative O
and O
red O
words O
are O
least O
indicative O
of O
depression O
. O

signal O
words O
in O
responses O
to O
generic O
backchannels O
; O
words O
such O
as O
“ O
theoretical O
” O
and O
“ O
plausible O
” O
constitute O
qualiﬁed O
language O
, O
and O
in O
the O
context O
of O
generic O
backchannels O
, O
the O
proposed O
model O
identiﬁes O
them O
as O
predictive O
of O
depression O
. O

Similarly O
, O
hypothesis O
H3 O
is O
also O
supported O
in O
responses O
togeneric O
backchannels O
. O

The O
model O
identiﬁes O
words O
related O
to O
community O
( O
“ O
kids O
, O
” O
“ O
neighborhood O
, O
” O
“ O
we O
” O
) O
as O
strong O
negative O
signals O
for O
depression O
, O
supporting O
that O
depressed O
language O
reﬂects O
detachment O
from O
community O
. O

However O
, O
the O
model O
only O
focuses O
on O
these O
semantic O
themes O
in O
responses O
to O
generic O
backchannelcategories O
. O

As O
we O
found O
in O
our O
evaluation O
of O
hypothesis O
H1 O
, O
the O
model O
localizes O
cues O
for O
depression O
to O
speciﬁc O
contexts O
. O

Signal O
words O
for O
depression O
in O
responses O
to O
the O
starters O
category O
are O
more O
reﬂective O
of O
our O
ﬁndings O
for O
hypothesis O
H1 O
: O
the O
model O
focuses O
on O
short O
, O
low O
- O
semantic O
- O
content O
words O
that O
could O
indicate O
social O
skill O
deﬁcit O
. O

For O
example O
, O
Figure O
3 O
shows O
we O
identiﬁed O
“ O
wow O
” O
as O
a O
signal O
word O
for O
the O
starters O
category O
. O

In O
one O
example O
from O
the O
corpus O
, O
a O
depressed O
participant O
uses O
“ O
wow O
” O
to O
express O
uncomfortability O
with O
an O
emotional O
question O
: O
the O
interviewer O
asks O
, O
“ O
Tell O
me O
about O
the O
last O
time O
you O
were O
really O
happy O
, O
” O
and O
the O
interviewee O
responds O
, O
“ O
wow O
( O
laughter O
) O
um O
. O
” O

For O
responses O
to O
speciﬁc O
backchannels O
, O
strong O
signal O
words O
reﬂect O
themes O
of O
goals O
and O
desires14(“wished O
, O
” O
“ O
mission O
, O
” O
“ O
accomplished O
” O
) O
. O

Psychologists O
have O
observed O
a O
correlation O
between O
depression O
and O
goal O
commitment O
and O
pursuit O
( O
Vergara O
and O
Roberts O
, O
2011 O
; O
Klossek O
, O
2015 O
) O
, O
and O
our O
ﬁnding O
indicates O
that O
depressed O
individuals O
discuss O
goal O
- O
related O
themes O
as O
response O
to O
speciﬁc O
backchannels O
. O

Overall O
, O
our O
model O
’s O
design O
not O
only O
helps O
in O
reducing O
its O
opacity O
but O
also O
informs O
psycholinguistic O
analysis O
, O
making O
it O
more O
useful O
as O
part O
of O
an O
informed O
decision O
- O
making O
process O
. O

Our O
analysis O
indicates O
that O
even O
though O
research O
has O
shown O
strong O
correlation O
between O
depression O
and O
various O
interpersonal O
factors O
such O
as O
social O
skills O
, O
self O
- O
focus O
and O
usage O
of O
qualiﬁed O
language O
, O
clinical O
support O
tools O
should O
focus O
on O
these O
factors O
in O
light O
of O
conversational O
cues O
. O

4.7 O
Sources O
of O
Error O
In O
this O
section O
, O
we O
analyze O
major O
sources O
of O
error O
. O

We O
apply O
a O
similar O
reverse O
engineering O
method O
as O
in O
Section O
4.6 O
. O

For O
prompts O
in O
each O
category O
, O
we O
consider O
corresponding O
responses O
that O
result O
in O
strong O
incorrect O
signals O
( O
false O
positive O
or O
false O
negative O
) O
based O
on O
the O
category O
’s O
weights O
in O
the O
decision O
layer O
. O

We O
focus O
on O
the O
categories O
with O
the O
most O
signiﬁcance O
presence O
in O
the O
dataset O
: O
the O
categories O
corresponding O
to O
starters O
, O
the O
“ O
mhm O
” O
backchannel O
, O
and O
the O
prompt O
“ O
Have O
you O
been O
diagnosed O
with O
depression O
? O
” O
. O

For O
the O
starters O
category O
, O
false O
positive O
- O
signal O
responses O
tend O
to O
contain O
a O
high O
presence O
of O
ﬁllers O
and O
discourse O
markers O
( O
“ O
uh O
, O
” O
“ O
huh O
, O
” O
“ O
post O
mm O
traumatic O
stress O
uh O
no O
uh O
uh O
, O
” O
“ O
hmm O
” O
) O
. O

It O
is O
possible O
that O
because O
the O
model O
learned O
to O
focus O
on O
short O
, O
low O
- O
semantic O
- O
content O
responses O
, O
it O
incorrectly O
correlates O
presence O
of O
ﬁllers O
and O
discourse O
markers O
with O
depression O
. O

For O
the O
“ O
mhm O
” O
category O
, O
we O
identiﬁed O
several O
false O
negatives O
, O
in O
which O
the O
responses O
included O
concrete O
words O
like O
“ O
uh O
nice O
environment O
” O
, O
“ O
I O
love O
the O
landscape O
” O
, O
and O
“ O
I O
love O
the O
waters O
” O
. O

Since O
the O
“ O
mhm O
” O
category O
focuses O
on O
vague O
, O
qualiﬁed O
language O
to O
predict O
depression O
( O
see O
Figure O
3 O
) O
, O
the O
presence O
of O
concrete O
words O
in O
these O
responses O
could O
have O
misled O
the O
model O
. O

For O
the O
“ O
Have O
you O
been O
diagnosed O
with O
depression O
? O
” O
category O
, O
the O
misclassiﬁed O
interviews O
contained O
short O
responses O
to O
this O
prompt O
like O
“ O
so O
, O
” O
“ O
never O
, O
” O
“ O
yes O
, O
” O
“ O
yeah O
, O
” O
and O
“ O
no O
, O
” O
as O
well O
as O
statements O
containing O
the O
word O
“ O
depression O
. O
” O

For O
this O
category O
, O
the O
model O
seems O
to O
incorrectly O
correlate O
short O
re O
- O
sponses O
and O
direct O
mentions O
of O
depression O
with O
the O
depressed O
class O
. O

5 O
Related O
Work O
Much O
work O
exists O
at O
the O
intersection O
of O
natural O
language O
processing O
( O
NLP O
) O
, O
psycholinguistics O
, O
and O
clinical O
psychology O
. O

For O
example O
, O
exploring B-TaskName
correlations I-TaskName
between I-TaskName
counselor I-TaskName
- I-TaskName
patient I-TaskName
interaction I-TaskName
dynamics I-TaskName
and I-TaskName
counseling I-TaskName
outcomes I-TaskName
( O
Althoff O
et O
al O
. O
, O
2016 O
) O
; O
studying O
linguistic O
development O
of O
mental O
healthcare O
counsellors O
( O
Zhang O
et O
al O
. O
, O
2019 O
) O
; O
identifying B-TaskName
differences I-TaskName
in I-TaskName
how I-TaskName
people I-TaskName
disclose I-TaskName
mental I-TaskName
illnesses I-TaskName
across I-TaskName
gender I-TaskName
and I-TaskName
culture I-TaskName
( O
De O
Choudhury O
et O
al O
. O
, O
2017 O
) O
; O
predicting B-TaskName
a I-TaskName
variety I-TaskName
of I-TaskName
mental I-TaskName
health I-TaskName
conditions I-TaskName
from I-TaskName
social I-TaskName
media I-TaskName
posts I-TaskName
( O
Sekulic O
and O
Strube O
, O
2019 O
; O
De O
Choudhury O
et O

al O
. O
, O
2013a O
; O
Guntuku O
et O
al O
. O
, O
2019 O
; O
Coppersmith O
et O

al O
. O
, O
2014 O
) O
; O
and O
analyzing O
well O
- O
being O
( O
Smith O
et O
al O
. O
, O
2016 O
) O
and O
distress O
( O
Buechel O
et O
al O
. O
, O
2018 O
) O
. O

Speciﬁcally O
, O
many O
researchers O
have O
used O
NLP O
methods O
for O
identifying B-TaskName
depression I-TaskName
( O
Morales O
et O
al O
. O
, O
2017 O
) O
. O

They O
focus O
on O
for O
predicting B-TaskName
depression I-TaskName
from I-TaskName
Twitter I-TaskName
posts I-TaskName
( O
Resnik O
et O
al O
. O
, O
2015 O
; O
De O
Choudhury O
et O
al O
. O
, O
2013b O
; O
Jamil O
et O
al O
. O
, O
2017 O
) O
, O
Facebook O
updates O
( O
Schwartz O
et O
al O
. O
, O
2014 O
) O
, O
student O
essays O
( O
Resnik O
et O
al O
. O
, O
2013 O
) O
, O
etc O
. O

Previous O
works O
have O
also O
focused O
on O
predicting O
depression O
severity O
from O
screening O
interview O
data O
( O
Yang O
et O
al O
. O
, O
2016 O
; O

Sun O
et O
al O
. O
, O
2017 O
; O
Pampouchidou O
et O
al O
. O
, O
2016 O
) O
. O

Unlike O
ours O
, O
these O
approaches O
rely O
on O
audio O
, O
visual O
, O
and O
text O
input O
. O

More O
recent O
approaches O
are O
based O
on O
deep O
learning O
. O

Yang O
et O
al O
. O

( O
2017 O
) O
propose O
a O
CNNbased O
model O
leveraging O
jointly O
trained O
paragraph O
vectorizations O
, O
Al O
Hanai O
et O
al O
. O

( O
2018 O
) O
propose O
an O
LSTM O
- O
based O
model O
fusing O
audio O
features O
with O
Doc2Vec O
representations O
of O
response O
text O
, O
Makiuchi O
et O
al O
. O

( O
2019 O
) O
combine O
LSTM O
and O
CNN O
components O
, O
and O
Mallol O
- O
Ragolta O
et O
al O
. O

( O
2019 O
) O
propose O
a O
model O
that O
uses O
a O
hierarchical O
attention O
mechanism O
. O

However O
, O
these O
approaches O
are O
more O
opaque O
and O
difﬁcult O
to O
interpret O
. O

Other O
approaches O
are O
similar O
to O
ours O
in O
the O
sense O
that O
they O
utilize O
the O
structure O
provided O
by O
interview O
prompts O
. O

Al O
Hanai O
et O
al O
. O

( O
2018 O
) O
and O

Gong O
and O
Poellabauer O
( O
2017 O
) O
propose O
models O
that O
extract O
separate O
sets O
of O
features O
for O
responses O
to O
each O
unique O
prompt O
in O
their O
corpus O
. O

However O
, O
these O
approaches O
require O
manually O
identifying O
unique O
prompts O
. O

Our O
model O
can O
instead O
automatically O
learn O
new O
, O
task O
- O
speciﬁc O
categorization O
of O
prompts.15Lubis O
et O
al O
. O

( O
2018 O
) O
perform O
a O
K B-MethodName
- I-MethodName
means I-MethodName
clustering I-MethodName
of I-MethodName
prompt I-MethodName
to O
assign O
prompts O
to O
latent O
dialogue O
act O
categories O
. O

These O
are O
used O
as O
features O
in O
a O
neural O
dialogue O
system O
. O

Our O
approach O
expands O
upon O
this O
idea O
of O
incorporating O
a O
separate O
unsupervised O
clustering O
step O
by O
allowing O
the O
learning O
goal O
to O
inﬂuence O
the O
clustering O
. O

Our O
approach O
is O
also O
related O
to O
that O
of O
Chaturvedi O
et O
al O
. O

( O
2014 O
) O
in O
that O
it O
automatically O
categorizes O
various O
parts O
of O
the O
conversation O
. O

However O
, O
they O
use O
domain O
- O
speciﬁc O
handcrafted O
features O
and O
discrete O
latent O
variables O
for O
this O
categorization O
. O

Our O
approach O
instead O
can O
leverage O
the O
neural O
architecture O
to O
automatically O
identify O
features O
useful O
for O
this O
categorization O
. O

To O
the O
best O
of O
our O
knowledge O
, O
our O
approach O
is O
the O
ﬁrst O
deep O
learning O
approach O
that O
jointly O
categorizes O
prompts O
to O
learn O
context O
- O
dependent O
patterns O
in O
responses O
. O

6 O
Conclusion O
This O
paper O
addressed O
the O
problem O
of O
identifying O
depression O
from O
interview O
transcripts O
. O

The O
proposed O
model O
analyzes O
the O
participant O
’s O
responses O
in O
light O
of O
various O
categories O
of O
prompts O
provided O
by O
the O
interviewer O
. O

The O
model O
jointly O
learns O
these O
prompt O
categories O
while O
identifying O
depression O
. O

We O
show O
that O
the O
model O
outperforms O
competitive O
baselines O
and O
we O
use O
the O
prompt O
categorization O
to O
investigate O
various O
psycholinguistic O
hypotheses O
. O

Depression O
prediction O
is O
a O
difﬁcult O
task O
which O
requires O
especially O
trained O
experts O
to O
conduct O
interviews O
and O
do O
their O
detailed O
analysis O
( O
Lakhan O
et O
al O
. O
, O
2010 O
) O
. O

While O
the O
absolute O
performance O
of O
our O
model O
is O
low O
for O
immediate O
practical O
deployment O
, O
it O
improves O
upon O
existing O
methods O
and O
at O
the O
same O
time O
, O
unlike O
modern O
methods O
, O
provides O
insight O
about O
the O
model O
’s O
workﬂow O
. O

For O
example O
, O
our O
ﬁndings O
show O
how O
language O
of O
depressed O
individuals O
changes O
when O
interviewers O
use O
backchannels O
to O
encourage O
continued O
speech O
. O

We O
hope O
that O
this O
combination O
will O
encourage O
the O
research O
community O
to O
make O
more O
progress O
in O
this O
direction O
. O

Future O
work O
can O
further O
investigate O
temporal O
patterns O
in O
how O
language O
used O
by O
depressed O
people O
evolves O
over O
the O
course O
of O
an O
interaction O
. O

References O
Tuka O
Al O
Hanai O
, O
Mohammad O
Ghassemi O
, O
and O
James O
Glass O
. O

2018 O
. O

Detecting B-TaskName
Depression I-TaskName
with O
Audio O
/ O
Text O
Sequence O
Modeling O
of O
Interviews O
. O

In O
Interspeech O
2018 O
, O
19th O
Annual O
Conference O
of O
the O
InternationalSpeech O
Communication O
Association O
, O
Hyderabad O
, O
India O
, O
2 O
- O
6 O
September O
2018 O
, O
pages O
1716–1720 O
. O
Tim O
Althoff O
, O
Kevin O
Clark O
, O
and O
Jure O
Leskovec O
. O

2016 O
. O

Large O
- O
scale O
Analysis O
of O
Counseling O
Conversations O
: O
An O
Application O
of O
Natural O
Language O
Processing O
to O
Mental O
Health O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
4:463–476 O
. O

Nancy O
J. O
C. O
Andreasen O
. O

1976 O
. O

Linguistic O
Analysis O
of O
Speech O
in O
Affective O
Disorders O
. O

Archives O
of O
General O
Psychiatry O
, O
33(11):1361 O
. O

Janet O
B. O
Bavelas O
, O
Linda O
Coates O
, O
and O
Trudy O
Johnson O
. O
2000 O
. O

Listeners O
as O
co O
- O
narrators O
. O

Journal O
of O
Personality O
and O
Social O
Psychology O
, O
79(6):941–952 O
. O

Sven O
Buechel O
, O
Anneke O
Buffone O
, O
Barry O
Slaff O
, O
Lyle O
Ungar O
, O
and O
Jo O
˜ao O
Sedoc O
. O

2018 O
. O

Modeling O
Empathy O
and O
Distress O
in O
Reaction O
to O
News O
Stories O
. O

In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
Brussels O
, O
Belgium O
, O
October O
31 O
- O
November O
4 O
, O
2018 O
, O
pages O
4758–4765 O
. O

Snigdha O
Chaturvedi O
, O
Dan O
Goldwasser O
, O
and O
Hal O
Daum O
´ O
e O
III O
. O

2014 O
. O

Predicting B-TaskName
instructor I-TaskName
’s I-TaskName
intervention I-TaskName
in I-TaskName
MOOC I-TaskName
forums I-TaskName
. O

In O
Proceedings O
of O
the O
52nd O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1501–1511 O
, O
Baltimore O
, O
Maryland O
. O

Association O
for O
Computational O
Linguistics O
. O

Nicholas O
Christenfeld O
. O

1995 O
. O

Does O
it O
hurt O
to O
say O
um O
? O
Journal O
of O
Nonverbal O
Behavior O
, O
19:171–186 O
. O

Glen O
Coppersmith O
, O
Mark O
Dredze O
, O
and O
Craig O
Harman O
. O
2014 O
. O

Quantifying O
Mental O
Health O
Signals O
in O
Twitter O
. O

In O
Proceedings O
of O
the O
Workshop O
on O
Computational O
Linguistics O
and O
Clinical O
Psychology O
: O
From O
Linguistic O
Signal O
to O
Clinical O
Reality O
, O
pages O
51–60 O
, O
Baltimore O
, O
Maryland O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O

Munmun O
De O
Choudhury O
, O
Scott O
Counts O
, O
and O
Eric O
Horvitz O
. O

2013a O
. O

Predicting B-TaskName
postpartum I-TaskName
changes I-TaskName
in I-TaskName
emotion I-TaskName
and I-TaskName
behavior I-TaskName
via I-TaskName
social I-TaskName
media I-TaskName
. O

In O
2013 O
ACM O
SIGCHI O
Conference O
on O
Human O
Factors O
in O
Computing O
Systems O
, O
CHI O
’ O
13 O
, O
Paris O
, O
France O
, O
April O
27 O
- O
May O
2 O
, O
2013 O
, O
pages O
3267–3276 O
. O

Munmun O
De O
Choudhury O
, O
Michael O
Gamon O
, O
Scott O
Counts O
, O
and O
Eric O
Horvitz O
. O

2013b O
. O

Predicting B-TaskName
Depression I-TaskName
via I-TaskName
Social I-TaskName
Media I-TaskName
. O

In O
Proceedings O
of O
the O
Seventh O
International O
Conference O
on O
Weblogs O
and O
Social O
Media O
, O
ICWSM O
2013 O
, O
Cambridge O
, O
Massachusetts O
, O
USA O
, O
July O
8 O
- O
11 O
, O
2013 O
. O

Munmun O
De O
Choudhury O
, O
Sanket O
S. O
Sharma O
, O
Tomaz O
Logar O
, O
Wouter O
Eekhout O
, O
and O
Ren O
´ O
e O
Clausen O
Nielsen O
. O
2017 O
. O

Gender O
and O
Cross O
- O
Cultural O
Differences O
in O
Social O
Media O
Disclosures O
of O
Mental O
Illness O
. O

In O
Proceedings O
of O
the O
2017 O
ACM O
Conference O
on O
Computer O
Supported O
Cooperative O
Work O
and O
Social O
Computing O
- O
CSCW O
’ O
17 O
, O
pages O
353–369 O
, O
Portland O
, O
Oregon O
, O
USA O
. O

ACM O
Press.16David O
DeVault O
, O
Ron O
Artstein O
, O
Grace O
Benn O
, O
Teresa O
Dey O
, O
Edward O
Fast O
, O
Alesia O
Gainer O
, O
Kallirroi O
Georgila O
, O
Jonathan O
Gratch O
, O
Arno O
Hartholt O
, O
Margaux O
Lhommet O
, O
Gale O
M. O
Lucas O
, O
Stacy O
Marsella O
, O
Fabrizio O
Morbini O
, O
Angela O
Nazarian O
, O
Stefan O
Scherer O
, O
Giota O
Stratou O
, O
Apar O
Suri O
, O
David O
R. O
Traum O
, O
Rachel O
Wood O
, O
Yuyu O
Xu O
, O
Albert O
A. O
Rizzo O
, O
and O
Louis O
- O
Philippe O
Morency O
. O

2014 O
. O

SimSensei O
Kiosk O
: O
A O
Virtual O
Human O
Interviewer O
for O
Healthcare O
Decision O
Support O
. O

In O
International O
conference O
on O
Autonomous O
Agents O
and O
Multi O
- O
Agent O
Systems O
, O
AAMAS O
’ O
14 O
, O
Paris O
, O
France O
, O
May O
5 O
- O
9 O
, O
2014 O
, O
pages O
1061–1068 O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT B-MethodName
: O
Pre B-MethodName
- I-MethodName
training I-MethodName
of I-MethodName
Deep I-MethodName
Bidirectional I-MethodName
Transformers I-MethodName
for I-MethodName
Language I-MethodName
Understanding I-MethodName
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
NAACL O
- O
HLT O
2019 O
, O
Minneapolis O
, O
MN O
, O
USA O
, O
June O
2 O
- O
7 O
, O
2019 O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
. O

Jean O
E. O
Fox O
Tree O
. O

2010 O
. O

Discourse O
Markers O
across O
Speakers O
and O
Settings O
. O

Language O
and O
Linguistics O
Compass O
, O
4(5):269–281 O
. O

Yuan O
Gong O
and O
Christian O
Poellabauer O
. O

2017 O
. O

Topic O
Modeling O
Based O
Multi O
- O
modal O
Depression O
Detection O
. O

InProceedings O
of O
the O
7th O
Annual O
Workshop O
on O
Audio O
/ O
Visual O
Emotion O
Challenge O
- O
AVEC O
’ O
17 O
, O
pages O
69–76 O
, O
Mountain O
View O
, O
California O
, O
USA O
. O
ACM O
Press O
. O

Charles O
Goodwin O
. O

1986 O
. O

Between O
and O
within O
: O
Alternative O
sequential O
treatments O
of O
continuers O
and O
assessments O
. O

Human O
Studies O
, O
9(2 O
- O
3):205–217 O
. O

Yves O
Grandvalet O
and O
Yoshua O
Bengio O
. O

2005 O
. O

Semisupervised B-MethodName
Learning I-MethodName
by I-MethodName
Entropy I-MethodName
Minimization I-MethodName
. O
page O
8 O
. O

Jonathan O
Gratch O
, O
Ron O
Artstein O
, O
Gale O
Lucas O
, O
Giota O
Stratou O
, O
Stefan O
Scherer O
, O
Angela O
Nazarian O
, O
Rachel O
Wood O
, O
Jill O
Boberg O
, O
David O
DeVault O
, O
Stacy O
Marsella O
, O
David O
Traum O
, O
Skip O
Rizzo O
, O
and O
Louis O
- O
Philippe O
Morency O
. O
2014 O
. O

The O
Distress O
Analysis O
Interview O
Corpus O
of O
human O
and O
computer O
interviews O
. O

In O
Proceedings O
of O
the O
Ninth O
International O
Conference O
on O
Language O
Resources O
and O
Evaluation O
, O
LREC O
2014 O
, O
Reykjavik O
, O
Iceland O
, O
May O
26 O
- O
31 O
, O
2014 O
, O
pages O
3123–3128 O
. O

Sharath O
Chandra O
Guntuku O
, O
Daniel O
Preotiuc O
- O
Pietro O
, O
Johannes O
C. O
Eichstaedt O
, O
and O
Lyle O
H. O
Ungar O
. O

2019 O
. O

What O
Twitter O
Proﬁle O
and O
Posted O
Images O
Reveal O
about O
Depression O
and O
Anxiety O
. O

In O
Proceedings O
of O
the O
Thirteenth O
International O
Conference O
on O
Web O
and O
Social O
Media O
, O
ICWSM O
2019 O
, O
Munich O
, O
Germany O
, O
June O
11 O
- O
14 O
, O
2019 O
, O
pages O
236–246 O
. O

John O
Heritage O
and O
Jeffrey O
Robinson O
. O

2006 O
. O

The O
Structure O
of O
Patients O
’ O
Presenting O
Concerns O
: O
Physicians O
’ O
Opening O
Questions O
. O

Health O
communication O
, O
19:89 O
– O
102.Zunaira O
Jamil O
, O
Diana O
Inkpen O
, O
Prasadith O
Buddhitha O
, O
and O
Kenton O
White O
. O
2017 O
. O

Monitoring O
Tweets O
for O
Depression O
to O
Detect O
At O
- O
risk O
Users O
. O

In O
Proceedings O
of O
the O
Fourth O
Workshop O
on O
Computational O
Linguistics O
and O
Clinical O
Psychology O
— O
From O
Linguistic O
Signal O
to O
Clinical O
Reality O
, O
pages O
32–40 O
, O
Vancouver O
, O
BC O
. O
Association O
for O
Computational O
Linguistics O
. O

Ulrike O
Klossek O
. O

2015 O
. O

The O
Role O
of O
Goals O
and O
Goal O
Orientation O
as O
Predisposing O
Factors O
for O
Depression O
. O

Ph.D. O
thesis O
, O
University O
of O
Exeter O
. O

Johanna O
K. O
Lake O
, O
Karin O
R. O
Humphreys O
, O
and O
Shannon O
Cardy O
. O
2011 O
. O

Listener O
vs. O
speaker O
- O
oriented O
aspects O
of O
speech O
: O
Studying O
the O
disﬂuencies O
of O
individuals O
with O
autism O
spectrum O
disorders O
. O

Psychonomic O
Bulletin O
& O
Review O
, O
18(1):135–140 O
. O

Shaheen O
E O
Lakhan O
, O
Karen O
Vieira O
, O
and O
Elissa O
Hamlat O
. O

2010 O
. O

Biomarkers O
in O
psychiatry O
: O
drawbacks O
and O
potential O
for O
misuse O
. O

International O
Archives O
of O
Medicine O
, O
3(1):1 O
. O
Nurul O
Lubis O
, O
Sakriani O
Sakti O
, O
Koichiro O
Yoshino O
, O
and O
Satoshi O
Nakamura O
. O
2018 O
. O

Unsupervised O
Counselor O
Dialogue O
Clustering O
for O
Positive O
Emotion O
Elicitation O
in O
Neural O
Dialogue O
System O
. O

In O
Proceedings O
of O
the O
19th O
Annual O
SIGdial O
Meeting O
on O
Discourse O
and O
Dialogue O
, O
pages O
161–170 O
, O
Melbourne O
, O
Australia O
. O

Association O
for O
Computational O
Linguistics O
. O

Mariana O
Rodrigues O
Makiuchi O
, O
Tifani O
Warnita O
, O
Kuniaki O
Uto O
, O
and O
Koichi O
Shinoda O
. O

2019 O
. O

Multimodal B-MethodName
Fusion I-MethodName
of I-MethodName
BERT I-MethodName
- I-MethodName
CNN I-MethodName
and I-MethodName
Gated I-MethodName
CNN I-MethodName
Representations I-MethodName
for O
Depression B-TaskName
Detection I-TaskName
. O

In O
Proceedings O
of O
the O
9th O
International O
on O
Audio O
/ O
Visual O
Emotion O
Challenge O
and O
Workshop O
- O
AVEC O
’ O
19 O
, O
pages O
55–63 O
, O
Nice O
, O
France O
. O

ACM O
Press O
. O

Adria O
Mallol O
- O
Ragolta O
, O
Ziping O
Zhao O
, O
Lukas O
Stappen O
, O
Nicholas O
Cummins O
, O
and O
Bj O
¨orn O
W. O
Schuller O
. O

2019 O
. O

A O
Hierarchical B-MethodName
Attention I-MethodName
Network I-MethodName
- O
Based O
Approach O
for O
Depression B-TaskName
Detection B-TaskName
from O
Transcribed O
Clinical O
Interviews O
. O

In O
Interspeech O
2019 O
, O
pages O
221–225 O
. O

ISCA O
. O

Margaret O
Mcallister O
, O
Beth O
Matarasso O
, O
Barbara O
Dixon O
, O
and O
C O
Shepperd O
. O

2004 O
. O

Conversation O
starters O
: O
reexamining O
and O
reconstructing O
ﬁrst O
encounters O
within O
the O
therapeutic O
relationship O
. O

Journal O
of O
Psychiatric O
and O
Mental O
Health O
Nursing O
, O
11 O
. O
Michelle O
Morales O
, O
Stefan O
Scherer O
, O
and O
Rivka O
Levitan O
. O
2017 O
. O

A O
Cross O
- O
modal O
Review O
of O
Indicators O
for O
Depression O
Detection O
Systems O
. O

In O
Proceedings O
of O
the O
Fourth O
Workshop O
on O
Computational O
Linguistics O
and O
Clinical O
Psychology O
— O
From O
Linguistic O
Signal O
to O
Clinical O
Reality O
, O
pages O
1–12 O
, O
Vancouver O
, O
BC O
. O

Association O
for O
Computational O
Linguistics O
. O

Anastasia O
Pampouchidou O
, O
Kostas O
Marias O
, O
Fan O
Yang O
, O
Manolis O
Tsiknakis O
, O
Olympia O
Simantiraki O
, O
Amir O
Fazlollahi O
, O
Matthew O
Pediaditis O
, O
Dimitris O
Manousos O
, O
Alexandros O
Roniotis O
, O
Georgios O
Giannakakis O
, O
Fabrice O
Meriaudeau O
, O
and O
Panagiotis O
Simos O
. O
2016 O
. O

Depression O
Assessment O
by O
Fusing O
High O
and O
Low O
Level17Features O
from O
Audio O
, O
Video O
, O
and O
Text O
. O

In O
Proceedings O
of O
the O
6th O
International O
Workshop O
on O
Audio O
/ O
Visual O
Emotion O
Challenge O
- O
AVEC O
’ O
16 O
, O
pages O
27–34 O
, O
Amsterdam O
, O
The O
Netherlands O
. O

ACM O
Press O
. O

Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christopher O
D. O
Manning O
. O

2014 O
. O

Glove B-MethodName
: O
Global B-MethodName
Vectors I-MethodName
for I-MethodName
Word I-MethodName
Representation I-MethodName
. O

In O
Proceedings O
of O
the O
2014 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
EMNLP O
2014 O
, O
October O
25 O
- O
29 O
, O
2014 O
, O
Doha O
, O
Qatar O
, O
A O
meeting O
of O
SIGDAT O
, O
a O
Special O
Interest O
Group O
of O
the O
ACL O
, O
pages O
1532–1543 O
. O

Philip O
Resnik O
, O
William O
Armstrong O
, O
Leonardo O
Claudino O
, O
Thang O
Nguyen O
, O
Viet O
- O
An O
Nguyen O
, O
and O
Jordan O
Boyd O
- O
Graber O
. O
2015 O
. O

Beyond O
LDA O
: O
Exploring O
Supervised O
Topic O
Modeling O
for O
Depression O
- O
Related O
Language O
in O
Twitter O
. O

In O
Proceedings O
of O
the O
2nd O
Workshop O
on O
Computational O
Linguistics O
and O
Clinical O
Psychology O
: O
From O
Linguistic O
Signal O
to O
Clinical O
Reality O
, O
pages O
99–107 O
, O
Denver O
, O
Colorado O
. O
Association O
for O
Computational O
Linguistics O
. O

Philip O
Resnik O
, O
Anderson O
Garron O
, O
and O
Rebecca O
Resnik O
. O
2013 O
. O

Using O
Topic O
Modeling O
to O
Improve O
Prediction O
of O
Neuroticism O
and O
Depression O
in O
College O
Students O
. O

InProceedings O
of O
the O
2013 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1348–1353 O
, O
Seattle O
, O
Washington O
, O
USA O
. O

Association O
for O
Computational O
Linguistics O
. O

Stephanie O
Rude O
, O
Eva O
- O
Maria O
Gortner O
, O
and O
James O
Pennebaker O
. O

2004 O
. O

Language O
use O
of O
depressed O
and O
depression O
- O
vulnerable O
college O
students O
. O

Cognition O
& O
Emotion O
, O
18(8):1121–1133 O
. O

H. O
Andrew O
Schwartz O
, O
Johannes O
Eichstaedt O
, O
Margaret O
L. O
Kern O
, O
Gregory O
Park O
, O
Maarten O
Sap O
, O
David O
Stillwell O
, O
Michal O
Kosinski O
, O
and O
Lyle O
Ungar O
. O

2014 O
. O

Towards O
Assessing O
Changes O
in O
Degree O
of O
Depression O
through O
Facebook O
. O

In O
Proceedings O
of O
the O
Workshop O
on O
Computational O
Linguistics O
and O
Clinical O
Psychology O
: O
From O
Linguistic O
Signal O
to O
Clinical O
Reality O
, O
pages O
118–125 O
, O
Baltimore O
, O
Maryland O
, O
USA O
. O

Association O
for O
Computational O
Linguistics O
. O

Chris O
Segrin O
. O

1990 O
. O

A O
meta O
- O
analytic O
review O
of O
social O
skill O
deﬁcits O
in O
depression O
. O

Communication O
Monographs O
, O
57(4):292–308 O
. O

Ivan O
Sekulic O
and O
Michael O
Strube O
. O

2019 O
. O

Adapting O
Deep O
Learning O
Methods O
for O
Mental O
Health O
Prediction O
on O
Social O
Media O
. O

In O
Proceedings O
of O
the O
5th O
Workshop O
on O
Noisy O
User O
- O
generated O
Text O
( O
W O
- O
NUT O
2019 O
) O
, O
pages O
322–327 O
, O
Hong O
Kong O
, O
China O
. O

Association O
for O
Computational O
Linguistics O
. O

Laura O
Smith O
, O
Salvatore O
Giorgi O
, O
Rishi O
Solanki O
, O
Johannes O
Eichstaedt O
, O
H. O
Andrew O
Schwartz O
, O
Muhammad O
Abdul O
- O
Mageed O
, O
Anneke O
Buffone O
, O
and O
Lyle O
Ungar O
. O
2016 O
. O

Does O
‘ O
well O
- O
being O
’ O
translate O
on O
Twitter O
? O

InProceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2042–2047 O
, O
Austin O
, O
Texas O
. O

Association O
for O
Computational O
Linguistics O
. O

Bo O
Sun O
, O
Yinghui O
Zhang O
, O
Jun O
He O
, O
Lejun O
Yu O
, O
Qihua O
Xu O
, O
Dongliang O
Li O
, O
and O
Zhaoying O
Wang O
. O
2017 O
. O

A O
Random B-MethodName
Forest I-MethodName
Regression I-MethodName
Method O
With O
Selected O
- O
Text O
Feature O
For O
Depression B-TaskName
Assessment I-TaskName
. O

In O
Proceedings O
of O
the O
7th O
Annual O
Workshop O
on O
Audio O
/ O
Visual O
Emotion O
Challenge O
- O
AVEC O
’ O
17 O
, O
pages O
61–68 O
, O
Mountain O
View O
, O
California O
, O
USA O
. O
ACM O
Press O
. O

Jackson O
Tolins O
and O
Jean O
E. O
Fox O
Tree O
. O

2014 O
. O

Addressee O
backchannels O
steer O
narrative O
development O
. O

Journal O
of O
Pragmatics O
, O
70:152–164 O
. O

Chrystal O
Vergara O
and O
John O
E. O
Roberts O
. O
2011 O
. O

Motivation O
and O
goal O
orientation O
in O
vulnerability O
to O
depression O
. O

Cognition O
and O
Emotion O
, O
25(7):1281–1290 O
. O
A. O
H. O
Weinberger O
, O
M. O
Gbedemah O
, O
A. O
M. O
Martinez O
, O
D. O
Nash O
, O
S. O
Galea O
, O
and O
R. O
D. O
Goodwin O
. O

2018 O
. O

Trends O
in O
depression O
prevalence O
in O
the O
USA O
from O
2005 O
to O
2015 O
: O
widening O
disparities O
in O
vulnerable O
groups O
. O

Psychological O
Medicine O
, O
48(8):1308–1315 O
. O

Le O
Yang O
, O
Dongmei O
Jiang O
, O
Lang O
He O
, O
Ercheng O
Pei O
, O
Meshia O
C O
´ O
edric O
Oveneke O
, O
and O
Hichem O
Sahli O
. O

2016 O
. O

Decision B-MethodName
Tree I-MethodName
Based I-MethodName
Depression I-MethodName
Classiﬁcation I-MethodName
from O
Audio O
Video O
and O
Language O
Information O
. O

In O
Proceedings O
of O
the O
6th O
International O
Workshop O
on O
Audio O
/ O
Visual O
Emotion O
Challenge O
- O
AVEC O
’ O
16 O
, O
pages O
89–96 O
, O
Amsterdam O
, O
The O
Netherlands O
. O

ACM O
Press O
. O

Le O
Yang O
, O
Dongmei O
Jiang O
, O
Xiaohan O
Xia O
, O
Ercheng O
Pei O
, O
Meshia O
C O
´ O
edric O
Oveneke O
, O
and O
Hichem O
Sahli O
. O
2017 O
. O

Multimodal O
Measurement O
of O
Depression O
Using O
Deep O
Learning O
Models O
. O

In O
Proceedings O
of O
the O
7th O
Annual O
Workshop O
on O
Audio O
/ O
Visual O
Emotion O
Challenge O
- O
AVEC O
’ O
17 O
, O
pages O
53–59 O
, O
Mountain O
View O
, O
California O
, O
USA O
. O
ACM O
Press O
. O

V O
. O

H. O
Yngve O
. O

1970 O
. O

On O
getting O
a O
word O
in O
edgewise O
. O

In O
Chicago O
Linguistics O
Society O
, O
6th O
Meeting O
, O
pages O
567–578 O
. O

Justine O
Zhang O
, O
Robert O
Filbin O
, O
Christine O
Morrison O
, O
Jaclyn O
Weiser O
, O
and O
Cristian O
Danescu O
- O
Niculescu O
- O
Mizil O
. O
2019 O
. O

Finding O
Your O
Voice O
: O
The O
Linguistic O
Development O
of O
Mental O
Health O
Counselors O
. O

In O
Proceedings O
of O
the O
57th O
Conference O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
ACL O
2019 O
, O
Florence O
, O
Italy O
, O
July O
28- O
August O
2 O
, O
2019 O
, O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
946–947 O
. O

A O
Appendices O
A.1 O
Continuous O
representation O
of O
utterances O
For O
continuous O
representation O
using O
the O
GloVe O
model O
, O
we O
use O
the O
pretrained O
100 O
- O
dimensional O
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

The O
representation O
of O
an O
utterance O
is O
computed O
as O
the O
average O
of O
embeddings O
for O
words O
in O
the O
utterance O
, O
with0100used O
to O
represent O
words O
not O
in O
the O
pretrained O
vocabulary O
. O

Based O
on O
the O
pretrained O
vocabulary O
, O
contractions O
( O
e.g. O
“ O
ca O
n’t O
” O
) O
are O
decomposed O
. O

For O
continuous O
representation O
with O
the18 O
BERT B-MethodName
model O
, O
utterances O
are O
split O
into O
sequences O
of O
sub O
- O
word O
tokens O
following O
the O
authors O
’ O
speciﬁcations O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
and O
the O
pretrained O
BERT B-MethodName
( O
Base O
, O
Uncased O
) O
model O
computes O
a O
768dimensional O
position O
- O
dependent O
representation O
. O

A.2 O
Training O
the O
BERT B-MethodName
Model O
For O
the O
BERT B-MethodName
model O
, O
all O
interviews O
were O
truncated O
to O
ﬁt O
the O
maximum O
sequence O
length O
of O
the O
pretrained O
BERT B-MethodName
model O
( O
Base O
, O
Uncased O
): O
512 O
subword O
tokens O
. O

Truncation O
occurs O
by O
alternating O
between O
removing O
prompt O
and O
response O
tokens O
until O
the O
interview O
length O
in O
tokens O
is O
adequate O
. O

Devlin O
et O

al O
. O

( O
2019 O
) O
suggest O
trying O
a O
limited O
number O
of O
combinations O
of O
learning O
rate O
and O
training O
epochs O
to O
optimize O
the O
BERT B-MethodName
classiﬁcation O
model O
. O

Speciﬁcally O
, O
the O
paper O
recommends O
combinations O
of O
2 O
, O
3 O
, O
or O
4 O
epochs O
and O
learning O
rates O
of O
2E-5 O
, O
3E-5 O
, O
and O
5E-5 O
. O

We O
noted O
that O
validation O
and O
test O
scores O
were O
surprisingly O
low O
( O
signiﬁcantly O
below O
random O
) O
using O
these O
combinations O
, O
and O
posited O
that O
the O
small O
number O
of O
suggested O
epochs O
could O
have O
resulted O
from O
the O
authors O
only O
evaluating O
BERT B-MethodName
on O
certain O
types O
of O
datasets O
. O

Accordingly O
, O
we O
evaluated O
up O
to O
50 O
epochs O
with O
the O
suggested O
learning O
rates O
and O
selected O
a O
learning O
rate O
of O
2E-5 O
with O
15 O
epochs O
based O
on O
validation O
results O
. O

A.3 O
Exclusion O
of O
prompts O
The O
goal O
of O
removing O
prompts O
is O
to O
prevent O
a O
classiﬁer O
from O
identifying O
participants O
as O
depressed O
based O
on O
certain O
prompts O
simply O
being O
present O
in O
the O
interview O
, O
such O
as O
“ O
How O
long O
ago O
were O
you O
diagnosed O
[ O
with O
depression O
] O
? O
” O

While O
some O
prompts O
are O
clear O
indicators O
, O
early O
tests O
showed O
that O
even O
with O
these O
prompts O
removed O
, O
other O
prompts O
were O
predictors O
for O
the O
participant O
being O
depressed O
for O
no O
obvious O
reason O
, O
indicating O
a O
bias O
in O
the O
design O
in O
the O
interview O
. O

Rather O
than O
using O
arbitrary O
means O
to O
determine O
whether O
prompts O
could O
be O
predictive O
, O
we O
used O
a O
machine O
- O
learning O
based O
algorithm O
to O
identify O
and O
remove O
predictive O
prompts O
from O
interviews O
. O

After O
the O
division O
of O
interviews O
into O
turns O
as O
described O
in O
Section O
3.1 O
, O
we O
extracted O
the O
set O
of O
distinct O
prompts O
Pdistinct O
from O
all O
interviews O
( O
with O
no O
additional O
preprocessing O
) O
. O

We O
then O
iteratively O
performed O
10 O
logistic O
regression O
experiments O
using O
the O
same O
set O
of O
splits O
described O
in O
Section O
4.2 O
. O

In O
a O
given O
experiment O
, O
each O
interview O
was O
represented O
as O
an O
indicator O
vector O
with O
jPdistinctjdi O
- O
mensions O
, O
such O
that O
position O
pis O
set O
to O
1if O
prompt O
p2f1;;jPdistinctjgis O
present O
in O
the O
interview O
, O
and0otherwise O
. O

Logistic O
Regression O
was O
optimized O
on O
the O
vector O
representations O
for O
the O
training O
interviews O
. O

The O
predicted O
F1 B-MetricName
score O
for O
the O
depressed O
class O
on O
the O
validation O
set O
was O
recorded O
for O
each O
experiment O
. O

The O
average O
weight O
vector O
for O
the O
10 O
Logistic O
regression O
models O
was O
computed O
. O

The O
prompt O
corresponding O
to O
the O
highest O
weight O
was O
removed O
fromPdistinct O
and O
added O
to O
a O
separate O
set O
Dof O
predictive O
prompts O
. O

The O
process O
was O
repeated O
until O
the O
mean O
validation O
F1 B-MetricName
score O
was O
less O
than O
the O
random O
baseline O
for O
the O
dataset O
( O
see O
Section O
4.3 O
) O
. O

The O
ﬁnal O
set O
of O
31 O
prompts O
Dhad O
to O
be O
removed O
from O
the O
dataset O
before O
the O
baselines O
and O
proposed O
approaches O
could O
be O
evaluated O
. O

The O
design O
of O
the O
DAIC B-DatasetName
interview O
posed O
a O
challenge O
, O
however O
: O
the O
same O
prompt O
can O
appear O
in O
many O
interviews O
, O
but O
preceded O
by O
unique O
interjections O
by O
the O
interviewer O
, O
such O
as O
“ O
mhm O
, O
” O
“ O
nice O
, O
” O
and O
“ O
I O
see O
” O
. O

We O
refer O
to O
this O
interjections O
as O
“ O
preﬁxes O
. O
” O

We O
manually O
compiled O
a O
list O
of O
37 O
preﬁxes O
that O
commonly O
reoccur O
in O
interviews O
. O

For O
all O
interviews O
, O
if O
a O
prompt O
fromPdistinct O
occurred O
in O
the O
interview O
after O
preﬁxes O
were O
ignored O
, O
then O
both O
the O
prompt O
and O
its O
corresponding O
response O
were O
removed O
from O
the O
interview O
before O
training O
. O

This O
resulted O
in O
an O
removing O
an O
average O
of O
13.64 O
turns O
from O
each O
interview O
in O
the O
dataset O
. O


Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
351–357 O
July O
5 O
- O
10 O
, O
2020 O
. O

c O

 O
2020 O
Association O
for O
Computational O
Linguistics351Text O
Classiﬁcation O
with O
Negative O
Supervision O
Sora O
Ohashiy O
, O
Junya O
Takayamay O
, O
Tomoyuki O
Kajiwaraz O
, O
Chenhui O
Chuz O
, O
Yuki O
Arasey O
yGraduete O
School O
of O
Information O
Science O
and O
Technology O
, O
Osaka O
University O
zInstitute O
of O
Datability O
Science O
, O
Osaka O
University O
yfohashi.sora O
, O
takayama.junya O
, O
arase O
g@ist.osaka-u.ac.jp O
zfkajiwara O
, O
chu O
g@ids.osaka-u.ac.jp O
Abstract O
Advanced O
pre O
- O
trained O
models O
for O
text O
representation O
have O
achieved O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
various O
text O
classiﬁcation O
tasks O
. O

However O
, O
the O
discrepancy O
between O
the O
semantic O
similarity O
of O
texts O
and O
labelling O
standards O
affects O
classiﬁers O
, O
i.e.leading O
to O
lower O
performance O
in O
cases O
where O
classiﬁers O
should O
assign O
different O
labels O
to O
semantically O
similar O
texts O
. O

To O
address O
this O
problem O
, O
we O
propose O
a O
simple O
multitask O
learning O
model O
that O
uses O
negative O
supervision O
. O

Speciﬁcally O
, O
our O
model O
encourages O
texts O
with O
different O
labels O
to O
have O
distinct O
representations O
. O

Comprehensive O
experiments O
show O
that O
our O
model O
outperforms O
the O
stateof O
- O
the O
- O
art O
pre O
- O
trained O
model O
on O
both O
singleand O
multi O
- O
label O
classiﬁcations O
, O
sentence O
and O
document O
classiﬁcations O
, O
and O
classiﬁcations O
in O
three O
different O
languages O
. O

1 O
Introduction O
Text O
classiﬁcation O
generally O
consists O
of O
two O
processes O
: O
an O
encoder O
that O
converts O
texts O
to O
numerical O
representations O
and O
a O
classiﬁer O
that O
estimates O
hidden O
relations O
between O
the O
representations O
and O
class O
labels O
. O

The O
text O
representations O
are O
generated O
usingN O
- O
gram O
statistics O
( O
Wang O
and O
Manning O
, O
2012 O
) O
, O
word O
embeddings O
( O
Joulin O
et O
al O
. O
, O
2017 O
; O
Wang O
et O
al O
. O
, O
2018 O
) O
, O
convolutional O
neural O
networks O
( O
Kalchbrenner O
et O

al O
. O
, O
2014 O
; O
Zhang O
et O
al O
. O
, O
2015 O
; O
Shen O
et O
al O
. O
, O
2018 O
) O
, O
and O
recurrent O
neural O
networks O
( O
Yang O
et O

al O
. O
, O
2016 O
, O
2018 O
) O

. O

Recently O
, O
powerful O
pre O
- O
trained O
models O
for O
text O
representations O
, O
e.g. O
Bidirectional O
Encoder O
Representations O
from O
Transformers O
( O
BERT O
) O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
have O
shown O
stateof O
- O
the O
- O
art O
performance O
on O
text O
classiﬁcation O
tasks O
using O
only O
the O
simple O
classiﬁer O
of O
a O
fully O
connected O
layer O
. O

However O
, O
a O
problem O
occurs O
when O
a O
classiﬁcation O
task O
is O
adversarial O
to O
text O
encoders O
. O

Encoders O
aim O
to O
represent O
the O
meanings O
of O
texts O
; O
hence O
, O
seman O
- O
Sentence O
Label O
BERT B-MethodName
A O
cold O
is O
a O
legit O
disease O
. O

– O
Cold O
Oh O
my O
god O
! O

I O
caught O
a O
cold O
! O

Both O
sentences O
are O
about O
the O
common O
cold O
. O

Only O
the O
second O
example O
indicates O
that O
the O
writer O
had O
a O
cold O
. O

BERT B-MethodName
misclassiﬁed O
the O
ﬁrst O
sentence O
. O

tically O
similar O
texts O
tend O
to O
have O
closer O
representations O
. O

Meanwhile O
, O
a O
classiﬁer O
should O
distinguish O
subtle O
differences O
that O
lead O
to O
different O
label O
assignments O
, O
although O
the O
texts O
are O
semantically O
similar O
. O

Table O
1 O
shows O
an O
example O
of O
classiﬁcation O
results O
using O
BERT B-MethodName
for O
the O
MedWeb B-DatasetName
dataset O
( O
Wakamiya O
et O
al O
. O
, O
2017 O
) O
. O

This O
task O
requires O
the O
labelling O
of O
a O
disease O
contracted O
by O
the O
writer O
of O
a O
text O
. O

Although O
both O
texts O
in O
Table O
1 O
refer O
to O
the O
common O
cold O
, O
only O
the O
second O
example O
implies O
that O
the O
writer O
had O
a O
cold O
. O

BERT B-MethodName
mistakenly O
labelled O
both O
texts O
asCold1 O
, O
likely O
owing O
to O
their O
semantic O
relatedness O
. O

When O
the O
standard O
of O
class O
label O
assignments O
disagrees O
with O
the O
semantic O
similarity O
, O
the O
classiﬁer O
tends O
to O
be O
error O
- O
prone O
owing O
to O
the O
excessive O
effects O
of O
the O
semantic O
similarity O
. O

To O
address O
this O
problem O
, O
we O
propose O
utilizing O
negative O
examples O
, O
i.e.texts O
with O
different O
labels O
, O
to O
enable O
negative O
supervision O
of O
the O
encoder O
for O
generating O
distinct O
representations O
for O
each O
class O
. O

In O
this O
study O
, O
we O
design O
a O
simple O
multitask O
learning O
model O
that O
trains O
two O
models O
simultaneously O
with O
a O
shared O
text O
encoder O
. O

The O
ﬁrst O
model O
learns O
an O
ordinary O
classiﬁcation O
task O
( O
herein O
referred O
to O
as O
the O
main O
task O
) O
. O

Meanwhile O
, O
the O
second O
model O
encourages O
representations O
with O
different O
class O
labels O
to O
be O
distinct O
( O
herein O
referred O
to O
as O
the O
auxiliary O
1We O
use O
the O
typewriter O
font O
to O
indicate O
a O
class O
label O
throughout O
this O
paper.352 O
EncoderI O
caught O
a O
cold O
. O

A O
cold O
is O
a O
legit O
diseaseI’m O
coughingClassifierDiscriminatorColdMain O
TaskAuxiliary O
TaskFigure O
1 O
: O
Our O
model O
consists O
of O
a O
classiﬁer O
, O
discriminator O
, O
and O
shared O
text O
encoder O
. O

The O
main O
task O
learns O
classiﬁcation O
, O
while O
the O
auxiliary O
task O
gives O
negative O
supervision O
to O
generate O
distinct O
representations O
for O
sentences O
with O
different O
labels O
. O
task O
) O
. O

We O
empirically O
show O
the O
effectiveness O
of O
our O
model O
using O
the O
following O
standard O
benchmarks O
of O
ﬁve O
single O
- O
label O
and O
four O
multi O
- O
label O
classiﬁcation O
datasets O
. O

This O
study O
has O
two O
main O
contributions O
. O

Our O
multi O
- O
tasking O
learning O
model O
consistently O
outperforms O
the O
state O
- O
of O
- O
the O
- O
art O
model O
in O
terms O
of O
both O
single O
and O
multi O
- O
label O
classiﬁcations O
, O
sentence O
and O
document O
classiﬁcations O
, O
and O
classiﬁcations O
in O
three O
languages O
. O

Our O
model O
is O
simple O
and O
easily O
applicable O
to O
any O
text O
encoders O
and O
classiﬁers O
. O

2 O
Multitask O
Learning O
Framework O
Figure O
1 O
shows O
an O
overview O
of O
our O
multitask O
learning O
framework O
that O
consists O
of O
main O
and O
auxiliary O
tasks O
. O

Herein O
, O
we O
refer O
to O
the O
model O
for O
the O
main O
task O
as O
a O
classiﬁer O
and O
the O
model O
for O
the O
auxiliary O
task O
as O
a O
discriminator O
. O

The O
overall O
loss O
function O
L O
sums O
the O
loss O
of O
the O
main O
task O
Lmand O
that O
of O
the O
auxiliary O
taskLa O
: O
L O
= O
Lm+La O
: O
The O
classiﬁer O
and O
discriminator O
share O
and O
jointly O
optimize O
the O
text O
encoder O
, O
which O
encodes O
an O
input O
text O
into O
ad O
- O
dimensional O
vector O
v2Rd O
. O

In O
this O
paper O
, O
we O
use O
the O
terms O
of O
text O
and O
representation O
interchangeably O
when O
the O
intention O
is O
obvious O
from O
the O
context.2.1 O
Main O
Task O
The O
main O
task O
is O
the O
primary O
classiﬁcation O
task O
to O
optimize O
. O

We O
use O
a O
simple O
classiﬁer O
as O
employed O
in O
BERT B-MethodName
. O

The O
classiﬁer O
takes O
an O
input O
vector O
vm O
and O
calculates O
probabilities O
p2RjCjto O
assign O
a O
set O
of O
class O
labels O
C O
: O
p O
= O
g(Wvm+b O
) O
; O
where O
W2RjCjdandb2RjCjare O
parameters O
of O
the O
classiﬁer O
, O
in O
which O
jjcounts O
the O
number O
of O
elements O
in O
a O
set O
. O

Forg O
, O
we O
employ O
a O
softmax O
function O
for O
singlelabel O
classiﬁcation O
and O
a O
sigmoid O
function O
for O
multilabel O
classiﬁcation O
. O

In O
both O
cases O
, O
Lmis O
a O
negative O
log O
- O
likelihood O
of O
predictions O
. O

2.2 O
Auxiliary O
Task O
The O
auxiliary O
task O
aims O
to O
give O
negative O
supervision O
to O
encourage O
distinct O
representations O
of O
texts O
with O
different O
labels O
. O

The O
discriminator O
samples O
a O
set O
of O
ntextsva O
1;:::;va O
nfrom O
the O
same O
batch O
as O
vm O
, O
all O
of O
which O
have O
different O
labels O
from O
vm O
. O

To O
encourage O
these O
texts O
to O
have O
distinct O
representations O
, O
we O
designed O
the O
loss O
function O
Laas O
La=1 O
nX O
ism O
i O
; O
sm O
j= O
1 O
+ O
cossim O
( O
vm;va O
j O
) O
; O
where O
the O
cossim O
function O
computes O
the O
cosine O
similarity O
between O
the O
representations O
. O

This O
loss O
function O
intuitively O
encourages O
the O
negative O
examples O
to O
have O
smaller O
cosine O
similarities O
. O

3 O
Experiments O
We O
conducted O
a O
comprehensive O
evaluation O
to O
investigate O
the O
performance O
of O
our O
model O
in O
terms O
of O
( O
a O
) O
single- O
and O
multi O
- O
label O
classiﬁcations O
, O
( O
b O
) O
sentence- O
and O
document O
- O
level O
classiﬁcation O
, O
and O
( O
c O
) O
different O
languages O
. O

We O
collected O
the O
standard O
evaluation O
datasets O
from O
heterogeneous O
sources O
, O
as O
summarised O
in O
Table O
2 O
. O
3.1 O
Single O
- O
Label O
Classiﬁcation O
As O
datasets O
assigned O
single O
labels O
to O
sentences O
, O
we O
used O
the O
following O
datasets O
from O
the O
SentEval B-DatasetName
( O
Conneau O
and O
Kiela O
, O
2018)2benchmark O
. O

MR B-DatasetName
Binary O
classiﬁcation O
of O
sentiment O
polarity O
of O
movie O
reviews O
. O

2https://github.com/facebookresearch/SentEval353Input O

The O
upper O
group O
is O
single O
- O
label O
classiﬁcation O
tasks O
, O
whereas O
the O
bottom O
group O
is O
multi O
- O
label O
classiﬁcation O
tasks O
. O

CR B-DatasetName
Binary O
classiﬁcation O
of O
sentiment O
polarity O
of O
product O
reviews O
. O

SST-5 B-DatasetName
Multi O
- O
class O
classiﬁcation O
of O
the O
ﬁnegrained O
sentiment O
polarity O
of O
movie O
reviews O
. O

Labels O
are O
Positive O
, O
Somewhat O
Positive O
, O
Neutral O
, O
Somewhat O
Negative O
, O
andNegative O
. O

TREC B-DatasetName
Multi O
- O
class O
classiﬁcation O
of O
question O
types.3 O
SUBJ B-DatasetName
Binary O
classiﬁcation O
of O
subjectivity O
. O

Because O
the O
MR B-DatasetName
, O
CR B-DatasetName
, O
and O
SUBJ B-DatasetName
datasets O
do O
not O
separate O
validation O
and O
test O
sets O
, O
we O
split O
20 O
% O
of O
each O
dataset O
for O
testing O
and O
20 O
% O
of O
the O
remainder O
for O
validation O
. O

The O
evaluation O
metric O
for O
these O
single B-MetricName
- I-MetricName
label I-MetricName
classiﬁcation I-MetricName
tasks O
is O
accuracy B-MetricName
. O

3.2 O
Multi B-MetricName
- I-MetricName
Label I-MetricName
Classiﬁcation I-MetricName
We O
used O
the O
NTCIR-13 B-DatasetName
MedWeb I-DatasetName
( O
Wakamiya O
et O
al O
. O
, O
2017 O
) O
and O
arXiv B-DatasetName
datasets O
( O
Yang O
et O
al O
. O
, O
2018 O
) O
for O
multi B-MetricName
- I-MetricName
label I-MetricName
classiﬁcation I-MetricName
. O

MedWeb B-DatasetName
Assigning O
disease O
labels O
that O
a O
writer O
of O
a O
sentence O
contracted.4 O
arXiv B-DatasetName
Classiﬁcation O
of O
areas O
of O
abstracts O
extracted O
from O
papers O
in O
the O
computer O
science O
ﬁeld.5 O
Because O
the O
arXiv B-DatasetName
dataset O
released O
by O
Yang O
et O

al O
. O
( O
2018 O
) O
removed O
all O
line O
breaks O
, O
we O
created O
one O
ourselves O
. O

We O
collected O
abstracts O
and O
categories O
of O
papers O
submitted O
to O
arXiv O
from O
January O
1st O
, O
2019 O
to O
June O
4th O
, O
2019 O
using O
arXiv O
API.6 O
3All O
question O
types O
are O
in O
the O
appendix O
. O

4http://research.nii.ac.jp/ntcir/permission/ntcir-13/permja-MedWeb.html O
5The O
labels O
of O
these O
two O
tasks O
are O
in O
the O
appendix O
. O

6https://arxiv.org/help/apiThe O
evaluation O
metric O
for O
multi B-MetricName
- I-MetricName
label I-MetricName
classiﬁcation I-MetricName
is O
Exact B-MetricName
- I-MetricName
Match I-MetricName
. O

ExactMatch B-MetricName
= O
1 O
MMX O
i=1I(yi=^yi O
) O
; O
whereyiand^yiare O
one O
- O
hot O
vectors O
of O
gold O
and O
predicted O
labels O
, O
respectively O
, O
and O
I(x)takes O
1when O
xis O
true O
and O
takes O
0otherwise O
. O

Mis O
the O
size O
of O
a O
test O
set O
. O

3.3 O
Settings O
As O
a O
text O
encoder O
, O
we O
employed O
BERT B-MethodName
and O
a O
Hierarchical B-MethodName
Attention I-MethodName
Network I-MethodName
( O
HAN B-MethodName
) O
( O
Yang O
et O
al O
. O
, O
2016 O
) O
for O
generating O
sentence O
and O
document O
representation O
, O
respectively O
. O

For O
BERT B-MethodName
, O
we O
used O
the O
pre O
- O
trained O
BERT B-MethodName
- O
base7(d= O
768 B-HyperparameterValue
) O
. O

We O
implemented O
the O
HAN O
following O
Yang O
et O
al O
. O

( O
2016 O
) O
who O
used O
the O
bi O
- O
directional O
Gated O
Recurrent O
Unit O
as O
the O
encoder O
with O
the O
hidden B-HyperparameterName
size I-HyperparameterName
of O
50 B-HyperparameterValue
(d= O
50 B-HyperparameterValue
) O
. O

The O
embedding O
layer O
of O
the O
HAN O
was O
initialised O
using O
CBOW O
( O
Mikolov O
et O
al O
. O
, O
2013 O
) O
embeddings O
( O
with O
dimensions B-HyperparameterName
of O
200 B-HyperparameterValue
) O
, O
which O
were O
trained O
using O
negative O
sampling O
on O
the O
training O
and O
development O
sets O
of O
each O
task O
. O

For O
systematic O
comparison O
, O
we O
investigated O
the O
performance O
of O
the O
following O
models O
. O

As O
a O
baseline O
, O
we O
compared O
models O
that O
conduct O
only O
the O
main O
task O
( O
referred O
to O
as O
Baseline O
) O
, O
which O
corresponds O
to O
the O
ﬁne O
- O
tuned O
BERT B-MethodName
- I-MethodName
base I-MethodName
for O
sentence B-MetricName
classiﬁcation I-MetricName
and O
the O
original O
HAN B-MethodName
for O
document B-MetricName
classiﬁcation I-MetricName
. O

Note O
that O
this O
BERT B-MethodName
baseline O
signiﬁcantly O
outperforms O
previous O
state O
- O
of O
- O
the O
- O
art O
methods O
, O
which O
were O
also O
compared O
in O
the O
experiment O
. O

To O
investigate O
the O
effects O
of O
negative O
supervision O
at O
7https://github.com/google-research/bert354 O
MR B-DatasetName
CR B-DatasetName
SST-5 B-DatasetName
TREC B-DatasetName

SUBJ B-DatasetName
MedWeb B-DatasetName
arXiv B-DatasetName

The O
best O
scores O
are O
presented O
in O
the O
bold O
font O
, O
and O
scores O
higher O
than O
the O
Baseline O
are O
underlined O
. O

Our O
models O
consistently O
outperform O
the O
baseline O
and O
ACE B-MethodName
, O
which O
indicates O
the O
effectiveness O
of O
negative O
supervision O
through O
the O
auxiliary O
task O
. O

Previous O
SOTA O
results O
are O
reported O
by O
Du O
et O
al O
. O

( O
2019 O
) O
( O
MR B-DatasetName
) O
, O

Zhou O
et O

al O
. O

( O
2016 O
) O
( O
CR B-DatasetName
, O
SST-5 B-DatasetName
) O
, O
Howard O
and O
Ruder O
( O
2018 O
) O
( O
TREC B-DatasetName
) O
, O

Zhao O
et O
al O
. O
( O
2015 O
) O
( O
SUBJ B-DatasetName
) O
and O
Iso O
et O

al O
. O

( O
2017 O
) O
( O
MedWeb B-DatasetName
) O
. O

the O
auxiliary O
task O
, O
we O
compared O
our O
model O
to O
one O
that O
predicts O
a O
sentence O
with O
the O
same O
label O
. O

Accurately O
, O
this O
model O
conducts O
classiﬁcation O
given O
cosine O
similarities O
using O
cross O
entropy O
loss O
( O
referred O
to O
as O
ACE B-MethodName
( O
the O
auxiliary B-MethodName
task I-MethodName
with I-MethodName
cross I-MethodName
entropy I-MethodName
loss I-MethodName
) O
) O
. O

Furthermore O
, O
we O
evaluated O
two O
variations O
of O
our O
model O
. O

The O
ﬁrst O
purely O
gives O
negative O
supervision O
, O
i.e. O
, O
the O
auxiliary O
task O
only O
encourages O
the O
generation O
of O
distinct O
representation O
to O
negative O
examples O
, O
as O
described O
in O
Section O
2.2 O
( O
referred O
to O
as O
AAN B-MethodName
( O
the O
auxiliary B-MethodName
task I-MethodName
using I-MethodName
all I-MethodName
negative I-MethodName
examples I-MethodName
) O
) O
. O

The O
second O
uses O
the O
following O
margin O
- O
based O
loss O
asLawith O
a O
positive O
example O
as O
well O
as O
negative O
examples O
: O
La= O
max0 O
@0; sm O
k+1 O
n 1X O
i6 O
= O
ksm O
i1 O
A O
; O
where O
thek O
- O
th O
sample O
is O
selected O
to O
have O
the O
same O
label O
as O
the O
input O
vmto O
the O
main O
task O
and O
is O
the O
margin B-HyperparameterName
empirically O
set O
to O
0.4 B-HyperparameterValue
(referred O
to O
as O
AM B-MethodName
( O
the O
auxiliary B-MethodName
task I-MethodName
with I-MethodName
the I-MethodName
margin I-MethodName
- I-MethodName
based I-MethodName
loss I-MethodName
) O
) O
. O

The O
intuition O
is O
that O
texts O
with O
the O
same O
label O
should O
have O
more O
similar O
representations O
than O
negative O
examples O
. O

We O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
of O
the O
main O
task O
to O
16 B-HyperparameterValue
and O
set O
n B-HyperparameterName
to O
four B-HyperparameterValue
in O
the O
auxiliary O
task O
, O
which O
performed O
best O
on O
the O
validation O
set O
of O
the O
MR B-DatasetName
task O
. O

We O
used O
early O
stopping O
to O
cease O
training O
when O
the O
validation O
score O
did O
not O
improve O
for O
10 B-HyperparameterValue
epochs O
. O

The O
optimization B-HyperparameterName
algorithm I-HyperparameterName
used O
was O
Adam O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
 O
1= O
0.999 B-HyperparameterValue
and O
 O
2= O
0.9 B-HyperparameterValue
. O

For O
each O
task O
, O
we O
selected O
the O
best O
learning B-HyperparameterName
rate I-HyperparameterName
among O
1e-5 B-HyperparameterValue
, O
3e-5 B-HyperparameterValue
, O
and O
5e-5 B-HyperparameterValue
using O
the O
validation O
set O
. O

To O
alleviate O
randomness O
owing O
to O
initialization O
, O
wereported O
average O
scores O
of O
10time O
trials O
excluding O
the O
best O
and O
worst O
results O
. O

3.4 O
Results O
Table O
3 O
shows O
the O
performance O
of O
all O
compared O
methods O
as O
well O
as O
the O
performance O
of O
the O
previous O
state O
- O
of O
- O
the O
- O
art O
methods O
( O
referred O
to O
as O
SOTA O
) O
. O

The O
results O
in O
Table O
3 O
indicate O
that O
our O
models O
of O
AM B-MethodName
and O
AAN B-MethodName
consistently O
outperform O
the O
strong O
Baselines O
on O
both O
single B-MetricName
- I-MetricName
label I-MetricName
and O
multi B-MetricName
- I-MetricName
label I-MetricName
classiﬁcations I-MetricName
, O
sentence B-MetricName
and I-MetricName
document I-MetricName
classiﬁcations I-MetricName
, O
and O
classiﬁcations B-MetricName
in I-MetricName
different I-MetricName
languages I-MetricName
. O

Most O
notably O
, O
our O
models O
are O
effective O
even O
for O
multi B-MetricName
- I-MetricName
label I-MetricName
classiﬁcation I-MetricName
, O
which O
is O
more O
challenging O
than O
its O
single O
- O
label O
counterpart O
. O

In O
general O
, O
AAN B-MethodName
achieved O
greater O
performance O
than O
AM B-MethodName
. O

However O
, O
their O
effectiveness O
turned O
out O
to O
be O
task O
- O
dependent O
. O

Unlike O
AM B-MethodName
and O
AAN B-MethodName
, O
ACE B-MethodName
degraded O
the O
performance O
of O
the O
Baseline O
except O
for O
the O
MedWeb B-DatasetName
Japanese O
task O
. O

This O
result O
shows O
that O
simple O
multitask O
learning O
is O
ineffective O
and O
that O
our O
design O
using O
negative O
supervision O
is O
crucial O
. O

SST-5 B-DatasetName
is O
an O
exception O
wherein O
our O
models O
degraded O
the O
performance O
of O
the O
Baseline O
. O

We O
hypothesise O
that O
this O
is O
because O
its O
class O
labels O
are O
gradational O
, O
e.g. O
Somewhat O
Negative O
is O
closer O
toNegative O
rather O
than O
Positive O
sentences O
. O

AM B-MethodName
and O
AAN B-MethodName
treat O
all O
negative O
examples O
equally O
, O
disregarding O
variables O
, O
such O
as O
relations O
between O
class O
labels O
. O

Future O
work O
should O
focus O
on O
the O
semantic O
relations O
among O
class O
labels O
in O
the O
auxiliary O
task O
. O

4 O
Related O
Work O
Multitask O
learning O
has O
been O
employed O
to O
improve O
the O
performance O
of O
text B-MetricName
classiﬁcation I-MetricName
( O
Liu O
et O
al O
. O
,3552019 O
; O
Xiao O
et O
al O
. O
, O
2018 O
) O
. O

Previous O
studies O
aimed O
to O
improve O
multiple O
tasks O
; O
hence O
, O
they O
required O
multiple O
sets O
of O
annotated O
datasets O
. O

In O
contrast O
, O
our O
method O
does O
not O
require O
any O
extra O
labelled O
datasets O
and O
is O
easily O
applicable O
to O
various O
classiﬁcation O
tasks O
. O

The O
methods O
proposed O
in O
Arase O
and O
Tsujii O
( O
2019 O
) O
and O
Phang O
et O
al O
. O

( O
2018 O
) O
improved O
the O
BERT B-MethodName
classiﬁcation O
performance O
by O
further O
training O
the O
pre O
- O
trained O
model O
using O
natural O
language O
inference O
and O
paraphrase O
recognition O
. O

Similar O
to O
multitask O
learning O
, O
both O
methods O
require O
an O
additional O
largescale O
labelled O
dataset O
. O

Furthermore O
, O
these O
previous O
studies O
revealed O
that O
the O
similarity O
of O
tasks O
in O
training O
affects O
the O
models O
’ O
ﬁnal O
performance O
( O
Xiao O
et O
al O
. O
, O
2018 O
; O
Arase O
and O
Tsujii O
, O
2019 O
) O
. O

Our O
method O
achieved O
consistent O
improvements O
across O
tasks O
, O
indicating O
its O
wider O
applicability O
. O

5 O
Conclusion O
In O
this O
paper O
, O
we O
proposed O
a O
simple O
multitask O
learning O
model O
that O
uses O
negative O
supervision O
to O
generate O
distinct O
representations O
for O
texts O
with O
different O
labels O
. O

Comprehensive O
evaluation O
empirically O
conﬁrmed O
that O
our O
model O
consistently O
outperformed O
BERT B-MethodName
and O
HAN B-MethodName
models O
on O
single- B-MetricName
and I-MetricName
multi I-MetricName
- I-MetricName
label I-MetricName
classiﬁcations I-MetricName
, O
sentence B-MetricName
and I-MetricName
document I-MetricName
classiﬁcations I-MetricName
, O
and O
classiﬁcations B-MetricName
in I-MetricName
different I-MetricName
languages I-MetricName
. O

Our O
multitask O
learning O
model O
provides O
a O
general O
framework O
that O
is O
easily O
applicable O
to O
existing O
text B-MetricName
classiﬁcation I-MetricName
models O
. O

In O
future O
work O
, O
we O
will O
examine O
semantic O
relations O
between O
class O
labels O
in O
the O
auxiliary O
task O
. O

Moreover O
, O
we O
will O
adapt O
our O
model O
to O
text O
generation O
tasks O
. O

We O
expect O
that O
our O
model O
will O
encourage O
a O
generation O
model O
to O
generate O
texts O
with O
different O
labels O
, O
such O
as O
styles O
, O
have O
distinct O
representations O
, O
which O
will O
result O
in O
class O
speciﬁc O
expressions O
. O

Acknowledgement O
This O
work O
was O
supported O
by O
JST O
AIP O
- O
PRISM O
Grant O
Number O
JPMJCR18Y1 O
, O
Japan O
. O

Would O
you O
Rather O
? O

A O
New O
Benchmark O
for O
Learning O
Machine O
Alignment O
with O
Cultural O
Values O
and O
Social B-TaskName
Preferences I-TaskName
yYi O
Tay,[Donovan O
Ong,]Jie O
Fu O
, O
yAlvin O
Chan,[Nancy O

F. O
Chen O
 O
 O
Luu O
Anh O
Tuan,]zChristopher O
Pal O
yNanyang O
Technological O
University O
, O
Singapore O
] O
Polytechnique O
Montreal O
, O
Mila O
, O
zCanada O
CIFAR O
AI O
Chair O
[ O
A*STAR O
, O
Singapore,MIT O
CSAIL O
, O
 O
VinAI O
Research O
ytay017@gmail.com O
, O
ongyl@i2r.a-star.edu.sg O
Abstract O
Understanding O
human O
preferences O
, O
along O
with O
cultural O
and O
social O
nuances O
, O
lives O
at O
the O
heart O
of O
natural B-TaskName
language I-TaskName
understanding I-TaskName
. O

Concretely O
, O
we O
present O
a O
new O
task O
and O
corpus O
for O
learning O
alignments O
between O
machine O
and O
human O
preferences O
. O

Our O
newly O
introduced O
problem O
is O
concerned O
with O
predicting B-TaskName
the I-TaskName
preferable I-TaskName
options I-TaskName
from O
two O
sentences O
describing O
scenarios O
that O
may O
involve O
social O
and O
cultural O
situations O
. O

Our O
problem O
is O
framed O
as O
a O
natural B-TaskName
language I-TaskName
inference I-TaskName
task O
with O
crowd O
- O
sourced O
preference O
votes O
by O
human O
players O
, O
obtained O
from O
a O
gamiﬁed O
voting O
platform O
. O

We O
benchmark O
several O
state O
- O
of O
- O
the O
- O
art O
neural O
models O
, O
along O
with O
BERT B-MethodName
and O
friends O
on O
this O
task O
. O

Our O
experimental O
results O
show O
that O
current O
state O
- O
ofthe O
- O
art O
NLP O
models O
still O
leave O
much O
room O
for O
improvement O
. O

1 O
Introduction O
The O
ability O
to O
understanding O
social O
nuances O
and O
human O
preferences O
is O
central O
to O
natural B-TaskName
language I-TaskName
understanding I-TaskName
. O

This O
also O
enables O
better O
alignment O
of O
machine O
learning O
models O
with O
human O
values O
, O
eventually O
leading O
to O
better O
human O
- O
compatible O
AI O
applications O
( O
Peterson O
et O
al O
. O
, O
2019 O
; O
Leslie O
, O
2019 O
; O
Rosenfeld O
and O
Kraus O
, O
2018 O
; O
Amodei O
et O
al O
. O
, O
2016 O
; O
Russell O
and O
Norvig O
, O
2016 O
) O
. O

There O
exist O
a O
plethora O
of O
work O
on O
studying O
optimal O
decision O
- O
making O
under O
a O
variety O
of O
situations O
( O
Edwards O
, O
1954 O
; O
Bottom O
, O
2004 O
; O
Plonsky O
et O
al O
. O
, O
2019 O
; O
Peterson O
et O
al O
. O
, O
2019 O
) O
. O

On O
the O
other O
hand O
, O
cognitive O
models O
of O
human O
decision O
- O
making O
are O
usually O
based O
on O
small O
datasets O
( O
Peterson O
et O
al O
. O
, O
2019 O
) O
. O

Furthermore O
, O
these O
studies O
tend O
to O
only O
consider O
individuals O
in O
isolation O
. O

In O
contrast O
, O
we O
investigate O
the O
inﬂuence O
of O
cultural O
and O
social O
nuances O
for O
choice O
prediction O
at O
scale O
. O

In O
other O
words O
, O
we O
study O
the O
social B-TaskName
preference I-TaskName
as O
a O
whole O
, O
First O
two O
authors O
contributed O
equallynot O
those O
of O
an O
individual O
in O
isolation O
, O
which O
is O
arguably O
more O
challenging O
and O
largely O
unexplored O
. O

In O
this O
work O
, O
we O
propose O
a O
new O
benchmark O
dataset O
with O
a O
large O
number O
of O
200k O
data O
points O
, O
Machine O
Alignment O
with O
Cultural O
values O
and O
Social B-TaskName
preferences I-TaskName
( O
MACS B-DatasetName
) O
, O
for O
learning O
AI O
alignment O
with O
humans O
. O

Our O
dataset O
is O
based O
on O
a O
popular O
gamiﬁed O
voting O
platform O
, O
namely O
the O
game O
of O
‘ O
would O
you O
rather O
? O
’ O
. O

In O
this O
game O
, O
participants O
are O
given O
two O
choices O
and O
vote O
for O
the O
more O
preferable O
option O
. O

Examples O
from O
our O
dataset O
can O
be O
found O
at O
Table O
1 O
. O

To O
the O
best O
of O
our O
knowledge O
, O
our O
work O
is O
the O
ﬁrst O
work O
to O
incorporate O
voting O
- O
based O
language O
games O
as O
a O
language O
understanding O
benchmark O
. O

In O
many O
ways O
, O
our O
benchmark O
dataset O
is O
reminiscent O
of O
the O
natural B-TaskName
language I-TaskName
inference I-TaskName
problem O
( O
MacCartney O
, O
2009 O
; O
Bowman O
et O
al O
. O
, O
2015 O
) O
, O
social O
commonsense O
reasoning O
( O
Sap O
et O
al O
. O
, O
2019 O
) O
or O
other O
natural B-TaskName
language I-TaskName
understanding I-TaskName
problems O
( O
Wang O
et O
al O
. O
, O
2018 O
; O
Zellers O
et O
al O
. O
, O
2018 O
) O
. O

To O
this O
end O
, O
our O
problem O
is O
framed O
in O
a O
way O
that O
enables O
convenient O
benchmarking O
of O
existing O
state O
- O
of O
- O
the O
- O
art O
NLU O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
or O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
. O

That O
said O
, O
unlike O
many O
NLU O
datasets O
that O
rely O
on O
few O
annotators O
, O
the O
key O
differentiator O
lies O
in O
the O
fact O
that O
our O
dataset O
aggregates O
across O
hundreds O
or O
thousands O
and O
beyond O
for O
each O
data O
point O
. O

Options O
are O
also O
crowd O
- O
sourced O
and O
gamiﬁed O
which O
may O
encourage O
less O
monotonic O
samples O
, O
ie O
. O
, O
encouraging O
players O
to O
come O
up O
with O
questionss O
that O
are O
difﬁcult O
for O
other O
players O
. O

Additionally O
, O
our O
dataset O
comprises O
of O
country O
- O
level O
statistics O
, O
which O
enable O
us O
to O
perform O
cultural O
- O
level O
prediction O
of O
preferences O
. O

Our O
Contributions O
All O
in O
all O
, O
the O
prime O
contribution O
of O
this O
work O
is O
as O
follows O
: O
We O
propose O
a O
new O
NLU O
benchmark O
based O
onan O
online O
gamiﬁed O
voting O
platform O
. O

We O
propose O
several O
ways O
to O
formulate O
the O
problem O
, O
including O
absolute O
and O
relative O
preference B-TaskName
prediction I-TaskName
. O

We O
also O
introduce O
a O
cultural O
- O
level O
NLU O
problem O
formulation O
. O

We O
investigate O
state O
- O
of O
- O
the O
- O
art O
NLU O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
RobERTA B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
and O
XLNET B-MethodName
( O
Yang O
et O
al O
. O
, O
2019 O
) O
on O
this O
dataset O
. O

Empirical O
results O
suggests O
that O
our O
benchmark O
is O
reasonably O
difﬁcult O
and O
there O
is O
a O
huge O
room O
for O
improvement O
. O

2 O
Learning O
Alignment O
with O
Human O
Preferences O
This O
section O
describes O
the O
proposed O
dataset O
and O
problem O
formulation O
. O

2.1 O
Dataset O
We O
look O
to O
crowdsourcing O
platforms O
to O
construct O
our O
dataset O
. O

Our O
dataset O
is O
constructed O
from O
https://www.rrrather.com/ O
, O
an O
online O
platform1for O
gamiﬁed O
voting O
. O

The O
platform O
is O
modeled O
after O
the O
famous O
internet O
game O
- O
would O
you O
rather O
? O

, O
which O
pits O
two O
supposedly O
comparable O
choices O
together O
. O

Whenever O
a O
player O
votes O
, O
their O
vote O
is O
recorded O
in O
the O
system O
. O

Players O
generally O
vote O
to O
see O
how O
well O
their O
vote O
aligns O
with O
the O
majority O
and O
consensus O
with O
everyone O
else O
. O

We O
provide O
samples O
of O
the O
problem O
space O
in O
Table O
1 O
. O

We O
crawled O
data O
from O
the O
said O
platform O
and O
ﬁltered O
away O
posts O
with O
less O
than O
500total O
votes O
. O

In O
total O
, O
we O
amassed O
194,525 O
data O
points O
, O
which O
we O
split O
into O
train O
/ O
dev O
/ O
test O
splits O
in O
an O
80/10/10 O
fashion O
. O

Dataset O
statistics O
are O
provided O
in O
Table O
2 O
. O
Train O
Dev O
Test O
Total O
Data O
155,621 O
19,452 O
19,452 O
194,525 O
` O
max O
678 O
351 O
298 O
` O
mean O
8 O
8 O
8 O
` O
min O
1 O
2 O
2 O
Table O
2 O
: O
Dataset O
statistics O
of O
the O
MACS B-DatasetName
dataset O
. O

2.2 O
Why O
is O
this O
interesting O
? O

This O
section O
outlines O
the O
beneﬁts O
of O
our O
proposed O
dataset O
as O
a O
language O
understanding O
benchmark O
. O

1The O
authors O
have O
obtained O
written O
permission O
from O
the O
owner O
of O
the O
platform O
to O
crawl O
and O
use O
their O
data O
for O
academic O
research O
. O

The O
questions O
, O
answers O
or O
discussions O
do O
not O
represent O
opinions O
of O
the O
authors O
in O
this O
paper.(1 O
) O
Understanding O
before O
Interaction O
. O

In O
our O
dataset O
and O
problem O
formulation O
, O
complex O
understanding O
of O
each O
option O
text O
is O
often O
required O
ﬁrst O
before O
modeling O
the O
relative O
preference O
between O
two O
options O
. O

This O
is O
unlike O
NLI O
or O
questionanswering O
based O
NLU O
benchmarks O
, O
where O
matching O
signals O
can O
be O
used O
to O
predict O
the O
outcome O
easily O
. O

In O
our O
dataset O
and O
task O
, O
it O
is O
imperative O
that O
any O
form O
of O
word O
overlap O
can O
be O
hardly O
used O
to O
determine O
the O
outcome O
. O

( O
2 O
) O
A O
good O
coverage O
of O
social B-TaskName
preferences I-TaskName
. O

Upon O
closer O
inspection O
of O
our O
proposed O
benchmark O
, O
we O
ﬁnd O
there O
is O
a O
good O
representation O
of O
samples O
which O
cover O
social O
and O
cultural O
themes O
. O

Social B-TaskName
preferences I-TaskName
( O
such O
as O
the O
preference O
of O
brands O
) O
are O
captured O
in O
samples O
such O
as O
example O
( O
6 O
) O
. O

( O
3 O
) O
Completely O
natural O
. O

Our O
MACS B-DatasetName
dataset O
completely O
exists O
in O
the O
wild O
naturally O
. O

This O
is O
unlike O
datasets O
that O
have O
to O
be O
annotated O
by O
mechanical O
turkers O
or O
paid O
raters O
. O

In O
general O
, O
there O
is O
a O
lack O
of O
incentives O
for O
turkers O
to O
provide O
highquality O
ratings O
, O
which O
often O
results O
in O
problems O
such O
as O
annotation O
artifacts O
. O

Unlike O
these O
datasets O
, O
our O
MACS B-DatasetName
dataset O
completely O
exists O
in O
the O
wild O
naturally O
. O

The O
choices O
are O
often O
created O
by O
other O
human O
players O
. O

Hence O
, O
in O
the O
spirit O
of O
competitiveness O
, O
this O
means O
that O
the O
data O
is O
meant O
to O
be O
deliberately O
challenging O
. O

Moreover O
, O
there O
are O
at O
least O
500 O
annotators O
for O
each O
sample O
, O
which O
makes O
the O
assigned O
label O
less O
susceptible O
to O
noisy O
raters O
. O

2.3 O
Problem O
Formulation O
Given O
Q(prompt O
) O
, O
two O
sentences O
S1andS2and O
V(:)which O
computes O
the O
absolute O
votes O
to O
each O
option O
, O
we O
explore O
different O
sub O
- O
tasks O
( O
or O
variant O
problem O
formulation O
) O
. O

Predicting B-TaskName
Preference I-TaskName

This O
task O
is O
primarily O
concerned O
with O
predicting O
if O
V(S1 O
) O
> O
V(S2)or O
otherwise O
. O

Intuitively O
, O
if O
a O
model O
is O
able O
to O
solve O
this O
task O
( O
perform O
equivalent O
to O
a O
human O
player O
) O
, O
we O
consider O
it O
to O
have O
some O
fundamental O
understanding O
of O
human O
values O
and O
social B-TaskName
preferences I-TaskName
. O

We O
frame O
this O
task O
in O
two O
ways O
. O

The O
ﬁrst O
is O
a O
straightforward O
binary O
classiﬁcation O
problem O
, O
i.e. O
, O
V(S1 O
) O
> O
V(S2 O
) O
. O

The O
second O
task O
is O
a O
three O
- O
way O
classiﬁcation O
problem O
with O
a O
third O
class O
predicting O
if O
the O
differencejV(S1) V(S2)jis O
less O
than O
5 O
% O
of O
the O
total O
votes O
. O

In O
short O
, O
this O
means O
that O
two O
options O
are O
almost O
in O
a O
draw O
. O

Prompt O
Option O
A O
Option O
B O
( O
1 O
) O
Would O
you O
rather O
ﬁt O
into O
any O
group O
but O
never O
be O
popular O
only O
ﬁt O
into O
the O
popular O
group O
( O
2 O
) O
Would O
you O
rather O
have O
no O
one O
attend O
yourfuneral O
wedding O
( O
3 O
) O
Would O
you O
rather O
have O
free O
starbucks O
for O
an O
entire O
year O
free O
itunes O
forever O
( O
4 O
) O
Would O
you O
rather O
Look O
unhealthy O
and O
unattractive O
, O
but O
be O
in O
perfect O
health O
. O

Be O
absolutely O
beautiful O
and O
look O
healthy O
, O
but O
be O
in O
extremely O
bad O
health O
. O

( O
5 O
) O
Would O
you O
rather O
Win O
the O
lottery O
Live O
twice O
as O
long O
( O
6 O
) O
Would O
you O
rather O
have O
a O
Mac O
a O
PC O
( O
7 O
) O
Would O
you O
rather O
spend O
the O
daySurﬁng O
on O
the O
ocean O

Surﬁng O
the O
Internet O
Table O
1 O
: O
Samples O
from O
our O
MACS B-DatasetName
dataset O
. O

Standard O
Cultural O
Binary O
Three O
- O
way O
Binary O
Three O
- O
way O
Model O
Dev O
Test O
Dev O
Test O
Dev O
Test O
Dev O
Test O
BERT B-MethodName
61.02 O
60.38 O
56.71 O
55.85 O
62.42 O
62.88 O
57.42 O
58.21 O
XLNEt B-MethodName
56.12 O
56.84 O
55.72 O
56.34 O
51.77 O
51.42 O
57.08 O
57.39 O
RoBERTa B-MethodName
64.75 O
64.15 O
61.04 O
61.19 O
64.39 O
64.71 O
59.28 O
61.22 O
Table O
3 O
: O
Experimental O
results O
on O
predicting B-TaskName
preference I-TaskName
( O
standard O
and O
cultural O
) O
with O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
, O
XLNEt B-MethodName
( O
Yang O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
on O
MACS B-DatasetName
dataset O
. O

Predicting B-TaskName
Cultural I-TaskName
Preferences I-TaskName
We O
consider O
a O
variant O
of O
the O
preference B-TaskName
prediction I-TaskName
problem O
. O

Our O
MACS B-DatasetName
dataset O
has O
culture O
- O
level O
preference O
votes O
which O
are O
the O
voting O
scores O
with O
respect O
to O
a O
particular O
cultural O
demographic O
. O

We O
extend O
the O
same O
setting O
as O
Task O
1 O
by O
requiring O
the O
model O
to O
produce O
culture O
- O
level O
predictions O
. O

In O
order O
to O
do O
this O
, O
we O
prepend O
the O
input O
sentence O
with O
a O
culture O
embedding O
token O
. O

For O
example O
, O
Input O
= O

[ O
Culture O
] O

+ O
[ O
Choice O
A O
] O

+ O
[ O
Sep O
] O
+ O

[ O
Choice O
B O
] O
. O

The O
task O
is O
identical O
, O
predicting O
the O
greater O
of O
Choice O
A O
OR O
Choice O
B O
, O
with O
respect O
to O
the O
cultural O
ground O
truth O
. O

The O
dataset O
is O
augmented O
at O
the O
culture O
level O
and O
the O
same O
example O
is O
duplicated O
for O
each O
culture O
, O
e.g. O
, O
we O
duplicate O
the O
sample O
for O
countries O
’ O
USA O
’ O
and O
’ O
Europe O
’ O
. O

We O
consider O
only O
culturelevel O
votes O
with O
a O
threshold O
above O
25votes O
in O
the O
dataset O
for O
train O
/ O
dev O
/ O
test O
sets O
. O

Predicting B-TaskName
Relative I-TaskName
Preference I-TaskName
The O
third O
variant O
is O
a O
ﬁne O
- O
grained O
regression O
task O
where O
we O
want O
to O
identify O
if O
our O
model O
is O
able O
to O
learn O
the O
extent O
of O
preference O
given O
by O
human O
players O
. O

This O
problem O
is O
framed O
as O
a O
regression O
problem O
that O
is O
normalized O
from O
[ O
0;1]with O
respect O
to O
the O
total O
number O
of O
votes O
in O
the O
data O
point O
3 O
Experiments O
This O
section O
outlines O
our O
experimental O
setup O
and O
results.3.1 O
Experimental O
Setup O
We O
implement O
and O
run O
several O
models O
on O
this O
dataset O
. O

( O
1 O
) O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
- O

Deep O
Bidirectional O
Transformers O
is O
the O
state O
- O
of O
- O
the O
- O
art O
pretrained O
transformer O
model O
for O
a O
wide O
range O
of O
NLP O
tasks O
. O

( O
2 O
) O
XLNet B-MethodName
( O
Yang O
et O
al O
. O
, O
2019 O
) O
is O
a O
large O
pretrained O
model O
based O
on O
Transformer O
- O
XL O
. O

( O
3 O
) O
RoBertA B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
is O
a O
robustly O
optimized O
improvement O
over O
the O
vanilla O
BERT B-MethodName
model O
. O

All O
models O
were O
run O
using O
the O
ﬁnetune O
methodology O
using O
the O
standard O
Pytorch O
Huggingface2repository O
. O

We O
train O
( O
ﬁnetune O
) O
all O
models O
for O
3 B-HyperparameterValue
epochs B-HyperparameterName
using O
the O
default B-HyperparameterName
hyperparameters I-HyperparameterName
.. O

Metrics O

The O
evaluation O
metrics O
for O
classiﬁcation O
tasks O
is O
the O
standard O
accuracy B-MetricName
score O
. O

For O
regression O
tasks O
, O
we O
use O
the O
correlation B-MetricName
, O
Pearson B-MetricName
, O
and O
Spearman B-MetricName
metrics O
. O

3.2 O
Experimental O
Results O
Table O
3 O
reports O
our O
results O
on O
binary O
and O
three O
- O
way O
classiﬁcation O
on O
the O
MACS B-DatasetName
dataset O
. O

In O
general O
, O
we O
ﬁnd O
that O
RoBERTa B-MethodName
performs O
the O
best O
. O

However O
, O
in O
most O
cases O
, O
the O
performance O
of O
all O
three O
models O
still O
leaves O
a O
lot O
to O
be O
desired O
. O

An O
accuracy B-MetricName
of60%+ B-MetricValue
shows O
that O
state O
- O
of O
- O
the O
- O
art O
models O
still O
struggle O
at O
this O
task O
. O

On O
the O
other O
hand O
, O
results O
on O
regression O
task O
are O
also O
similarly O
lacklustre O
, O
and O
2https://github.com/huggingface/ O
transformersDev O
Test O
Model O
Correlation B-MetricName
Pearson B-MetricName
Spearman B-MetricName
Correlation B-MetricName
Pearson B-MetricName
Spearman B-MetricName
BERT B-MethodName
0.234 O
0.256 O

0.214 O
0.229 O
0.250 O
0.208 O
XLNEt B-MethodName
0.225 O
0.243 O
0.206 O
0.228 O
0.250 O
0.206 O
RoBERTa B-MethodName
0.258 O
0.279 O
0.236 O
0.256 O
0.278 O
0.235 O
Table O
4 O
: O
Experimental O
results O
on O
predicting B-TaskName
relative I-TaskName
preference I-TaskName
on O
MACS B-DatasetName
dataset O
. O

Prompt O
Option O
A O
Option O
B O
V O
ote O
A O
V O
ote O
B O
Pred O
( O
1 O
) O
Would O
you O
rather O
be O
happy O
and O
with O
friends O
popular O
and O
without O
friends O
95.39 O
% O
4.61 O
% O
7 O
( O
2 O
) O
Would O
you O
rather O
.... O

Own O
a O
self O
reﬁlling O
fridge O
. O

Have O
a O
self O
cleaning O
bed O

room.74.10 O
% O
25.9 O
% O
7 O
( O
3 O
) O
Which O
art O
style O
do O
you O
preferPhotography O
Poetry O
69.62 O
% O
30.38 O
% O
7 O
( O
4 O
) O
Would O
you O
rather O
Be O
A O
Millionare O
Be O
the O
kindest O
, O
loveing O
most O
talented O
human O
being O
living O
and O
will O
ever O
live47.32 O
% O
52.68 O
% O
3 O
( O
5 O
) O
Would O
you O
rather O
Be O
the O
ﬁrst O
to O
invent O
an O
Invisibility O
cloakBe O
the O
ﬁrst O
to O
invent O
a O
Teleportation O
device47.32 O
% O
52.68 O
% O
3 O
Table O
5 O
: O
Model O
predictions O
from O
MACS B-DatasetName
dataset O
using O
ﬁnetuned O
BERT B-MethodName
. O
show O
that O
models O
like O
BERT B-MethodName
and O
RoBERTa B-MethodName
are O
unable O
to O
perform O
well O
on O
this O
task O
. O

On O
a O
whole O
, O
it O
is O
good O
to O
note O
that O
RoBERTa B-MethodName
performs O
the O
best O
out O
of O
the O
three O
compared O
models O
. O

Overall O
, O
this O
encourages O
further O
research O
on O
cultural O
and O
social O
commonsense O
reasoning O
in O
the O
current O
state O
- O
of O
- O
the O
- O
art O
in O
natural B-TaskName
language I-TaskName
understanding I-TaskName
. O

All O
in O
all O
, O
we O
hope O
our O
benchmark O
serves O
as O
a O
useful O
tool O
for O
understanding O
the O
social O
capabilities O
of O
these O
models O
. O

3.3 O
Qualitative O
Evaluation O
Table O
5 O
reports O
some O
sample O
of O
our O
model O
outputs O
, O
shedding O
light O
on O
examples O
in O
which O
our O
model O
does O
well O
and O
otherwise O
. O

We O
observe O
that O
the O
model O
often O
gets O
the O
answer O
wrong O
even O
when O
the O
ground O
truth O
is O
overwhelmingly O
swayed O
towards O
one O
side O
. O

On O
the O
other O
hand O
, O
occasionally O
, O
we O
also O
observe O
that O
the O
model O
can O
get O
questionable O
questions O
such O
as O
( O
4 O
) O
and O
( O
5 O
) O
correctly O
even O
despite O
the O
tight O
draw O
between O
human O
voters O
. O

4 O
Conclusion O
We O
propose O
MACS B-DatasetName
( O
Machine B-DatasetName
Alignment I-DatasetName
with I-DatasetName
Cultural I-DatasetName
and I-DatasetName
Social I-DatasetName
Preferences I-DatasetName
) O
, O
a O
new O
benchmark O
dataset O
for O
learning O
machine O
alignment O
with O
human O
cultural O
and O
social B-TaskName
preferences I-TaskName
. O

MACS B-DatasetName
encompasses O
and O
requires O
social O
and O
cultural O
reasoning O
to O
solve O
and O
an O
overall O
holistic O
understanding O
of O
humanity O
. O

It O
is O
designed O
to O
be O
challenging O
where O
state O
- O
of O
- O
the O
- O
art O
NLP O
models O
still O
struggle O
at O
60%.Broader O
Impact O
In O
this O
paper O
, O
we O
are O
not O
promoting O
the O
use O
of O
https://www.rrrather.com/ O
as O
the O
training O
source O
, O
but O
rather O
the O
study O
of O
the O
alignment O
of O
machine O
learning O
models O
with O
social B-TaskName
preference I-TaskName
of O
a O
large O
population O
. O

Unfortunately O
, O
there O
might O
be O
some O
issues O
of O
bias O
, O
fairness O
and O
representation O
due O
to O
the O
curation O
of O
the O
training O
data O
from O
Internet O
, O
which O
might O
lead O
models O
to O
give O
prejudiced O
or O
stereotyped O
outputs O
. O

Evaluating O
bias O
, O
fairness O
and O
representation O
in O
language O
models O
and O
the O
training O
data O
is O
an O
important O
research O
area O
( O
Nadeem O
et O
al O
. O
, O
2020 O
; O
Huang O
et O
al O
. O
, O
2019 O
) O
. O

As O
for O
future O
works O
, O
it O
is O
important O
to O
characterize O
and O
intervene O
biases O
when O
designing O
such O
tasks O
. O

Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
619–624 O
July O
5 O
- O
10 O
, O
2020 O
. O

c O

 O
2020 O
Association O
for O
Computational O
Linguistics619Learning O
to O
Tag O
OOV O
Tokens O
by O
Integrating O
Contextual O
Representation O
and O
Background O
Knowledge O
Keqing O
He O
, O
Yuanmeng O
Yan O
, O
Weiran O
Xu O
Pattern O
Recognition O
& O
Intelligent O
System O
Laboratory O
School O
of O
Information O
and O
Communication O
Engineering O
Beijing O
University O
of O
Posts O
and O
Telecommunications O
, O
Beijing O
, O
China O
fkqin O
, O
yanyuanmeng O
, O
xuweiran O
g@bupt.edu.cn O
Abstract O
Neural O
- O
based O
context B-MethodName
- I-MethodName
aware I-MethodName
models O
for O
slot B-TaskName
tagging I-TaskName
have O
achieved O
state O
- O
of O
- O
the O
- O
art O
performance O
. O

However O
, O
the O
presence O
of O
OOV(outof O
- O
vocab O
) O
words O
signiﬁcantly O
degrades O
the O
performance O
of O
neural O
- O
based O
models O
, O
especially O
in O
a O
few O
- O
shot O
scenario O
. O

In O
this O
paper O
, O
we O
propose O
a O
novel O
knowledge O
- O
enhanced O
slot B-TaskName
tagging I-TaskName
model O
to O
integrate O
contextual O
representation O
of O
input O
text O
and O
the O
large O
- O
scale O
lexical O
background O
knowledge O
. O

Besides O
, O
we O
use O
multilevel O
graph O
attention O
to O
explicitly O
model O
lexical O
relations O
. O

The O
experiments O
show O
that O
our O
proposed O
knowledge O
integration O
mechanism O
achieves O
consistent O
improvements O
across O
settings O
with O
different O
sizes O
of O
training O
data O
on O
two O
public O
benchmark O
datasets O
. O

1 O
Introduction O
Slot B-TaskName
tagging I-TaskName
is O
a O
critical O
component O
of O
spoken O
language O
understanding(SLU O
) O
in O
dialogue O
systems O
. O

It O
aims O
at O
parsing O
semantic O
concepts O
from O
user O
utterances O
. O

For O
instance O
, O
given O
the O
utterance O
” O
I O
’d O
also O
like O
to O
have O
lunch O
during O
my O
ﬂight O
” O
from O
the O
ATIS B-DatasetName
dataset O
, O
a O
slot B-TaskName
tagging I-TaskName
model O
might O
identify O
lunch O
as O
ameal O
description O
type O
. O

Given O
sufﬁcient O
training O
data O
, O
recent O
neural O
- O
based O
models O
( O
Mesnil O
et O
al O
. O
, O
2014 O
; O
Liu O
and O
Lane O
, O
2015 O
, O
2016 O
; O

Goo O
et O
al O
. O
, O
2018 O
; O
Haihong O
et O

al O
. O
, O
2019 O
; O
He O
et O
al O
. O
, O
2020 O
) O
have O
achieved O
remarkably O
good O
results O
. O

However O
, O
these O
works O
often O
suffer O
from O
poor O
slot B-TaskName
tagging I-TaskName
accuracy O
when O
rare O
words O
or O
OOV O
( O
out O
- O
of O
- O
vocab O
) O
words O
exist O
. O

( O
Ray O
et O
al O
. O
, O
2018 O
) O
has O
veriﬁed O
the O
presence O
of O
OOV O
words O
further O
degrades O
the O
performance O
of O
neural O
- O
based O
models O
, O
especially O
in O
a O
few O
- O
shot O
scenario O
where O
training O
data O
can O
not O
provide O
adequate O
contextual O
semantics O
. O

Previous O
context O
- O
aware O
models O
merely O
focus O
on O
how O
to O
capture O
deep O
contextual O
semantics O
to O
aid O
Weiran O
Xu O
is O
the O
corresponding O
author O
. O

playlist O
broadcastmusic O
genre O
classical O
  O
music O
popular O
jazz O
scat O
  O
singinghyponym O
s O
sister O
term O
train O
setcan O
  O
you O
  O
append O
  O
some O
   O
classical O
   O
music O
   O
to O
  O
my O
  O
playlist O
    O
O O
            O

O O
            O

O O
            O
O O
     O
B O
-music_type O
   O

I O
-music_type O
   O

O O
      O

O O
       O

O O
find O
  O
and O
  O
add O
   O
some O
    O
scat O
  O
singing O
     O
to O
   O
my O
  O
broadcast O
    O

O O
          O

O O
           O

O O
          O

O O
                                                    O

O O
         O

O O
          O
Otest O
setsemantic O
   O
synset O
O O
          O

O O
( O
context O
-aware O
model)B O
- O
music_type O
   O

I O
-music_type O
( O
knowledge O
integration)Figure O
1 O
: O
An O
example O
of O
slot B-TaskName
tagging I-TaskName
in O
the O
few O
- O
shot O
scenario O
where O
scat O
singing O
is O
unseen O
in O
the O
training O
set O
. O

The O
prior O
context O
- O
aware O
model O
fails O
to O
recognize O
its O
correct O
type O
because O
of O
low O
- O
coverage O
contextual O
information O
. O

After O
integrating O
background O
knowledge O
from O
WordNet O
, O
it O
succeeds O
to O
reason O
the O
correct O
type O
via O
lexical O
relations O
. O

in O
recognizing O
slot O
entities O
, O
while O
neglecting O
ontology O
behind O
the O
words O
or O
large O
- O
scale O
background O
knowledge O
. O

Explicit O
lexical O
relations O
are O
vital O
to O
recognizing O
unseen O
words O
when O
there O
is O
not O
adequate O
training O
data O
, O
that O
is O
, O
few O
- O
shot O
scenarios O
. O

Fig O
1 O
gives O
a O
motivating O
example O
of O
slot B-TaskName
tagging I-TaskName
to O
explain O
the O
phenomenon O
. O

This O
example O
suggests O
slot B-TaskName
tagging I-TaskName
requires O
not O
only O
understanding O
the O
complex O
linguistic O
context O
constraints O
but O
also O
reasoning O
explicit O
lexical O
relations O
via O
large O
- O
scale O
background O
knowledge O
graphs O
. O

Previous O
state O
- O
of O
- O
the O
- O
art O
context O
- O
aware O
models O
( O
Goo O
et O
al O
. O
, O
2018 O
; O
Haihong O
et O
al O
. O
, O
2019 O
) O
only O
learn O
contextual O
information O
based O
on O
a O
multi O
- O
layer O
BiLSTM O
encoder O
and O
self O
- O
attention O
layer O
. O

( O
Dugas O
and O
Nichols O
, O
2016 O
; O
Williams O
, O
2019 O
; O
Shah O
et O
al O
. O
, O
2019 O
) O
use O
handcrafted B-MethodName
lexicons I-MethodName
( O
also O
known O
as O
gazettes O
or O
dictionaries O
) O
, O
which O
are O
typically O
collections O
of O
phrases O
semantically O
related O
, O
to O
improve O
slot B-TaskName
tagging I-TaskName
. O

One O
major O
limitation O
is O
that O
lexicons O
collected O
by O
domain O
experts O
are O
relatively O
small O
on O
the O
scale O
and O
fail O
to O
model O
complicated O
relations620between O
words O
, O
such O
as O
relation O
hierarchy O
. O

In O
this O
paper O
, O
we O
propose O
a O
novel O
knowledge B-MethodName
enhanced I-MethodName
method O
for O
slot B-TaskName
tagging I-TaskName
by O
integrating O
contextual O
representation O
of O
input O
text O
and O
the O
largescale O
lexical O
background O
knowledge O
, O
enabling O
the O
model O
to O
reason O
explicit O
lexical O
relations O
. O

We O
aim O
to O
leverage O
both O
linguistic O
regularities O
covered O
by O
deep O
LMs O
and O
high O
- O
quality O
knowledge O
derived O
from O
curated O
KBs O
. O

Consequently O
, O
our O
model O
could O
infer O
rare O
and O
unseen O
words O
in O
the O
test O
dataset O
by O
incorporating O
contextual O
semantics O
learned O
from O
the O
training O
dataset O
and O
lexical O
relations O
from O
ontology O
. O

As O
depicted O
in O
Fig O
2 O
, O
given O
an O
input O
sequence O
, O
we O
ﬁrst O
retrieve O
potentially O
relevant O
KB O
entities O
and O
encode O
them O
into O
distributed O
representations O
that O
describe O
global O
graph O
- O
structured O
information O
. O

Then O
we O
employ O
a O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
encoder O
layer O
to O
capture O
context O
- O
aware O
representations O
of O
the O
sequence O
and O
attend O
to O
the O
KB O
embeddings O
using O
multi O
- O
level O
graph O
attention O
. O

Finally O
, O
we O
integrate O
BERT B-MethodName
embeddings O
and O
the O
desired O
KB O
embeddings O
to O
predict O
the O
slot O
type O
. O

Our O
main O
contributions O
are O
three O
- O
fold O
: O
( O
1 O
) O
We O
investigate O
and O
demonstrate O
the O
feasibility O
of O
applying O
lexical O
ontology O
to O
facilitate O
recognizing O
OOV O
words O
in O
the O
few O
- O
shot O
scenario O
. O

To O
the O
best O
of O
our O
knowledge O
, O
this O
is O
the O
ﬁrst O
to O
consider O
the O
large O
- O
scale O
background O
knowledge O
for O
enhancing O
context O
- O
aware O
slot B-TaskName
tagging I-TaskName
models O
. O

( O
2 O
) O
We O
propose O
a O
knowledge B-MethodName
integration I-MethodName
mechanism I-MethodName
and O
use O
multi O
- O
level O
graph O
attention O
to O
model O
explicit O
lexical O
relations O
. O

( O
3 O
) O
Plenty O
of O
experiments O
on O
two O
benchmark O
datasets O
show O
that O
our O
proposed O
method O
achieves O
consistently O
better O
performance O
than O
various O
state O
- O
of O
- O
theart O
context O
- O
aware O
methods O
. O

2 O
Our O
Approach O
In O
this O
work O
, O
we O
consider O
the O
slot B-TaskName
tagging I-TaskName
task O
in O
the O
few O
- O
shot O
scenario O
, O
especially O
for O
OOV O
tokens O
. O

Given O
a O
sequence O
with O
n O
tokens O
X O
= O
fxign O
i=1 O
, O
our O
goal O
is O
to O
predict O
a O
corresponding O
tagging O
sequenceY O
= O
fyign O
i=1 O
. O

This O
section O
ﬁrst O
explains O
our O
BERT B-MethodName
- O
based O
model O
and O
then O
introduces O
the O
proposed O
knowledge O
integration O
mechanism O
for O
inducing O
background O
commonsense O
. O

The O
overall O
model O
architecture O
is O
illustrated O
in O
Fig O
2 O
. O
2.1 O
BERT B-MethodName
- O
Based O
Model O
for O
Slot B-TaskName
Tagging I-TaskName
The O
model O
architecture O
of O
BERT B-MethodName
is O
a O
multi O
- O
layer O
bidirectional O
Transformer O
encoder O
. O

The O
input O
representation O
is O
a O
concatenation O
of O
WordPiece O
emKnowledge O
Integration O
  O
Layer O

x1 O
x2 O

xnh1 O
h2 O
hny1 O
y2 O
yn O
hic1c2 O
cm O
sentinelC1(xi O
) O
C2(xi)fi O
... O
… O
… O
  O

BiLSTM O
Matching O
LayerCRF O
LayerFigure O
2 O
: O
The O
overall O
architecture O
of O
the O
proposed O
slot B-TaskName
tagging I-TaskName
model O
. O
beddings O
( O
Wu O
et O
al O
. O
, O
2016 O
) O
, O
positional O
embeddings O
, O
and O
the O
segment O
embeddings O
. O

Inspired O
by O
previous O
RNN B-MethodName
- I-MethodName
based I-MethodName
works O
( O
Mesnil O
et O
al O
. O
, O
2014 O
; O
Liu O
and O
Lane O
, O
2016 O
) O
, O
we O
extend O
BERT B-MethodName
to O
a O
slot B-TaskName
tagging I-TaskName
model O
. O

We O
ﬁrst O
feed O
the O
input O
sequence O
X O
= O
fxign O
i=1to O
a O
pre O
- O
trained O
BERT B-MethodName
encoding O
layer O
and O
then O
get O
ﬁnal O
hidden O
states O
H= O
( O
h1;:::;h O
n O
) O
. O

To O
make O
this O
procedure O
compatible O
with O
the O
original O
BERT B-MethodName
tokenization O
, O
we O
feed O
each O
input O
word O
into O
a O
WordPiece O
tokenizer O
and O
use O
the O
hidden O
state O
corresponding O
to O
the O
ﬁrst O
sub O
- O
word O
as O
input O
to O
the O
softmax O
classiﬁer O
. O

yi= O
softmax O
( O
Whi+b);i21:::n O
( O
1 O
) O
wherehi2Rd1is O
the O
hidden O
state O
corresponding O
to O
the O
ﬁrst O
sub O
- O
word O
of O
the O
i O
- O
th O
input O
word O
xiand O
yiis O
the O
slot O
label O
. O

2.2 O
Knowledge O
Integration O
Mechanism O
The O
knowledge O
integration O
mechanism O
aims O
at O
enhancing O
the O
deep O
contextual O
representation O
of O
input O
text O
via O
leveraging O
the O
large O
- O
scale O
lexical O
background O
knowledge O
, O
Wordnet O
( O
Miller O
, O
1995 O
) O
, O
to O
recognize O
unseen O
tokens O
in O
the O
training O
set O
. O

Essentially O
, O
it O
applies O
multi O
- O
level O
graph O
attention O
to O
KB O
embeddings O
with O
the O
BERT B-MethodName
representations O
from O
the O
previous O
layer O
to O
enhance O
the O
contextual O
BERT B-MethodName
embeddings O
with O
human O
- O
curated O
background O
knowledge O
. O

We O
ﬁrst O
introduce O
the O
KB O
embedding O
and O
retrieval O
process O
. O

In O
this O
paper O
, O
we O
use O
the O
lexical O
KB O
, O
WordNet O
, O
stored O
as O
( O
subject O
, O
relation O
, O
object O
) O
triples O
, O
where O
each O
triple O
indicates O
a O
speciﬁc O
relation O
between O
word O
synsets O
, O
e.g. O
, O
( O
state O
, O
hypernym-621of O
, O
california O
) O
. O

Each O
synset O
expresses O
a O
distinct O
concept O
, O
organized O
by O
a O
human O
- O
curated O
tree O
hierarchy O
. O

KB O
Embeddings O
We O
represent O
KB O
concepts O
as O
continuous O
vectors O
in O
this O
paper O
. O

The O
goal O
is O
that O
the O
KB O
tuples O
( O
s;r;o O
) O
can O
be O
measured O
in O
the O
dense O
vector O
space O
based O
on O
the O
embeddings O
. O

We O
adopt O
the O
BILINEAR O
model O
( O
Yang O
et O
al O
. O
, O
2014 O
) O
which O
measures O
the O
relevance O
via O
a O
bilinear O
function O
: O
f(s;r;o O
) O

= O
sTMro O
, O
where O
s;o2Rd2are O
the O
vector O
embeddings O
for O
s;orespectively O
and O
and O
Mris O
a O
relation O
- O
speciﬁc O
embedding O
matrix O
. O

Then O
we O
train O
the O
embeddings O
using O
the O
max O
- O
margin O
ranking O
objective O
: O
X O
q=(s;r;o O
) O
2TX O
q0=(s;r;o0)2T0max O
0;1 Sq+Sq0 O
	  O
( O
2 O
) O
whereTdenotes O
the O
set O
of O
triples O
in O
the O
KB O
and O
T0 O
denotes O
the O
negative O
triples O
that O
are O
not O
observed O
in O
the O
KB O
. O

Finally O
we O
can O
acquire O
vector O
representations O
for O
concepts O
of O
the O
KB O
. O

Because O
we O
mainly O
focus O
on O
the O
slot B-TaskName
tagging I-TaskName
task O
, O
and O
the O
datasets O
are O
relatively O
small O
for O
joint O
learning O
KB O
embeddings O
. O

Furthermore O
, O
the O
KB O
contains O
many O
triplets O
not O
present O
in O
the O
ATIS B-DatasetName
and O
Snips B-DatasetName
dataset O
. O

Therefore O
we O
pre O
- O
train O
the O
KB O
vectors O
and O
keep O
them O
ﬁxed O
while O
training O
the O
whole O
model O
to O
reduce O
the O
complexity O
. O

KB O
Concepts O
Retrieval O
We O
need O
to O
retrieve O
all O
the O
concepts O
or O
synsets O
relevant O
to O
the O
input O
word O
xi O
from O
the O
KB O
. O

Different O
from O
( O
Yang O
and O
Mitchell O
, O
2017 O
; O
Yang O
et O
al O
. O
, O
2019 O
) O
, O
for O
a O
word O
xi O
, O
we O
ﬁrst O
return O
its O
synsets O
as O
the O
ﬁrst O
- O
level O
candidate O
set O
C1(xi)of O
KB O
concepts O
. O

Then O
we O
construct O
the O
second O
- O
level O
candidate O
set O
C2(xi)by O
retrieving O
all O
the O
direct O
hyponyms O
of O
each O
synset O
in O
C1(xi O
) O
, O
as O
shown O
in O
the O
right O
part O
of O
Fig O
2 O
. O
Multi O
- O
Level O
Graph O
Attention O
After O
obtaining O
the O
two O
- O
level O
concept O
candidate O
sets O
, O
we O
apply O
the O
BERT B-MethodName
embedding O
hiof O
input O
token O
xito O
attending O
over O
the O
multi O
- O
level O
memory O
. O

The O
ﬁrst O
- O
level O
attention O
, O
 O
, O
is O
calculated O
by O
a O
bilinear O
operation O
between O
hiand O
each O
synset O
cjin O
the O
ﬁrst O
level O
set O
C1(xi O
): O
 O
ij O
/ O
exp(cT O
jW1hi O
) O
( O
3 O
) O

Then O
we O
add O
an O
additional O
sentinel O
vector O
c(Yang O
and O
Mitchell O
, O
2017 O
) O
and O
accumulate O
all O
the O
embeddings O
as O
follows O
: O
s1 O
i O
= O
X O
j O
 O
ijcj+ O

 O

 O
iis O
similar O
to O
 O
ijandP O
j O
 O
ij+ O

 O
i= O
1 O
. O

Heres1 O
iis O
regarded O
as O
a O
one O
- O
hop O
knowledge O
state O
vector O
for O
it O
only O
represents O
its O
directly O
linked O
synsets O
. O

Therefore O
, O
we O
perform O
the O
second O
- O
level O
graph O
attention O
to O
encode O
the O
hyponyms O
of O
its O
direct O
synsets O
to O
enrich O
the O
information O
of O
original O
synsets O
. O

Intuitively O
the O
second O
- O
level O
attention O
over O
the O
hyponyms O
can O
be O
viewed O
as O
a O
relational O
reasoning O
process O
. O

Because O
once O
a O
synset O
belongs O
to O
an O
entity O
type O
, O
its O
hyponyms O
always O
conform O
to O
the O
same O
type O
. O

Likewise O
, O
the O
second O
- O
level O
attention O
overC2(xi)is O
calculated O
: O
 O
ijk O
/ O
exp(cT O
jkW2hi O
) O
( O
5 O
) O
where O
cjis O
thej O
- O
th O
synset O
linked O
to O
token O
xiand O
cjkthek O
- O
th O
hyponym O
of O
cj O
. O

So O
we O
can O
obtain O
the O
multi O
- O
hop O
knowledge O
state O
vector O
s2 O
i O
: O
s2 O
i O
= O
X O
jX O
k O
 O
ij O
 O
ijkcjk O
( O
6 O
) O
Then O
we O
concat O
multi O
- O
level O
knowledge O
- O
aware O
vectors1 O
i;s2 O
i O
, O
and O
original O
BERT B-MethodName
representation O
hi O
, O
and O
output O
fi= O

[ O
s1 O
i;s2 O
i;hi O
] O
. O

We O
also O
add O
a O
BiLSTM O
matching O
layer O
which O
takes O
as O
input O
the O
knowledge O
- O
enriched O
representationsfi O
. O

Then O
we O
forward O
the O
hidden O
states O
to O
a O
CRF O
layer O
and O
predict O
the O
ﬁnal O
results O
. O

The O
training O
objective O
is O
the O
sum O
of O
log O
- O
likelihood O
of O
all O
the O
words O
. O

3 O
Experiments O
3.1 O
Setup O
Datasets O
To O
evaluate O
our O
approach O
, O
we O
conduct O
experiments O
on O
two O
public O
benchmark O
datasets O
, O
ATIS B-DatasetName
( O
T¨ur O
et O
al O
. O
, O
2010 O
) O
and O
Snips B-DatasetName
( O
Coucke O
et O
al O
. O
, O
2018 O
) O
. O

ATIS B-DatasetName
contains O
4,478 O
utterances O
in O
the O
training O
set O
and O
893 O
utterances O
in O
the O
test O
set O
, O
while O
Snips B-DatasetName
contains O
13,084 O
and O
700 O
utterances O
, O
respectively O
. O

% O
represents O
how O
much O
training O
data O
we O
randomly O
choose O
from O
the O
original O
training O
set O
. O

We O
report O
the O
F1 B-MetricName
scores O
on O
the O
same O
test O
sets O
. O

Samples O
in O
Snips B-DatasetName
are O
from O
different O
topics O
, O
such O
as O
getting O
weather O
and O
booking O
a O
restaurant O
, O
resulting O
in O
a O
larger O
vocabulary O
. O

By O
contrast O
, O
samples O
in O
ATIS B-DatasetName
are O
all O
about O
ﬂight O
information O
with O
similar O
vocabularies O
across O
them O
. O

Therefore O
, O
Snips B-DatasetName
is O
much O
more O
complicated O
, O
mainly O
due O
to O
data O
diversity O
and O
the O
large O
vocabulary O
. O

The O
full O
statistics O
are O
shown O
in O
the O
Table O
1 O
. O

To O
simulate O
the O
few O
- O
shot O
scenarios O
, O
we O
downsample O
the O
original O
training O
sets O
of O
ATIS B-DatasetName
and O
Snips B-DatasetName
to O
different O
extents O
while O
keeping O
valid O
and O
test O
sets O
ﬁxed O
. O

We O
aim O
to O
evaluate O
the O
effectiveness O
of O
integrating O
external O
KB O
under O
the O
settings O
of O
varied O
sizes O
of O
training O
data O
available O
. O

Evaluation O
We O
evaluate O
the O
performance O
of O
slot B-TaskName
tagging I-TaskName
using O
the O
F1 B-MetricName
score O
metric O
. O

In O
the O
experiments O
, O
we O
use O
the O
English O
uncased O
BERT B-MethodName
- O
base O
model O
, O
which O
has O
12 B-HyperparameterValue
layers B-HyperparameterName
, O
768 B-HyperparameterValue
hidden B-HyperparameterName
states I-HyperparameterName
, O
and O
12 B-HyperparameterValue
heads B-HyperparameterName
. O

The O
hidden B-HyperparameterName
size I-HyperparameterName
for O
the O
BiLSTM O
layer O
is O
set O
to O
128 B-HyperparameterValue
. O

Adam O
( O
Kingma O
and O
Ba O
, O
2014 O
) O
is O
used O
for O
optimization O
with O
an O
initial O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-5 B-HyperparameterValue
. O

The O
dropout B-HyperparameterName
probability I-HyperparameterName
is O
0.1 B-HyperparameterValue
, O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
is O
64 B-HyperparameterValue
. O

We O
ﬁnetune O
all O
hyperparameters O
on O
the O
valid O
set O
. O

3.2 O
Baselines O
Attention B-MethodName
- I-MethodName
Based I-MethodName
( O
Liu O
and O
Lane O
, O
2016 O
) O
uses O
an O
RNN O
layer O
and O
a O
self O
- O
attention O
layer O
to O
encode O
the O
input O
text O
. O

Slot B-MethodName
- I-MethodName
Gated I-MethodName
( O
Goo O
et O
al O
. O
, O
2018 O
) O
, O
which O
has O
two O
variants O
, O
Full O
Atten O
andIntent O
Atten O
, O
applies O
the O
information O
of O
intent O
detection O
task O
to O
enhance O
slot B-TaskName
tagging I-TaskName
. O

SF B-MethodName
- I-MethodName
ID I-MethodName
Network O
( O
Haihong O
et O
al O
. O
, O
2019 O
) O

designs O
a O
multiple O
iteration O
mechanism O
to O
construct O
bi O
- O
directional O
interrelated O
connections O
between O
slot B-TaskName
tagging I-TaskName
and O
intent O
detection O
. O

Most O
of O
the O
previous O
methods O
consider O
improving O
the O
performance O
of O
slot B-TaskName
tagging I-TaskName
by O
joint O
learning O
with O
intent O
detection O
. O

However O
, O
the O
effectiveness O
of O
background O
knowledge O
for O
slot B-TaskName
tagging I-TaskName
is O
still O
unexplored O
. O

Con O
- O
sequently O
, O
our O
proposed O
approach O
intends O
to O
integrate O
the O
large O
- O
scale O
lexical O
background O
knowledge O
, O
WordNet O
, O
to O
enhance O
the O
deep O
contextual O
representation O
of O
input O
text O
. O

We O
hope O
to O
further O
improve O
the O
performance O
of O
slot B-TaskName
tagging I-TaskName
, O
especially O
in O
the O
fewshot O
scenario O
where O
there O
is O
no O
plenty O
of O
training O
data O
available.1 O
3.3 O
Overall O
Results O
We O
display O
the O
experiment O
results O
in O
Table O
2 O
, O
where O
we O
choose O
two O
model O
architectures O
RNN B-MethodName
and O
BERT B-MethodName
as O
the O
encoding O
layer O
. O

Table O
2 O
shows O
that O
our O
proposed O
knowledge O
integration O
mechanism O
signiﬁcantly O
outperforms O
the O
baselines O
for O
both O
datasets O
, O
demonstrating O
that O
explicitly O
integrating O
the O
largescale O
background O
knowledge O
and O
contextual O
representation O
can O
beneﬁt O
slot B-TaskName
tagging I-TaskName
effectively O
. O

Moreover O
, O
the O
improvement O
of O
0.72 B-MetricValue
% I-MetricValue
over O
strong O
baseline O
BERT B-MethodName
on O
Snips B-DatasetName
is O
considerably O
higher O
than O
0.27 B-MetricValue
% I-MetricValue
on O
ATIS B-DatasetName
. O

Considering O
the O
distinct O
complexity O
of O
the O
two O
datasets O
, O
the O
probable O
reason O
is O
that O
a O
simpler O
slot B-TaskName
tagging I-TaskName
task O
, O
such O
as O
ATIS B-DatasetName
, O
does O
not O
require O
much O
background O
knowledge O
to O
achieve O
good O
results O
. O

Because O
the O
vocabulary O
of O
ATIS B-DatasetName
is O
extremely O
smaller O
than O
that O
of O
Snips B-DatasetName
, O
therefore O
the O
context O
- O
aware O
models O
are O
capable O
of O
providing O
enough O
cues O
for O
recognizing O
rare O
or O
OOV O
words O
. O

Hence O
, O
our O
method O
makes O
a O
notable O
difference O
in O
a O
scenario O
where O
samples O
are O
linguistically O
diverse O
, O
and O
large O
vocab O
exists O
. O

The O
results O
also O
demonstrate O
that O
incorporating O
external O
knowledge O
will O
not O
bring O
in O
much O
noise O
since O
we O
use O
a O
knowledge O
sentinel O
for O
the O
better O
tradeoff O
between O
the O
impact O
of O
background O
knowledge O
and O
information O
from O
the O
context O
. O

On O
the O
other O
hand O
, O
the O
main O
results O
of O
the O
1We O
do O
not O
choose O
( O
Williams O
, O
2019 O
) O
as O
a O
baseline O
since O
it O
only O
performs O
experiments O
on O
private O
industrial O
datasets O
and O
does O
not O
open O
source O
. O

We O
can O
hardly O
ﬁgure O
out O
the O
details O
of O
manually O
collecting O
lexicons O
from O
the O
dataset.623 O
0.01 O
0.02 O
0.05 O
0.10 O
0.50 O
1.00 O
Train O
set O
size0.00.51.01.52.02.53.0Relative O
F1 B-MetricName
improvement O
( O
% O
) O

1.41 O
1.06 O
0.82 O
0.54 O
0.330.282.89 O
2.32 O
1.74 O
1.37 O
1.24 O
0.76ATIS O
SNIPSFigure O
3 O
: O
Relative O
F1 B-MetricName
improvement O
over O
BERT B-MethodName
baseline O
under O
the O
different O
sizes O
of O
training O
data O
. O

RNN B-MethodName
- I-MethodName
based I-MethodName
models O
are O
95.17 B-MetricValue
( O
+0.46 B-MetricValue
) O
on O
ATIS B-DatasetName
and O
89.30 B-MetricValue
( O
+1.51 B-MetricValue
) O
on O
Snips B-DatasetName
, O
where O
the O
scores O
in O
the O
brackets O
are O
the O
absolute O
improvements O
arisen O
by O
KB O
. O

Compared O
to O
the O
BERT B-MethodName
- O
based O
models O
, O
95.98 B-MetricValue
( O
+0.27 B-MetricValue
) O
on O
ATIS B-DatasetName
and O
95.17 B-MetricValue
( O
+0.72 B-MetricValue
) O
on O
Snips B-DatasetName
, O
the O
RNN B-MethodName
- I-MethodName
based I-MethodName
model O
achieves O
more O
signiﬁcant O
improvements O
in O
BERT B-MethodName
- O
based O
models O
. O

We O
believe O
BERT B-MethodName
can O
effectively O
transfer O
prior O
linguistic O
context O
constraints O
, O
so O
that O
background O
knowledge O
beneﬁts O
RNN B-MethodName
- I-MethodName
based I-MethodName
models O
more O
. O

BERT B-MethodName
does O
improve O
the O
model O
’s O
ability O
to O
solve O
the O
OOV O
problem O
since O
it O
has O
learned O
linguistic O
knowledge O
from O
the O
large O
corpus O
. O

However O
, O
our O
method O
focuses O
more O
on O
the O
effect O
of O
using O
human O
- O
curated O
structured O
background O
knowledge O
and O
further O
enhances O
BERT B-MethodName
in O
a O
distinct O
way O
. O

4 O
Qualitative O
Analysis O
4.1 O
Effect O
of O
Training O
Data O
Size O
Fig O
3 O
shows O
the O
relative O
improvement O
percentages O
on O
ATIS B-DatasetName
and O
Snips B-DatasetName
using O
different O
sizes O
of O
training O
data O
. O

Results O
substantiate O
knowledge O
integration O
better O
facilitates O
few O
- O
shot O
slot B-TaskName
tagging I-TaskName
. O

This O
is O
because O
traditional O
context O
- O
aware O
models O
can O
not O
learn O
enough O
contextual O
semantics O
well O
while O
only O
given O
several O
samples O
. O

Explicit O
lexical O
relations O
become O
essentially O
necessary O
when O
there O
is O
not O
adequate O
training O
data O
, O
especially O
for O
rare O
words O
or O
OOV O
words O
. O

Background O
KB O
enables O
the O
model O
to O
reason O
explicit O
lexical O
relations O
and O
helps O
recognize O
rare O
and O
unseen O
words O
. O

Meanwhile O
, O
incorporating O
background O
knowledge O
can O
also O
enhance O
the O
original O
representation O
of O
BERT B-MethodName
, O
which O
can O
provide O
direct O
lexical O
relations O
. O

4.2 O
Ablation O
Study O
To O
study O
the O
effect O
of O
each O
component O
of O
our O
method O
, O
we O
conduct O
ablation O
analysis O
under O
the O
10 O
% O
training O
data O
setting O
( O
Table O
3 O
) O
. O

We O
can O
see O
that O
knowledge O
integration O
is O
crucial O
to O
the O
improvements O
. O

Besides O
, O
the O
ﬁrst O
- O
level O
graph O
attention O
acquires O
better O
performance O
gain O
than O
the O
secondlevel O
attention O
. O

We O
assume O
that O
directly O
linked O
synsets O
are O
more O
signiﬁcant O
than O
the O
hyponyms O
. O

The O
matching O
layer O
and O
CRF O
also O
play O
a O
role O
. O

The O
reason O
why O
the O
RNN O
matching O
layer O
matters O
is O
partly O
to O
build O
explicit O
interactions O
between O
knowledge O
vectors O
and O
context O
vectors O
. O

5 O
Conclusion O
We O
present O
a O
novel O
knowledge O
integration O
mechanism O
of O
incorporating O
background O
KB O
and O
deep O
contextual O
representations O
to O
facilitate O
the O
few O
- O
shot O
slot B-TaskName
tagging I-TaskName
task O
. O

Experiments O
conﬁrm O
the O
effectiveness O
of O
modeling O
explicit O
lexical O
relations O
, O
which O
has O
not O
yet O
been O
explored O
by O
previous O
works O
. O

Moreover O
, O
we O
ﬁnd O
that O
our O
method O
delivers O
more O
beneﬁts O
to O
data O
scarcity O
scenarios O
. O

We O
hope O
to O
provide O
new O
guidance O
for O
the O
future O
slot B-TaskName
tagging I-TaskName
work O
. O

Acknowledgments O

The O
authors O
would O
like O
to O
thank O
the O
reviewers O
for O
their O
valuable O
comments O
. O

This O
work O
was O
partially O
supported O
by O
National O
Key O
R&D O
Program O
of O
China O
No O
. O
2019YFF0303300 O
and O
Subject O
II O

No O
. O
2019YFF0303302 O
, O
DOCOMO O
Beijing O
Communications O
Laboratories O
Co. O
, O
Ltd O
, O
MoE O
- O
CMCC O
” O
Artifical O
Intelligence O
” O
Project O
No O
. O
MCM20190701 O
. O

Proceedings O
of O
the O
60th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
2134 O
- O
2148 O
May O
22 O
- O
27 O
, O
2022 O
c O

 O
2022 O
Association O
for O
Computational O
Linguistics O
bert2BERT B-MethodName
: O
Towards O
Reusable O
Pretrained O
Language O
Models O
Cheng O
Chen1† O
, O
Yichun O
Yin2 O
, O
Lifeng O
Shang2 O
, O
Xin O
Jiang2 O
, O
Yujia O
Qin1 O
, O
Fengyu O
Wang1 O
, O
Zhi O
Wang3,4‡ O
, O
Xiao O
Chen2 O
, O
Zhiyuan O
Liu1 O
, O
Qun O
Liu2 O
1Department O
of O
Computer O
Science O
and O
Technology O
, O
Tsinghua O
University O
2Huawei O
Noah O
’s O
Ark O
Lab,3Tsinghua O
Shenzhen O
International O
Graduate O
School O
4Peng O
Cheng O
Laboratory O
{ O
c-chen19,qyj20,wangfy20}@mails.tsinghua.edu.cn O
{ O
yinyichun,shang.lifeng,jiang.xin,chen.xiao2,qun.liu}@huawei.com O

wangzhi@sz.tsinghua.edu.cn,liuzy@tsinghua.edu.cn O
Abstract O

In O
recent O
years O
, O
researchers O
tend O
to O
pre O
- O
train O
ever O
- O
larger O
language O
models O
to O
explore O
the O
upper O
limit O
of O
deep O
models O
. O

However O
, O
large O
language O
model O
pre O
- O
training O
costs O
intensive O
computational O
resources O
, O
and O
most O
of O
the O
models O
are O
trained O
from O
scratch O
without O
reusing O
the O
existing O
pre O
- O
trained O
models O
, O
which O
is O
wasteful O
. O

In O
this O
paper O
, O
we O
propose O
bert2BERT1 B-MethodName
, O
which O
can O
effectively O
transfer O
the O
knowledge O
of O
an O
existing O
smaller O
pre O
- O
trained O
model O
to O
a O
large O
model O
through O
parameter O
initialization O
and O
significantly O
improve O
the O
pre O
- O
training O
efficiency O
of O
the O
large O
model O
. O

Specifically O
, O
we O
extend O
the O
previous O
function O
- O
preserving O
( O
Chen O
et O
al O
. O
, O
2016 O
) O
method O
proposed O
in O
computer O
vision O
on O
the O
Transformer B-MethodName
- O
based O
language O
model O
, O
and O
further O
improve O
it O
by O
proposing O
a O
novel O
method O
, O
advanced O
knowledge O
for O
the O
large O
model O
’s O
initialization O
. O

In O
addition O
, O
a O
two O
- O
stage O
learning O
method O
is O
proposed O
to O
further O
accelerate O
the O
pre O
- O
training O
. O

We O
conduct O
extensive O
experiments O
on O
representative O
PLMs O
( O
e.g. O
, O
BERT B-MethodName
and O
GPT B-MethodName
) O
and O
demonstrate O
that O
( O
1 O
) O
our O
method O
can O
save O
a O
significant O
amount O
of O
training O
cost O
compared O
with O
baselines O
including O
learning O
from O
scratch O
, O
StackBERT B-MethodName
( O
Gong O
et O
al O
. O
, O
2019 O
) O
and O
MSLT B-MethodName
( O
Yang O
et O
al O
. O
, O
2020 O
) O
; O
( O
2 O
) O
our O
method O
is O
generic O
and O
applicable O
to O
different O
types O
of O
pretrained O
models O
. O

In O
particular O
, O
bert2BERT B-MethodName
saves O
about O
45 O
% O
and O
47 O
% O
computational O
cost O
of O
pretraining O
BERT B-MethodName
BASE I-MethodName
and O
GPT B-MethodName
BASE I-MethodName
by O
reusing O
the O
models O
of O
almost O
their O
half O
sizes O
. O

1 O
Introduction O
Pre O
- O
trained O
language O
models O
( O
PLMs O
) O
, O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
GPT B-MethodName
( O
Radford O
et O
al O
. O
, O
2018 O
, O
2019 O
; O
Brown O
et O
al O
. O
, O
2020 O
) O
, O
ELECTRA B-MethodName
( O
Clark O
et O
al O
. O
, O
2020 O
) O
, O
XLNet B-MethodName
( O
Yang O
et O
al O
. O
, O
2019 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
, O
have O
achieved O
great O
†This O
work O
is O
done O
when O
Cheng O
Chen O
is O
an O
intern O
at O
Huawei O
Noah O
’s O
Ark O
Lab O
. O

‡Corresponding O
author O
. O

1Our O
code O
is O
available O
at O
https://github.com/ O
huawei O
- O
noah O
/ O
Pretrained O
- O
Language O
- O
Model O
. O

0 O
1 O
2 O
3 O
4 O
5 O
6 O
7 O
8 O
FLOPs O
( O
1e19)1.41.61.82.02.22.42.6MLM O
Loss O
4 O
5 O
6 O
71.401.421.441.461.48 O
1.437 O
100 O
% O
75.7 O
% O
54.8%BERTBASE O
StackBERTbert2BERTFigure O
1 O
: O
Loss O
curves O
of O
bert2BERT O
and O
baselines O
. O
StackBERT O
( O
Gong O
et O
al O
. O
, O
2019 O
) O
is O
based O
on O
the O
progressive O
training O
setting O
. O

More O
details O
are O
shown O
in O
Table O
2 O
. O
success O
in O
natural O
language O
processing O
( O
NLP O
) O
. O

However O
, O
the O
pre O
- O
training O
process O
of O
large O
PLMs O
can O
be O
extremely O
computationally O
expensive O
and O
produces O
huge O
carbon O
footprints O
. O

For O
example O
, O
GPT-3 B-MethodName
uses O
3.1E+6 O
GPU O
hours O
for O
training O
, O
at O
an O
estimated O
cost O
of O
$ O
4.6 O
million2 O
, O
consuming O
a O
lot O
of O
computing O
resources O
. O

Therefore O
, O
how O
to O
reduce O
the O
training O
cost O
of O
PLM O
is O
of O
great O
importance O
to O
Green O
AI O
( O
Schwartz O
et O
al O
. O
, O
2020 O
) O
. O

Recently O
, O
there O
is O
a O
trend O
of O
training O
extremely O
large O
models O
to O
explore O
the O
upper O
limits O
of O
PLMs O
. O

For O
example O
, O
large O
pre O
- O
trained O
models O
, O
including O
GPT-3 B-MethodName
( O
Brown O
et O
al O
. O
, O
2020 O
) O
( O
175B O
) O
, O
PanGuα(Zeng O
et O
al O
. O
, O
2021 O
) O
( O
200B O
) O
and O
Switch B-MethodName
Transformers I-MethodName
( O
Fedus O
et O
al O
. O
, O
2021 O
) O
( O
1571B O
) O
, O
have O
been O
proved O
promising O
in O
language O
understanding O
and O
generation O
. O

However O
, O
these O
models O
are O
all O
pre O
- O
trained O
from O
scratch O
independently O
without O
utilizing O
the O
knowledge O
of O
smaller O
ones O
that O
have O
already O
been O
trained O
. O

On O
the O
other O
hand O
, O
our O
empirical O
studies O
show O
that O
the O
pre O
- O
trained O
models O
of O
different O
scales O
could O
share O
similar O
knowledge O
, O
for O
example O
in O
Figure O
2 O
, O
the O
attention O
patterns O
of O
the O
two O
PLMs O
with O
different O
sizes O
are O
similar O
. O

To O
save O
the O
training O
cost O
of O
large O
models O
, O
we O
2https://lambdalabs.com/blog/ O
demystifying O
- O
gpt-3/2134L2 O
H1 O
  O
L2 O
H2 O
  O
L2 O
H4 O
  O
L2 O
H5 O
  O
L12 O
H4 O
  O
L12 O
H5 O
  O
L12 O
H10 O
  O
L12 O
H11 O
L2 O
H8 O
  O
L2 O
H2 O
  O
L2 O
H4 O
  O
L2 O
H5 O
  O
L12 O
H3 O
  O
L12 O
H8 O
  O
L12 O

H5 O
  O
L12 O
H7Figure O
2 O
: O
The O
comparisons O
of O
attention O
patterns O
between O
small O
and O
large O
PLMs O
. O

The O
upper O
ones O
are O
the O
attention O
patterns O
of O
BERT O
BASE O
model O
whose O
architecture O
is O
{ O
L=12,D=768 O
} O
, O
and O
the O
lower O
ones O
are O
the O
attention O
patterns O
of O
one O
small O
BERT O
model O
whose O
architecture O
is O
{ O
L=12,D=512 O
} O
. O

We O
find O
that O
there O
are O
a O
large O
number O
of O
similar O
attention O
patterns O
in O
the O
same O
layer O
of O
the O
two O
models O
, O
indicating O
the O
possibility O
of O
reusing O
parameters O
of O
trained O
small O
PLMs O
to O
speed O
up O
the O
pre O
- O
training O
of O
large O
PLMs O
. O

The O
attention O
maps O
of O
PLMs O
with O
different O
layers O
are O
also O
similar O
, O
which O
is O
visualized O
in O
previous O
work O
( O
Gong O
et O
al O
. O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2020 O
) O
. O
propose O
the O
bert2BERT B-MethodName
method O
, O
which O
can O
efficiently O
transfer O
the O
learned O
knowledge O
of O
the O
smaller O
model O
to O
the O
large O
model O
. O

bert2BERT B-MethodName
consists O
of O
two O
components O
: O
( O
1 O
) O
For O
parameter O
initialization O
, O
we O
first O
extend O
the O
function O
preserving O
training O
( O
Chen O
et O
al O
. O
, O
2016 O
) O
to O
PLMs O
by O
duplicating O
and O
stacking O
the O
parameters O
of O
the O
existing O
smaller O
PLM O
, O
which O
we O
call O
function O
- O
preserving O
initialization O
( O
FPI O
) O
. O

FPI O
ensures O
that O
the O
initialized O
large O
model O
has O
almost O
the O
same O
behavior O
as O
the O
small O
model O
, O
so O
that O
the O
large O
model O
has O
a O
good O
starting O
point O
for O
later O
optimization O
. O

We O
also O
find O
that O
duplicating O
the O
weights O
of O
the O
upper O
layer O
to O
the O
current O
layer O
can O
further O
accelerate O
the O
convergence O
of O
the O
large O
model O
, O
which O
we O
call O
advanced O
knowledge O
initialization O
( O
AKI O
) O
. O

Although O
the O
AKI O
somewhat O
violates O
the O
principle O
of O
function O
preserving O
, O
we O
find O
that O
empirically O
it O
also O
has O
a O
good O
starting O
point O
as O
shown O
in O
Table O
1 O
, O
which O
leads O
to O
a O
faster O
convergence O
rate O
and O
achieves O
higher O
training O
efficiency O
. O

( O
2 O
) O
Secondly O
, O
a O
two O
- O
stage O
training O
strategy O
is O
further O
applied O
to O
the O
large O
model O
to O
accelerate O
the O
training O
process O
. O

To O
demonstrate O
the O
superiority O
of O
our O
method O
, O
we O
conduct O
extensive O
experiments O
on O
two O
representative O
PLMs O
: O
BERT B-MethodName
and O
GPT B-MethodName
, O
with O
different O
source O
model O
sizes O
. O

The O
results O
show O
that O
: O
( O
1 O
) O
our O
method O
can O
save O
a O
significant O
amount O
of O
computation O
in O
pre O
- O
training O
compared O
to O
the O
traditional O
way O
of O
learning O
from O
scratch O
and O
progressive O
stacking O
methods O
such O
as O
StackBERT B-MethodName
( O
Gong O
et O
al O
. O
, O
2019 O
) O
and O
MSLT B-MethodName
( O
Yang O
et O
al O
. O
, O
2020 O
) O
; O
( O
2 O
) O
our O
method O
is O
model O
- O
agnostic O
, O
which O
can O
be O
applied O
on O
a O
wide O
range O
of O
Transformer O
- O
based O
PLMs O
. O

One O
typical O
example O
is O
that O
, O
when O
using O
a O
small O
pre O
- O
trainedmodel O
with O
half O
the O
size O
of O
BERT B-MethodName
BASE I-MethodName
for O
initialization O
, O
bert2BERT B-MethodName
saves O
45 O
% O
computation O
cost O
of O
the O
original O
BERT B-MethodName
BASE I-MethodName
pre O
- O
training O
. O

In O
general O
, O
our O
contributions O
are O
summarized O
as O
follows O
: O
( O
1 O
) O
We O
explore O
a O
new O
direction O
for O
the O
efficient O
pre O
- O
training O
by O
reusing O
the O
trained O
parameters O
of O
small O
models O
to O
initialize O
the O
large O
model O
; O
( O
2 O
) O
We O
successfully O
extend O
function O
preserving O
method O
( O
Chen O
et O
al O
. O
, O
2016 O
) O
on O
BERT B-MethodName
and O
further O
propose O
advanced O
knowledge O
initialization O
, O
which O
can O
effectively O
transfer O
the O
knowledge O
of O
the O
trained O
small O
model O
to O
the O
big O
model O
and O
improve O
the O
pre O
- O
training O
efficiency O
; O
( O
3 O
) O
The O
proposed O
method O
outperforms O
other O
training O
methods O
and O
achieves O
45 O
% O
computation O
reduction O
on O
BERT B-MethodName
BASE I-MethodName
; O
( O
4 O
) O
Our O
method O
is O
generic O
, O
effective O
for O
both O
the O
BERT B-MethodName
and O
GPT B-MethodName
models O
, O
and O
have O
great O
potential O
to O
become O
an O
energy O
- O
efficient O
solution O
for O
pre O
- O
training O
super O
large O
- O
scale O
language O
models O
. O

2 O
Related O
Work O
Efficient O
Pre O
- O
training O
in O
NLP O
. O

The O
efficiency O
of O
pre O
- O
training O
has O
been O
explored O
by O
previous O
work O
. O

Some O
works O
( O
Gong O
et O
al O
. O
, O
2019 O
; O
Yang O
et O
al O
. O
, O
2020 O
; O
Gu O
et O
al O
. O
, O
2021 O
) O
propose O
progressive O
learning O
to O
accelerate O
the O
pre O
- O
training O
, O
which O
are O
motivated O
by O
the O
fact O
that O
different O
layers O
have O
some O
similar O
knowledge O
( O
e.g. O
, O
attention O
patterns O
) O
. O

They O
start O
pre O
- O
training O
a O
small O
model O
with O
fewer O
Transformer O
layers O
, O
and O
then O
iteratively O
expand O
the O
model O
by O
stacking O
the O
already O
trained O
layers O
on O
the O
top O
. O

Another O
line O
of O
work O
proposes O
to O
“ O
back O
distill O
” O
the O
knowledge O
of O
the O
small O
models O
into O
large O
models O
, O
which O
is O
termed O
as O
knowledge O
inheritance O
( O
Qin O
et O
al O
. O
, O
2021 O
) O
. O

Some O
works O
focus O
on O
the O
data O
effi-2135ciency O

( O
Wu O
et O
al O
. O
, O
2021 O
) O
and O
take O
notes O
for O
rare O
words O
during O
the O
pre O
- O
training O
process O
to O
help O
the O
model O
understand O
them O
when O
they O
occur O
next O
. O

ELECTRA B-MethodName
( O
Clark O
et O
al O
. O
, O
2020 O
) O
proposes O
a O
task O
of O
replaced O
token O
detection O
to O
predict O
whether O
each O
token O
in O
the O
input O
was O
replaced O
or O
not O
, O
which O
improves O
the O
pre O
- O
training O
efficiency O
. O

Our O
method O
is O
orthogonal O
to O
this O
kind O
of O
work O
and O
the O
combination O
of O
ELECTRA B-MethodName
and O
bert2BERT B-MethodName
could O
achieve O
better O
efficiency O
. O

In O
addition O
, O
there O
are O
several O
other O
orthogonal O
techniques O
for O
efficient O
pre O
- O
training O
: O
mixed O
- O
precision O
training O
( O
Shoeybi O
et O

al O
. O
, O
2019 O
) O
, O
large O
batch O
optimization O
( O
You O
et O
al O
. O
, O
2020 O
) O
, O
model O
architecture O
innovation O
( O
Lan O
et O
al O
. O
, O
2020 O
) O
, O
layer O
dropping O
technique O
( O
Zhang O
and O
He O
, O
2020 O
) O
, O
etc O
. O

Reusable O
Neural O
Network O
. O

Reusable O
neural O
network O
, O
a O
topic O
related O
to O
transfer O
learning O
( O
Pan O
and O
Yang O
, O
2010 O
) O
, O
is O
introduced O
to O
accelerate O
the O
model O
training O
in O
computer O
vision O
. O

One O
classical O
work O
is O
Net2Net B-MethodName
( O
Chen O
et O
al O
. O
, O
2016 O
) O
, O
which O
first O
proposes O
the O
concept O
of O
the O
function O
- O
preserving O
transformation O
to O
make O
neural O
networks O
reusable O
. O

However O
, O
Net2Net B-MethodName
randomly O
selects O
the O
neurons O
to O
be O
split O
. O

To O
handle O
this O
problem O
, O
some O
works O
( O
Wu O
et O
al O
. O
, O
2019 O
, O
2020b O
; O
Wang O
et O
al O
. O
, O
2019b O
; O
Wu O
et O
al O
. O
, O
2020a O
) O
leverage O
a O
functional O
steepest O
descent O
idea O
to O
decide O
the O
optimal O
subset O
of O
neurons O
to O
be O
split O
. O

The O
pruning O
technique O
( O
Han O
et O
al O
. O
, O
2015 O
) O
is O
also O
introduced O
for O
reusable O
neural O
networks O
( O
Feng O
and O
Panda O
, O
2020 O
) O
. O

In O
this O
paper O
, O
we O
study O
the O
reusable O
pre O
- O
trained O
language O
model O
and O
propose O
a O
new O
method O
, O
bert2BERT B-MethodName
, O
to O
accelerate O
the O
pre O
- O
training O
of O
BERT B-MethodName
and O
GPT B-MethodName
. O

3 O
Preliminary O
BERT B-MethodName
consists O
of O
one O
embedding O
layer O
and O
multiple O
Transformer B-MethodName
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
layers O
. O

3.1 O
Embedding O
Layer O
The O
embedding O
layer O
first O
maps O
the O
tokens O
in O
a O
sentence O
into O
vectors O
with O
an O
embedding O
matrix O
WE O
. O

Then O
one O
normalization O
layer O
is O
employed O
to O
produce O
the O
initial O
hidden O
states O
H0 O
. O

3.2 O
Transformer O
Layer O
The O
hidden O
states O
are O
iteratively O
processed O
by O
multiple O
Transformer O
layers O
as O
follows O
: O
Hl= O
Transformer O
l(Hl−1 O
) O
, O
l∈[1 O
, O
L O
] O
( O
1 O
) O
where O
Ldenotes O
the O
number O
of O
Transformer O
layers O
, O
each O
including O
a O
multi O
- O
head O
attention O
( O
MHA O
) O
and O
a O
feed O
- O
forward O
network O
( O
FFN).MHA O
. O

It O
is O
composed O
of O
multiple O
parallel O
selfattention O
heads O
. O

The O
hidden O
states O
of O
the O
previous O
layer O
are O
fed O
into O
each O
head O
and O
then O
the O
outputs O
of O
all O
heads O
are O
summed O
to O
obtain O
the O
final O
output O
as O
follows O
: O
Qi O
, O
Ki O
, O
Vi O
= O
Hl−1WQ O
l O
, O
i O
, O
Hl−1WK O
l O
, O
i O
, O
Hl−1WV O
l O
, O
i O
, O
HHEAD O
l O
, O
i O
= O
softmax(QiKiT O
√dk)ViWO O
l O
, O
i O
, O
MHA(Hl−1 O
) O

= O
aX O
i=1HHEAD O
l O
, O
i O
, O
HMHA O
l O
= O
LayerNorm O
( O
Hl−1 O
+ O
MHA O
( O
Hl−1 O
) O
) O
. O

( O
2 O
) O
Hl−1is O
linearly O
projected O
to O
queries O
( O
Qi O
) O
, O
keys O
( O
Ki O
) O
and O
values O
( O
Vi O
) O
using O
WQ O
l O
, O
i O
, O
WK O
l O
, O
i O
, O
WV O
l O
, O
irespectively O
. O

HHEAD O
l O
, O
iindicates O
the O
context O
- O
aware O
vector O
which O
is O
obtained O
by O
the O
scaled O
dot O
- O
product O
of O
queries O
and O
keys O
in O
the O
i O
- O
th O
attention O
head O
. O

a O
represents O
the O
number O
of O
self O
- O
attention O
heads O
. O

dk O
is O
the O
head O
dimension O
acting O
as O
the O
scaling O
factor O
. O

FFN O
. O

It O
consists O
of O
two O
linear O
layers O
and O
one O
GeLU O
activation O
function O
( O
Hendrycks O
and O
Gimpel O
, O
2016 O
) O
, O
that O
is O
: O
HFFN O
l= O
GeLU O
( O
HMHA O
lW1 O
l+b1 O
l)W2 O
l+b2 O
l O
, O
Hl= O
LayerNorm O
( O
HMHA O
l O
+ O
HFFN O
l O
) O
. O

( O
3 O
) O
Layer O
Normalization O
. O

Both O
the O
modules O
of O
MHA O
and O
FFN O
have O
one O
layer O
normalization O
( O
Ba O
et O
al O
. O
, O
2016 O
) O
that O
stabilizes O
the O
dynamics O
of O
the O
hidden O
state O
in O
the O
Transformer O
. O

Formally O
, O
it O
is O
written O
as O
: O
LayerNorm O
( O
H O
) O
= O
( O
H−µH O
σH)⊙WLN+bLN O
, O
( O
4 O
) O
where O
⊙means O
the O
element O
- O
wise O
multiplication O
. O

The O
statistics O
of O
µHandσHare O
the O
mean O
and O
variance O
of O
hidden O
states O
Hrespectively O
. O
4 O
Methodology O
4.1 O
Problem O
Statement O
We O
aim O
to O
accelerate O
the O
pre O
- O
training O
of O
target O
model O
T(Lt O
, O
Dt)by O
transferring O
the O
knowledge O
of O
an O
existing O
pre O
- O
trained O
source O
model O
S(Ls O
, O
Ds O
) O
, O
where O
Ls|tmeans O
the O
numbers O
of O
Transformer O
layer O
and O
Ds|tmeans O
the O
model O
width O
( O
i.e. O
, O
hidden O
size O
) O
, O
satisfying O
Ls≤LtandDs≤Dt O
. O

Formally O
, O
our O
problem O
is O
two O
- O
fold O
: O
( O
1 O
) O
how O
to O
perform O
an O
effective O
parameter O
initialization O
for O
Tby O
reusing O
the O
trained O
parameters O
of O
S O
, O
and O
( O
2 O
) O
how O
to O
efficiently2136train O
the O
initialized O
T O
, O
so O
that O
Tcan O
have O
a O
faster O
convergence O
rate O
in O
pre O
- O
training O
. O

4.2 O
Overview O
Targeting O
the O
above O
problems O
, O
bert2BERT B-MethodName
first O
initializes O
the O
target O
model O
Twith O
the O
parameters O
of O
the O
existing O
model O
Sby O
the O
width O
- O
wise O
expansion O
( O
Ds→Dt O
) O
and O
depth O
- O
wise O
expansion O
( O
Ls→Lt O
) O
. O

Through O
this O
expansion O
, O
the O
knowledge O
contained O
in O
the O
parameters O
of O
the O
source O
model O
is O
directly O
transferred O
to O
the O
target O
model O
. O

Then O
we O
further O
pre O
- O
train O
the O
initialized O
target O
model O
with O
a O
twostage O
pre O
- O
training O
method O
. O

The O
overall O
workflow O
is O
illustrated O
in O
Section O
4.5 O
. O

Essentially O
, O
the O
width O
- O
wise O
expansion O
can O
be O
decomposed O
into O
expansions O
of O
parameter O
matrices O
( O
or O
vectors3 O
) O
. O

As O
illustrated O
in O
Figure O
3 O
, O
the O
matrix O
expansion O
enlarges O
W∈ O
Rdw O
in∗dw O
outofSto O
U∈ O
Rdu O
in∗du O

outofTby O
two O
kinds O
of O
operations O
: O
in O
- O
dimension O
and O
out O
- O
dimension O
expansion O
. O

In O
the O
following O
sections O
, O
we O
first O
introduce O
two O
strategies O
of O
width O
- O
wise O
expansion O
: O
functionpreserving O
and O
advanced O
knowledge O
initialization O
. O

Then O
, O
we O
introduce O
the O
depth O
- O
wise O
expansion O
and O
detail O
the O
two O
- O
stage O
pre O
- O
training O
process O
. O

4.3 O
Width O
- O
wise O
Expansion O
For O
the O
paper O
clarity O
, O
we O
introduce O
two O
index O
mapping O
functions O
: O
ginandgout O
, O
where O
gin(i)means O
thei O
- O
th O
in O
- O
dimension O
of O
Ureuses O
the O
gin(i)-th O
indimension O
parameters O
of O
W O
, O
gout(j)means O
the O
j O
- O
th O
out O
- O
dimension O
of O
Ureuses O
the O
gout(j)-th O
outdimension O
parameters O
of O
W. O
Both O
our O
two O
methods O
are O
defined O
with O
these O
two O
mapping O
functions O
. O

W(i O
, O
j)means O
the O
parameter O
element O
, O
iandjrefer O
to O
the O
i O
- O
th O
in O
- O
dimension O
index O
and O
j O
- O
th O
outdimension O
index O
respectively O
. O

As O
shown O
in O
Figure O
3 O
, O
the O
i O
- O
th O
in O
- O
dimension O
parameters O
of O
Ware O
the O
parameters O
of O
the O
i O
- O
th O
input O
neuron O
of O
Wor O
the O
i O
- O
th O
column O
of O
W. O
4.3.1 O
Function O
Preserving O
Initialization O
Function O
preserving O
initialization O
( O
FPI O
) O
( O
Chen O
et O
al O
. O
, O
2016 O
) O
aims O
to O
make O
the O
initialized O
target O
model O
have O
the O
same O
function O
as O
the O
source O
model O
, O
which O
means O
that O
given O
the O
same O
input O
, O
the O
initialized O
target O
model O
has O
the O
same O
output O
as O
the O
source O
model O
. O

In O
this O
paper O
, O
we O
extend O
FPI O
on O
a O
different O
architecture O
, O
Transformer O
- O
based O
pre O
- O
trained O
language O
model O
. O

We O
give O
an O
example O
in O
Figure O
3 O
to O
illustrate O
3We O
omit O
the O
expansion O
of O
bias O
( O
vector O
) O
for O
simplicity O
. O

It O
follows O
a O
similar O
process O
as O
the O
matrix O
expansion O
. O

h1 O
𝑑out𝑤𝑑in𝑤❶ O
❷h2 O
x1 O
x2y1 O
y2 O
𝑜h1 O
h2 O
x1 O
x2y1 O
y2 O
x1h1 O

h2 O
x1 O
x2y1 O
y2 O
x1h2 O
𝑔in O
{ O
1:1,2:2,𝟑:𝟏}𝑔out O
{ O
1:1,2:2,𝟑:𝟐 O
} O
𝑝 O
𝑞𝑟𝑜𝑞𝑝 O

𝑟𝑎𝑏𝑐 O
𝑑 O
𝑜 O
2𝑞 O
2𝑞 O
2𝑜 O
2 O
𝑞 O
2𝑟𝑞 O

2 O
𝑜 O
2 O
𝑞 O
2𝑟𝑝𝑜 O
2𝑞 O
2𝑞 O
2𝑟𝑞 O
2𝑜 O
2𝑝𝑜 O
2 O
𝑞 O
2𝑟𝑞 O
2𝑑in𝑢𝑑in𝑢 O
𝑑out𝑢𝑏 O
2𝑏 O
2 O
𝑑 O
2𝑑 O
2❸ O
COPY O
& O
RE O
- O
SCALECOPY O
𝑾𝑼 O
𝑼~ O
Change O
. O

20211112𝑞 O
2𝑟𝑞 O
2Figure O
3 O
: O
Overview O
of O
the O
function O
preserving O
initialization O
( O
FPI O
) O
. O

Given O
the O
same O
input O
{ O
x1,x2 O
} O
, O
FPI O
ensures O
the O
initialized O
target O
model O
has O
the O
same O
output O
{ O
y1,y2 O
} O
with O
the O
source O
model O
. O

The O
first O
and O
the O
second O
steps O
are O
expanding O
the O
in O
- O
dimension O
and O
out O
- O
dimension O
of O
the O
parameter O
matrix O
according O
to O
mapping O
functions O
ginandgoutrespectively O
. O

After O
we O
expand O
the O
matrix O
WintoU O
, O
we O
use O
the O
in O
- O
dimension O
expansion O
on O
the O
upper O
parameter O
matrix O
again O
to O
ensure O
the O
output O
{ O
y1 O
, O
y2 O
} O
same O
as O
the O
original O
one O
. O

From O
the O
view O
of O
neurons O
, O
FPI O
copies O
the O
corresponding O
input O
and O
output O
neurons O
to O
expand O
the O
neural O
network O
. O
FPI O
. O

Formally O
, O
the O
mapping O
functions O
are O
defined O
as O
follows O
: O
gin(i O
) O
=( O

i O
i O
∈[1 O
, O
dw O
in O
] O
f({1,2 O
, O
... O
, O
dw O
in})i∈(dw O
in O
, O
du O
in],(5 O
) O
gout(j O
) O
=( O
j O
j O
∈[1 O
, O
dw O
out O
] O
f({1,2 O
, O
... O
, O
dw O
out})j∈(dw O
out O
, O
du O
out O
] O
, O
( O
6 O
) O
where O
f(·)is O
uniform O
sampling O
. O

We O
denote O
the O
weight O
expansion O
as O
U= O
EXPN O
( O
W;gin O
, O
gout O
) O
, O
which O
includes O
in O
- O
dimension O
expansion O
( O
Eq O
. O
7 O
) O
and O
out O
- O
dimension O
expansion O
( O
Eq O
. O
8) O
: O
Cgin(i)=du O
inX O
i′=1I(gin(i′ O
) O
= O
gin(i O
) O
) O
eU(i,∗)=1 O
Cgin(i)W(gin(i),∗),(7 O
) O
U(∗,j)=eU(∗,gout(j O
) O
) O
, O
( O
8) O
where O
I(·)is O
an O
indicator O
function O
, O
and O
Cgin(i)is O
the O
count O
of O
gin(i)in O
the O
values O
of O
gin O
( O
· O
) O
, O
which O
is O
used O
to O
re O
- O
scale O
the O
original O
parameters O
to O
keep O
the O
function O
preserving O
property O
. O

Expansion O
for O
All O
Modules O
. O

We O
apply O
FPI O
for O
all O
modules O
of O
BERT O
via O
matrix O
expansion O
EXPN O
( O
· O
) O
. O

Specifically O
, O
for O
the O
embedding O
matrix O
WE O
, O
we O
only O
conduct O
the O
out O
- O
dimension O
expansion O
: O
UE O
( O
∗,j)=WE O
( O
∗,ge O
out(j O
) O
) O
. O

( O
9 O
) O
MHA O
module O
can O
be O
decomposed O
into O
multiple O
parallel O
self O
- O
attention O
heads O
and O
we O
conduct O
the O
head O
- O
wise O
expansion O
for O
this O
module O
, O
which O
means2137increasing O
the O
number O
of O
attention O
heads O
. O

The O
headwise O
expansion O
is O
formulated O
as O
: O
UQ|K|V|O= O
EXPN O
( O
WQ|K|V|O;gq|k|v|o O
in O
, O
gq|k|v|o O
out O
) O
. O

( O
10 O
) O
Specifically O
, O
the O
head O
- O
wise O
expansion O
means O
that O
we O
reuse O
the O
head O
group O
parameters O
to O
construct O
the O
new O
matrices O
. O

The O
i O
- O
th O
head O
group O
in O
l O
- O
th O
layer O
contains O
WQ O
l O
, O
i|WK O
l O
, O
i|WV O
l O
, O
i|WO O
l O
, O
iin O
Eq O
. O

2 O
and O
the O
outdimension O
expansion O
for O
WQ O
l O
, O
i|WK O
l O
, O
i|WV O
l O
, O
iis O
: O
gq|k|v O
out(j O
) O
=( O
j O
j O
∈[1 O
, O
as O
] O
f({1,2 O
, O
... O
, O
as})j∈(as O
, O
at O
] O
, O
( O
11 O
) O
where O
jis O
the O
head O
index O
and O
as|tmean O
the O
head O
numbers O
of O
source O
model O
and O
target O
model O
respectively O
. O

The O
module O
has O
three O
constraints O
: O
{ O
ge O
out O
= O
gq|k|v O
in;gq|k|v O
out O
= O
go O
in;gq|k|v O
in O
= O
go O
out O
} O
, O
with O
the O
first O
two O
constraints O
for O
hidden O
dimension O
consistency O
( O
Wen O
et O
al O
. O
, O
2018 O
; O
Chen O
et O
al O
. O
, O
2021 O
) O
and O
the O
third O
one O
for O
residual O
connection O
( O
Eq O
. O
2 O
) O
. O

For O
the O
FFN O
module O
, O
we O
perform O
the O
expansion O
on O
the O
parameter O
matrices O
W1|2(Eq O
. O

3 O
) O
as O
follows O
: O
U1|2= O
EXPN O
( O
W1|2;g1|2 O
in O
, O
g1|2 O
out O
) O
. O

( O
12 O
) O
Similar O
to O
the O
MHA O
module O
, O
the O
mapping O
functions O
of O
FFN O
also O
have O
three O
constraints O
: O
{ O
go O
out O
= O
g1 O
in O
; O
g1 O
out O
= O
g2 O
in;g1 O
in O
= O
g2 O
out O
} O
. O

For O
the O
layer O
normalization O
, O
we O
take O
the O
layer O
normalization O
of O
FFN O
as O
an O
example O
, O
its O
expansion O
is O
formulated O
as O
: O
ULN O
j O
= O
WLN O
g2 O
out(j O
) O
. O

( O
13 O
) O
Note O
that O
in O
layer O
normalization O
( O
Eq O
. O
4 O
) O
, O
the O
mean O
µ O
and O
variance O
σare O
calculated O
based O
on O
the O
hidden O
representations O
H. O
Thus O
, O
the O
expansion O
of O
this O
parameter O
inevitably O
induces O
a O
gap O
and O
prevents O
the O
target O
model O
from O
strictly O
following O
the O
function O
preserving O
principle O
. O

However O
, O
we O
empirically O
find O
that O
the O
gap O
is O
so O
small O
that O
it O
can O
hardly O
affect O
the O
initialization O
and O
convergence O
of O
the O
target O
model O
. O

Thus O
we O
ignore O
this O
discrepancy O
. O

We O
have O
validated O
the O
effectiveness O
of O
the O
adapted O
FPI O
in O
different O
settings O
in O
Table O
1 O
. O

The O
results O
show O
that O
the O
initialized O
model O
Tachieves O
almost O
the O
same O
loss O
as O
S O
, O
demonstrating O
that O
FPI O
successfully O
retains O
the O
knowledge O
of O
the O
small O
model O
when O
performing O
parameter O
expansion O
. O

4.3.2 O
Advanced O
Knowledge O
Initialization O
To O
further O
improve O
the O
convergence O
rate O
of O
the O
pretraining O
target O
model O
, O
we O
propose O
the O
advanced O
knowledge O
initialization O
( O
AKI O
) O
, O
which O
expands O
newMethod O
S(12,384 O
) O
S(12,512 O
) O

Original O
1.89 O
1.67 O
Rand O
10.40 O
10.42 O
DirectCopy O
9.05 O
6.45 O
FPI O
1.89 O
1.70 O
AKI O
2.08 O
1.96 O
Table O
1 O
: O
The O
comparison O
of O
MLM O
losses O
between O
FPI O
and O
baselines O
. O

“ O
Original O
” O
refers O
to O
the O
MLM O
losses O
of O
source O
pre O
- O
trained O
models O
S. O
“ O
Rand O
” O
refers O
to O
the O
MLM O
losses O
of O
randomly O
initialized O
target O
models O
. O

“ O
DirectCopy O
” O
refers O
to O
a O
naive O
method O
that O
directly O
copies O
the O
source O
model O
to O
the O
target O
model O
and O
the O
unfilled O
part O
is O
randomly O
initialized O
, O
“ O
FPI O
” O
represents O
the O
function O
preserving O
method O
. O

We O
expand O
both O
models O
to O
the O
target O
model O
T(12,768 O
) O
and O
find O
that O
FPI O
can O
make O
the O
target O
model O
have O
similar O
losses O
with O
these O
trained O
source O
models O
. O

The O
loss O
gap O
between O
FPI O
and O
Original O
is O
brought O
by O
layer O
normalization O
. O

“ O
AKI O
” O
represents O
the O
advanced O
knowledge O
initialization O
method O
. O

matrices O
based O
on O
not O
only O
the O
parameters O
of O
the O
same O
layer O
but O
also O
the O
parameters O
of O
the O
upper O
layer O
in O
the O
source O
model O
. O

The O
intuition O
is O
based O
on O
previous O
findings O
( O
Jawahar O
et O
al O
. O
, O
2019 O
; O
Clark O
et O
al O
. O
, O
2019 O
) O
that O
adjacent O
Transformer O
layers O
have O
similar O
functionality O
, O
which O
ensures O
that O
it O
will O
not O
damage O
the O
knowledge O
contained O
in O
the O
parameters O
of O
the O
current O
layer O
. O

Moreover O
, O
the O
knowledge O
that O
comes O
from O
adjacent O
layers O
can O
break O
the O
symmetry O
( O
Chen O
et O
al O
. O
, O
2016 O
) O
appeared O
in O
FPI O
, O
which O
has O
been O
demonstrated O
beneficial O
. O

We O
give O
an O
illustrative O
example O
in O
Figure O
4 O
and O
formulate O
AKI O
as O
: O
Ul= O
EXPN O
( O
Wl O
, O
Wl+1;gl|l+1 O
in O
, O
gl O
out).(14 O
) O

Specifically O
, O
we O
first O
do O
the O
in O
- O
dimension O
expansion O
forWl|l+1 O
. O

Here O
we O
take O
Wlas O
an O
example O
: O
Cgl O
in(i)=du O
inX O
i′=1I(gl O
in(i′ O
) O
= O
gl O
in(i O
) O
) O

eUl O
( O
i,∗)=1 O
Cgl O
in(i)Wl O
( O
gl O
in(i),∗).(15 O
) O
It O
is O
similar O
with O
Eq O
. O

7 O
. O
Then O
we O
stack O
the O
expanded O
matrices O
of O
eUlandeUl+1to O
construct O
the O
final O
matrix O
: O

Ul O
( O
∗,j)=(eUl O
( O
∗,j)j∈[1 O
, O
dw O
out O
] O
eUl+1 O
( O
∗,gl O
out(j))j∈(dw O
out O
, O
du O
out].(16 O
) O
We O
directly O
copy O
the O
expanded O
eUlas O
the O
top O
part O
of O
the O
new O
matrix O
and O
place O
the O
sampled O
parameters O
fromeUl+1on O
the O
bottom O
of O
the O
new O
matrix O
. O

We O
aggregate O
upper O
- O
layer O
information O
into O
a O
new O
matrix O
for O
two O
intuitions O
: O
( O
1 O
) O
it O
breaks O
the O
FPI O
symmetry O
that O
hinders O
model O
convergence O
( O
Chen O
et O
al O
. O

, O
2138𝑜𝑝 O
𝑞𝑟 O
𝑎𝑏 O
𝑐𝑑𝑔in𝑙 O
{ O
1:1,2:2,𝟑:𝟏}𝑜 O
2 O
𝑞 O
2𝑟𝑝𝑜 O
2𝑞 O
2 O
𝑏 O
2𝑑 O
2𝑐𝑎𝑏 O
2𝑑 O
2𝑔in𝑙+1 O
{ O
1:1,2:2,𝟑:𝟐}𝑜 O
2 O
𝑞 O
2𝑟𝑝𝑜 O
2𝑞 O
2𝑔out𝑙 O
{ O
1:1,2:2,𝟑:𝟐 O
} O
𝑑 O
2𝑐𝑑 O
2❶ O
❷ O
Cheng O
20211113COPY𝑑in𝑤 O
𝑑out𝑤𝑑in𝑢𝑑in𝑢 O
𝑑out𝑢 O
𝑾𝑙+1𝑾𝑙 O
෩𝑼𝑙 O
෩𝑼𝑙+1𝑼𝑙Figure O
4 O
: O
Overview O
of O
AKI O
. O

It O
first O
performs O
the O
indimension O
expansion O
on O
both O
the O
matrixes O
of O
current O
and O
upper O
layers O
. O

Then O
it O
uses O
the O
widened O
matrix O
of O
the O
current O
layer O
as O
the O
top O
part O
of O
the O
new O
matrix O
and O
samples O
the O
row O
of O
the O
widened O
matrix O
of O
the O
upper O
layer O
as O
the O
bottom O
part O
of O
the O
new O
matrix O
. O
2016 O
) O
. O

For O
example O
, O
FPI O
makes O
the O
attention O
patterns O
in O
the O
same O
layer O
repeated O
, O
which O
is O
redundant O
and O
called O
symmetry O
; O
( O
2 O
) O
upper O
- O
layer O
information O
can O
be O
used O
as O
similar O
but O
high O
- O
level O
knowledge O
to O
guide O
the O
model O
to O
converge O
faster O
. O

We O
display O
the O
attention O
patterns O
of O
the O
target O
model O
initialized O
by O
AKI O
in O
Appendix O
E O
and O
find O
that O
the O
target O
model O
can O
maintain O
the O
attention O
patterns O
of O
both O
current O
and O
upper O
layers O
very O
well O
. O

Expansion O
for O
All O
Modules O
. O

For O
embedding O
matrix O
, O
we O
only O
do O
the O
out O
- O
dimension O
expansion O
as O
Eq O
. O
9 O
in O
the O
FPI O
. O

Both O
the O
modules O
of O
MHA O
and O
FFN O
do O
the O
matrix O
expansion O
by O
following O
the O
defined O
operation O
in O
Eq O
. O
15 O
and O
Eq O
. O

16 O
. O

The O
constraints O
of O
mapping O
functions O
follow O
the O
setting O
of O
FPI O
. O

Empirically O
, O
we O
find O
that O
the O
AKI O
method O
outperforms O
FPI O
, O
while O
the O
performance O
is O
worse O
if O
we O
build O
a O
new O
matrix O
based O
on O
the O
matrix O
of O
the O
lower O
layer O
( O
or O
low O
- O
level O
knowledge O
) O
. O

How O
to O
construct O
the O
optimal O
initialization O
for O
the O
target O
model O
with O
the O
parameters O
of O
different O
layers O
remains O
an O
open O
question O
and O
we O
leave O
it O
as O
future O
work O
. O

For O
more O
details O
, O
we O
give O
a O
clear O
illustration O
of O
the O
FPI O
and O
AKI O
process O
in O
Appendix O
F. O
4.4 O
Depth O
- O
wise O
Expansion O
After O
the O
width O
- O
wise O
expansion O
, O
we O
obtain O
a O
widened O
model O
with O
the O
same O
width O
as O
the O
target O
model O
. O

To O
bridge O
the O
depth O
gap O
, O
we O
perform O
depthwise O
expansion O
to O
increase O
model O
depth O
to O
the O
depth O
of O
the O
target O
model O
. O

We O
illustrate O
this O
process O
in O
Algorithm O
1 O
and O
the O
main O
idea O
is O
to O
iteratively O
stack O
the O
widened O
model O
until O
its O
depth O
is O
equal O
to O
the O
target O
model O
( O
Gong O
et O
al O
. O
, O
2019 O
) O
. O

4.5 O
Two O
- O
stage O
Pre O
- O
training O
To O
further O
improve O
the O
pre O
- O
training O
efficiency O
of O
initialized O
target O
model O
, O
we O
propose O
a O
two O
- O
stage O
training O
method O
: O
( O
1 O
) O
train O
sub O
- O
models O
with O
differentAlgorithm O
1 O
Target O
Model O
Initialization O
Input O
: O
the O
target O
model O
T(Lt O
, O
Dt)and O
the O
source O
model O
S(Ls O
, O
Ds O
) O
. O

1 O
: O
T1(Ls O
, O
Dt)←do O
AKI O
or O
FPI O
with O
S(Ls O
, O
Ds O
) O
2 O
: O
k← O
⌊Lt O
/ O
Ls⌋ O
3 O
: O
fort= O
2→kdo O
4 O
: O
Tt(Ls·t O
, O
Dt)←stackT1on O
top O
of O
Tt−1 O
5 O
: O
end O
for O
6 O
: O
T O
← O
stack O
top O
Lt−Ls·klayers O
of O
T1 O
. O

Output O
: O
the O
initialized O
model O
T(Lt O
, O
Dt O
) O
Algorithm O
2 O
Two O
- O
stage O
Pre O
- O
training O
Input O
: O
the O
initialized O
model O
T O
, O
large O
- O
scale O
unsupervised O
dataset O
D O
, O
the O
epoch O
number O
of O
submodel O
training O
Eband O
the O
epoch O
number O
of O
whole O
training O
process O
E O
, O
the O
layer O
number O
lb O
. O
1 O
: O
Construct O
sub O
- O
models O
and O
these O
models O
have O
the O
layer O
numbers O
of O
{ O
lb,2·lb O
, O
. O
. O
. O

, O
Lt O
} O
. O

2 O
: O
fore= O
1→Ebdo O
3 O
: O
forbatch O
inDdo O
4 O
: O
T′←sample O
one O
sub O
- O
model O
. O
5 O
: O
Perform O
forward O
and O
backward O
of O
T′. O
6 O
: O
Update O
only O
top O
lblayers O
of O
T′. O
7 O
: O
end O
for O
8 O
: O
end O
for O
9 O
: O
fore O
= O
Eb→Edo O
10 O
: O
forbatch O
inDdo O
11 O
: O
Perform O
forward O
and O
backward O
of O
T. O
12 O
: O
Update O
whole O
model O
T. O
13 O
: O
end O
for O
14 O
: O
end O
for O
Output O
: O
the O
pre O
- O
trained O
model O
T O
layers O
in O
a O
random O
manner O
to O
make O
the O
complete O
model O
converge O
at O
a O
low O
cost O
. O

These O
sub O
- O
models O
are O
built O
with O
bottom O
Transformer O
layers O
of O
the O
initialized O
target O
model O
and O
share O
one O
classification O
layer O
. O

At O
each O
optimization O
step O
, O
we O
randomly O
sample O
one O
sub O
- O
model O
and O
only O
update O
its O
top O
Transformer O
layers O
and O
the O
shared O
classification O
layer O
. O

( O
2 O
) O
After O
the O
sub O
- O
structure O
training O
, O
we O
further O
perform O
the O
traditional O
full O
- O
model O
training O
. O

The O
details O
of O
our O
method O
are O
displayed O
in O
Algorithm O
2 O
. O
5 O
Experiment O
5.1 O
Experimental O
Setup O
Pre O
- O
training O
Details O
. O

We O
use O
the O
English B-DatasetName
Wikipedia I-DatasetName
and O
Toronto B-DatasetName
Book I-DatasetName
Corpus I-DatasetName
( O
Zhu O
et O
al O
. O
, O
2015 O
) O
as O
the O
pre O
- O
training O
data O
. O

The O
settings O
of O
pretraining O
are O
: O
peak O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-4 B-HyperparameterValue
, O
warmup2139Model O
FLOPs O
Ratio O
Loss O
SQuADv1.1 O
SST-2 O
MNLI O
MRPC O
CoLA O
QNLI O
QQP O
STS O
- O
B O
Avg O
. O
( O
×1e19 O
) O
( O
Saving O
) O
( O
MLM O
) O
( O
F1 O
) O
( O
Acc O
) O
( O
Acc O
) O
( O
Acc O
) O
( O
Mcc O
) O
( O
Acc O
) O
( O
Acc O
) O
( O
Acc O
) O
BERT O
BASE O
( O
Google O
) O
- O
- O
- O
88.4(0.1 O
) O
93.6(0.2 O
) O
84.7(0.1 O
) O
87.9(0.9 O
) O
59.6(1.5 O
) O
91.6(0.1 O
) O
91.4(0.1 O
) O
89.6(0.5 O
) O
85.8(0.1 O
) O
BERT O
BASE†(Ours O
) O
7.3 O
0 O
% O
1.437 O
89.6(0.1 O
) O
92.7(0.2 O
) O
84.6(0.2 O
) O
88.6(0.5 O
) O
57.3(4.0 O
) O
90.6(0.7 O
) O
90.6(0.1 O
) O
89.9(0.3 O
) O
85.5(0.5 O
) O
Progressive O
Training O
MSLT† O
6.5 O
10.7 O
% O
1.436 O
90.4(0.2 O
) O
92.9(0.2 O
) O
85.1(0.2 O
) O
87.9(2.1 O
) O
55.6(4.1 O
) O
90.7(0.2 O
) O
90.6(0.2 O
) O
88.2(0.6 O
) O
85.2(0.7 O
) O
StackBERT† O
5.5 O
24.3 O
% O
1.433 O
90.4(0.2 O
) O
92.6(0.4 O
) O
85.3(0.1 O
) O
88.2(1.0 O
) O
63.2(0.9 O
) O
91.0(0.4 O
) O
91.0(0.1 O
) O
86.7(0.7 O
) O
86.0(0.2 O
) O
bert2BERT O
: O
S(12 O
, O
512 O
) O
→ O
T O
( O
12 O
, O
768 O
) O
DirectCopy O
6.4 O
12.2 O
% O
1.436 O
89.8(0.2 O
) O
92.9(0.3 O
) O
84.7(0.2 O
) O
86.2(0.6 O
) O
62.2(0.7 O
) O
90.2(0.6 O
) O
90.4(0.1 O
) O
89.2(0.1 O
) O
85.7(0.1 O
) O
FPI O
5.1 O
30.4 O
% O
1.436 O
90.0(0.2 O
) O
92.6(0.4 O
) O
85.2(0.1 O
) O
87.1(0.5 O
) O
61.5(0.9 O
) O
90.9(0.6 O
) O
90.8(0.2 O
) O
89.7(0.2 O
) O
86.0(0.1 O
) O
AKI O
4.5 O
38.4 O
% O
1.434 O
90.4(0.1 O
) O
92.5(0.4 O
) O
85.3(0.4 O
) O
87.8(0.9 O
) O
61.0(1.4 O
) O
91.2(0.2 O
) O
90.5(0.1 O
) O
89.5(0.2 O
) O
86.0(0.2 O
) O
bert2BERT O
4.0 O
45 O
.2 O
% O
1.433 O
90.0(0.2 O
) O
92.9(0.1 O
) O
85.1(0.1 O
) O
87.7(0.7 O
) O
60.0(1.2 O
) O
90.5(0.8 O
) O
90.4(0.1 O
) O
89.2(0.2 O
) O
85.7(0.4 O
) O
Table O
2 O
: O
Comparison O
between O
bert2BERT B-MethodName
and O
baselines O
. O

We O
report O
mean O
( O
and O
standard O
deviation O
) O
performance O
over O
3 O
runs O
on O
the O
dev O
set O
. O

bert2BERT B-MethodName
means O
the O
combination O
of O
AKI O
and O
two O
- O
stage O
pre O
- O
training O
here O
. O

FPI O
and O
AKI O
mean O
that O
the O
function O
preserving O
initialization O
, O
advanced O
knowledge O
initialization O
respectively O
. O

†means O
the O
re O
- O
implemented O
results O
, O
where O
the O
BERT B-MethodName
BASE I-MethodName
and O
StackBERT B-MethodName
achieve O
similar O
results O
with O
the O
original O
paper O
, O
and O
the O
MSLT B-MethodName
result O
is O
different O
from O
the O
original O
paper O
may O
be O
due O
to O
the O
different O
training O
settings O
( O
e.g. O
, O
in O
the O
original O
paper O
, O
it O
uses O
the O
LAMB O
optimizer O
( O
You O
et O
al O
. O
, O
2020 O
) O
and O
only O
trains O
the O
corpus O
with O
a O
max O
sequence O
length O
of O
128 O
) O
. O

steps B-HyperparameterName
of O
10k B-HyperparameterValue
, O
training O
epochs B-HyperparameterName
of O
E=40 B-HyperparameterValue
, O
batch B-HyperparameterName
size I-HyperparameterName
of O
512 B-HyperparameterValue
, O
sub B-HyperparameterName
- I-HyperparameterName
model I-HyperparameterName
training I-HyperparameterName
epochs I-HyperparameterName
of O
Eb=5 B-HyperparameterValue
, O
layer B-HyperparameterName
number I-HyperparameterName
of O
lb=3 B-HyperparameterValue
. O

Unless O
otherwise O
noted O
, O
all O
methods O
including O
bert2BERT B-MethodName
and O
baselines O
use O
the O
same O
pre O
- O
training O
settings O
for O
fair O
comparisons O
. O

In O
the O
settings O
of O
bert2BERT B-MethodName
, O
the O
target O
model O
has O
a O
BERT B-MethodName
BASE I-MethodName
architecture O
of O
T(12,768 O
) O
and O
the O
source O
model O
has O
an O
architecture O
of O
S(12,512 O
) O
. O

Fine O
- O
tuning O
Details O
. O

For O
the O
evaluation O
, O
we O
use O
tasks O
from O
GLUE B-DatasetName
benchmark O
( O
Wang O
et O
al O
. O
, O
2019a O
) O
and O
SQuADv1.1 B-DatasetName
( O
Rajpurkar O
et O
al O
. O
, O
2016 O
) O
. O

We O
report O
F1 B-MetricName
for O
SQuADv1.1 B-DatasetName
, O
Matthews B-MetricName
correlation I-MetricName
coefficient I-MetricName
( O
Mcc O
) O
for O
CoLA B-DatasetName

( O
Warstadt O
et O
al O
. O
, O
2019 O
) O
and O
accuracy B-MetricName
( O
Acc O
) O
for O
other O
tasks O
. O

For O
the O
GLUE B-DatasetName
tasks O
fine O
- O
tuning O
, O
we O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
32 B-HyperparameterValue
, O
choose O
the O
learning B-HyperparameterName
rate I-HyperparameterName
from O
{ O
5e-6 B-HyperparameterValue
, O
1e-5 B-HyperparameterValue
, O
2e-5 B-HyperparameterValue
, O
3e-5 B-HyperparameterValue
} O
and O
epochs B-HyperparameterName
from O
{ O
4 B-HyperparameterValue
, O
5 B-HyperparameterValue
, O
10 B-HyperparameterValue
} O
. O

For O
the O
SQuADv1.1 B-DatasetName
finetuning O
, O
we O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
16 B-HyperparameterValue
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
3e-5 B-HyperparameterValue
, O
and O
the O
number B-HyperparameterName
of I-HyperparameterName
training I-HyperparameterName
epochs I-HyperparameterName
to O
4 B-HyperparameterValue
. O

All O
results O
are O
the O
average O
of O
3 O
runs O
on O
the O
dev O
set O
. O
Baselines O
. O

We O
first O
introduce O
a O
naive O
bert2BERT B-MethodName
baseline O
named O
DirectCopy O
, O
which O
directly O
copies O
the O
small O
model O
to O
the O
target O
model O
and O
randomly O
initializes O
the O
unfilled O
parameters O
. O

StackBERT B-MethodName
( O
Gong O
et O
al O
. O
, O
2019 O
) O
and O
MSLT B-MethodName
( O
Yang O
et O
al O
. O
, O
2020 O
) O
are O
also O
included O
as O
the O
baselines O
. O

Both O
of O
them O
are O
trained O
in O
a O
progressive O
manner O
. O

Following O
the O
original O
setting O
, O
for O
the O
StackBERT B-MethodName
, O
we O
first O
train O
the O
3 O
- O
layer O
BERT B-MethodName
for O
5 B-HyperparameterValue
epochs B-HyperparameterName
, O
stack O
it O
twice O
into O
a O
6 O
- O
layer O
BERT B-MethodName
and O
then O
train O
it O
for O
7 B-HyperparameterValue
epochs B-HyperparameterName
. O

In O
the O
final O
step O
, O
we O
stack O
the O
6 O
- O
layer O
model O
into O
BERT B-MethodName
BASE I-MethodName
and O
further O
train O
it O
with O
28 B-HyperparameterValue
epochs B-HyperparameterName
. O

For O
MSLT B-MethodName
, O
we O
first O
perform O
4 O
- O
stage O
training O
. O

In O
each O
stage O
, O
we O
add O
the O
top O
3 O
layers O
of O
the O
model O
already O
trained O
to O
the O
top O
of O
the O
model O
and O
then O
pre O
- O
train O
the O
new O
model O
by O
partially O
updatingthe O
top O
3 O
layers O
. O

Each O
stage O
of O
the O
partial O
training O
process O
has O
8 B-HyperparameterValue
epochs B-HyperparameterName
. O

Finally O
, O
we O
further O
perform O
20 O
full O
- O
model O
training O
epochs4to O
achieve O
the O
same O
loss O
as O
BERT B-MethodName
BASE I-MethodName
trained O
from O
scratch O
. O

The O
baselines O
are O
trained O
using O
the O
same O
optimizer O
, O
training B-HyperparameterName
steps I-HyperparameterName
, O
and O
warmup B-HyperparameterName
steps I-HyperparameterName
as O
the O
bert2BERT B-MethodName
. O
5.2 O
Results O
and O
Analysis O
We O
demonstrate O
the O
effectiveness O
of O
the O
proposed O
method O
on O
the O
SQuAD B-DatasetName
and O
GLUE B-DatasetName
benchmark O
. O

The O
results O
are O
shown O
in O
Table O
2 O
. O

We O
also O
represent O
the O
loss O
curves O
in O
Figure O
1 O
and O

Appendix O
A. O

The O
results O
show O
that O
: O
( O
1 O
) O
DirectCopy O
only O
saves O
12.2 O
% O
computational O
costs O
, O
which O
indicates O
this O
naive O
method O
of O
directly O
copying O
the O
trained O
parameters O
of O
the O
source O
model O
to O
the O
target O
model O
is O
not O
effective O
; O
( O
2 O
) O
our O
proposed O
methods O
, O
FPI O
and O
AKI O
, O
achieve O
better O
performances O
than O
the O
baselines O
. O

Although O
AKI O
does O
not O
follow O
the O
function O
preserving O
, O
it O
has O
a O
bigger O
loss O
than O
FPI O
at O
the O
start O
of O
training O
, O
AKI O
achieves O
a O
faster O
convergence O
rate O
by O
using O
the O
advanced O
knowledge O
and O
breaking O
the O
symmetry O
; O
( O
3 O
) O
by O
performing O
the O
two O
- O
stage O
pre O
- O
training O
on O
the O
target O
model O
initialized O
by O
AKI O
, O
we O
can O
save O
45.2 O
% O
computational O
costs O
. O

Note O
that O
the O
total O
parameters O
of O
the O
source O
model O
are O
half O
of O
those O
of O
the O
target O
model O
( O
54 O
M O
vs. O
110 O
M O
) O
. O

The O
loss O
of O
bert2BERT B-MethodName
in O
Figure O
1 O
is O
high O
at O
the O
stage O
of O
sub O
- O
model O
training O
because O
it O
represents O
the O
average O
loss O
of O
all O
submodels O
. O

We O
also O
compare O
the O
attention O
patterns O
of O
the O
target O
models O
initialized O
by O
DirectCopy O
, O
FPI O
, O
and O
AKI O
. O

The O
attention O
patterns O
and O
their O
discussions O
are O
displayed O
in O
Appendix O
E. O
4We O
have O
tried O
the O
same O
setting O
as O
the O
original O
paper O
with O
8 O
epoch O
full O
- O
model O
running O
but O
it O
does O
not O
achieve O
the O
same O
loss O
with O
BERT O
BASE O
( O
1.511 O
vs. O
1.437).2140bert2BERT O
with O
Smaller O
Source O
Model O
. O

We O
also O
evaluate O
bert2BERT B-MethodName
on O
different O
settings O
, O
where O
the O
source O
model O
S(6 O
, O
512 O
) O
, O
S(8 O
, O
512 O
) O
, O
S(10 O
, O
512 O
) O
are O
significantly O
smaller O
than O
the O
target O
model O
( O
35 O
M O
| O
42 O
M O
| O
48 O
M O
vs. O
110 O
M O
) O
. O

The O
results O
are O
shown O
in O
Table O
3 O
and O
loss O
curves O
are O
displayed O
in O
Appendix O

B. O

We O
observe O
that O
DirectCopy O
for O
S(6 O
, O
512 O
) O
achieves O
no O
efficiency O
improvement O
over O
the O
original O
pre O
- O
training O
, O
which O
indicates O
that O
the O
significant O
size O
gap O
between O
the O
source O
and O
target O
model O
greatly O
reduces O
the O
benefit O
of O
DirectCopy O
methods O
. O

Compared O
with O
DirectCopy O
, O
our O
proposed O
method O
reduces O
the O
computation O
cost O
by O
23.3 O
% O
, O
which O
again O
demonstrates O
the O
effectiveness O
of O
bert2BERT B-MethodName
. O

The O
results O
show O
that O
the O
smaller O
the O
size O
gap O
between O
the O
source O
model O
and O
target O
model O
, O
the O
greater O
the O
cost O
savings O
of O
bert2BERT B-MethodName
. O

We O
also O
note O
that O
it O
is O
more O
challenging O
to O
speed O
up O
the O
target O
model O
with O
a O
small O
source O
model O
S(6 O
, O
512 O
) O
. O

We O
encourage O
future O
work O
to O
explore O
to O
transfer O
the O
knowledge O
from O
smaller O
source O
models O
to O
improve O
the O
pre O
- O
training O
efficiency O
of O
the O
target O
model O
. O

Settings O
Model O
FLOPs O
Ratio O
Loss O
Avg O
. O
( O
×1e19 O
) O
( O
Saving O
) O
( O
MLM O
) O
S(6 O
, O
512)DirectCopy O
7.3 O
0 O
% O
1.440 O
89.1 O
bert2BERT O
5.6 O
23.3 O
% O
1.435 O
89.3 O
S(8 O
, O
512 O
) O

bert2BERT O
4.6 O
36.8 O
% O
1.435 O
89.2 O
S(10 O
, O
512 O
) O
bert2BERT O
4.2 O
42.7 O
% O
1.434 O
89.1 O
Table O
3 O
: O
bert2BERT O
with O
smaller O
source O
model O
. O

Avg O
means O
the O
average O
score O
of O
SST-2 O
/ O
MNLI O
/ O
SQuADv1.1 O
. O

Effect O
of O
Sub O
- O
model O
Training O
Epochs O
. O

Our O
training O
procedure O
includes O
two O
stages O
: O
sub O
- O
model O
training O
and O
full O
- O
model O
training O
. O

Here O
, O
we O
study O
the O
effect O
of O
the O
number O
of O
sub O
- O
model O
training O
epochs O
by O
performing O
bert2BERT O
on O
the O
different O
settings O
ofEb={0 O
, O
5 O
, O
10 O
, O
20 O
} O
. O

The O
results O
are O
presented O
in O
Table O
4 O
and O
the O
loss O
curves O
are O
displayed O
in O
Appendix O
C. O

We O
observe O
that O
our O
method O
achieves O
the O
best O
efficiency O
when O
the O
epoch O
number O
is O
set O
to O
5 O
, O
while O
a O
larger O
or O
smaller O
epoch O
number O
will O
bring O
a O
negative O
impact O
. O

Model O
FLOPs O
Ratio O
Loss O
Avg O
. O
( O
×1e19 O
) O
( O
Saving O
) O
( O
MLM O
) O
bert2BERT O
: O
S(12 O
, O
512 O
) O
→ O
T O
( O
12 O
, O
768 O
) O
bert2BERT O
( O
Eb= O
0 O
) O
4.5 O
38.4 O
% O
1.434 O
89.4 O
bert2BERT O
( O
Eb= O
5 O
) O
4.0 O
45 O
.2 O
% O
1.433 O
89.3 O
bert2BERT O
( O
Eb= O
10 O
) O
4.1 O
43.9 O
% O
1.436 O
89.3 O
bert2BERT O
( O
Eb= O
20 O
) O
5.4 O
25.4 O
% O
1.448 O
89.1 O
Table O
4 O
: O
Effect O
of O
sub O
- O
model O
training O
epochs O
. O

Avg O
means O
the O
average O
score O
of O
SST-2 O
/ O
MNLI O
/ O
SQuADv1.1.5.3 O
Application O
on O
GPT B-MethodName
Datasets O
. O

To O
demonstrate O
that O
our O
method O
is O
generic O
, O
following O
the O
BERT O
setting O
, O
we O
also O
use O
the O
English O
Wikipedia O
and O
Book O
Corpus O
in O
the O
GPTtraining O
. O

For O
the O
evaluation O
, O
we O
use O
the O
datasets O
of O
WikiText-2 O
, O
PTB O
, O
and O
WikiText103 O
and O
evaluate O
these O
models O
under O
the O
zero O
- O
shot O
setting O
without O
fine O
- O
tuning O
on O
the O
training O
set O
. O

Implementation O
Details O
. O

We O
use O
the O
architecture O
of O
{ O
L=12,D=768 O
} O
for O
the O
GPT O
target O
model O
, O
and O
pre O
- O
train O
it O
with O
the O
learning O
rate O
of O
1e-4 O
, O
training O
epochs O
of O
20 O
. O

For O
bert2BERT O
, O
we O
use O
the O
source O
model O
with O
an O
architecture O
of O
{ O
L=12 O
, O
D=512 O
} O
, O
initialize O
the O
target O
model O
with O
AKI O
, O
and O
pre O
- O
train O
it O
by O
the O
full O
- O
model O
training O
( O
Eb=0 O
) O
. O
Results O
and O
Analysis O
. O

We O
compare O
the O
original O
pre O
- O
training O
method O
and O
bert2BERT O
, O
the O
results O
are O
shown O
in O
Table O
5 O
and O

Appendix O
D. O

We O
observe O
that O
the O
proposed O
method O
saves O
47 O
% O
computation O
cost O
of O
GPT O
pre O
- O
training O
, O
exhibiting O
a O
similar O
trend O
to O
BERT O
pre O
- O
training O
. O

Although O
GPT O
and O
BERT O
have O
different O
architectures O
( O
e.g. O
, O
postLN O
and O
pre O
- O
LN O
( O
Xiong O
et O
al O
. O
, O
2020 O
) O
) O
and O
are O
pretrained O
with O
different O
tasks O
, O
bert2BERT O
saves O
a O
significant O
amount O
of O
training O
cost O
on O
both O
these O
two O
models O
, O
which O
shows O
that O
the O
proposed O
method O
is O
generic O
and O
is O
effective O
for O
different O
kinds O
of O
PLMs O
. O

Model O
FLOPs O
PTB O
WikiText-2 O
WikiText103 O
( O
×1e19 O
) O
( O
w/o O
FT O
) O
( O
w/o O
FT O
) O
( O
w/o O
FT O
) O
bert2BERT O
: O
S(12 O
, O
512 O
) O
→ O
T O
( O
12 O
, O
768 O
) O
GPT O
4.9 O
133.8 O
47.0 O
53.5 O
bert2BERT O
2.6(47%↓ O
) O
132.1 O
47.9 O
53.0 O
Table O
5 O
: O
Experiments O
on O
GPT O
. O

We O
report O
the O
perplexity O
for O
these O
tasks O
. O

“ O
w/o O
FT O
” O
means O
that O
the O
pre O
- O
trained O
model O
is O
directly O
evaluated O
on O
the O
test O
set O
without O
finetuning O
on O
the O
train O
set O
. O

5.4 O
Application O
on O
T5 B-MethodName
Datasets O
. O

To O
demonstrate O
that O
our O
method O
can O
be O
used O
to O
train O
larger O
models O
, O
we O
use O
the O
Baidu B-DatasetName
Wikipedia I-DatasetName
, O
Sougou B-DatasetName
Wikipedia I-DatasetName
, O
and O
Zhihu B-DatasetName
to O
train O
the O
T5 B-MethodName
model O
( O
Raffel O
et O
al O
. O
, O
2020 O
) O
. O

For O
the O
evaluation O
, O
we O
use O
the O
dataset O
of O
the O
original O
Chinese B-TaskName
natural I-TaskName
language I-TaskName
inference I-TaskName
task O
( O
OCNLI B-DatasetName
) O

( O
Hu O
et O
al O
. O
, O
2020 O
) O
. O

Implementation O
Details O
. O

Since O
the O
bert2BERT B-MethodName
method O
is O
suitable O
for O
BERT B-MethodName
and O
GPT B-MethodName
, O
it O
can O
also O
be O
used O
for O
the O
T5 B-MethodName
model O
, O
which O
consists O
of O
an O
encoder O
and O
a O
decoder O
. O

The O
target O
T5 B-MethodName
model O
’s O
architecture O
is O
{ O
Le=12,Ld=12,D=1024 O
, O
A=16 O
} O
, O
where2141LeandLdmeans O
the O
numbers O
of O
encoder O
and O
decoder O
Transformer O
layers O
respectively O
, O
Dmeans O
the O
hidden O
size O
, O
Ameans O
the O
number O
of O
attention O
heads O
. O

We O
pre O
- O
train O
it O
with O
the O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-4 B-HyperparameterValue
, O
batch B-HyperparameterName
size I-HyperparameterName
of O
1024 B-HyperparameterValue
. O

For O
bert2BERT B-MethodName
, O
we O
use O
the O
source O
model O
with O
an O
architecture O
of O
{ O
Le=12 O
, O
Ld=12,D=256 O
, O
A=4 O
} O
, O
initialize O
the O
target O
model O
with O
FPI O
, O
and O
pre O
- O
train O
it O
by O
the O
full O
- O
model O
training O
( O
Eb=0 O
) O
. O

Note O
that O
the O
scale O
gap O
between O
the O
source O
model O
and O
the O
target O
model O
is O
over O
10 O
times O
( O
31 O
M O
vs. O
360 O
M O
) O
, O
which O
is O
a O
challenging O
setting O
. O
Results O
and O
Analysis O
. O

We O
compare O
the O
original O
pre O
- O
training O
method O
and O
bert2BERT B-MethodName
method O
on O
the O
T5 B-MethodName
model O
, O
the O
results O
are O
shown O
in O
Table O
6 O
. O

We O
observe O
that O
the O
proposed O
method O
saves O
at O
least O
25 O
% O
computation O
cost O
of O
T5 B-MethodName
pre O
- O
training O
. O

It O
demonstrates O
the O
effectiveness O
of O
the O
method O
on O
larger O
models O
. O

Model O
FLOPs O
Loss O
OCNLI O
( O
×1e20 O
) O
( O
MLM O
) O
( O
Acc O
) O
bert2BERT O
: O
S(12 O
, O
12 O
, O
256 O
, O
4 O
) O
→ O
T O
( O
12 O
, O
12 O
, O
1024 O
, O
16 O
) O
T5 O
1.6 O
1.90 O
72.03 O
bert2BERT O
1.2(25%↓ O
) O
1.90 O
72.75 O
Table O
6 O
: O
Experiments O
on O
the O
T5 O
model O
. O

6 O
Conclusion O
and O
Future O
Work O
This O
paper O
proposes O
an O
efficient O
pre O
- O
training O
method O
, O
bert2BERT B-MethodName
, O
which O
reuses O
the O
parameters O
of O
the O
small O
trained O
model O
as O
the O
initialization O
parameters O
of O
the O
large O
model O
. O

We O
employ O
the O
proposed O
method O
in O
BERT B-MethodName
and O
GPT B-MethodName
under O
different O
settings O
of O
model O
sizes O
. O

The O
extensive O
results O
show O
that O
bert2BERT B-MethodName
is O
generic O
to O
Transformerbased O
models O
and O
saves O
a O
significant O
amount O
of O
computation O
cost O
. O

Moreover O
, O
the O
detailed O
analysis O
shows O
that O
our O
techniques O
, O
function O
- O
preserving O
, O
advanced O
knowledge O
initialization O
, O
and O
two O
- O
stage O
pre O
- O
training O
, O
are O
all O
effective O
. O

In O
the O
future O
, O
we O
will O
apply O
bert2BERT B-MethodName
on O
training O
super O
largescale O
language O
models O
( O
e.g. O
, O
use O
the O
10B O
source O
model O
to O
train O
the O
100B O
target O
model O
) O
and O
extends O
its O
scope O
to O
other O
PLMs O
such O
as O
ELECTRA B-MethodName
and O
BART B-MethodName
( O
Lewis O
et O
al O
. O
, O
2020 O
) O
. O

Acknowledgements O
This O
work O
is O
supported O
in O
part O
by O
NSFC O
( O
Grant O
No O
. O
61872215 O
) O
, O
and O
Shenzhen O
Science O
and O
Technology O
Program O
( O
Grant O
No O
. O
RCYX20200714114523079 O
) O
. O

We O
would O
like O
to O
thank O
Yifeng O
Liu O
, O
Binbin O
Deng O
, O
Ziliang O
Yang O
, O
Jiaxin O
Shi O
for O
their O
support O
of O
this O
work O
. O

References O
Jimmy O
Lei O
Ba O
, O
Jamie O
Ryan O
Kiros O
, O
and O
Geoffrey O
E. O
Hinton O
. O
2016 O
. O

Layer O
normalization O
. O

ArXiv O
preprint O
, O
abs/1607.06450 O
. O

Tom O
B. O
Brown O
, O
Benjamin O
Mann O
, O
Nick O
Ryder O
, O
Melanie O
Subbiah O
, O
Jared O
Kaplan O
, O
Prafulla O
Dhariwal O
, O
Arvind O
Neelakantan O
, O
Pranav O
Shyam O
, O
Girish O
Sastry O
, O
Amanda O
Askell O
, O
Sandhini O
Agarwal O
, O
Ariel O
Herbert O
- O
V O
oss O
, O
Gretchen O
Krueger O
, O
Tom O
Henighan O
, O
Rewon O
Child O
, O
Aditya O
Ramesh O
, O
Daniel O
M. O
Ziegler O
, O
Jeffrey O
Wu O
, O
Clemens O
Winter O
, O
Christopher O
Hesse O
, O
Mark O
Chen O
, O
Eric O
Sigler O
, O
Mateusz O
Litwin O
, O
Scott O
Gray O
, O
Benjamin O
Chess O
, O
Jack O
Clark O
, O
Christopher O
Berner O
, O
Sam O
McCandlish O
, O
Alec O
Radford O
, O
Ilya O
Sutskever O
, O
and O
Dario O
Amodei O
. O
2020 O
. O

Language O
models O
are O
few O
- O
shot O
learners O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
33 O
: O
Annual O
Conference O
on O
Neural O
Information O
Processing O
Systems O
2020 O
, O
NeurIPS O
2020 O
, O
December O
6 O
- O
12 O
, O
2020 O
, O
virtual O
. O

Cheng O
Chen O
, O
Yichun O
Yin O
, O
Lifeng O
Shang O
, O
Zhi O
Wang O
, O
Xin O
Jiang O
, O
Xiao O
Chen O
, O
and O
Qun O
Liu O
. O
2021 O
. O

Extract O
then O
distill O
: O
Efficient O
and O
effective O
task O
- O
agnostic O
BERT O
distillation O
. O

In O
Artificial O
Neural O
Networks O
and O
Machine O
Learning O
- O
ICANN O
2021 O
- O
30th O
International O
Conference O
on O
Artificial O
Neural O
Networks O
, O
Bratislava O
, O
Slovakia O
, O
September O
14 O
- O
17 O
, O
2021 O
, O
Proceedings O
, O
Part O
III O
, O
volume O
12893 O
of O
Lecture O
Notes O
in O
Computer O
Science O
, O
pages O
570–581 O
. O

Springer O
. O

Tianqi O
Chen O
, O
Ian O
J. O
Goodfellow O
, O
and O
Jonathon O
Shlens O
. O
2016 O
. O

Net2net B-MethodName
: O

Accelerating O
learning O
via O
knowledge O
transfer O
. O

In O
ICLR O
. O

Kevin O
Clark O
, O
Urvashi O
Khandelwal O
, O
Omer O
Levy O
, O
and O
Christopher O
D. O
Manning O
. O

2019 O
. O

What O
does O
BERT O
look O
at O
? O

an O
analysis O
of O
BERT B-MethodName
’s O
attention O
. O

In O
Proceedings O
of O
the O
2019 O
ACL O
Workshop O
BlackboxNLP O
: O
Analyzing O
and O
Interpreting O
Neural O
Networks O
for O
NLP O
, O
pages O
276–286 O
, O
Florence O
, O
Italy O
. O

Association O
for O
Computational O
Linguistics O
. O

Kevin O
Clark O
, O
Minh O
- O
Thang O
Luong O
, O
Quoc O
V O
. O

Le O
, O
and O
Christopher O
D. O
Manning O
. O

2020 O
. O

ELECTRA B-MethodName
: O
pretraining O
text O
encoders O
as O
discriminators O
rather O
than O
generators O
. O

In O
8th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2020 O
, O
Addis O
Ababa O
, O
Ethiopia O
, O
April O
26 O
- O
30 O
, O
2020 O
. O

OpenReview.net O
. O
Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT B-MethodName
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
, O
Minneapolis O
, O
Minnesota O
. O

Association O
for O
Computational O
Linguistics O
. O

William O
Fedus O
, O
Barret O
Zoph O
, O
and O
Noam O
Shazeer O
. O

2021 O
. O

Switch O
transformers O
: O
Scaling O
to O
trillion O
parameter O
models O
with O
simple O
and O
efficient O
sparsity O
. O

CoRR O
.2142A. O

Feng O
and O
P. O
Panda O
. O

2020 O
. O

Energy O
- O
efficient O
and O
robust O
cumulative O
training O
with O
net2net O
transformation O
. O

In O
IJCNN O
. O

Linyuan O
Gong O
, O
Di O
He O
, O
Zhuohan O
Li O
, O
Tao O
Qin O
, O
Liwei O
Wang O
, O
and O
Tie O
- O
Yan O
Liu O
. O
2019 O
. O

Efficient O
training O
of O
BERT O
by O
progressively O
stacking O
. O

In O
Proceedings O
of O
the O
36th O
International O
Conference O
on O
Machine O
Learning O
, O
ICML O
2019 O
, O
9 O
- O
15 O
June O
2019 O
, O
Long O
Beach O
, O
California O
, O
USA O
, O
volume O
97 O
of O
Proceedings O
of O
Machine O
Learning O
Research O
, O
pages O
2337–2346 O
. O

PMLR O
. O

Xiaotao O
Gu O
, O
Liyuan O
Liu O
, O
Hongkun O
Yu O
, O
Jing O
Li O
, O
Chen O
Chen O
, O
and O
Jiawei O
Han O
. O
2021 O
. O

On O
the O
transformer O
growth O
for O
progressive O
BERT O
training O
. O

In O
Proceedings O
of O
the O
2021 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
5174–5180 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Song O
Han O
, O
Jeff O
Pool O
, O
John O
Tran O
, O
and O
William O
Dally O
. O
2015 O
. O

Learning O
both O
weights O
and O
connections O
for O
efficient O
neural O
network O
. O

Advances O
in O
neural O
information O
processing O
systems O
, O
28 O
. O
Dan O
Hendrycks O
and O
Kevin O
Gimpel O
. O

2016 O
. O

Gaussian O
error O
linear O
units O
( O
gelus O
) O
. O

ArXiv O
preprint O
, O
abs/1606.08415 O
. O

Hai O
Hu O
, O
Kyle O
Richardson O
, O
Liang O
Xu O
, O
Lu O
Li O
, O
Sandra O
Kübler O
, O
and O
Lawrence O
S. O
Moss O
. O
2020 O
. O

OCNLI B-DatasetName
: O
original O
chinese O
natural O
language O
inference O
. O

In O
Findings O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
EMNLP O
2020 O
, O
Online O
Event O
, O
16 O
- O
20 O
November O
2020 O
, O
volume O
EMNLP O
2020 O
of O
Findings O
of O
ACL O
, O
pages O
3512–3526 O
. O

Association O
for O
Computational O
Linguistics O
. O

Ganesh O
Jawahar O
, O
Benoît O
Sagot O
, O
and O
Djamé O
Seddah O
. O
2019 O
. O

What O
does O
BERT O
learn O
about O
the O
structure O
of O
language O
? O

In O
ACL O
, O
pages O
3651–3657 O
, O
Florence O
, O
Italy O
. O
Association O
for O
Computational O
Linguistics O
. O

Zhenzhong O
Lan O
, O
Mingda O
Chen O
, O
Sebastian O
Goodman O
, O
Kevin O
Gimpel O
, O
Piyush O
Sharma O
, O
and O
Radu O
Soricut O
. O
2020 O
. O

ALBERT B-MethodName
: O

A O
lite O
BERT O
for O
self O
- O
supervised O
learning O
of O
language O
representations O
. O

In O
8th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2020 O
, O
Addis O
Ababa O
, O
Ethiopia O
, O
April O
26 O
- O
30 O
, O
2020 O
. O

OpenReview.net O
. O

Mike O
Lewis O
, O
Yinhan O
Liu O
, O
Naman O
Goyal O
, O
Marjan O
Ghazvininejad O
, O
Abdelrahman O
Mohamed O
, O
Omer O
Levy O
, O
Veselin O
Stoyanov O
, O
and O
Luke O
Zettlemoyer O
. O

2020 O
. O

BART B-MethodName
: O

Denoising O
sequence O
- O
to O
- O
sequence O
pre O
- O
training O
for O
natural O
language O
generation O
, O
translation O
, O
and O
comprehension O
. O

In O
ACL O
, O
pages O
7871–7880 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Yinhan O
Liu O
, O
Myle O
Ott O
, O
Naman O
Goyal O
, O
Jingfei O
Du O
, O
Mandar O
Joshi O
, O
Danqi O
Chen O
, O
Omer O
Levy O
, O
Mike O
Lewis O
, O
Luke O
Zettlemoyer O
, O
and O
Veselin O
Stoyanov O
. O

2019 O
. O

Roberta B-MethodName
: O
A O
robustly O
optimized O
BERT O
pretraining O
approach O
. O

CoRR O
.Sinno O

Jialin O
Pan O
and O
Qiang O
Yang O
. O

2010 O
. O

A O
survey O
on O
transfer O
learning O
. O

TKDE O
. O

Yujia O
Qin O
, O
Yankai O
Lin O
, O
Jing O
Yi O
, O
Jiajie O
Zhang O
, O
Xu O
Han O
, O
Zhengyan O
Zhang O
, O
YuSheng O
Su O
, O
Zhiyuan O
Liu O
, O
Peng O
Li O
, O
Maosong O
Sun O
, O
and O
Jie O
Zhou O
. O

2021 O
. O

Knowledge O
inheritance O
for O
pre O
- O
trained O
language O
models O
. O

CoRR O
. O

Alec O
Radford O
, O
Karthik O
Narasimhan O
, O
Tim O
Salimans O
, O
and O
Ilya O
Sutskever O
. O

2018 O
. O

Improving O
language O
understanding O
by O
generative O
pre O
- O
training O
. O

Alec O
Radford O
, O
Jeffrey O
Wu O
, O
Rewon O
Child O
, O
David O
Luan O
, O
Dario O
Amodei O
, O
and O
Ilya O
Sutskever O
. O
2019 O
. O

Language O
models O
are O
unsupervised O
multitask O
learners O
. O

OpenAI O
blog O
, O
1(8):9 O
. O

Colin O
Raffel O
, O
Noam O
Shazeer O
, O
Adam O
Roberts O
, O
Katherine O
Lee O
, O
Sharan O
Narang O
, O
Michael O
Matena O
, O
Yanqi O
Zhou O
, O
Wei O
Li O
, O
and O
Peter O
J. O
Liu O
. O
2020 O
. O

Exploring O
the O
limits O
of O
transfer O
learning O
with O
a O
unified O
text O
- O
to O
- O
text O
transformer O
. O

J. O
Mach O
. O

Learn O
. O

Res O
. O
, O
21:140:1–140:67 O
. O

Pranav O
Rajpurkar O
, O
Jian O
Zhang O
, O
Konstantin O
Lopyrev O
, O
and O
Percy O
Liang O
. O

2016 O
. O

SQuAD B-DatasetName
: O
100,000 O
+ O
questions O
for O
machine O
comprehension O
of O
text O
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2383–2392 O
, O
Austin O
, O
Texas O
. O
Association O
for O
Computational O
Linguistics O
. O
Roy O
Schwartz O
, O
Jesse O
Dodge O
, O
Noah O
A O
Smith O
, O
and O
Oren O
Etzioni O
. O

2020 O
. O

Green O
ai O
. O

Communications O
of O
the O
ACM O
, O
63(12):54–63 O
. O

Mohammad O
Shoeybi O
, O
Mostofa O
Patwary O
, O
Raul O
Puri O
, O
Patrick O
LeGresley O
, O
Jared O
Casper O
, O
and O
Bryan O
Catanzaro O
. O

2019 O
. O

Megatron O
- O
lm O
: O
Training O
multi O
- O
billion O
parameter O
language O
models O
using O
model O
parallelism O
. O

CoRR O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N. O
Gomez O
, O
Lukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
30 O
: O
Annual O
Conference O
on O
Neural O
Information O
Processing O
Systems O
2017 O
, O
December O
4 O
- O
9 O
, O
2017 O
, O
Long O
Beach O
, O
CA O
, O
USA O
, O
pages O
5998–6008 O
. O

Alex O
Wang O
, O
Amanpreet O
Singh O
, O
Julian O
Michael O
, O
Felix O
Hill O
, O
Omer O
Levy O
, O
and O
Samuel O
R. O
Bowman O
. O
2019a O
. O
GLUE B-DatasetName
: O

A O
multi O
- O
task O
benchmark O
and O
analysis O
platform O
for O
natural O
language O
understanding O
. O

In O
7th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2019 O
, O
New O
Orleans O
, O
LA O
, O
USA O
, O
May O
6 O
- O
9 O
, O
2019 O
. O

OpenReview.net O
. O

Dilin O
Wang O
, O
Meng O
Li O
, O
Lemeng O
Wu O
, O
Vikas O
Chandra O
, O
and O
Qiang O
Liu O
. O
2019b O
. O

Energy O
- O
aware O
neural O
architecture O
optimization O
with O
fast O
splitting O
steepest O
descent O
. O

CoRR O
. O

Alex O
Warstadt O
, O
Amanpreet O
Singh O
, O
and O
Samuel O
R. O
Bowman O
. O

2019 O
. O

Neural O
network O
acceptability O
judgments O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
7:625–641.2143Wei O
Wen O
, O
Yuxiong O
He O
, O
Samyam O
Rajbhandari O
, O
Minjia O
Zhang O
, O
Wenhan O
Wang O
, O
Fang O
Liu O
, O
Bin O
Hu O
, O
Yiran O
Chen O
, O
and O
Hai O
Li O
. O

2018 O
. O

Learning O
intrinsic O
sparse O
structures O
within O
long O
short O
- O
term O
memory O
. O

In O
6th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2018 O
, O
Vancouver O
, O
BC O
, O
Canada O
, O
April O
30 O
- O
May O
3 O
, O
2018 O
, O
Conference O
Track O
Proceedings O
. O

OpenReview.net O
. O

Lemeng O
Wu O
, O
Bo O
Liu O
, O
Peter O
Stone O
, O
and O
Qiang O
Liu O
. O
2020a O
. O

Firefly O
neural O
architecture O
descent O
: O
a O
general O
approach O
for O
growing O
neural O
networks O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
33 O
: O
Annual O
Conference O
on O
Neural O
Information O
Processing O
Systems O
2020 O
, O
NeurIPS O
2020 O
, O
December O
6 O
- O
12 O
, O
2020 O
, O
virtual O
. O

Lemeng O
Wu O
, O
Dilin O
Wang O
, O
and O
Qiang O
Liu O
. O
2019 O
. O

Splitting O
steepest O
descent O
for O
growing O
neural O
architectures O
. O

InNeurIPS O
, O
pages O
10655–10665 O
. O

Lemeng O
Wu O
, O
Mao O
Ye O
, O
Qi O
Lei O
, O
Jason O
D. O
Lee O
, O
and O
Qiang O
Liu O
. O

2020b O
. O

Steepest O
descent O
neural O
architecture O
optimization O
: O

Escaping O
local O
optimum O
with O
signed O
neural O
splitting O
. O

CoRR O
. O

Qiyu O
Wu O
, O
Chen O
Xing O
, O
Yatao O
Li O
, O
Guolin O
Ke O
, O
Di O
He O
, O
and O
Tie O
- O
Yan O
Liu O
. O
2021 O
. O

Taking O
notes O
on O
the O
fly O
helps O
language O
pre O
- O
training O
. O

In O
ICLR O
. O

Ruibin O
Xiong O
, O
Yunchang O
Yang O
, O
Di O
He O
, O
Kai O
Zheng O
, O
Shuxin O
Zheng O
, O
Chen O
Xing O
, O
Huishuai O
Zhang O
, O
Yanyan O
Lan O
, O
Liwei O
Wang O
, O
and O
Tie O
- O
Yan O
Liu O
. O
2020 O
. O

On O
layer O
normalization O
in O
the O
transformer O
architecture O
. O

In O
Proceedings O
of O
the O
37th O
International O
Conference O
on O
Machine O
Learning O
, O
ICML O
2020 O
, O
13 O
- O
18 O
July O
2020 O
, O
Virtual O
Event O
, O
volume O
119 O
of O
Proceedings O
of O
Machine O
Learning O
Research O
, O
pages O
10524–10533 O
. O

PMLR O
. O

Cheng O
Yang O
, O
Shengnan O
Wang O
, O
Chao O
Yang O
, O
Yuechuan O
Li O
, O
Ru O
He O
, O
and O
Jingqiao O
Zhang O
. O

2020 O
. O

Progressively O
stacking O
2.0 O
: O
A O
multi O
- O
stage O
layerwise O
training O
method O
for O
bert O
training O
speedup O
. O

arXiv O
preprint O
arXiv:2011.13635 O
. O

Zhilin O
Yang O
, O
Zihang O
Dai O
, O
Yiming O
Yang O
, O
Jaime O
G. O
Carbonell O
, O
Ruslan O
Salakhutdinov O
, O
and O
Quoc O
V O
. O

Le O
. O
2019 O
. O

Xlnet B-MethodName
: O

Generalized O
autoregressive O
pretraining O
for O
language O
understanding O
. O

In O
NeurIPS O
, O
pages O
5754–5764 O
. O

Yang O
You O
, O
Jing O
Li O
, O
Sashank O
J. O
Reddi O
, O
Jonathan O
Hseu O
, O
Sanjiv O
Kumar O
, O
Srinadh O
Bhojanapalli O
, O
Xiaodan O
Song O
, O
James O
Demmel O
, O
Kurt O
Keutzer O
, O
and O
Cho O
- O
Jui O
Hsieh O
. O
2020 O
. O

Large O
batch O
optimization O
for O
deep O
learning O
: O
Training O
BERT O
in O
76 O
minutes O
. O

In O
8th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2020 O
, O
Addis O
Ababa O
, O
Ethiopia O
, O
April O
26 O
- O
30 O
, O
2020 O
. O

OpenReview.net O
. O

Wei O
Zeng O
, O
Xiaozhe O
Ren O
, O
Teng O
Su O
, O
Hui O
Wang O
, O
Yi O
Liao O
, O
Zhiwei O
Wang O
, O
Xin O
Jiang O
, O
ZhenZhang O
Yang O
, O
Kaisheng O
Wang O
, O
Xiaoda O
Zhang O
, O
Chen O
Li O
, O
Ziyan O
Gong O
, O
Yifan O
Yao O
, O
Xinjing O
Huang O
, O
Jun O
Wang O
, O
Jianfeng O
Yu O
, O
Qi O
Guo O
, O
Yue O
Yu O
, O
Yan O
Zhang O
, O
Jin O
Wang O
, O
Hengtao O
Tao O
, O
Dasen O
Yan O
, O
Zexuan O
Yi O
, O
Fang O
Peng O
, O
Fangqing O
Jiang O
, O
Han O
Zhang O
, O
Lingfeng O
Deng O
, O
Yehong O
Zhang O
, O
Zhe O
Lin O
, O
Chao O
Zhang O
, O
Shaojie O
Zhang O
, O
Mingyue O
Guo O
, O
Shanzhi O
Gu O
, O
Gaojun O
Fan O
, O
Yaowei O
Wang O
, O
Xuefeng O
Jin O
, O
Qun O
Liu O
, O
and O
Yonghong O
Tian O
. O
2021 O
. O

Panguα O
: O
Large O
- O
scale O
autoregressive O
pretrained O
chinese O
language O
models O
with O
auto O
- O
parallel O
computation O
. O

Minjia O
Zhang O
and O
Yuxiong O
He O
. O
2020 O
. O

Accelerating O
training O
of O
transformer O
- O
based O
language O
models O
with O
progressive O
layer O
dropping O
. O

In O
Advances O
in O
Neural O
Information O
Processing O
Systems O
33 O
: O
Annual O
Conference O
on O
Neural O
Information O
Processing O
Systems O
2020 O
, O
NeurIPS O
2020 O
, O
December O
6 O
- O
12 O
, O
2020 O
, O
virtual O
. O

Yukun O
Zhu O
, O
Ryan O
Kiros O
, O
Richard O
S. O
Zemel O
, O
Ruslan O
Salakhutdinov O
, O
Raquel O
Urtasun O
, O
Antonio O
Torralba O
, O
and O
Sanja O
Fidler O
. O

2015 O
. O

Aligning O
books O
and O
movies O
: O
Towards O
story O
- O
like O
visual O
explanations O
by O
watching O
movies O
and O
reading O
books O
. O

In O
2015 O
IEEE O
International O
Conference O
on O
Computer O
Vision O
, O
ICCV O
2015 O
, O
Santiago O
, O
Chile O
, O
December O
7 O
- O
13 O
, O
2015 O
, O
pages O
19–27 O
. O

IEEE O
Computer O
Society.2144A O
Ablation O
Study O
of O
bert2BERT O
0 O
1 O
2 O
3 O
4 O
5 O
6 O
7 O
8 O
FLOPs O
( O
1e19)1.41.61.82.02.22.42.6MLM O
Loss O
4 O
5 O
6 O
71.401.421.441.461.48 O
1.437 O
100 O
% O
87.8 O
% O
69.6 O
% O
61.6 O
% O
54.8%BERTBASE O
DirectCopyFPI O
AKIbert2BERT O
Figure O
5 O
: O
Ablation O
study O
of O
bert2BERT O
. O

bert2BERT O
means O
the O
combination O
of O
AKI O
and O
two O
- O
stage O
pretraining O
. O

The O
ablation O
study O
of O
bert2BERT O
is O
displayed O
in O
Table O
5 O
. O

From O
the O
table O
, O
we O
observe O
that O
: O
( O
1 O
) O
all O
the O
proposed O
methods O
is O
better O
than O
the O
original O
pre O
- O
training O
method O
and O
DirectCopy O
; O
( O
2 O
) O
although O
AKI O
has O
a O
worse O
initialization O
than O
FPI O
, O
it O
achieves O
faster O
convergence O
rate O
than O
FPI O
; O
( O
3 O
) O
the O
two O
- O
stage O
pre O
- O
training O
furthers O
reduce O
the O
cost O
from O
61.6 O
% O
to O
54.8 O
% O
; O
( O
4 O
) O
the O
FPI O
curve O
has O
an O
upward O
trend O
at O
the O
beginning O
. O

We O
conjecture O
that O
it O
is O
due O
to O
the O
symmetry O
brought O
by O
FPI O
and O
the O
model O
needs O
some O
optimization O
time O
to O
break O
this O
symmetry O
. O

B O
bert2BERT O
with O
smaller O
source O
model O
0 O
1 O
2 O
3 O
4 O
5 O
6 O
7 O
8 O
FLOPs O
( O
1e19)1.41.61.82.02.22.42.6MLM O
Loss O
4 O
5 O
6 O
71.401.421.441.461.48 O
1.437 O
100 O
% O
76.7 O
% O
63.2 O
% O
57.3%DirectCopy_L6 O
bert2BERT_L6bert2BERT_L8 O
bert2BERT_L10BERTBASE O
Figure O
6 O
: O
Loss O
curves O
of O
bert2BERT O
and O
baselines O
with O
smaller O
source O
models O
. O

We O
test O
bert2BERT O
with O
different O
source O
models O
and O
the O
loss O
curves O
are O
represented O
in O
Figure O
6 O
. O

C O
Effect O
of O
sub O
- O
model O
training O
epochs O
We O
study O
the O
effect O
of O
sub O
- O
model O
training O
epochs O
on O
the O
pre O
- O
training O
efficiency O
. O

The O
loss O
curves O
are O
represented O
in O
Figure O
7 O
. O

Note O
that O
the O
setting O
Eb= O
20has O
not O
achieved O
the O
same O
loss O
( O
1.437 O
) O
as O
the O
baseline O
BERT O
BASE O
in O
the O
40 O
training O
epochs O
. O

D O
Application O
on O
GPT B-MethodName
The O
loss O
curve O
of O
our O
method O
on O
GPT B-MethodName
application O
is O
displayed O
in O
Figure O
8 O
. O
0 O
1 O
2 O
3 O
4 O
5 O
6 O
7 O
8 O
FLOPs O
( O
1e19)1.41.61.82.02.22.42.6MLM O
Loss O
4 O
5 O
6 O
71.401.421.441.461.48 O
1.437 O
100 O
% O
61.6 O
% O
54.8%BERTBASE O
Eb=0Eb=5 O
Eb=10Eb=20Figure O
7 O
: O
Loss O
curves O
of O
bert2BERT O
with O
different O
submodel O
training O
epochs O
. O

0 O
1 O
2 O
3 O
4 O
5 O
FLOPs O
( O
1e19)2.83.03.23.43.63.84.0Loss O
2.5 O
3.0 O
3.5 O
4.0 O
4.5 O
5.02.842.862.882.90 O
2.870 O
100 O
% O
52.4%GPT O
bert2BERT O
Figure O
8 O
: O
Pre O
- O
training O
loss O
curves O
of O
GPT O
. O
E O
Comparisons O
of O
Attention O
Patterns O
We O
take O
the O
source O
model O
S(4,256 O
) O
and O
target O
model O
T(4,512 O
) O
as O
an O
example O
to O
analyze O
the O
attention O
patterns O
of O
DirectCopy O
in O
Figure O
10 O
, O
FPI O
in O
Figure O
11 O
and O
AKI O
in O
Figure O
12 O
. O

We O
display O
the O
attention O
patterns O
of O
the O
source O
model O
S(4,256 O
) O
in O
Figure O
9 O
. O

Compared O
with O
the O
source O
model O
, O
we O
observe O
that O
the O
newly O
added O
attention O
patterns O
of O
DirectCopy O
are O
messy O
, O
and O
the O
randomly O
initialized O
parameters O
destroy O
the O
attention O
patterns O
of O
the O
source O
model O
. O

The O
proposed O
FPI O
method O
makes O
the O
new O
model O
have O
the O
same O
attention O
patterns O
as O
the O
source O
model O
, O
thus O
the O
knowledge O
of O
the O
source O
model O
is O
preserved O
. O

However O
, O
FPI O
always O
induces O
symmetrical O
attention O
patterns O
in O
the O
same O
layer O
. O

This O
symmetry O
will O
hinder O
the O
convergence O
. O

To O
handle O
this O
problem O
, O
we O
use O
AKI O
method O
to O
reuse O
the O
parameters O
of O
the O
upper O
layer O
( O
advanced O
knowledge O
) O
to O
break O
the O
symmetry O
, O
and O
meanwhile O
make O
the O
knowledge O
in O
the O
same O
layer O
richer O
. O

Through O
the O
AKI O
method O
, O
the O
attention O
patterns O
of O
the O
upper O
layer O
can O
be O
also O
maintained O
well O
in O
the O
target O
model O
. O

For O
example O
, O
as O
shown O
in O
Figure O
12 O
, O
the O
newly O
added O
attention O
patterns O
of O
the O
1st O
layer O
in O
the O
target O
model O
are O
similar O
to O
the O
ones O
of O
the O
2nd O
layer O
in O
the O
source O
model O
. O

F O
Illustration O
of O
FPI O
and O
AKI O
process O
We O
illustrate O
the O
process O
of O
FPI O
and O
AKI O
in O
Figure O
13 O
and O
14 O
respectively.2145L1 O
H0 O
  O
L1 O
H1 O
  O
L1 O
H2 O
  O
L1 O
H3 O
L2 O
H0 O
  O
L2 O
H1 O
  O
L2 O
H2 O
  O
L2 O
H3 O
L3 O
H0 O
  O
L3 O
H1 O
  O
L3 O
H2 O
  O
L3 O
H3 O
L4 O

H0 O
  O
L4 O
H1 O
  O
L4 O
H2 O
  O
L4 O
H3Figure O
9 O
: O
Attention O
patterns O
of O
the O
source O
model O
S(4,256 O
) O
, O
which O
has O
4 O
attention O
heads O
in O
each O
layer O
. O

L1 O
H0 O
  O
L1 O
H1 O
  O
L1 O
H2 O
  O
L1 O
H3 O
  O
L1 O
H4 O
  O
L1 O
H5 O
  O
L1 O

H6 O
  O
L1 O
H7 O
L2 O
H0 O
  O
L2 O
H1 O
  O
L2 O
H2 O
  O
L2 O
H3 O
  O
L2 O
H4 O
  O
L2 O
H5 O
  O
L2 O
H6 O
  O
L2 O
H7 O
L3 O
H0 O
  O
L3 O
H1 O
  O
L3 O
H2 O
  O
L3 O
H3 O
  O
L3 O
H4 O
  O
L3 O
H5 O
  O
L3 O

H6 O
  O
L3 O
H7 O
L4 O
H0 O
  O
L4 O
H1 O
  O
L4 O
H2 O
  O
L4 O
H3 O
  O
L4 O
H4 O
  O
L4 O
H5 O
  O
L4 O
H6 O
  O
L4 O
H7 O
Figure O
10 O
: O
Attention O
patterns O
of O
the O
target O
model O
T(4,512 O
) O
based O
on O
the O
baseline O
DirectCopy O
method O
. O

The O
first O
4 O
attention O
patterns O
( O
H0 O
- O
H3 O
) O
in O
each O
row O
correspond O
to O
the O
source O
model O
’s O
attention O
patterns O
, O
and O
the O
last O
4 O
attention O
patterns O
( O
H4 O
- O
H7 O
) O
are O
newly O
added.2146L1 O
H0 O
  O
L1 O
H1 O
  O
L1 O
H2 O
  O
L1 O
H3 O
  O
L1 O
H4 O
  O
L1 O

H5 O
  O
L1 O

H6 O
  O
L1 O
H7 O
L2 O
H0 O
  O
L2 O
H1 O
  O
L2 O
H2 O
  O
L2 O
H3 O
  O
L2 O
H4 O
  O
L2 O
H5 O
  O
L2 O
H6 O
  O
L2 O
H7 O
L3 O
H0 O
  O
L3 O
H1 O
  O
L3 O
H2 O
  O
L3 O
H3 O
  O
L3 O
H4 O
  O
L3 O
H5 O
  O
L3 O

H6 O
  O
L3 O
H7 O
L4 O
H0 O
  O
L4 O
H1 O
  O
L4 O
H2 O
  O
L4 O
H3 O
  O
L4 O
H4 O
  O
L4 O
H5 O
  O
L4 O
H6 O
  O
L4 O
H7Figure O
11 O
: O
Attention O
patterns O
of O
the O
target O
model O
T(4,512 O
) O
based O
on O
our O
FPI O
method O
. O

The O
last O
4 O
attention O
patterns O
( O
H4 O
- O
H7 O
) O
in O
each O
row O
are O
obtained O
by O
FPI O
expansion O
. O

L1 O
H0 O
  O
L1 O
H1 O
  O
L1 O
H2 O
  O
L1 O
H3 O
  O
L1 O
H4 O
  O
L1 O
H5 O
  O
L1 O

H6 O
  O
L1 O
H7 O
L2 O
H0 O
  O
L2 O
H1 O
  O
L2 O
H2 O
  O
L2 O
H3 O
  O
L2 O
H4 O
  O
L2 O
H5 O
  O
L2 O
H6 O
  O
L2 O
H7 O
L3 O
H0 O
  O
L3 O
H1 O
  O
L3 O
H2 O
  O
L3 O
H3 O
  O
L3 O
H4 O
  O
L3 O
H5 O
  O
L3 O

H6 O
  O
L3 O
H7 O
L4 O
H0 O
  O
L4 O
H1 O
  O
L4 O
H2 O
  O
L4 O
H3 O
  O
L4 O
H4 O
  O
L4 O
H5 O
  O
L4 O
H6 O
  O
L4 O
H7 O
Figure O
12 O
: O
Attention O
patterns O
of O
the O
target O
model O
T(4,512 O
) O
based O
on O
our O
AKI O
method O
. O

The O
last O
4 O
attention O
patterns O
( O
H4 O
- O
H7 O
) O
in O
each O
row O
are O
obtained O
by O
AKI O
expansion.2147abc O

d O
efg O

h O

i O
j O
abc O

d O
ef O
a O
bcg/2 O
Layer O
Norm:𝑢=𝐻.𝑚𝑒𝑎𝑛 O
𝑠=(𝐻−𝑢)2.𝑚𝑒𝑎𝑛LayerNorm O
会出现难以避 O
免的不同但影响不大。当大klm O
nClassifier O
EmbeddingMulti O
-Head O
  O
Attention O
( O
MHA)Add O
& O
NormFeed O
Forward O
  O
Network O
( O
FFN)Add O
& O
Norm O

N O
xop O
q O
rst O

uv O
g/2 O
h O
i/2i/2jk O
lm O
nk O
lo/2o/2 O
pq/2 O
q/2rs O
tu O
vs O
tab O
c O
defa O
b O
c O
g/2g/2 O
hi/2 O
i/2 O
jg/2 O
hg/2k/2k/2 O
lm/2 O
m/2 O
nk/2 O
lk/2o/2o/2 O
pq/2 O
q/2 O
ro/2 O
po/2s/2s/2 O
tu/2 O
u/2vs/2 O
ts/2ab O
c O
defa O
b O
c O
ade O
fbc O
w1 O
w2w1 O
w2w1 O
w2 O
w1 O
w1 O
w2 O
w1 O
w1 O
w2W1/2 O
w2W1/2 O
w1 O
w1 O
w2w1 O
w1 O
w2w2W1/2 O
W1/2MHA O
/ O
FFN O
Expansion O
step O
1 O
MHA O
Expansion O
step O
2 O
FFN O
Expansion O
step O
2 O
𝑊𝑙𝑄|𝐾|𝑉 O
𝑊𝐸𝑀𝐵𝑊𝑙𝑂 O
𝑊𝐿𝑁 O
abc O

d O
ef O
a O
bc𝑊𝐿𝑁𝑊𝑙1𝑊𝑙2𝑊𝐿𝑁𝑊𝐸𝑀𝐵𝑇 O
𝑔𝑖𝑛={1,2,1}FFN O
MHA O
abc O

d O
ef O
a O
bcg/2g/2 O
hi/2 O
i/2 O
jk O
lm O
nk O
lo/2o/2 O
pq/2 O
q/2rs O
tu O
vs O
tab O
c O
defa O
b O
c O
w1 O
w1 O
w2 O
w1 O
w1 O
w2W1/2 O
w2W1/2FPI O
hidden O
neuron O
embedding O
neuron O
different O
heads O
’ O
neuron O
FFN O
neuronFigure O
13 O
: O
FPI O
process O
. O

We O
use O
the O
FPI O
method O
to O
widen O
the O
source O
model O
with O
a O
width O
of O
2 O
into O
a O
target O
model O
with O
a O
width O
of O
3 O
. O

In O
the O
example O
, O
the O
source O
model O
and O
the O
target O
model O
have O
2 O
and O
3 O
attention O
heads O
respectively O
. O

And O
the O
head O
dimension O
is O
1 O
. O

To O
facilitate O
the O
illustration O
, O
we O
reduce O
the O
number O
of O
neurons O
in O
the O
FFN O
layer O
. O

We O
also O
note O
that O
since O
the O
MLM O
classifier O
of O
BERT O
is O
a O
transposition O
of O
the O
Embedding O
layer O
, O
they O
share O
a O
parameter O
matrix O
. O

Therefore O
, O
in O
step O
1 O
, O
we O
expand O
the O
MLM O
classifier O
by O
re O
- O
scaling O
the O
parameter O
values O
of O
the O
LN O
layer O
below O
the O
MLM O
classifier O
instead O
of O
following O
formula O
7 O
. O

Layer O
Norm:𝑢=𝐻.𝑚𝑒𝑎𝑛 O
𝑠=(𝐻−𝑢)2.𝑚𝑒𝑎𝑛LayerNorm O
会出现难以避 O
免的不同但影响不大。当大Classifier O
EmbeddingMulti O
-Head O
  O
Attention O
( O
MHA)Add O
& O
NormFeed O
Forward O
  O
Network O
( O
FFN)Add O
& O
Norm O
N O
x O
g/2g/2 O
hi/2 O
i/2 O
jg’/2 O
h’g’/2k/2k’/2 O
lm/2 O
m’/2 O
nk/2 O
lk’/2o/2o/2 O
pq/2 O
q/2 O
ro’/2 O
p’o’/2s/2s’/2 O
tu/2 O
u’/2vs/2 O
ts’/2ab O
c O
defa O
b O
c O
w1 O
w1 O
w2w1 O
w1 O
w2w2W1/2 O
W1/2MHA O
Expansion O
step O
2FFN O
Expansion O
step O
2 O
abc O
d O
ef O
a O
bc O
𝑔𝑖𝑛={1,2,1}FFN O
MHA O
abc O

d O
ef O
a O
bcg/2g/2 O
hi/2 O
i/2 O
jk O
lm O
nk O
lo/2o/2 O
pq/2 O
q/2rs O
tu O
vs O
tab O
c O
defa O
b O
c O
w1 O
w1 O
w2 O
w1 O
w1 O
w2W1/2 O
w2W1/2 O
𝑊𝑙1𝑊𝑙2 O
𝑊𝑙𝑄|𝐾|𝑉𝑊𝑙𝑂 O
g’/2g’/2 O
h’i’/2 O
i’/2 O
j’k’l’m’n’k O
’ O
l’o’/2o’/2 O
p’q’/2 O
q’/2 O
r O
’s O
’ O
t’u O
’ O
v O
’s O
’ O
t O
’ O
w’1 O
w’1 O
w’2𝑊𝑙+11𝑊𝑙+12 O

𝑊𝑙+1𝑄|𝐾|𝑉𝑊𝑙+1𝑂𝑈𝑙2 O
𝑈𝑙1 O
𝑈𝑙𝑄|𝐾|𝑉𝑈𝑙𝑂AKI O
hidden O
neuron O
embedding O
neuron O
different O
heads O
’ O
neurondifferent O
layers O
’ O
FFN O
neuron O
𝒍-thlayer O
Transformer O
𝒍+𝟏 O
-thlayer O

Transformer O
Figure O
14 O
: O
AKI O
process O
. O

We O
ignore O
its O
first O
step O
because O
it O
is O
the O
same O
as O
FPI O
’s O
first O
step O
. O

The O
main O
difference O
between O
AKI O
and O
FPI O
is O
that O
in O
step O
2 O
, O
AKI O
copies O
some O
attention O
heads O
in O
MHA O
and O
some O
parameters O
in O
FFN O
of O
the(l+ O
1 O
) O
-th O
layer O
instead O
of O
only O
copying O
the O
l O
- O
th O
layer O
to O
construct O
the O
new O
l O
- O
th O
layer O
Transformer.2148 O


Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
3982–3992 O
, O
Hong O
Kong O
, O
China O
, O
November O
3–7 O
, O
2019 O
. O

c O

 O
2019 O
Association O
for O
Computational O
Linguistics3982Sentence O
- O
BERT B-MethodName
: O
Sentence O
Embeddings O
using O
Siamese O
BERT B-MethodName
- O
Networks O
Nils O
Reimers O
and O

Iryna O
Gurevych O
Ubiquitous O
Knowledge O
Processing O
Lab O
( O
UKP O
- O
TUDA O
) O
Department O
of O
Computer O
Science O
, O
Technische O
Universit O
¨at O
Darmstadt O
www.ukp.tu-darmstadt.de O
Abstract O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
and O
RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
has O
set O
a O
new O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
sentence O
- O
pair O
regression O
tasks O
like O
semantic B-MetricName
textual I-MetricName
similarity I-MetricName
( O
STS B-MetricName
) O
. O

However O
, O
it O
requires O
that O
both O
sentences O
are O
fed O
into O
the O
network O
, O
which O
causes O
a O
massive O
computational O
overhead O
: O
Finding O
the O
most O
similar O
pair O
in O
a O
collection O
of O
10,000 O
sentences O
requires O
about O
50 O
million O
inference O
computations O
( O
~65 O
hours O
) O
with O
BERT B-MethodName
. O

The O
construction O
of O
BERT B-MethodName
makes O
it O
unsuitable O
for O
semantic O
similarity O
search O
as O
well O
as O
for O
unsupervised O
tasks O
like O
clustering O
. O

In O
this O
publication O
, O
we O
present O
Sentence B-MethodName
- I-MethodName
BERT I-MethodName
( O
SBERT B-MethodName
) O
, O
a O
modiﬁcation O
of O
the O
pretrained O
BERT B-MethodName
network O
that O
use O
siamese O
and O
triplet O
network O
structures O
to O
derive O
semantically O
meaningful O
sentence O
embeddings O
that O
can O
be O
compared O
using O
cosine O
- O
similarity O
. O

This O
reduces O
the O
effort O
for O
ﬁnding O
the O
most O
similar O
pair O
from O
65 O
hours O
with O
BERT B-MethodName
/ O
RoBERTa B-MethodName
to O
about O
5 O
seconds O
with O
SBERT B-MethodName
, O
while O
maintaining O
the O
accuracy O
from O
BERT B-MethodName
. O

We O
evaluate O
SBERT B-MethodName
and O
SRoBERTa B-MethodName
on O
common O
STS B-MetricName
tasks O
and O
transfer O
learning O
tasks O
, O
where O
it O
outperforms O
other O
state O
- O
of O
- O
the O
- O
art O
sentence O
embeddings O
methods.1 O
1 O
Introduction O
In O
this O
publication O
, O
we O
present O
Sentence O
- O
BERT B-MethodName
( O
SBERT B-MethodName
) O
, O
a O
modiﬁcation O
of O
the O
BERT B-MethodName
network O
using O
siamese O
and O
triplet O
networks O
that O
is O
able O
to O
derive O
semantically O
meaningful O
sentence O
embeddings2 O
. O

This O
enables O
BERT B-MethodName
to O
be O
used O
for O
certain O
new O
tasks O
, O
which O
up O
- O
to O
- O
now O
were O
not O
applicable O
for O
BERT B-MethodName
. O

These O
tasks O
include O
large O
- O
scale O
seman1Code O
available O
: O
https://github.com/UKPLab/ O
sentence O
- O
transformers O
2With O
semantically O
meaningful O

we O
mean O
that O
semantically O
similar O
sentences O
are O
close O
in O
vector O
space.tic O
similarity O
comparison O
, O
clustering O
, O
and O
information O
retrieval O
via O
semantic O
search O
. O

BERT B-MethodName
set O
new O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
various O
sentence O
classiﬁcation O
and O
sentence O
- O
pair O
regression O
tasks O
. O

BERT B-MethodName
uses O
a O
cross O
- O
encoder O
: O
Two O
sentences O
are O
passed O
to O
the O
transformer O
network O
and O
the O
target O
value O
is O
predicted O
. O

However O
, O
this O
setup O
is O
unsuitable O
for O
various O
pair O
regression O
tasks O
due O
to O
too O
many O
possible O
combinations O
. O

Finding O
in O
a O
collection O
of O
n= O
10 O
000 O
sentences O
the O
pair O
with O
the O
highest O
similarity O
requires O
with O
BERT B-MethodName
n(n 1)=2 O
= O
49 O
995 O
000 O
inference O
computations O
. O

On O
a O
modern O
V100 O
GPU O
, O
this O
requires O
about O
65 O
hours O
. O

Similar O
, O
ﬁnding O
which O
of O
the O
over O
40 O
million O
existent O
questions O
of O
Quora O
is O
the O
most O
similar O
for O
a O
new O
question O
could O
be O
modeled O
as O
a O
pair O
- O
wise O
comparison O
with O
BERT B-MethodName
, O
however O
, O
answering O
a O
single O
query O
would O
require O
over O
50 O
hours O
. O

A O
common O
method O
to O
address O
clustering O
and O
semantic O
search O
is O
to O
map O
each O
sentence O
to O
a O
vector O
space O
such O
that O
semantically O
similar O
sentences O
are O
close O
. O

Researchers O
have O
started O
to O
input O
individual O
sentences O
into O
BERT B-MethodName
and O
to O
derive O
ﬁxedsize O
sentence O
embeddings O
. O

The O
most O
commonly O
used O
approach O
is O
to O
average O
the O
BERT B-MethodName
output O
layer O
( O
known O
as O
BERT B-MethodName
embeddings O
) O
or O
by O
using O
the O
output O
of O
the O
ﬁrst O
token O
( O
the O
[ O
CLS O
] O
token O
) O
. O

As O
we O
will O
show O
, O
this O
common O
practice O
yields O
rather O
bad O
sentence O
embeddings O
, O
often O
worse O
than O
averaging O
GloVe O
embeddings O
( O
Pennington O
et O
al O
. O
, O
2014 O
) O
. O

To O
alleviate O
this O
issue O
, O
we O
developed O
SBERT B-MethodName
. O

The O
siamese O
network O
architecture O
enables O
that O
ﬁxed O
- O
sized O
vectors O
for O
input O
sentences O
can O
be O
derived O
. O

Using O
a O
similarity O
measure O
like O
cosinesimilarity O
or O
Manhatten O
/ O
Euclidean O
distance O
, O
semantically O
similar O
sentences O
can O
be O
found O
. O

These O
similarity O
measures O
can O
be O
performed O
extremely O
efﬁcient O
on O
modern O
hardware O
, O
allowing O
SBERT B-MethodName
to O
be O
used O
for O
semantic B-MetricName
similarity I-MetricName
search O
as O
well O
as O
for O
clustering O
. O

The O
complexity O
for O
ﬁnding O
the3983most O
similar O
sentence O
pair O
in O
a O
collection O
of O
10,000 O
sentences O
is O
reduced O
from O
65 O
hours O
with O
BERT B-MethodName
to O
the O
computation O
of O
10,000 O
sentence O
embeddings O
( O
~5 O
seconds O
with O
SBERT B-MethodName
) O
and O
computing O
cosinesimilarity O
( O
~0.01 O
seconds O
) O
. O

By O
using O
optimized O
index O
structures O
, O
ﬁnding O
the O
most O
similar O
Quora O
question O
can O
be O
reduced O
from O
50 O
hours O
to O
a O
few O
milliseconds O
( O
Johnson O
et O
al O
. O
, O
2017 O
) O
. O

We O
ﬁne O
- O
tune O
SBERT B-MethodName
on O
NLI B-DatasetName
data O
, O
which O
creates O
sentence O
embeddings O
that O
signiﬁcantly O
outperform O
other O
state O
- O
of O
- O
the O
- O
art O
sentence O
embedding O
methods O
like O
InferSent O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
and O
Universal O
Sentence O
Encoder O
( O
Cer O
et O
al O
. O
, O
2018 O
) O
. O

On O
seven O
Semantic B-MetricName
Textual I-MetricName
Similarity I-MetricName
( O
STS B-MetricName
) O
tasks O
, O
SBERT B-MethodName
achieves O
an O
improvement O
of O
11.7 O
points O
compared O
to O
InferSent O
and O
5.5 O
points O
compared O
to O
Universal O
Sentence O
Encoder O
. O

On O
SentEval O
( O
Conneau O
and O
Kiela O
, O
2018 O
) O
, O
an O
evaluation O
toolkit O
for O
sentence O
embeddings O
, O
we O
achieve O
an O
improvement O
of O
2.1 O
and O
2.6 O
points O
, O
respectively O
. O

SBERT B-MethodName
can O
be O
adapted O
to O
a O
speciﬁc O
task O
. O

It O
sets O
new O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
a O
challenging O
argument O
similarity O
dataset O
( O
Misra O
et O
al O
. O
, O
2016 O
) O
and O
on O
a O
triplet O
dataset O
to O
distinguish O
sentences O
from O
different O
sections O
of O
a O
Wikipedia O
article O
( O
Dor O
et O
al O
. O
, O
2018 O
) O
. O

The O
paper O
is O
structured O
in O
the O
following O
way O
: O
Section O
3 O
presents O
SBERT B-MethodName
, O
section O
4 O
evaluates O
SBERT B-MethodName
on O
common O
STS B-MetricName
tasks O
and O
on O
the O
challenging O
Argument O
Facet O
Similarity O
( O
AFS O
) O
corpus O
( O
Misra O
et O
al O
. O
, O
2016 O
) O
. O

Section O
5 O
evaluates O
SBERT B-MethodName
on O
SentEval O
. O

In O
section O
6 O
, O
we O
perform O
an O
ablation O
study O
to O
test O
some O
design O
aspect O
of O
SBERT B-MethodName
. O

In O
section O
7 O
, O
we O
compare O
the O
computational O
efﬁciency O
of O
SBERT B-MethodName
sentence O
embeddings O
in O
contrast O
to O
other O
state O
- O
of O
- O
the O
- O
art O
sentence O
embedding O
methods O
. O

2 O
Related O
Work O
We O
ﬁrst O
introduce O
BERT B-MethodName
, O
then O
, O
we O
discuss O
stateof O
- O
the O
- O
art O
sentence O
embedding O
methods O
. O

BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2018 O
) O
is O
a O
pre O
- O
trained O
transformer O
network O
( O
Vaswani O
et O
al O
. O
, O
2017 O
) O
, O
which O
set O
for O
various O
NLP O
tasks O
new O
state O
- O
of O
- O
the O
- O
art O
results O
, O
including O
question O
answering O
, O
sentence O
classiﬁcation O
, O
and O
sentence O
- O
pair O
regression O
. O

The O
input O
for O
BERT B-MethodName
for O
sentence O
- O
pair O
regression O
consists O
of O
the O
two O
sentences O
, O
separated O
by O
a O
special O
[ O
SEP O
] O
token O
. O

Multi O
- O
head O
attention O
over O
12 O
( O
base O
- O
model O
) O
or O
24 O
layers O
( O
large O
- O
model O
) O
is O
applied O
and O
the O
output O
is O
passed O
to O
a O
simple O
regression O
function O
to O
derive O
the O
ﬁnal O
label O
. O

Using O
this O
setup O
, O
BERT B-MethodName
set O
anew O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
the O
Semantic B-MetricName
Textual I-MetricName
Semilarity I-MetricName
( O
STS B-MetricName
) O
benchmark O
( O
Cer O
et O
al O
. O
, O
2017 O
) O
. O

RoBERTa B-MethodName
( O
Liu O
et O
al O
. O
, O
2019 O
) O
showed O
, O
that O
the O
performance O
of O
BERT B-MethodName
can O
further O
improved O
by O
small O
adaptations O
to O
the O
pre O
- O
training O
process O
. O

We O
also O
tested O
XLNet O
( O
Yang O
et O
al O
. O
, O
2019 O
) O
, O
but O
it O
led O
in O
general O
to O
worse O
results O
than O
BERT B-MethodName
. O

A O
large O
disadvantage O
of O
the O
BERT B-MethodName
network O
structure O
is O
that O
no O
independent O
sentence O
embeddings O
are O
computed O
, O
which O
makes O
it O
difﬁcult O
to O
derive O
sentence O
embeddings O
from O
BERT B-MethodName
. O

To O
bypass O
this O
limitations O
, O
researchers O
passed O
single O
sentences O
through O
BERT B-MethodName
and O
then O
derive O
a O
ﬁxed O
sized O
vector O
by O
either O
averaging O
the O
outputs O
( O
similar O
to O
average O
word O
embeddings O
) O
or O
by O
using O
the O
output O
of O
the O
special O
CLS O
token O
( O
for O
example O
: O

May O
et O
al O
. O
( O
2019 O
) O
; O
Zhang O
et O
al O
. O
( O
2019 O
) O
; O
Qiao O
et O
al O
. O
( O
2019 O
) O
) O
. O

These O
two O
options O
are O
also O
provided O
by O
the O
popular O
bert B-MethodName
- O
as O
- O
a O
- O
service O
- O
repository3 O
. O

Up O
to O
our O
knowledge O
, O
there O
is O
so O
far O
no O
evaluation O
if O
these O
methods O
lead O
to O
useful O
sentence O
embeddings O
. O

Sentence O
embeddings O
are O
a O
well O
studied O
area O
with O
dozens O
of O
proposed O
methods O
. O

Skip O
- O
Thought O
( O
Kiros O
et O
al O
. O
, O
2015 O
) O
trains O
an O
encoder O
- O
decoder O
architecture O
to O
predict O
the O
surrounding O
sentences O
. O

InferSent O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
uses O
labeled O
data O
of O
the O
Stanford O
Natural O
Language O
Inference O
dataset O
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
and O
the O
MultiGenre O
NLI B-DatasetName
dataset O
( O
Williams O
et O
al O
. O
, O
2018 O
) O
to O
train O
a O
siamese O
BiLSTM O
network O
with O
max O
- O
pooling O
over O
the O
output O
. O

Conneau O
et O
al O
. O
showed O
, O
that O
InferSent O
consistently O
outperforms O
unsupervised O
methods O
like O
SkipThought O
. O

Universal O
Sentence O
Encoder O
( O
Cer O
et O
al O
. O
, O
2018 O
) O
trains O
a O
transformer O
network O
and O
augments O
unsupervised O
learning O
with O
training O
on O
SNLI B-DatasetName
. O

Hill O
et O

al O
. O

( O
2016 O
) O
showed O
, O
that O
the O
task O
on O
which O
sentence O
embeddings O
are O
trained O
signiﬁcantly O
impacts O
their O
quality O
. O

Previous O
work O
( O
Conneau O
et O
al O
. O
, O
2017 O
; O
Cer O
et O
al O
. O
, O
2018 O
) O
found O
that O
the O
SNLI B-DatasetName
datasets O
are O
suitable O
for O
training O
sentence O
embeddings O
. O

Yang O
et O
al O
. O

( O
2018 O
) O
presented O
a O
method O
to O
train O
on O
conversations O
from O
Reddit O
using O
siamese O
DAN O
and O
siamese O
transformer O
networks O
, O
which O
yielded O
good O
results O
on O
the O
STS B-MetricName
benchmark O
dataset O
. O

Humeau O
et O
al O
. O

( O
2019 O
) O
addresses O
the O
run O
- O
time O
overhead O
of O
the O
cross O
- O
encoder O
from O
BERT B-MethodName
and O
present O
a O
method O
( O
poly O
- O
encoders O
) O
to O
compute O
a O
score O
between O
mcontext O
vectors O
and O
pre3https://github.com/hanxiao/ O
bert B-MethodName
- O
as O
- O
service/3984 O
Sentence O
A O
  O
Sentence O
B O
  O
BERT B-MethodName
BERT B-MethodName
u O
v O
  O
pooling O
  O
pooling O
  O
( O
u O
, O
v O
, O
|u O
-v| O
) O

Softmax O
classifier O
  O
Figure O
1 O
: O
SBERT B-MethodName
architecture O
with O
classiﬁcation O
objective O
function O
, O
e.g. O
, O
for O
ﬁne O
- O
tuning O
on O
SNLI B-DatasetName
dataset O
. O

The O
two O
BERT B-MethodName
networks O
have O
tied O
weights O
( O
siamese O
network O
structure O
) O
. O

computed O
candidate O
embeddings O
using O
attention O
. O

This O
idea O
works O
for O
ﬁnding O
the O
highest O
scoring O
sentence O
in O
a O
larger O
collection O
. O

However O
, O
polyencoders O
have O
the O
drawback O
that O
the O
score O
function O
is O
not O
symmetric O
and O
the O
computational O
overhead O
is O
too O
large O
for O
use O
- O
cases O
like O
clustering O
, O
which O
would O
require O
O(n2)score O
computations O
. O

Previous O
neural O
sentence O
embedding O
methods O
started O
the O
training O
from O
a O
random O
initialization O
. O

In O
this O
publication O
, O
we O
use O
the O
pre O
- O
trained O
BERT B-MethodName
and O
RoBERTa B-MethodName
network O
and O
only O
ﬁne O
- O
tune O
it O
to O
yield O
useful O
sentence O
embeddings O
. O

This O
reduces O
signiﬁcantly O
the O
needed O
training O
time O
: O
SBERT B-MethodName
can O
be O
tuned O
in O
less O
than O
20 O
minutes O
, O
while O
yielding O
better O
results O
than O
comparable O
sentence O
embedding O
methods O
. O

3 O
Model O
SBERT B-MethodName
adds O
a O
pooling O
operation O
to O
the O
output O
of O
BERT B-MethodName
/ O
RoBERTa B-MethodName
to O
derive O
a O
ﬁxed O
sized O
sentence O
embedding O
. O

We O
experiment O
with O
three O
pooling O
strategies O
: O
Using O
the O
output O
of O
the O
CLS O
- O
token O
, O
computing O
the O
mean O
of O
all O
output O
vectors O
( O
MEAN O
strategy O
) O
, O
and O
computing O
a O
max O
- O
over O
- O
time O
of O
the O
output O
vectors O
( O
MAX O
- O
strategy O
) O
. O

The O
default O
conﬁguration O
is O
MEAN O
. O

In O
order O
to O
ﬁne O
- O
tune O
BERT B-MethodName
/ O
RoBERTa B-MethodName
, O
we O
create O
siamese O
and O
triplet O
networks O
( O
Schroff O
et O
al O
. O
, O
2015 O
) O
to O
update O
the O
weights O
such O
that O
the O
produced O
sentence O
embeddings O
are O
semantically O
meaningful O
and O
can O
be O
compared O
with O
cosine O
- O
similarity O
. O

The O
network O
structure O
depends O
on O
the O
available O
Sentence O
A O
  O
Sentence O
B O
  O
BERT B-MethodName
BERT B-MethodName
u O
v O
  O
pooling O
  O
pooling O
  O
cosine O
-sim(u O
, O
v O
) O
-1 O
… O
1 O
  O
Figure O
2 O
: O
SBERT B-MethodName
architecture O
at O
inference O
, O
for O
example O
, O
to O
compute O
similarity O
scores O
. O

This O
architecture O
is O
also O
used O
with O
the O
regression O
objective O
function O
. O

training O
data O
. O

We O
experiment O
with O
the O
following O
structures O
and O
objective O
functions O
. O

Classiﬁcation O
Objective O
Function O
. O

We O
concatenate O
the O
sentence O
embeddings O
uandvwith O
the O
element O
- O
wise O
difference O
ju vjand O
multiply O
it O
with O
the O
trainable O
weight O
Wt2R3nk O
: O

o O
= O
softmax O
( O
Wt(u;v;ju vj O
) O
) O
wherenis O
the O
dimension O
of O
the O
sentence O
embeddings O
and O
kthe O
number O
of O
labels O
. O

We O
optimize O
cross O
- O
entropy O
loss O
. O

This O
structure O
is O
depicted O
in O
Figure O
1 O
. O

Regression O
Objective O
Function O
. O

The O
cosinesimilarity O
between O
the O
two O
sentence O
embeddings O
uandvis O
computed O
( O
Figure O
2 O
) O
. O

We O
use O
meansquared O
- O
error O
loss O
as O
the O
objective O
function O
. O

Triplet O
Objective O
Function O
. O

Given O
an O
anchor O
sentencea O
, O
a O
positive O
sentence O
p O
, O
and O
a O
negative O
sentencen O
, O
triplet O
loss O
tunes O
the O
network O
such O
that O
the O
distance O
between O
aandpis O
smaller O
than O
the O
distance O
between O
aandn O
. O

Mathematically O
, O
we O
minimize O
the O
following O
loss O
function O
: O
max(jjsa spjj jjsa snjj+;0 O
) O
withsxthe O
sentence O
embedding O
for O
a O
/ O
n O
/ O
p O
, O
jjjj O
a O
distance O
metric O
and O
margin O
. O
Marginensures O
thatspis O
at O
leastcloser O
tosathansn O
. O

As O
metric O
we O
use O
Euclidean O
distance O

and O
we O
set O
= O
1 O
in O
our O
experiments O
. O

3.1 O
Training O
Details O
We O
train O
SBERT B-MethodName
on O
the O
combination O
of O
the O
SNLI B-DatasetName
( O
Bowman O
et O
al O
. O
, O
2015 O
) O
and O
the O
Multi O
- O
Genre O
NLI3985Model O
STS12 O
STS13 O
STS14 O
STS15 O
STS16 O
STSb B-DatasetName
SICK O
- O
R O
Avg O
. O
Avg O
. O
GloVe O
embeddings O
55.14 O
70.66 O
59.73 O
68.25 O
63.66 O
58.02 O
53.76 O
61.32 O
Avg O
. O
BERT B-MethodName
embeddings O
38.78 O
57.98 O
57.98 O
63.15 O
61.06 O
46.35 O
58.40 O
54.81 O
BERT B-MethodName
CLS O
- O
vector O
20.16 O
30.01 O
20.09 O
36.88 O
38.08 O
16.50 O
42.63 O
29.19 O
InferSent O
- O
Glove O
52.86 O
66.75 O
62.15 O
72.77 O
66.87 O
68.03 O
65.65 O
65.01 O
Universal O
Sentence O
Encoder O
64.49 O
67.80 O
64.61 O
76.83 O
73.18 O
74.92 O
76.69 O
71.22 O
SBERT B-MethodName
- O
NLI O
- O
base O
70.97 O
76.53 O
73.19 O
79.09 O
74.30 O
77.03 O
72.91 O
74.89 O
SBERT B-MethodName
- O
NLI O
- O
large O
72.27 O
78.46 O
74.90 O
80.99 O
76.25 O
79.23 O
73.75 O
76.55 O
SRoBERTa B-MethodName
- O
NLI O
- O
base O
71.54 O
72.49 O
70.80 O
78.74 O
73.69 O
77.77 O
74.46 O
74.21 O
SRoBERTa B-MethodName
- O
NLI O
- O
large O
74.53 O
77.00 O
73.18 O
81.85 O
76.82 O
79.10 O
74.29 O
76.68 O
Table O
1 O
: O
Spearman O
rank O
correlation O
between O
the O
cosine O
similarity O
of O
sentence O
representations O
and O
the O
gold O
labels O
for O
various O
Textual B-MetricName
Similarity I-MetricName
( O
STS B-MetricName
) O
tasks O
. O

Performance O
is O
reported O
by O
convention O
as O
100 O
. O

STS12 O
- O
STS16 O
: O
SemEval O
2012 O
- O
2016 O
, O
STSb B-DatasetName
: O
STSbenchmark B-DatasetName
, O
SICK O
- O
R O
: O
SICK O
relatedness O
dataset O
. O

( O
Williams O
et O
al O
. O
, O
2018 O
) O
dataset O
. O

The O
SNLI B-DatasetName
is O
a O
collection O
of O
570,000 O
sentence O
pairs O
annotated O
with O
the O
labels O
contradiction O
, O
eintailment O
, O
and O
neutral O
. O

MultiNLI B-DatasetName
contains O
430,000 O
sentence O
pairs O
and O
covers O
a O
range O
of O
genres O
of O
spoken O
and O
written O
text O
. O

We O
ﬁne O
- O
tune O
SBERT B-MethodName
with O
a O
3 O
- O
way O
softmaxclassiﬁer O
objective O
function O
for O
one O
epoch O
. O

We O
used O
a O
batch B-HyperparameterName
- I-HyperparameterName
size I-HyperparameterName
of O
16 B-HyperparameterValue
, O
Adam O
optimizer O
with O
learning B-HyperparameterName
rate I-HyperparameterName
2e-5 B-HyperparameterValue
, O
and O
a O
linear B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
warm I-HyperparameterName
- I-HyperparameterName
up I-HyperparameterName
over O
10 B-HyperparameterValue
% I-HyperparameterValue
of O
the O
training O
data O
. O

Our O
default O
pooling O
strategy O
is O
MEAN O
. O

4 O
Evaluation O
- O
Semantic B-MetricName
Textual I-MetricName
Similarity I-MetricName
We O
evaluate O
the O
performance O
of O
SBERT B-MethodName
for O
common O
Semantic B-MetricName
Textual I-MetricName
Similarity I-MetricName
( O
STS B-MetricName
) O
tasks O
. O

State O
- O
of O
- O
the O
- O
art O
methods O
often O
learn O
a O
( O
complex O
) O
regression O
function O
that O
maps O
sentence O
embeddings O
to O
a O
similarity O
score O
. O

However O
, O
these O
regression O
functions O
work O
pair O
- O
wise O
and O
due O
to O
the O
combinatorial O
explosion O
those O
are O
often O
not O
scalable O
if O
the O
collection O
of O
sentences O
reaches O
a O
certain O
size O
. O

Instead O
, O
we O
always O
use O
cosine O
- O
similarity O
to O
compare O
the O
similarity O
between O
two O
sentence O
embeddings O
. O

We O
ran O
our O
experiments O
also O
with O
negative O
Manhatten O
and O
negative O
Euclidean O
distances O
as O
similarity O
measures O
, O
but O
the O
results O
for O
all O
approaches O
remained O
roughly O
the O
same O
. O

4.1 O
Unsupervised O
STS B-MetricName
We O
evaluate O
the O
performance O
of O
SBERT B-MethodName
for O
STS B-MetricName
without O
using O
any O
STS B-MetricName
speciﬁc O
training O
data O
. O

We O
use O
the O
STS B-MetricName
tasks O
2012 O
- O
2016 O
( O
Agirre O
et O
al O
. O
, O
2012 O
, O
2013 O
, O
2014 O
, O
2015 O
, O
2016 O
) O
, O
the O
STS B-MetricName
benchmark O
( O
Cer O
et O
al O
. O
, O
2017 O
) O
, O
and O
the O
SICK O
- O
Relatedness O
dataset O
( O
Marelli O
et O
al O
. O
, O
2014 O
) O
. O

These O
datasets O
provide O
labels O
between O
0 O
and O
5 O
on O
the O
semantic O
relatedness O
of O
sentence O
pairs O
. O

We O
showed O
in O
( O
Reimers O
et O
al O
. O
, O
2016 O
) O
that O
Pearson O
correlation O
is O
badly O
suited O
for O
STS B-MetricName
. O

Instead O
, O
we O
compute O
the O
Spearman O
’s O
rank O
correlation O
between O
the O
cosine O
- O
similarity O
of O
the O
sentence O
embeddings O
and O
the O
gold O
labels O
. O

The O
setup O
for O
the O
other O
sentence O
embedding O
methods O
is O
equivalent O
, O
the O
similarity O
is O
computed O
by O
cosinesimilarity O
. O

The O
results O
are O
depicted O
in O
Table O
1 O
. O

The O
results O
shows O
that O
directly O
using O
the O
output O
of O
BERT B-MethodName
leads O
to O
rather O
poor O
performances O
. O

Averaging O
the O
BERT B-MethodName
embeddings O
achieves O
an O
average O
correlation O
of O
only O
54.81 O
, O
and O
using O
the O
CLStoken O
output O
only O
achieves O
an O
average O
correlation O
of O
29.19 O
. O

Both O
are O
worse O
than O
computing O
average O
GloVe O
embeddings O
. O

Using O
the O
described O
siamese O
network O
structure O
and O
ﬁne O
- O
tuning O
mechanism O
substantially O
improves O
the O
correlation O
, O
outperforming O
both O
InferSent O
and O
Universal O
Sentence O
Encoder O
substantially O
. O

The O
only O
dataset O
where O
SBERT B-MethodName
performs O
worse O
than O
Universal O
Sentence O
Encoder O
is O
SICK O
- O
R. O
Universal O
Sentence O
Encoder O
was O
trained O
on O
various O
datasets O
, O
including O
news O
, O
question O
- O
answer O
pages O
and O
discussion O
forums O
, O
which O
appears O
to O
be O
more O
suitable O
to O
the O
data O
of O
SICK O
- O
R. O
In O
contrast O
, O
SBERT B-MethodName
was O
pre O
- O
trained O
only O
on O
Wikipedia O
( O
via O
BERT B-MethodName
) O
and O
on O
NLI B-DatasetName
data O
. O

While O
RoBERTa B-MethodName
was O
able O
to O
improve O
the O
performance O
for O
several O
supervised O
tasks O
, O
we O
only O
observe O
minor O
difference O
between O
SBERT B-MethodName
and O
SRoBERTa B-MethodName
for O
generating O
sentence O
embeddings O
. O

4.2 O
Supervised O
STS B-MetricName
The O
STS B-MetricName
benchmark O
( O
STSb B-DatasetName
) O
( O
Cer O
et O
al O
. O
, O
2017 O
) O
provides O
is O
a O
popular O
dataset O
to O
evaluate O
supervised O
STS B-MetricName
systems O
. O

The O
data O
includes O
8,628 O
sentence O
pairs O
from O
the O
three O
categories O
captions O
, O
news O
, O
and O
forums O
. O

It O
is O
divided O
into O
train O
( O
5,749 O
) O
, O
dev O
( O
1,500 O
) O
and O
test O
( O
1,379 O
) O
. O

BERT B-MethodName
set O
a O
new O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
this O
dataset O
by O
passing O
both O
sentences O
to O
the O
network O
and O
using O
a O
simple O
regres-3986sion O
method O
for O
the O
output O
. O

Model O
Spearman O
Not O
trained O
for O
STS B-MetricName
Avg O
. O
GloVe O
embeddings O
58.02 B-MetricValue

Avg O
. O
BERT B-MethodName
embeddings O
46.35 B-MetricValue
InferSent O
- O
GloVe O
68.03 B-MetricValue
Universal O
Sentence O
Encoder O
74.92 B-MetricValue
SBERT B-MethodName
- O
NLI B-DatasetName
- O
base O
77.03 B-MetricValue
SBERT B-MethodName
- O
NLI B-DatasetName
- O
large O
79.23 B-MetricValue
Trained O
on O
STS B-MetricName
benchmark O
dataset O
BERT B-MethodName
- O
STSb B-DatasetName
- O
base O
84.30 B-MetricValue
+0.76 B-MetricValue
SBERT B-MethodName
- O
STSb B-DatasetName
- O
base O
84.67 B-MetricValue
-0.19 B-MetricValue
SRoBERTa B-MethodName
- O
STSb B-DatasetName
- O
base O
84.92 B-MetricValue
+0.34 B-MetricValue
BERT B-MethodName
- O
STSb B-DatasetName
- O
large O
85.64 B-MetricValue
+0.81 B-MetricValue
SBERT B-MethodName
- O
STSb B-DatasetName
- O
large O
84.45 B-MetricValue
-0.43 B-MetricValue
SRoBERTa B-MethodName
- O
STSb B-DatasetName
- O
large O
85.02 B-MetricValue
-0.76 B-MetricValue
Trained O
on O
NLI B-DatasetName
data O
+ O

STS B-MetricName
benchmark O
data O
BERT B-MethodName
- O
NLI B-DatasetName
- O
STSb B-DatasetName
- O
base O
88.33 B-MetricValue
0.19 B-MetricValue
SBERT B-MethodName
- O
NLI B-DatasetName
- O
STSb B-DatasetName
- O
base O
85.35 B-MetricValue
+0.17 B-MetricValue
SRoBERTa B-MethodName
- O
NLI B-DatasetName
- O
STSb B-DatasetName
- O
base O
84.79 B-MetricValue
+0.38 B-MetricValue
BERT B-MethodName
- O
NLI B-DatasetName
- O
STSb B-DatasetName
- O
large O
88.77 B-MetricValue
-0.46 B-MetricValue
SBERT B-MethodName
- O
NLI B-DatasetName
- O
STSb B-DatasetName
- O
large O
86.10 B-MetricValue
0.13 B-MetricValue
SRoBERTa B-MethodName
- O
NLI B-DatasetName
- O
STSb B-DatasetName
- O
large O
86.15 B-MetricValue
0.35 B-MetricValue
Table O
2 O
: O
Evaluation O
on O
the O
STS B-MetricName
benchmark O
test O
set O
. O

BERT B-MethodName
systems O
were O
trained O
with O
10 O
random O
seeds O
and O
4 O
epochs O
. O

SBERT B-MethodName
was O
ﬁne O
- O
tuned O
on O
the O
STSb B-DatasetName
dataset O
, O
SBERT B-MethodName
- O
NLI B-DatasetName
was O
pretrained O
on O
the O
NLI B-DatasetName
datasets O
, O
then O
ﬁne O
- O
tuned O
on O
the O
STSb B-DatasetName
dataset O
. O

We O
use O
the O
training O
set O
to O
ﬁne O
- O
tune O
SBERT B-MethodName
using O
the O
regression O
objective O
function O
. O

At O
prediction O
time O
, O
we O
compute O
the O
cosine O
- O
similarity O
between O
the O
sentence O
embeddings O
. O

All O
systems O
are O
trained O
with O
10 O
random O
seeds O
to O
counter O
variances O
( O
Reimers O
and O
Gurevych O
, O
2018 O
) O
. O

The O
results O
are O
depicted O
in O
Table O
2 O
. O

We O
experimented O
with O
two O
setups O
: O
Only O
training O
on O
STSb B-DatasetName
, O
and O
ﬁrst O
training O
on O
NLI B-DatasetName
, O
then O
training O
on O
STSb B-DatasetName
. O

We O
observe O
that O
the O
later O
strategy O
leads O
to O
a O
slight O
improvement O
of O
1 O
- O
2 O
points O
. O

This O
two O
- O
step O
approach O
had O
an O
especially O
large O
impact O
for O
the O
BERT B-MethodName
cross O
- O
encoder O
, O
which O
improved O
the O
performance O
by O
3 O
- O
4 O
points O
. O

We O
do O
not O
observe O
a O
signiﬁcant O
difference O
between O
BERT B-MethodName
and O
RoBERTa B-MethodName
. O

4.3 O
Argument O
Facet O
Similarity O
We O
evaluate O
SBERT B-MethodName
on O
the O
Argument O
Facet O
Similarity O
( O
AFS O
) O
corpus O
by O
Misra O
et O
al O
. O

( O
2016 O
) O

. O

The O
AFS O
corpus O
annotated O
6,000 O
sentential O
argument O
pairs O
from O
social O
media O
dialogs O
on O
three O
controversial O
topics O
: O
gun O
control O
, O
gay O
marriage O
, O
and O
death O
penalty O
. O

The O
data O
was O
annotated O
on O
a O
scale O
from O
0 O
( O
“ O
different O
topic O
” O
) O
to O
5 O
( O
“ O
completely O
equivalent O
” O
) O
. O

The O
similarity O
notion O
in O
the O
AFS O
corpus O
is O
fairly O
different O
to O
the O
similarity O
notion O
in O
the O
STS B-MetricName
datasets O
from O
SemEval O
. O

STS B-MetricName
data O
is O
usuallydescriptive O
, O
while O
AFS O
data O
are O
argumentative O
excerpts O
from O
dialogs O
. O

To O
be O
considered O
similar O
, O
arguments O
must O
not O
only O
make O
similar O
claims O
, O
but O
also O
provide O
a O
similar O
reasoning O
. O

Further O
, O
the O
lexical O
gap O
between O
the O
sentences O
in O
AFS O
is O
much O
larger O
. O

Hence O
, O
simple O
unsupervised O
methods O
as O
well O
as O
state O
- O
of O
- O
the O
- O
art O
STS B-MetricName
systems O
perform O
badly O
on O
this O
dataset O
( O
Reimers O
et O
al O
. O
, O
2019 O
) O
. O

We O
evaluate O
SBERT B-MethodName
on O
this O
dataset O
in O
two O
scenarios O
: O
1 O
) O
As O
proposed O
by O
Misra O
et O
al O
. O
, O
we O
evaluate O
SBERT B-MethodName
using O
10 O
- O
fold O
cross O
- O
validation O
. O

A O
drawback O
of O
this O
evaluation O
setup O
is O
that O
it O
is O
not O
clear O
how O
well O
approaches O
generalize O
to O
different O
topics O
. O

Hence O
, O
2 O
) O
we O
evaluate O
SBERT B-MethodName
in O
a O
cross O
- O
topic O
setup O
. O

Two O
topics O
serve O
for O
training O
and O
the O
approach O
is O
evaluated O
on O
the O
left O
- O
out O
topic O
. O

We O
repeat O
this O
for O
all O
three O
topics O
and O
average O
the O
results O
. O

SBERT B-MethodName
is O
ﬁne O
- O
tuned O
using O
the O
Regression O
Objective O
Function O
. O

The O
similarity O
score O
is O
computed O
using O
cosine O
- O
similarity O
based O
on O
the O
sentence O
embeddings O
. O

We O
also O
provide O
the O
Pearson O
correlationrto O
make O
the O
results O
comparable O
to O
Misra O
et O
al O
. O

However O
, O
we O
showed O
( O
Reimers O
et O
al O
. O
, O
2016 O
) O
that O
Pearson O
correlation O
has O
some O
serious O
drawbacks O
and O
should O
be O
avoided O
for O
comparing O
STS B-MetricName
systems O
. O

The O
results O
are O
depicted O
in O
Table O
3 O
. O

Unsupervised O
methods O
like O
tf O
- O
idf O
, O
average O
GloVe O
embeddings O
or O
InferSent O
perform O
rather O
badly O
on O
this O
dataset O
with O
low O
scores O
. O

Training O
SBERT B-MethodName
in O
the O
10 O
- O
fold O
cross O
- O
validation O
setup O
gives O
a O
performance O
that O
is O
nearly O
on O
- O
par O
with O
BERT B-MethodName
. O

However O
, O
in O
the O
cross O
- O
topic O
evaluation O
, O
we O
observe O
a O
performance O
drop O
of O
SBERT B-MethodName
by O
about O
7 O
points O
Spearman O
correlation O
. O

To O
be O
considered O
similar O
, O
arguments O
should O
address O
the O
same O
claims O
and O
provide O
the O
same O
reasoning O
. O

BERT B-MethodName
is O
able O
to O
use O
attention O
to O
compare O
directly O
both O
sentences O
( O
e.g. O
word O
- O
by O
- O
word O
comparison O
) O
, O
while O
SBERT B-MethodName
must O
map O
individual O
sentences O
from O
an O
unseen O
topic O
to O
a O
vector O
space O
such O
that O
arguments O
with O
similar O
claims O
and O
reasons O
are O
close O
. O

This O
is O
a O
much O
more O
challenging O
task O
, O
which O
appears O
to O
require O
more O
than O
just O
two O
topics O
for O
training O
to O
work O
on O
- O
par O
with O
BERT B-MethodName
. O
4.4 O
Wikipedia O
Sections O
Distinction O
Dor O
et O

al O
. O

( O
2018 O
) O
use O
Wikipedia O
to O
create O
a O
thematically O
ﬁne O
- O
grained O
train O
, O
dev O
and O
test O
set O
for O
sentence O
embeddings O
methods O
. O

Wikipedia O
articles O
are O
separated O
into O
distinct O
sections O
focusing O
on O
certain O
aspects O
. O

Dor O
et O

al O
. O
assume O
that O
sen-3987Model O
r O
Unsupervised O
methods O
tf O
- O
idf O
46.77 O
42.95 O
Avg O
. O
GloVe O
embeddings O
32.40 O
34.00 O
InferSent O
- O
GloVe O
27.08 O
26.63 O
10 O
- O
fold O
Cross O
- O
Validation O
SVR O
( O
Misra O
et O
al O
. O
, O
2016 O
) O
63.33 O
BERT B-MethodName
- O
AFS O
- O
base O
77.20 O
74.84 O
SBERT B-MethodName
- O
AFS O
- O
base O
76.57 O
74.13 O
BERT B-MethodName
- O
AFS O
- O
large O
78.68 O
76.38 O
SBERT B-MethodName
- O
AFS O
- O
large O
77.85 O
75.93 O
Cross O
- O
Topic O
Evaluation O
BERT B-MethodName
- O
AFS O
- O
base O
58.49 O
57.23 O
SBERT B-MethodName
- O
AFS O
- O
base O
52.34 O
50.65 O
BERT B-MethodName
- O
AFS O
- O
large O
62.02 O
60.34 O
SBERT B-MethodName
- O
AFS O
- O
large O
53.82 O
53.10 O
Table O
3 O
: O
Average O
Pearson O
correlation O
rand O
average O
Spearman O
’s O
rank O
correlation O
on O
the O
Argument O
Facet O
Similarity O
( O
AFS O
) O
corpus O
( O
Misra O
et O
al O
. O
, O
2016 O
) O
. O

Misra O
et O
al O
. O
proposes O
10 O
- O
fold O
cross O
- O
validation O
. O

We O
additionally O
evaluate O
in O
a O
cross O
- O
topic O
scenario O
: O
Methods O
are O
trained O
on O
two O
topics O
, O
and O
are O
evaluated O
on O
the O
third O
topic O
. O

tences O
in O
the O
same O
section O
are O
thematically O
closer O
than O
sentences O
in O
different O
sections O
. O

They O
use O
this O
to O
create O
a O
large O
dataset O
of O
weakly O
labeled O
sentence O
triplets O
: O
The O
anchor O
and O
the O
positive O
example O
come O
from O
the O
same O
section O
, O
while O
the O
negative O
example O
comes O
from O
a O
different O
section O
of O
the O
same O
article O
. O

For O
example O
, O
from O
the O
Alice O
Arnold O
article O
: O
Anchor O
: O
Arnold O
joined O
the O
BBC O
Radio O
Drama O
Company O
in O
1988 O
. O

, O
positive O
: O
Arnold O
gained O
media O
attention O
in O
May O
2012 O
. O

, O
negative O
: O
Balding O
and O
Arnold O
are O
keen O
amateur O
golfers O
. O

We O
use O
the O
dataset O
from O
Dor O
et O

al O
. O

We O
use O
the O
Triplet O
Objective O
, O
train O
SBERT B-MethodName
for O
one O
epoch O
on O
the O
about O
1.8 O
Million O
training O
triplets O
and O
evaluate O
it O
on O
the O
222,957 O
test O
triplets O
. O

Test O
triplets O
are O
from O
a O
distinct O
set O
of O
Wikipedia O
articles O
. O

As O
evaluation O
metric O
, O
we O
use O
accuracy O
: O
Is O
the O
positive O
example O
closer O
to O
the O
anchor O
than O
the O
negative O
example O
? O

Results O
are O
presented O
in O
Table O
4 O
. O

Dor O
et O

al O
. O
ﬁnetuned O
a O
BiLSTM O
architecture O
with O
triplet O
loss O
to O
derive O
sentence O
embeddings O
for O
this O
dataset O
. O

As O
the O
table O
shows O
, O
SBERT B-MethodName
clearly O
outperforms O
the O
BiLSTM O
approach O
by O
Dor O
et O

al O
. O
5 O
Evaluation O
- O
SentEval O
SentEval O
( O
Conneau O
and O
Kiela O
, O
2018 O
) O
is O
a O
popular O
toolkit O
to O
evaluate O
the O
quality O
of O
sentence O
embeddings O
. O

Sentence O
embeddings O
are O
used O
as O
features O
for O
a O
logistic O
regression O
classiﬁer O
. O

The O
logistic O
regression O
classiﬁer O
is O
trained O
on O
various O
tasks O
in O
a O
10 O
- O
fold O
cross O
- O
validation O
setup O
and O
the O
prediction O
accuracy O
is O
computed O
for O
the O
test O
- O
fold O
. O

Model O
Accuracy O
mean O
- O
vectors O
0.65 O
skip O
- O
thoughts O
- O
CS O
0.62 O
Dor O
et O

al O
. O
0.74 O
SBERT B-MethodName
- O
WikiSec O
- O
base O
0.8042 O
SBERT B-MethodName
- O
WikiSec O
- O
large O
0.8078 O
SRoBERTa B-MethodName
- O
WikiSec O
- O
base O
0.7945 O
SRoBERTa B-MethodName
- O
WikiSec O
- O
large O
0.7973 O
Table O
4 O
: O
Evaluation O
on O
the O
Wikipedia O
section O
triplets O
dataset O
( O
Dor O
et O
al O
. O
, O
2018 O
) O
. O

SBERT B-MethodName
trained O
with O
triplet O
loss O
for O
one O
epoch O
. O

The O
purpose O
of O
SBERT B-MethodName
sentence O
embeddings O
are O
not O
to O
be O
used O
for O
transfer O
learning O
for O
other O
tasks O
. O

Here O
, O
we O
think O
ﬁne O
- O
tuning O
BERT B-MethodName
as O
described O
by O
Devlin O
et O

al O
. O

( O
2018 O
) O
for O
new O
tasks O
is O
the O
more O
suitable O
method O
, O
as O
it O
updates O
all O
layers O
of O
the O
BERT B-MethodName
network O
. O

However O
, O
SentEval O
can O
still O
give O
an O
impression O
on O
the O
quality O
of O
our O
sentence O
embeddings O
for O
various O
tasks O
. O

We O
compare O
the O
SBERT B-MethodName
sentence O
embeddings O
to O
other O
sentence O
embeddings O
methods O
on O
the O
following O
seven O
SentEval O
transfer O
tasks O
: O
•MR O
: O
Sentiment O
prediction O
for O
movie O
reviews O
snippets O
on O
a O
ﬁve O
start O
scale O
( O
Pang O
and O
Lee O
, O
2005 O
) O
. O

•CR O
: O
Sentiment O
prediction O
of O
customer O
product O
reviews O
( O
Hu O
and O
Liu O
, O
2004 O
) O
. O

•SUBJ O
: O
Subjectivity O
prediction O
of O
sentences O
from O
movie O
reviews O
and O
plot O
summaries O
( O
Pang O
and O
Lee O
, O
2004 O
) O
. O

•MPQA O
: O
Phrase O
level O
opinion O
polarity O
classiﬁcation O
from O
newswire O
( O
Wiebe O
et O
al O
. O
, O
2005 O
) O
. O

•SST O
: O

Stanford O
Sentiment O
Treebank O
with O
binary O
labels O
( O
Socher O
et O
al O
. O
, O
2013 O
) O
. O

•TREC O
: O
Fine O
grained O
question O
- O
type O
classiﬁcation O
from O
TREC O
( O
Li O
and O
Roth O
, O
2002 O
) O
. O

•MRPC O
: O
Microsoft O
Research O
Paraphrase O
Corpus O
from O
parallel O
news O
sources O
( O
Dolan O
et O
al O
. O
, O
2004 O
) O
. O

The O
results O
can O
be O
found O
in O
Table O
5 O
. O

SBERT B-MethodName
is O
able O
to O
achieve O
the O
best O
performance O
in O
5 O
out O
of O
7 O
tasks O
. O

The O
average O
performance O
increases O
by O
about O
2 O
percentage O
points O
compared O
to O
InferSent O
as O
well O
as O
the O
Universal O
Sentence O
Encoder O
. O

Even O
though O
transfer O
learning O
is O
not O
the O
purpose O
of O
SBERT B-MethodName
, O
it O
outperforms O
other O
state O
- O
of O
- O
the O
- O
art O
sentence O
embeddings O
methods O
on O
this O
task.3988Model O
MR O
CR O
SUBJ O
MPQA O
SST O
TREC O
MRPC O
Avg O
. O
Avg O
. O
GloVe O
embeddings O
77.25 O
78.30 O
91.17 O
87.85 O
80.18 O
83.0 O
72.87 O
81.52 O
Avg O
. O
fast O
- O
text O
embeddings O
77.96 O
79.23 O
91.68 O
87.81 O
82.15 O
83.6 O
74.49 O
82.42 O

Avg O
. O
BERT B-MethodName
embeddings O
78.66 O
86.25 O
94.37 O
88.66 O
84.40 O
92.8 O
69.45 O
84.94 O
BERT B-MethodName
CLS O
- O
vector O
78.68 O
84.85 O
94.21 O
88.23 O
84.13 O
91.4 O
71.13 O
84.66 O
InferSent O
- O
GloVe O
81.57 O
86.54 O
92.50 O
90.38 O
84.18 O
88.2 O
75.77 O
85.59 O
Universal O
Sentence O
Encoder O
80.09 O
85.19 O
93.98 O
86.70 O
86.38 O
93.2 O
70.14 O
85.10 O
SBERT B-MethodName
- O
NLI B-DatasetName
- O
base O
83.64 O
89.43 O
94.39 O
89.86 O
88.96 O
89.6 O
76.00 O
87.41 O
SBERT B-MethodName
- O
NLI B-DatasetName
- O
large O
84.88 O
90.07 O
94.52 O
90.33 O
90.66 O
87.4 O
75.94 O
87.69 O
Table O
5 O
: O
Evaluation O
of O
SBERT B-MethodName
sentence O
embeddings O
using O
the O
SentEval O
toolkit O
. O

SentEval O
evaluates O
sentence O
embeddings O
on O
different O
sentence O
classiﬁcation O
tasks O
by O
training O
a O
logistic O
regression O
classiﬁer O
using O
the O
sentence O
embeddings O
as O
features O
. O

Scores O
are O
based O
on O
a O
10 O
- O
fold O
cross O
- O
validation O
. O

It O
appears O
that O
the O
sentence O
embeddings O
from O
SBERT B-MethodName
capture O
well O
sentiment O
information O
: O
We O
observe O
large O
improvements O
for O
all O
sentiment O
tasks O
( O
MR O
, O
CR O
, O
and O
SST O
) O
from O
SentEval O
in O
comparison O
to O
InferSent O
and O
Universal O
Sentence O
Encoder O
. O

The O
only O
dataset O
where O
SBERT B-MethodName
is O
signiﬁcantly O
worse O
than O
Universal O
Sentence O
Encoder O
is O
the O
TREC O
dataset O
. O

Universal O
Sentence O
Encoder O
was O
pre O
- O
trained O
on O
question O
- O
answering O
data O
, O
which O
appears O
to O
be O
beneﬁcial O
for O
the O
question O
- O
type O
classiﬁcation O
task O
of O
the O
TREC O
dataset O
. O

Average O
BERT B-MethodName
embeddings O
or O
using O
the O
CLStoken O
output O
from O
a O
BERT B-MethodName
network O
achieved O
bad O
results O
for O
various O
STS B-MetricName
tasks O
( O
Table O
1 O
) O
, O
worse O
than O
average O
GloVe O
embeddings O
. O

However O
, O
for O
SentEval O
, O
average O
BERT B-MethodName
embeddings O
and O
the O
BERT B-MethodName
CLS O
- O
token O
output O
achieves O
decent O
results O
( O
Table O
5 O
) O
, O
outperforming O
average O
GloVe O
embeddings O
. O

The O
reason O
for O
this O
are O
the O
different O
setups O
. O

For O
the O
STS B-MetricName
tasks O
, O
we O
used O
cosine O
- O
similarity O
to O
estimate O
the O
similarities O
between O
sentence O
embeddings O
. O

Cosine O
- O
similarity O
treats O
all O
dimensions O
equally O
. O

In O
contrast O
, O
SentEval O
ﬁts O
a O
logistic O
regression O
classiﬁer O
to O
the O
sentence O
embeddings O
. O

This O
allows O
that O
certain O
dimensions O
can O
have O
higher O
or O
lower O
impact O
on O
the O
classiﬁcation O
result O
. O

We O
conclude O
that O
average O
BERT B-MethodName
embeddings O
/ O
CLS O
- O
token O
output O
from O
BERT B-MethodName
return O
sentence O
embeddings O
that O
are O
infeasible O
to O
be O
used O
with O
cosinesimilarity O
or O
with O
Manhatten O
/ O
Euclidean O
distance O
. O

For O
transfer O
learning O
, O
they O
yield O
slightly O
worse O
results O
than O
InferSent O
or O
Universal O
Sentence O
Encoder O
. O

However O
, O
using O
the O
described O
ﬁne O
- O
tuning O
setup O
with O
a O
siamese O
network O
structure O
on O
NLI B-DatasetName
datasets O
yields O
sentence O
embeddings O
that O
achieve O
a O
new O
state O
- O
of O
- O
the O
- O
art O
for O
the O
SentEval O
toolkit O
. O

6 O
Ablation O
Study O
We O
have O
demonstrated O
strong O
empirical O
results O
for O
the O
quality O
of O
SBERT B-MethodName
sentence O
embeddings O
. O

Inthis O
section O
, O
we O
perform O
an O
ablation O
study O
of O
different O
aspects O
of O
SBERT B-MethodName
in O
order O
to O
get O
a O
better O
understanding O
of O
their O
relative O
importance O
. O

We O
evaluated O
different O
pooling O
strategies O
( O
MEAN O
, O
MAX O
, O
and O
CLS O
) O
. O

For O
the O
classiﬁcation O
objective O
function O
, O
we O
evaluate O
different O
concatenation O
methods O
. O

For O
each O
possible O
conﬁguration O
, O
we O
train O
SBERT B-MethodName
with O
10 O
different O
random O
seeds O
and O
average O
the O
performances O
. O

The O
objective O
function O
( O
classiﬁcation O
vs. O
regression O
) O
depends O
on O
the O
annotated O
dataset O
. O

For O
the O
classiﬁcation O
objective O
function O
, O
we O
train O
SBERT B-MethodName
base O
on O
the O
SNLI B-DatasetName
and O
the O
Multi O
- O
NLI B-DatasetName
dataset O
. O

For O
the O
regression O
objective O
function O
, O
we O
train O
on O
the O
training O
set O
of O
the O
STS B-MetricName
benchmark O
dataset O
. O

Performances O
are O
measured O
on O
the O
development O
split O
of O
the O
STS B-MetricName
benchmark O
dataset O
. O

Results O
are O
shown O
in O
Table O
6 O
. O

NLI B-DatasetName
STSb B-DatasetName
Pooling O
Strategy O
MEAN O
80.78 O
87.44 O
MAX O
79.07 O
69.92 O
CLS O
79.80 O
86.62 O
Concatenation O
( O
u;v O
) O
66.04 O
( O
ju vj O
) O
69.78 O
( O
uv O
) O
70.54 O
( O
ju vj;uv O
) O
78.37 O
( O
u;v;uv O
) O
77.44 O
( O
u;v;ju vj O
) O
80.78 O
( O
u;v;ju vj;uv)80.44 O
Table O
6 O
: O
SBERT B-MethodName
trained O
on O
NLI B-DatasetName
data O
with O
the O
classiﬁcation O
objective O
function O
, O
on O
the O
STS B-MetricName
benchmark O
( O
STSb B-DatasetName
) O
with O
the O
regression O
objective O
function O
. O

Conﬁgurations O
are O
evaluated O
on O
the O
development O
set O
of O
the O
STSb B-DatasetName
using O
cosine O
- O
similarity O
and O

Spearman O
’s O
rank O
correlation O
. O

For O
the O
concatenation O
methods O
, O
we O
only O
report O
scores O
with O
MEAN O
pooling O
strategy O
. O

When O
trained O
with O
the O
classiﬁcation O
objective O
function O
on O
NLI B-DatasetName
data O
, O
the O
pooling O
strategy O
has O
a O
rather O
minor O
impact O
. O

The O
impact O
of O
the O
concatenation O
mode O
is O
much O
larger O
. O

InferSent O
( O
Conneau3989et O
al O
. O
, O
2017 O
) O
and O
Universal O
Sentence O
Encoder O
( O
Cer O
et O
al O
. O
, O
2018 O
) O

both O
use O
( O
u;v;ju vj;uv)as O
input O
for O
a O
softmax O
classiﬁer O
. O

However O
, O
in O
our O
architecture O
, O
adding O
the O
element O
- O
wise O
uvdecreased O
the O
performance O
. O

The O
most O
important O
component O
is O
the O
elementwise O
differenceju vj O
. O

Note O
, O
that O
the O
concatenation O
mode O
is O
only O
relevant O
for O
training O
the O
softmax O
classiﬁer O
. O

At O
inference O
, O
when O
predicting O
similarities O
for O
the O
STS B-MetricName
benchmark O
dataset O
, O
only O
the O
sentence O
embeddings O
uandvare O
used O
in O
combination O
with O
cosine O
- O
similarity O
. O

The O
element O
- O
wise O
difference O
measures O
the O
distance O
between O
the O
dimensions O
of O
the O
two O
sentence O
embeddings O
, O
ensuring O
that O
similar O
pairs O
are O
closer O
and O
dissimilar O
pairs O
are O
further O
apart O
. O

When O
trained O
with O
the O
regression O
objective O
function O
, O
we O
observe O
that O
the O
pooling O
strategy O
has O
a O
large O
impact O
. O

There O
, O
the O
MAX O
strategy O
perform O
signiﬁcantly O
worse O
than O
MEAN O
orCLS O
- O
token O
strategy O
. O

This O
is O
in O
contrast O
to O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
, O
who O
found O
it O
beneﬁcial O
for O
the O
BiLSTM O
- O
layer O
of O
InferSent O
to O
use O
MAX O
instead O
of O
MEAN O
pooling O
. O

7 O
Computational O
Efﬁciency O
Sentence O
embeddings O
need O
potentially O
be O
computed O
for O
Millions O
of O
sentences O
, O
hence O
, O
a O
high O
computation O
speed O
is O
desired O
. O

In O
this O
section O
, O
we O
compare O
SBERT B-MethodName
to O
average O
GloVe O
embeddings O
, O
InferSent O
( O
Conneau O
et O
al O
. O
, O
2017 O
) O
, O
and O
Universal O
Sentence O
Encoder O
( O
Cer O
et O
al O
. O
, O
2018 O
) O
. O

For O
our O
comparison O
we O
use O
the O
sentences O
from O
the O
STS B-MetricName
benchmark O
( O
Cer O
et O
al O
. O
, O
2017 O
) O
. O

We O
compute O
average O
GloVe O
embeddings O
using O
a O
simple O
for O
- O
loop O
with O
python O
dictionary O
lookups O
and O
NumPy O
. O

InferSent4is O
based O
on O
PyTorch O
. O

For O
Universal O
Sentence O
Encoder O
, O
we O
use O
the O
TensorFlow O
Hub O
version5 O
, O
which O
is O
based O
on O
TensorFlow O
. O

SBERT B-MethodName
is O
based O
on O
PyTorch O
. O

For O
improved O
computation O
of O
sentence O
embeddings O
, O
we O
implemented O
a O
smart O
batching O
strategy O
: O
Sentences O
with O
similar O
lengths O
are O
grouped O
together O
and O
are O
only O
padded O
to O
the O
longest O
element O
in O
a O
mini O
- O
batch O
. O

This O
drastically O
reduces O
computational O
overhead O
from O
padding O
tokens O
. O

Performances O
were O
measured O
on O
a O
server O
with O
Intel O
i7 O
- O
5820 O
K O
CPU O
@ O
3.30GHz O
, O
Nvidia O
Tesla O
4https://github.com/facebookresearch/ O
InferSent O
5https://tfhub.dev/google/ O
universal O
- O
sentence O
- O
encoder O
- O
large/3V100 O
GPU O
, O
CUDA O
9.2 O
and O
cuDNN O
. O

The O
results O
are O
depicted O
in O
Table O
7 O
. O
Model O
CPU O
GPU O
Avg O
. O
GloVe O
embeddings O
6469 O
InferSent O
137 O
1876 O
Universal O
Sentence O
Encoder O
67 O
1318 O
SBERT B-MethodName
- O
base O
44 O
1378 O
SBERT B-MethodName
- O
base O
- O
smart O
batching O
83 O
2042 O
Table O
7 O
: O
Computation O
speed O
( O
sentences O
per O
second O
) O
of O
sentence O
embedding O
methods O
. O

Higher O
is O
better O
. O

On O
CPU O
, O
InferSent O
is O
about O
65 O
% O
faster O
than O
SBERT B-MethodName
. O

This O
is O
due O
to O
the O
much O
simpler O
network O
architecture O
. O

InferSent O
uses O
a O
single O
BiLSTM O
layer O
, O
while O
BERT B-MethodName
uses O
12 O
stacked O
transformer O
layers O
. O

However O
, O
an O
advantage O
of O
transformer O
networks O
is O
the O
computational O
efﬁciency O
on O
GPUs O
. O

There O
, O
SBERT B-MethodName
with O
smart O
batching O
is O
about O
9 O
% O
faster O
than O
InferSent O
and O
about O
55 O
% O
faster O
than O
Universal O
Sentence O
Encoder O
. O

Smart O
batching O
achieves O
a O
speed O
- O
up O
of O
89 O
% O
on O
CPU O
and O
48 O
% O
on O
GPU O
. O

Average O
GloVe O
embeddings O
is O
obviously O
by O
a O
large O
margin O
the O
fastest O
method O
to O
compute O
sentence O
embeddings O
. O

8 O
Conclusion O
We O
showed O
that O
BERT B-MethodName
out O
- O
of O
- O
the O
- O
box O
maps O
sentences O
to O
a O
vector O
space O
that O
is O
rather O
unsuitable O
to O
be O
used O
with O
common O
similarity O
measures O
like O
cosine O
- O
similarity O
. O

The O
performance O
for O
seven O
STS B-MetricName
tasks O
was O
below O
the O
performance O
of O
average O
GloVe O
embeddings O
. O

To O
overcome O
this O
shortcoming O
, O
we O
presented O
Sentence B-MethodName
- I-MethodName
BERT I-MethodName
( O
SBERT B-MethodName
) O
. O

SBERT B-MethodName
ﬁne O
- O
tunes O
BERT B-MethodName
in O
a O
siamese O
/ O
triplet O
network O
architecture O
. O

We O
evaluated O
the O
quality O
on O
various O
common O
benchmarks O
, O
where O
it O
could O
achieve O
a O
signiﬁcant O
improvement O
over O
state O
- O
of O
- O
the O
- O
art O
sentence O
embeddings O
methods O
. O

Replacing O
BERT B-MethodName
with O
RoBERTa B-MethodName
did O
not O
yield O
a O
signiﬁcant O
improvement O
in O
our O
experiments O
. O

SBERT B-MethodName
is O
computationally O
efﬁcient O
. O

On O
a O
GPU O
, O
it O
is O
about O
9 O
% O
faster O
than O
InferSent O
and O
about O
55 O
% O
faster O
than O
Universal O
Sentence O
Encoder O
. O

SBERT B-MethodName
can O
be O
used O
for O
tasks O
which O
are O
computationally O
not O
feasible O
to O
be O
modeled O
with O
BERT B-MethodName
. O

For O
example O
, O
clustering O
of O
10,000 O
sentences O
with O
hierarchical O
clustering O
requires O
with O
BERT B-MethodName
about O
65 O
hours O
, O
as O
around O
50 O
Million O
sentence O
combinations O
must O
be O
computed O
. O

With O
SBERT B-MethodName
, O
we O
were O
able O
to O
reduce O
the O
effort O
to O
about O
5 O
seconds.3990Acknowledgments O
This O
work O
has O
been O
supported O
by O
the O
German O
Research O
Foundation O
through O
the O
German O
- O
Israeli O
Project O
Cooperation O
( O
DIP O
, O
grant O
DA O
1600/1 O
- O
1 O
and O
grant O
GU O
798/17 O
- O
1 O
) O
. O

It O
has O
been O
co O
- O
funded O
by O
the O
German O
Federal O
Ministry O
of O
Education O
and O
Research O
( O
BMBF O
) O
under O
the O
promotional O
references O
03VP02540 O
( O
ArgumenText O
) O
. O

References O
Eneko O
Agirre O
, O
Carmen O
Banea O
, O
Claire O
Cardie O
, O
Daniel O
Cer O
, O
Mona O
Diab O
, O
Aitor O
Gonzalez O
- O
Agirre O
, O
Weiwei O
Guo O
, O
Inigo O
Lopez O
- O
Gazpio O
, O
Montse O
Maritxalar O
, O
Rada O
Mihalcea O
, O
German O
Rigau O
, O
Larraitz O
Uria O
, O
and O
Janyce O
Wiebe O
. O

2015 O
. O

SemEval-2015 O
Task O
2 O
: O
Semantic B-MetricName
Textual I-MetricName
Similarity I-MetricName
, O
English O
, O
Spanish O
and O
Pilot O
on O
Interpretability O
. O

In O
Proceedings O
of O
the O
9th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval O
2015 O
) O
, O
pages O
252–263 O
, O
Denver O
, O
Colorado O
. O

Association O
for O
Computational O
Linguistics O
. O

Eneko O
Agirre O
, O
Carmen O
Banea O
, O
Claire O
Cardie O
, O
Daniel O
Cer O
, O
Mona O
Diab O
, O
Aitor O
Gonzalez O
- O
Agirre O
, O
Weiwei O
Guo O
, O
Rada O
Mihalcea O
, O
German O
Rigau O
, O
and O
Janyce O
Wiebe O
. O

2014 O
. O

SemEval-2014 O
Task O
10 O
: O
Multilingual O
Semantic B-MetricName
Textual I-MetricName
Similarity I-MetricName
. O

In O
Proceedings O
of O
the O
8th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval O
2014 O
) O
, O
pages O
81–91 O
, O
Dublin O
, O
Ireland O
. O

Association O
for O
Computational O
Linguistics O
. O

Eneko O
Agirre O
, O
Carmen O
Banea O
, O
Daniel O
M. O
Cer O
, O
Mona O
T. O
Diab O
, O
Aitor O
Gonzalez O
- O
Agirre O
, O
Rada O
Mihalcea O
, O
German O
Rigau O
, O
and O
Janyce O
Wiebe O
. O

2016 O
. O

SemEval2016 O
Task O
1 O
: O
Semantic B-MetricName
Textual I-MetricName
Similarity I-MetricName
, O
Monolingual O
and O
Cross O
- O
Lingual O
Evaluation O
. O

In O
Proceedings O
of O
the O
10th O
International O
Workshop O
on O
Semantic O
Evaluation O
, O
SemEval@NAACL O
- O
HLT O
2016 O
, O
San O
Diego O
, O
CA O
, O
USA O
, O
June O
16 O
- O
17 O
, O
2016 O
, O
pages O
497–511 O
. O
Eneko O
Agirre O
, O
Daniel O
Cer O
, O
Mona O
Diab O
, O
Aitor O
GonzalezAgirre O
, O
and O
Weiwei O
Guo O
. O

2013 O
. O

* O
SEM O
2013 O
shared O
task O
: O

Semantic B-MetricName
Textual I-MetricName
Similarity I-MetricName
. O

In O
Second O
Joint O
Conference O
on O
Lexical O
and O
Computational O
Semantics O
( O
* O
SEM O
) O
, O
Volume O
1 O
: O
Proceedings O
of O
the O
Main O
Conference O
and O
the O
Shared O
Task O
: O

Semantic B-MetricName
Textual I-MetricName
Similarity I-MetricName
, O
pages O
32–43 O
, O
Atlanta O
, O
Georgia O
, O
USA O
. O

Association O
for O
Computational O
Linguistics O
. O

Eneko O
Agirre O
, O
Mona O
Diab O
, O
Daniel O
Cer O
, O
and O
Aitor O
Gonzalez O
- O
Agirre O
. O

2012 O
. O

SemEval-2012 O
Task O
6 O
: O
A O
Pilot O
on O
Semantic B-MetricName
Textual I-MetricName
Similarity I-MetricName
. O

In O
Proceedings O
of O
the O
First O
Joint O
Conference O
on O
Lexical O
and O
Computational O
Semantics O
- O
Volume O
1 O
: O
Proceedings O
of O
the O
Main O
Conference O
and O
the O
Shared O
Task O
, O
and O
Volume O
2 O
: O
Proceedings O
of O
the O
Sixth O
International O
Workshop O
on O
Semantic O
Evaluation O
, O
SemEval O
’ O
12 O
, O
pages O
385–393 O
, O
Stroudsburg O
, O
PA O
, O
USA O
. O

Association O
for O
Computational O
Linguistics O
. O

Samuel O
R. O
Bowman O
, O
Gabor O
Angeli O
, O
Christopher O
Potts O
, O
and O
Christopher O
D. O
Manning O
. O
2015 O
. O

A O
large O
annotated O
corpus O
for O
learning O
natural O
language O
inference O
. O

InProceedings O
of O
the O
2015 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
632–642 O
, O
Lisbon O
, O
Portugal O
. O

Association O
for O
Computational O
Linguistics O
. O

Daniel O
Cer O
, O
Mona O
Diab O
, O
Eneko O
Agirre O
, O
Iigo O
LopezGazpio O
, O
and O
Lucia O
Specia O
. O

2017 O
. O

SemEval-2017 O
Task O
1 O
: O
Semantic B-MetricName
Textual I-MetricName
Similarity I-MetricName
Multilingual O
and O
Crosslingual O
Focused O
Evaluation O
. O

In O
Proceedings O
of O
the O
11th O
International O
Workshop O
on O
Semantic O
Evaluation O
( O
SemEval-2017 O
) O
, O
pages O
1–14 O
, O
Vancouver O
, O
Canada O
. O
Daniel O
Cer O
, O
Yinfei O
Yang O
, O
Sheng O
- O
yi O
Kong O
, O
Nan O
Hua O
, O
Nicole O
Limtiaco O
, O
Rhomni O
St. O
John O
, O
Noah O
Constant O
, O
Mario O
Guajardo O
- O
Cespedes O
, O
Steve O
Yuan O
, O
Chris O
Tar O
, O
Yun O
- O
Hsuan O
Sung O
, O
Brian O
Strope O
, O
and O
Ray O
Kurzweil O
. O
2018 O
. O

Universal O
Sentence O
Encoder O
. O

arXiv O
preprint O
arXiv:1803.11175 O
. O

Alexis O
Conneau O
and O
Douwe O
Kiela O
. O

2018 O
. O

SentEval O
: O
An O
Evaluation O
Toolkit O
for O
Universal O
Sentence O
Representations O
. O

arXiv O
preprint O
arXiv:1803.05449 O
. O

Alexis O
Conneau O
, O
Douwe O
Kiela O
, O
Holger O
Schwenk O
, O
Lo O
¨ıc O
Barrault O
, O
and O
Antoine O
Bordes O
. O

2017 O
. O

Supervised O
Learning O
of O
Universal O
Sentence O
Representations O
from O
Natural O
Language O
Inference O
Data O
. O

In O
Proceedings O
of O
the O
2017 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
670–680 O
, O
Copenhagen O
, O
Denmark O
. O

Association O
for O
Computational O
Linguistics O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O

2018 O
. O

BERT B-MethodName
: O
Pre O
- O
training O
of O
Deep O
Bidirectional O
Transformers O
for O
Language O
Understanding O
. O

arXiv O
preprint O
arXiv:1810.04805 O
. O

Bill O
Dolan O
, O
Chris O
Quirk O
, O
and O
Chris O
Brockett O
. O

2004 O
. O

Unsupervised O
Construction O
of O
Large O
Paraphrase O
Corpora O
: O
Exploiting O
Massively O
Parallel O
News O
Sources O
. O

InProceedings O
of O
the O
20th O
International O
Conference O
on O
Computational O
Linguistics O
, O
COLING O
’ O
04 O
, O
Stroudsburg O
, O
PA O
, O
USA O
. O

Association O
for O
Computational O
Linguistics O
. O

Liat O
Ein O
Dor O
, O
Yosi O
Mass O
, O
Alon O
Halfon O
, O
Elad O
Venezian O
, O
Ilya O
Shnayderman O
, O
Ranit O
Aharonov O
, O
and O
Noam O
Slonim O
. O

2018 O
. O

Learning O
Thematic O
Similarity O
Metric O
from O
Article O
Sections O
Using O
Triplet O
Networks O
. O

In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
2 O
: O
Short O
Papers O
) O
, O
pages O
49–54 O
, O
Melbourne O
, O
Australia O
. O
Association O
for O
Computational O
Linguistics O
. O
Felix O
Hill O
, O
Kyunghyun O
Cho O
, O
and O
Anna O
Korhonen O
. O
2016 O
. O

Learning O
Distributed O
Representations O
of O
Sentences O
from O
Unlabelled O
Data O
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
1367 O
– O
1377 O
, O
San O
Diego O
, O
California O
. O

Association O
for O
Computational O
Linguistics.3991Minqing O
Hu O
and O
Bing O
Liu O
. O

2004 O
. O

Mining O
and O
Summarizing O
Customer O
Reviews O
. O

In O
Proceedings O
of O
the O
Tenth O
ACM O
SIGKDD O
International O
Conference O
on O
Knowledge O
Discovery O
and O
Data O
Mining O
, O
KDD O
’ O
04 O
, O
pages O
168–177 O
, O
New O
York O
, O
NY O
, O
USA O
. O
ACM O
. O

Samuel O
Humeau O
, O
Kurt O
Shuster O
, O
Marie O
- O
Anne O
Lachaux O
, O
and O
Jason O
Weston O
. O

2019 O
. O

Real O
- O
time O
Inference O
in O
Multi O
- O
sentence O
Tasks O
with O
Deep O
Pretrained O
Transformers O
. O

arXiv O
preprint O
arXiv:1905.01969 O
, O
abs/1905.01969 O
. O

Jeff O
Johnson O
, O
Matthijs O
Douze O
, O
and O
Herv O
´ O
e O
J´egou O
. O

2017 O
. O

Billion O
- O
scale O
similarity O
search O
with O
GPUs O
. O

arXiv O
preprint O
arXiv:1702.08734 O
. O

Ryan O
Kiros O
, O
Yukun O
Zhu O
, O
Ruslan O
R O
Salakhutdinov O
, O
Richard O
Zemel O
, O
Raquel O
Urtasun O
, O
Antonio O
Torralba O
, O
and O
Sanja O
Fidler O
. O

2015 O
. O

Skip O
- O
Thought O
Vectors O
. O

In O
C. O
Cortes O
, O
N. O
D. O
Lawrence O
, O
D. O
D. O
Lee O
, O
M. O
Sugiyama O
, O
and O
R. O
Garnett O
, O
editors O
, O
Advances O
in O
Neural O
Information O
Processing O
Systems O
28 O
, O
pages O
3294–3302 O
. O
Curran O
Associates O
, O
Inc. O
Xin O
Li O
and O
Dan O
Roth O
. O
2002 O
. O

Learning O
Question O
Classiﬁers O
. O

In O
Proceedings O
of O
the O
19th O
International O
Conference O
on O
Computational O
Linguistics O
- O
Volume O
1 O
, O
COLING O
’ O
02 O
, O
pages O
1–7 O
, O
Stroudsburg O
, O
PA O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O

Yinhan O
Liu O
, O
Myle O
Ott O
, O
Naman O
Goyal O
, O
Jingfei O
Du O
, O
Mandar O
Joshi O
, O
Danqi O
Chen O
, O
Omer O
Levy O
, O
Mike O
Lewis O
, O
Luke O
Zettlemoyer O
, O
and O
Veselin O
Stoyanov O
. O

2019 O
. O
RoBERTa B-MethodName
: O
A O
Robustly O
Optimized O
BERT B-MethodName
Pretraining O
Approach O
. O

arXiv O
preprint O
arXiv:1907.11692 O
. O

Marco O
Marelli O
, O
Stefano O
Menini O
, O
Marco O
Baroni O
, O
Luisa O
Bentivogli O
, O
Raffaella O
Bernardi O
, O
and O
Roberto O
Zamparelli O
. O

2014 O
. O

A O
SICK O
cure O
for O
the O
evaluation O
of O
compositional O
distributional O
semantic O
models O
. O

In O
Proceedings O
of O
the O
Ninth O
International O
Conference O
on O
Language O
Resources O
and O
Evaluation O
( O
LREC’14 O
) O
, O
pages O
216–223 O
, O
Reykjavik O
, O
Iceland O
. O

European O
Language O
Resources O
Association O
( O
ELRA O
) O
. O

Chandler O
May O
, O
Alex O
Wang O
, O
Shikha O
Bordia O
, O
Samuel O
R. O
Bowman O
, O
and O
Rachel O
Rudinger O
. O

2019 O
. O

On O
Measuring O
Social O
Biases O
in O
Sentence O
Encoders O
. O

arXiv O
preprint O
arXiv:1903.10561 O
. O

Amita O
Misra O
, O
Brian O
Ecker O
, O
and O
Marilyn O
A. O
Walker O
. O
2016 O
. O

Measuring O
the O
Similarity O
of O
Sentential O
Arguments O
in O
Dialogue O
. O

In O
Proceedings O
of O
the O
SIGDIAL O
2016 O
Conference O
, O
The O
17th O
Annual O
Meeting O
of O
the O
Special O
Interest O
Group O
on O
Discourse O
and O
Dialogue O
, O
13 O
- O
15 O
September O
2016 O
, O
Los O
Angeles O
, O
CA O
, O
USA O
, O
pages O
276–287 O
. O

Bo O
Pang O
and O
Lillian O
Lee O
. O
2004 O
. O

A O
Sentimental O
Education O
: O
Sentiment O
Analysis O
Using O
Subjectivity O
Summarization O
Based O
on O
Minimum O
Cuts O
. O

In O
Proceedings O
of O
the O
42nd O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
ACL’04 O
) O
, O
Main O
Volume O
, O
pages O
271–278 O
, O
Barcelona O
, O
Spain O
. O

Bo O
Pang O
and O
Lillian O
Lee O
. O
2005 O
. O

Seeing O
Stars O
: O
Exploiting O
Class O
Relationships O
for O
Sentiment O
Categorization O
with O
Respect O
to O
Rating O
Scales O
. O

In O
Proceedings O
of O
the O
43rd O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
ACL’05 O
) O
, O
pages O
115 O
– O
124 O
, O
Ann O
Arbor O
, O
Michigan O
. O

Association O
for O
Computational O
Linguistics O
. O

Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christopher O
D. O
Manning O
. O

2014 O
. O

GloVe O
: O
Global O
Vectors O
for O
Word O
Representation O
. O

In O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
1532 O
– O
1543 O
. O

Yifan O
Qiao O
, O
Chenyan O
Xiong O
, O
Zheng O
- O
Hao O
Liu O
, O
and O
Zhiyuan O
Liu O
. O

2019 O
. O

Understanding O
the O
Behaviors O
of O
BERT B-MethodName
in O
Ranking O
. O

arXiv O
preprint O
arXiv:1904.07531 O
. O

Nils O
Reimers O
, O
Philip O
Beyer O
, O
and O
Iryna O
Gurevych O
. O

2016 O
. O

Task O
- O
Oriented O
Intrinsic O
Evaluation O
of O
Semantic B-MetricName
Textual I-MetricName
Similarity I-MetricName
. O

In O
Proceedings O
of O
the O
26th O
International O
Conference O
on O
Computational O
Linguistics O
( O
COLING O
) O
, O
pages O
87–96 O
. O

Nils O
Reimers O
and O
Iryna O
Gurevych O
. O

2018 O
. O

Why O
Comparing O
Single O
Performance O
Scores O
Does O
Not O
Allow O
to O
Draw O
Conclusions O
About O
Machine O
Learning O
Approaches O
. O

arXiv O
preprint O
arXiv:1803.09578 O
, O
abs/1803.09578 O
. O

Nils O
Reimers O
, O
Benjamin O
Schiller O
, O
Tilman O
Beck O
, O
Johannes O
Daxenberger O
, O
Christian O
Stab O
, O
and O
Iryna O
Gurevych O
. O

2019 O
. O

Classiﬁcation O
and O
Clustering O
of O
Arguments O
with O
Contextualized O
Word O
Embeddings O
. O

InProceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
567 O
– O
578 O
, O
Florence O
, O
Italy O
. O

Association O
for O
Computational O
Linguistics O
. O

Florian O
Schroff O
, O
Dmitry O
Kalenichenko O
, O
and O
James O
Philbin O
. O
2015 O
. O

FaceNet O
: O

A O
Uniﬁed O
Embedding O
for O
Face O
Recognition O
and O
Clustering O
. O

arXiv O
preprint O
arXiv:1503.03832 O
, O
abs/1503.03832 O
. O

Richard O
Socher O
, O
Alex O
Perelygin O
, O
Jean O
Wu O
, O
Jason O
Chuang O
, O
Christopher O
D. O
Manning O
, O
Andrew O
Ng O
, O
and O
Christopher O
Potts O
. O

2013 O
. O

Recursive O
Deep O
Models O
for O
Semantic O
Compositionality O
Over O
a O
Sentiment O
Treebank O
. O

In O
Proceedings O
of O
the O
2013 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
1631–1642 O
, O
Seattle O
, O
Washington O
, O
USA O
. O
Association O
for O
Computational O
Linguistics O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
Łukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
All O
you O
Need O
. O

In O
I. O
Guyon O
, O
U. O
V O
. O

Luxburg O
, O
S. O
Bengio O
, O
H. O
Wallach O
, O
R. O
Fergus O
, O
S. O
Vishwanathan O
, O
and O
R. O
Garnett O
, O
editors O
, O
Advances O
in O
Neural O
Information O
Processing O
Systems O
30 O
, O
pages O
5998–6008 O
. O

Janyce O
Wiebe O
, O
Theresa O
Wilson O
, O
and O
Claire O
Cardie O
. O
2005 O
. O

Annotating O
Expressions O
of O
Opinions O
and O
Emotions O
in O
Language O
. O

Language O
Resources O
and O
Evaluation O
, O
39(2):165–210.3992Adina O
Williams O
, O
Nikita O
Nangia O
, O
and O
Samuel O
Bowman O
. O
2018 O
. O

A O
Broad O
- O
Coverage O
Challenge O
Corpus O
for O
Sentence O
Understanding O
through O
Inference O
. O

In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
Papers O
) O
, O
pages O
1112–1122 O
. O

Association O
for O
Computational O
Linguistics O
. O

Yinfei O
Yang O
, O
Steve O
Yuan O
, O
Daniel O
Cer O
, O
Sheng O
- O
Yi O
Kong O
, O
Noah O
Constant O
, O
Petr O
Pilar O
, O
Heming O
Ge O
, O
Yun O
- O
hsuan O
Sung O
, O
Brian O
Strope O
, O
and O
Ray O
Kurzweil O
. O

2018 O
. O

Learning O
Semantic B-MetricName
Textual I-MetricName
Similarity I-MetricName
from O
Conversations O
. O

In O
Proceedings O
of O
The O
Third O
Workshop O
on O
Representation O
Learning O
for O
NLP O
, O
pages O
164 O
– O
174 O
, O
Melbourne O
, O
Australia O
. O

Association O
for O
Computational O
Linguistics O
. O

Zhilin O
Yang O
, O
Zihang O
Dai O
, O
Yiming O
Yang O
, O
Jaime O
G. O
Carbonell O
, O
Ruslan O
Salakhutdinov O
, O
and O
Quoc O
V O
. O

Le O
. O
2019 O
. O

XLNet O
: O

Generalized O
Autoregressive O
Pretraining O
for O
Language O
Understanding O
. O

arXiv O
preprint O
arXiv:1906.08237 O
, O
abs/1906.08237 O
. O

Tianyi O
Zhang O
, O
Varsha O
Kishore O
, O
Felix O
Wu O
, O
Kilian O
Q. O
Weinberger O
, O
and O
Yoav O
Artzi O
. O

2019 O
. O

BERT B-MethodName
Score O
: O
Evaluating O
Text O
Generation O
with O
BERT B-MethodName
. O

arXiv O
preprint O
arXiv:1904.09675 O
. O


Proceedings O
of O
the O
60th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
5347 O
- O
5363 O
May O
22 O
- O
27 O
, O
2022 O
c O

 O
2022 O
Association O
for O
Computational O
Linguistics O
KinyaBERT B-MethodName
: O
a O
Morphology O
- O
aware O
Kinyarwanda O
Language O
Model O
Antoine O
Nzeyimana O
University O
of O
Massachusetts O
Amherst O
anthonzeyi@gmail.comAndre O
Niyongabo O
Rubungo O
Polytechnic O
University O
of O
Catalonia O
niyongabor.andre@gmail.com O
Abstract O
Pre O
- O
trained O
language O
models O
such O
as O
BERT B-MethodName
have O
been O
successful O
at O
tackling O
many O
natural O
language O
processing O
tasks O
. O

However O
, O
the O
unsupervised O
sub O
- O
word O
tokenization O
methods O
commonly O
used O
in O
these O
models O
( O
e.g. O
, O
byte O
- O
pair O
encoding O
– O
BPE O
) O
are O
sub O
- O
optimal O
at O
handling O
morphologically O
rich O
languages O
. O

Even O
given O
a O
morphological O
analyzer O
, O
naive O
sequencing O
of O
morphemes O
into O
a O
standard O
BERT B-MethodName
architecture O
is O
inefﬁcient O
at O
capturing O
morphological O
compositionality O
and O
expressing O
word O
- O
relative O
syntactic O
regularities O
. O

We O
address O
these O
challenges O
by O
proposing O
a O
simple O
yet O
effective O
twotier O
BERT B-MethodName
architecture O
that O
leverages O
a O
morphological O
analyzer O
and O
explicitly O
represents O
morphological O
compositionality O
. O

Despite O
the O
success O
of O
BERT B-MethodName
, O
most O
of O
its O
evaluations O
have O
been O
conducted O
on O
high O
- O
resource O
languages O
, O
obscuring O
its O
applicability O
on O
low O
- O
resource O
languages O
. O

We O
evaluate O
our O
proposed O
method O
on O
the O
low O
- O
resource O
morphologically O
rich O
Kinyarwanda O
language O
, O
naming O
the O
proposed O
model O
architecture O
KinyaBERT B-MethodName
. O

A O
robust O
set O
of O
experimental O
results O
reveal O
that O
KinyaBERT B-MethodName
outperforms O
solid O
baselines O
by O
2 O
% O
in O
F1 O
score O
on O
a O
named O
entity O
recognition O
task O
and O
by O
4.3 O
% O
in O
average O
score O
of O
a O
machine O
- O
translated O
GLUE O
benchmark O
. O

KinyaBERT B-MethodName
ﬁne O
- O
tuning O
has O
better O
convergence O
and O
achieves O
more O
robust O
results O
on O
multiple O
tasks O
even O
in O
the O
presence O
of O
translation O
noise.1 O
1 O
Introduction O
Recent O
advances O
in O
natural O
language O
processing O
( O
NLP O
) O
through O
deep O
learning O
have O
been O
largely O
enabled O
by O
vector O
representations O
( O
or O
embeddings O
) O
learned O
through O
language O
model O
pre O
- O
training O
( O
Bengio O
et O
al O
. O
, O
2003 O
; O
Mikolov O
et O
al O
. O
, O
2013 O
; O
Pennington O
et O
al O
. O
, O
2014 O
; O
Bojanowski O
et O

al O
. O
, O
2017 O
; O
Peters O
et O
al O
. O
, O
2018 O
; O
Devlin O
et O
al O
. O
, O
2019 O
) O
. O

Language O
models O
such O
as O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
are O
pre1Code O
and O
data O
are O
released O
at O
https://github O
. O
com O
/ O
anzeyimana O
/ O
kinyabert O
- O
acl2022trained O
on O
large O
text O
corpora O
and O
then O
ﬁne O
- O
tuned O
on O
downstream O
tasks O
, O
resulting O
in O
better O
performance O
on O
many O
NLP O
tasks O
. O

Despite O
attempts O
to O
make O
multilingual O
BERT B-MethodName
models O
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
, O
research O
has O
shown O
that O
models O
pre O
- O
trained O
on O
high O
quality O
monolingual O
corpora O
outperform O
multilingual O
models O
pre O
- O
trained O
on O
large O
Internet O
data O
( O
Scheible O
et O
al O
. O
, O
2020 O
; O
Virtanen O
et O

al O
. O
, O
2019 O
) O
. O

This O
has O
motivated O
many O
researchers O
to O
pretrain O
BERT B-MethodName
models O
on O
individual O
languages O
rather O
than O
adopting O
the O
“ O
language O
- O
agnostic O
” O
multilingual O
models O
. O

This O
work O
is O
partly O
motivated O
by O
the O
same O
ﬁndings O
, O
but O
also O
proposes O
an O
adaptation O
of O
the O
BERT B-MethodName
architecture O
to O
address O
representational O
challenges O
that O
are O
speciﬁc O
to O
morphologically O
rich O
languages O
such O
as O
Kinyarwanda O
. O

In O
order O
to O
handle O
rare O
words O
and O
reduce O
the O
vocabulary O
size O
, O
BERT B-MethodName
- O
like O
models O
use O
statistical O
sub O
- O
word O
tokenization O
algorithms O
such O
as O
byte O
pair O
encoding O
( O
BPE O
) O
( O
Sennrich O
et O
al O
. O
, O
2016 O
) O
. O

While O
these O
techniques O
have O
been O
widely O
used O
in O
language O
modeling O
and O
machine O
translation O
, O
they O
are O
not O
optimal O
for O
morphologically O
rich O
languages O
( O
Klein O
and O
Tsarfaty O
, O
2020 O
) O
. O

In O
fact O
, O
sub O
- O
word O
tokenization O
methods O
that O
are O
solely O
based O
on O
surface O
forms O
, O
including O
BPE O
and O
character O
- O
based O
models O
, O
can O
not O
capture O
all O
morphological O
details O
. O

This O
is O
due O
to O
morphological O
alternations O
( O
Muhirwe O
, O
2007 O
) O
and O
non O
- O
concatenative O
morphology O
( O
McCarthy O
, O
1981 O
) O
that O
are O
often O
exhibited O
by O
morphologically O
rich O
languages O
. O

For O
example O
, O
as O
shown O
in O
Table O
1 O
, O
a O
BPE O
model O
trained O
on O
390 O
million O
tokens O
of O
Kinyarwanda O
text O
can O
not O
extract O
the O
true O
sub O
- O
word O
lexical O
units O
( O
i.e. O
morphemes O
) O
for O
the O
given O
words O
. O

This O
work O
addresses O
the O
above O
problem O
by O
proposing O
a O
language O
model O
architecture O
that O
explicitly O
represents O
most O
of O
the O
input O
words O
with O
morphological O
parses O
produced O
by O
a O
morphological O
analyzer O
. O

In O
this O
architecture O
BPE O
is O
only O
used O
to O
handle O
words O
which O
can O
not O
be O
directly O
decomposed O
by O
the O
morphological O
analyzer O
such O
as O
misspellings,5347Word O
Morphemes O
Monolingual O
BPE O
Multilingual O
BPE O
twagezeyo O
‘ O
we O
arrived O
there O
’ O
tu O
. O

a O
. O
ger O
. O

ye O
. O

yo O
twag O
. O

ezeyo O
_ O
twa O
. O

ge O
. O

ze O
. O

yo O
ndabyizeye O
‘ O
I O
hope O
so O
’ O

n O
. O

ra O
. O

bi O
. O

izer O
. O

ye O
ndaby O
. O

izeye O
_ O
ndab O
. O

yiz O
. O

eye O
umwarimu O
‘ O
teacher O
’ O
u O
. O

mu O
. O

arimu O
umwarimu O

_ O
um O
. O

wari O
. O

mu O
Table O
1 O
: O
Comparison O
between O
morphemes O
and O
BPE O
- O
produced O
sub O
- O
word O
tokens O
. O

Stems O
are O
underlined O
. O

proper O
names O
and O
foreign O
language O
words O
. O

Given O
the O
output O
of O
a O
morphological O
analyzer O
, O
a O
second O
challenge O
is O
in O
how O
to O
incorporate O
the O
produced O
morphemes O
into O
the O
model O
. O

One O
naive O
approach O
is O
to O
feed O
the O
produced O
morphemes O
to O
a O
standard O
transformer O
encoder O
as O
a O
single O
monolithic O
sequence O
. O

This O
approach O
is O
used O
by O
Mohseni O
and O
Tebbifakhr O
( O
2019 O
) O
. O

One O
problem O
with O
this O
method O
is O
that O
mixing O
sub O
- O
word O
information O
and O
sentencelevel O
tokens O
in O
a O
single O
sequence O
does O
not O
encourage O
the O
model O
to O
learn O
the O
actual O
morphological O
compositionality O
and O
express O
word O
- O
relative O
syntactic O
regularities O
. O

We O
address O
these O
issues O
by O
proposing O
a O
simple O
yet O
effective O
two O
- O
tier O
transformer O
encoder O
architecture O
. O

The O
ﬁrst O
tier O
encodes O
morphological O
information O
, O
which O
is O
then O
transferred O
to O
the O
second O
tier O
to O
encode O
sentence O
level O
information O
. O

We O
call O
this O
new O
model O
architecture O
KinyaBERT B-MethodName
because O
it O
uses O
BERT B-MethodName
’s O
masked O
language O
model O
objective O
for O
pre O
- O
training O
and O
is O
evaluated O
on O
the O
morphologically O
rich O
Kinyarwanda O
language O
. O

This O
work O
also O
represents O
progress O
in O
low O
resource O
NLP O
. O

Advances O
in O
human O
language O
technology O
are O
most O
often O
evaluated O
on O
the O
main O
languages O
spoken O
by O
major O
economic O
powers O
such O
as O
English O
, O
Chinese O
and O
European O
languages O
. O

This O
has O
exacerbated O
the O
language O
technology O
divide O
between O
the O
highly O
resourced O
languages O
and O
the O
underrepresented O
languages O
. O

It O
also O
hinders O
progress O
in O
NLP O
research O
because O
new O
techniques O
are O
mostly O
evaluated O
on O
the O
mainstream O
languages O
and O
some O
NLP O
advances O
become O
less O
informed O
of O
the O
diversity O
of O
the O
linguistic O
phenomena O
( O
Bender O
, O
2019 O
) O
. O

Specifically O
, O
this O
work O
provides O
the O
following O
research O
contributions O
: O
•A O
simple O
yet O
effective O
two O
- O
tier O
BERT B-MethodName
architecture O
for O
representing O
morphologically O
rich O
languages O
. O

•New O
evaluation O
datasets O
for O
Kinyarwanda O
language O
including O
a O
machine O
- O
translated O
subset O
of O
the O
GLUE B-DatasetName
benchmark O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
and O
a O
news B-DatasetName
categorization I-DatasetName
dataset.•Experimental O
results O
which O
set O
a O
benchmark O
for O
future O
studies O
on O
Kinyarwanda O
language O
understanding O
, O
and O
on O
using O
machinetranslated O
versions O
of O
the O
GLUE B-DatasetName
benchmark O
. O

•Code O
and O
datasets O
are O
made O
publicly O
available O
for O
reproducibility1 O
. O

2 O
Morphology O
- O
aware O
Language O
Model O
Our O
modeling O
objective O
is O
to O
be O
able O
to O
express O
morphological O
compositionality O
in O
a O
Transformerbased O
( O
Vaswani O
et O

al O
. O
, O
2017 O
) O
language O
model O
. O

For O
morphologically O
rich O
languages O
such O
as O
Kinyarwanda O
, O
a O
set O
of O
morphemes O
( O
typically O
a O
stem O
and O
a O
set O
of O
functional O
afﬁxes O
) O
combine O
to O
produce O
a O
word O
with O
a O
given O
surface O
form O
. O

This O
requires O
an O
alternative O
to O
the O
ubiquitous O
BPE O
tokenization O
, O
through O
which O
exact O
sub O
- O
word O
lexical O
units O
( O
i.e. O
morphemes O
) O
are O
used O
. O

For O
this O
purpose O
, O
we O
use O
a O
morphological O
analyzer O
which O
takes O
a O
sentence O
as O
input O
and O
, O
for O
every O
word O
, O
produces O
a O
stem O
, O
zero O
or O
more O
afﬁxes O
and O
assigns O
a O
part O
of O
speech O
( O
POS O
) O

tag O
to O
each O
word O
. O

This O
section O
describes O
how O
this O
morphological O
information O
is O
obtained O
and O
then O
integrated O
in O
a O
two O
- O
tier O
transformer O
architecture O
( O
Figure O
1 O
) O
to O
learn O
morphology O
- O
aware O
input O
representations O
. O

2.1 O
Morphological O
Analysis O
and O
Part O
- O
of O
- O
Speech O
Tagging O
Kinyarwanda O
, O
the O
national O
language O
of O
Rwanda O
, O
is O
one O
of O
the O
major O
Bantu O
languages O
( O
Nurse O
and O
Philippson O
, O
2006 O
) O
spoken O
in O
central O
and O
eastern O
Africa O
. O

Kinyarwanda O
has O
16 O
noun O
classes O
. O

Modiﬁers O
( O
demonstratives O
, O
possessives O
, O
adjectives O
, O
numerals O
) O
carry O
a O
class O
marking O
morpheme O
that O
agrees O
with O
the O
main O
noun O
class O
. O

The O
verbal O
morphology O
( O
Nzeyimana O
, O
2020 O
) O
also O
includes O
subject O
and O
object O
markers O
that O
agree O
with O
the O
class O
of O
the O
subject O
or O
object O
. O

This O
agreement O
therefore O
enables O
users O
of O
the O
language O
to O
approximately O
disambiguate O
referred O
entities O
based O
on O
their O
classes O
. O

We O
leverage O
this O
syntactic O
agreement O
property O
in O
designing O
our O
unsupervised O
POS O
tagger.5348 O
    O
V5 O
    O
tu O

ara O
ha O
mu O
  O
bon O
ye O
NP35 O
  O
John O
  O
Morphological O
Analyser O
  O
John O
twarahamubonye O
biradutangaza O
    O
V9 O
   O
bi O
  O
ra O
  O

tu O
  O
tangar O
  O
y O
aSentence O
/ O
Document O
- O
Level O
Encoder O
  O
Morphology O
Encoder O
Morphology O
Encoder O
Morphology O
Encoder O
John O
bon O
tangar O
  O
( O
We O
were O
surprised O
to O
find O
John O
there O
) O
Figure O
1 O
: O
KinyaBERT B-MethodName
model O
architecture O
: O

Encoding O
of O
the O
sentence O
’ O
John O
twarahamusanze O
biradutangaza O
’ O
( O
We O
were O
surprised O
to O
ﬁnd O
John O
there O
) O
. O

The O
morphological O
analyzer O
produces O
morphemes O
for O
each O
word O
and O
assigns O
a O
POS O
tag O
to O
it O
. O

The O
two O
- O
tier O
transformer O
model O
then O
generates O
contextualized O
embeddings O
( O
blue O
vectors O
at O
the O
top O
) O
. O

The O
redcolored O
embeddings O
correspond O
to O
the O
POS O
tags O
, O
yellow O
is O
for O
the O
stem O
embeddings O
, O
green O
is O
for O
the O
variable O
length O
afﬁxes O
while O
the O
purple O
embeddings O
correspond O
to O
the O
afﬁx O
set O
. O

Our O
morphological O
analyzer O
for O
Kinyarwanda O
was O
built O
following O
ﬁnite O
- O
state O
two O
- O
level O
morphology O
principles O
( O
Koskenniemi O
, O
1983 O
; O
Beesley O
and O
Karttunen O
, O
2000 O
, O
2003 O
) O
. O

For O
every O
inﬂectable O
word O
type O
, O
we O
maintain O
a O
morphotactics O
model O
using O
a O
directed O
acyclic O
graph O
( O
DAG O
) O
that O
represents O
the O
regular O
sequencing O
of O
morphemes O
. O

We O
effectively O
model O
all O
inﬂectable O
word O
types O
in O
Kinyarwanda O
which O
include O
verbals O
, O
nouns O
, O
adjectives O
, O
possessive O
and O
demonstrative O
pronouns O
, O
numerals O
and O
quantiﬁers O
. O

The O
morphological O
analyzer O
also O
includes O
many O
hand O
- O
crafted O
rules O
for O
handling O
morphographemics O
and O
other O
linguistic O
regularities O
of O
the O
Kinyarwanda O
language O
. O

The O
morphological O
analyzer O
was O
independently O
developed O
and O
calibrated O
by O
native O
speakers O
as O
a O
closed O
source O
solution O
before O
the O
current O
work O
on O
language O
modeling O
. O

Similar O
to O
Nzeyimana O
( O
2020 O
) O
, O
we O
use O
a O
classiﬁer O
trained O
on O
a O
stemming O
dataset O
to O
disambiguate O
between O
competing O
outputs O
of O
the O
morphological O
analyzer O
. O

Furthermore O
, O
we O
improve O
the O
disambiguation O
quality O
by O
leveraging O
a O
POS O
tagger O
at O
the O
phrase O
level O
so O
that O
the O
syntactic O
context O
can O
be O
taken O
into O
consideration O
. O

We O
devise O
an O
unsupervised O
part O
of O
speech O
tagging O
algorithm O
which O
we O
explain O
here O
. O

Let O
x= O

( O
x1;x2;x3;:::x O
n)be O
a O
sequence O
of O
tokens O
( O
e.g. O
words O
) O
to O
be O
tagged O
with O
a O
corresponding O
sequence O
of O
tagsy= O
( O
y1;y2;y3;:::yn O
) O
. O

A O
sample O
of O
actual O
POS O
tags O
used O
for O
Kinyarwanda O
is O
given O
in O
Table O
12 O
the O
Appendix O
. O

Using O
Bayes O
’ O
rule O
, O
the O
optimal O
tagsequenceyis O
given O
by O
the O
following O
equation O
: O
y= O
arg O
max O
yP(yjx O
) O
= O
arg O
max O
yP(xjy)P(y O
) O
P(x O
) O
= O
arg O
max O
yP(xjy)P(y)(1 O
) O
A O
standard O
hidden O
Markov O
model O
( O
HMM O
) O
can O
decompose O
the O
result O
of O
Equation O
1 O
using O
ﬁrst O
order O
Markov O
assumption O
and O
independence O
assumptions O
into O
P(xjy O
) O

= O
Qn O
t=1P(xtjyt)and O
P(y O
) O
= O
Qn O
t=1P(ytjyt 1 O
) O
. O

The O
tag O
sequence O
y O
can O
then O
be O
efﬁciently O
decoded O
using O
the O
Viterbi O
algorithm O
( O
Forney O
, O
1973 O
) O
. O

A O
better O
decoding O
strategy O
is O
presented O
below O
. O

Inspired O
by O
Tsuruoka O
and O
Tsujii O
( O
2005 O
) O
, O
we O
devise O
a O
greedy O
heuristic O
for O
decoding O
yusing O
the O
same O
ﬁrst O
order O
Markov O
assumptions O
but O
with O
bidirectional O
decoding O
. O

First O
, O
we O
estimate O
the O
local O
emission O
probabilitiesP(xtjyt)using O
a O
factored O
model O
given O
in O
the O
following O
equation O
: O
P(xtjyt)/~P(xtjyt O
) O
~P(xtjyt O
) O

= O
~Pm(xtjyt)~Pp(xtjyt)~Pa(xtjyt)(2 O
) O

In O
Equation O
2 O
, O
~Pm(xtjyt)corresponds O
to O
the O
probability O
/ O
score O
returned O
by O
a O
morphological O
disambiguation O
classiﬁer O
, O
representing O
the O
uncertainty O
of O
the O
morphology O
of O
xt.~Pp(xtjyt)corresponds O
to O
a O
local O
precedence O
weight O
between O
competing O
POS O
tags O
. O

These O
precedence O
weights O
are O
man-5349ually O
crafted O
through O
qualitative O
evaluation O
( O
See O
Table O
12 O
in O
Appendix O
for O
examples O
) O
. O

~Pa(xtjyt O
) O
quantiﬁes O
the O
local O
neighborhood O
syntactic O
agreement O
between O
Bantu O
class O
markers O
. O

When O
there O
are O
two O
or O
more O
agreeing O
class O
markers O
in O
neighboring O
words O
, O
the O
tagger O
should O
be O
more O
conﬁdent O
of O
the O
agreeing O
parts O
of O
speech O
. O

A O
basic O
agreement O
score O
can O
be O
the O
number O
of O
agreeing O
class O
markers O
within O
a O
window O
of O
seven O
words O
around O
a O
given O
candidate O
xt O
. O

We O
manually O
designed O
a O
more O
elaborate O
set O
of O
agreement O
rules O
and O
their O
weights O
for O
different O
contexts O
. O

Therefore O
, O
the O
actual O
agreement O
score O
~Pa(xtjyt)is O
a O
weighted O
sum O
of O
the O
matched O
agreement O
rules O
. O

Each O
of O
the O
unnormalized O
measures O
~P O
in O
Equation O
2 O
is O
mapped O
to O
the O
[ O
0;1]range O
using O
a O
sigmoid O
function O
(zjzA;zB)given O
in O
Equation O
3 O
, O
wherezis O
the O
score O
of O
the O
measure O
and O
[ O
zA;zB]is O
its O
estimated O
active O
range O
. O
(zjzA;zB O
) O

= O

[ O
1 O
+ O
exp( 8z zA O
zB zA)] 8(3 O
) O
After O
estimating O
the O
local O
emission O
model O
, O
we O
greedily O
decode O
y O
t= O
arg O
maxyt O
~ O
P(ytjx)in O
decreasing O
order O
of O
~P(xtjyt)using O
a O
ﬁrst O
order O
bidirectional O
inference O
of O
~P(ytjx)as O
given O
in O
the O
following O
equation O
: O
~P(ytjx O
) O

= O
8 O
> O
> O
> O
> O
> O
> O
> O
> O
> O
> O
> O
< O
> O
> O
> O
> O
> O
> O
> O
> O
> O
> O
> O
: O
~P(xtjyt)~P(ytjy O
t 1;y O
t+1)~P(y O
t 1jx)~P(y O
t+1jx O
) O
if O
bothy O
t 1andy O
t+1have O
been O
decoded O
; O
~P(xtjyt)~P(ytjy O
t 1)~P(y O
t 1jx O
) O
if O
onlyy O
t 1has O
been O
decoded O
; O
~P(xtjyt)~P(ytjy O
t+1)~P(y O
t+1jx O
) O
if O
onlyy O
t+1has O
been O
decoded O
; O
~P(xtjyt)otherwise O
( O
4 O
) O

The O
ﬁrst O
order O
transition O
measures O
~P(ytjyt 1 O
) O
, O
~P(ytjyt+1)and O
~ O
P(ytjyt 1;yt+1)are O
estimated O
using O
count O
tables O
computed O
over O
the O
entire O
corpus O
by O
aggregating O
local O
emission O
marginals O
~P(yt O
) O
= O
P O
xt O
~ O
P(xt;yt)obtained O
through O
morphological O
analysis O
and O
disambiguation O
. O

2.2 O
Morphology O
Encoding O
The O
overall O
architecture O
of O
our O
model O
is O
depicted O
in O
Figure O
1 O
. O

This O
is O
a O
two O
- O
tier O
transformer O
encoder O
architecture O
made O
of O
a O
token O
- O
level O
morphology O
encoder O
that O
feeds O
into O
a O
sentence O
/ O
document O
- O
level O
encoder O
. O

The O
morphology O
encoder O
is O
made O
of O
a O
small O
transformer O
encoder O
that O
is O
applied O
to O
eachanalyzed O
token O
separately O
in O
order O
to O
extract O
its O
morphological O
features O
. O

The O
extracted O
morphological O
features O
are O
then O
concatenated O
with O
the O
token O
’s O
stem O
embedding O
to O
form O
the O
input O
vector O
fed O
to O
the O
sentence O
/ O
document O
encoder O
. O

The O
sentence O
/ O
document O
encoder O
is O
made O
of O
a O
standard O
transformer O
encoder O
as O
used O
in O
other O
BERT B-MethodName
models O
. O

The O
sentence O
/ O
document O
encoder O
uses O
untied O
position O
encoding O
with O
relative O
bias O
as O
proposed O
in O
Ke O
et O
al O
. O

( O
2020 O
) O
. O

The O
input O
to O
the O
morphology O
encoder O
is O
a O
set O
of O
embedding O
vectors O
, O
three O
vectors O
relating O
to O
the O
part O
of O
speech O
, O
one O
for O
the O
stem O
and O
one O
for O
each O
afﬁx O
when O
available O
. O

The O
transformer O
encoder O
operation O
is O
applied O
to O
these O
embedding O
vectors O
without O
any O
positional O
information O
. O

This O
is O
because O
positional O
information O
at O
the O
morphology O
level O
is O
inherent O
since O
no O
morpheme O
repeats O
and O
each O
morpheme O
always O
occupies O
a O
known O
( O
i.e. O
ﬁxed O
) O
slot O
in O
the O
morphotactics O
model O
. O

The O
extracted O
morphological O
features O
are O
four O
encoder O
output O
vectors O
corresponding O
to O
the O
three O
POS O
embeddings O
and O
one O
stem O
embedding O
. O

Vectors O
corresponding O
to O
the O
afﬁxes O
are O
left O
out O
since O
they O
are O
of O
variable O
length O
and O
the O
role O
of O
the O
afﬁxes O
in O
this O
case O
is O
to O
be O
attended O
to O
by O
the O
stem O
and O
the O
POS O
tag O
so O
that O
morphological O
information O
can O
be O
captured O
. O

The O
four O
morphological O
output O
feature O
vectors O
are O
further O
concatenated O
with O
another O
stem O
embedding O
at O
the O
sentence O
level O
to O
form O
the O
input O
vector O
for O
the O
main O
sentence O
/ O
document O
encoder O
. O

The O
choice O
of O
this O
transformer O
- O
based O
architecture O
for O
morphology O
encoding O
is O
motivated O
by O
two O
factors O
. O

First O
, O
Zaheer O
et O
al O
. O

( O
2020 O
) O
has O
demonstrated O
the O
importance O
of O
having O
“ O
global O
tokens O
” O
such O
as O
[ O
CLS O
] O
token O
in O
BERT B-MethodName
models O
. O

These O
are O
tokens O
that O
attend O
to O
all O
other O
tokens O
in O
the O
modeled O
sequence O
. O

These O
“ O
global O
tokens O
” O
effectively O
encapsulate O
some O
“ O
meaning O
” O
of O
the O
encoded O
sequence O
. O

Second O
, O
the O
POS O
tag O
and O
stem O
represent O
the O
high O
level O
information O
content O
of O
a O
word O
. O

Therefore O
, O
having O
the O
POS O
tag O
and O
stem O
embeddings O
be O
transformed O
into O
morphological O
features O
is O
a O
viable O
option O
. O

The O
POS O
tag O
and O
stem O
embeddings O
thus O
serve O
as O
the O
“ O
global O
tokens O
” O
at O
the O
morphology O
encoder O
level O
since O
they O
attend O
to O
all O
other O
morphemes O
that O
can O
be O
associated O
with O
them O
. O

In O
order O
to O
capture O
subtle O
morphological O
information O
, O
we O
make O
one O
of O
the O
three O
POS O
embeddings O
span O
an O
afﬁx O
set O
vocabulary O
that O
is O
a O
subset O
of O
the O
all O
afﬁxes O
power O
set O
. O

We O
form O
an O
afﬁx O
set O
vocabu-5350laryVathat O
is O
made O
of O
the O
Nmost O
frequent O
afﬁx O
combinations O
in O
the O
corpus O
. O

In O
fact O
, O
the O
morphological O
model O
of O
the O
language O
enforces O
constraints O
on O
which O
afﬁxes O
can O
go O
together O
for O
any O
given O
part O
of O
speech O
, O
resulting O
in O
an O
afﬁx O
set O
vocabulary O
that O
is O
much O
smaller O
than O
the O
power O
set O
of O
all O
afﬁxes O
. O

Even O
with O
limiting O
the O
afﬁx O
set O
vocabulary O
Vato O
a O
ﬁxed O
size O
, O
we O
can O
still O
map O
any O
afﬁx O
combination O
toVaby O
dropping O
zero O
or O
very O
few O
afﬁxes O
from O
the O
combination O
. O

Note O
that O
the O
afﬁx O
set O
embedding O
still O
has O
to O
attend O
to O
all O
morphemes O
at O
the O
morphology O
encoder O
level O
, O
making O
it O
adapt O
to O
the O
whole O
morphological O
context O
. O

The O
afﬁx O
set O
embedding O
is O
depicted O
by O
the O
purple O
units O
in O
Figure O
1 O
and O
a O
sample O
ofVais O
given O
in O
Table O
13 O
in O
the O
Appendix O
. O

2.3 O
Pre O
- O
training O
Objective O
Similar O
to O
other O
BERT B-MethodName
models O
, O
we O
use O
a O
masked O
language O
model O
objective O
. O

Speciﬁcally O
, O
15 O
% O
of O
all O
tokens O
in O
the O
training O
set O
are O
considered O
for O
prediction O
, O
of O
which O
80 O
% O
are O
replaced O
with O
[ O
MASK O
] O
tokens O
, O
10 O
% O
are O
replaced O
with O
random O
tokens O
and O
10 O
% O
are O
left O
unchanged O
. O

When O
prediction O
tokens O
are O
replaced O
with O
[ O
MASK O
] O
or O
random O
tokens O
, O
the O
corresponding O
afﬁxes O
are O
randomly O
omitted O
70 O
% O
of O
the O
time O
or O
left O
in O
place O
for O
30 O
% O
of O
the O
time O
, O
while O
the O
units O
corresponding O
to O
POS O
tags O
and O
afﬁx O
sets O
are O
also O
masked O
. O

The O
pre O
- O
training O
objective O
is O
then O
to O
predict O
stems O
and O
the O
associated O
afﬁxes O
for O
all O
tokens O
considered O
for O
prediction O
using O
a O
two O
- O
layer O
feed O
- O
forward O
module O
on O
top O
of O
the O
encoder O
output O
. O

For O
the O
afﬁx O
prediction O
task O
, O
we O
face O
a O
multilabel O
classiﬁcation O
problem O
where O
for O
each O
prediction O
token O
, O
we O
predict O
a O
variable O
number O
of O
afﬁxes O
. O

In O
our O
experiments O
, O
we O
tried O
two O
methods O
. O

For O
one O
, O
we O
use O
the O
Kullback O
– O
Leibler O
( O
KL O
) O
divergence2loss O
function O
to O
solve O
a O
regression O
task O
of O
predicting O
the O
N O
- O
length O
afﬁx O
distribution O
vector O
. O

For O
this O
case O
, O
we O
use O
a O
target O
afﬁx O
probability O
vector O
at2RNin O
which O
each O
target O
afﬁx O
index O
is O
assigned1 O
mprobability O
and O
0probability O
for O
non O
- O
target O
afﬁxes O
. O

Here O
mis O
the O
number O
of O
afﬁxes O
in O
the O
word O
to O
be O
predicted O
andNis O
the O
total O
number O
of O
all O
afﬁxes O
. O

We O
call O
this O
method O
“ O
Afﬁx O
Distribution O
Regression O
” O
( O
ADR O
) O
and O
model O
variant O
KinyaBERT B-MethodName
ADR O
. O

Alter O

natively O
, O
we O
use O
cross O
entropy O
loss O
and O
just O
predict O
the O
afﬁx O
set O
associated O
with O
the O
prediction O
word O
; O
we O
call O
this O
method O
“ O
Afﬁx O
Set O
Classiﬁcation O
” O
( O
ASC O
) O
and O
the O
model O
variant O
KinyaBERT B-MethodName
ASC O
. O

2https://en.wikipedia.org/wiki/ O
Kullback%E2%80%93Leibler_divergence3 O
Experiments O
In O
order O
to O
evaluate O
the O
proposed O
architecture O
, O
we O
pre O
- O
train O
KinyaBERT B-MethodName
( O
101 O
M O
parameters O
for O
KinyaBERT B-MethodName
ADR I-MethodName
and O
105 O
M O
for O
KinyaBERT B-MethodName
ASC I-MethodName
) O
on O
a O
2.4 O
GB O
of O
Kinyarwanda O
text O
along O
with O
3 O
baseline O
BERT B-MethodName
models O
. O

The O
ﬁrst O
baseline O
is O
a O
BERT B-MethodName
model O
pre O
- O
trained O
on O
the O
same O
Kinyarwanda O
corpus O
and O
with O
the O
same O
position O
encoding O
( O
Ke O
et O
al O
. O
, O
2020 O
) O
, O
same O
batch B-HyperparameterName
size I-HyperparameterName
and O
pre O
- O
training B-HyperparameterName
steps I-HyperparameterName
, O
but O
using O
the O
standard O
BPE O
tokenization O
. O

We O
call O
this O
ﬁrst O
baseline O
model O
BERT B-MethodName
BPE I-MethodName
( O
120 O
M O
parameters O
) O
. O

The O
second O
baseline O
is O
a O
similar O
BERT B-MethodName
model O
pretrained O
on O
the O
same O
Kinyarwanda B-DatasetName
corpus O
but O
tokenized O
by O
a O
morphological O
analyzer O
. O

For O
this O
model O
, O
the O
input O
is O
just O
a O
sequence O
of O
morphemes O
, O
in O
a O
similar O
fashion O
to O
Mohseni O
and O
Tebbifakhr O
( O
2019 O
) O
. O

We O
call O
this O
second O
baseline O
model O
BERT B-MethodName
MORPHO I-MethodName
( O
127 O
M O
parameters O
) O
. O

For O
BERT B-MethodName
MORPHO I-MethodName
, O
we O
found O
that O
predicting O
30 O
% O
of O
the O
tokens O
achieves O
better O
results O
than O
using O
15 O
% O
because O
of O
the O
many O
afﬁxes O
generated O
. O

The O
third O
baseline O
is O
XLM B-MethodName
- I-MethodName
R I-MethodName
( O
Conneau O
et O
al O
. O
, O
2020 O
) O
( O
270 O
M O
parameters O
) O
which O
is O
pretrained O
on O
2.5 O
TB O
of O
multilingual O
text O
. O

We O
evaluate O
the O
above O
models O
by O
comparing O
their O
performance O
on O
downstream O
NLP O
tasks O
. O

Language O
Kinyarwanda O
Publication O
Period O
2011 O
- O
2021 O
Websites O
/ O
Sources O
370 O
Documents O
/ O
Articles O
840 O
K O
Sentences O
16 O
M O
Tokens O
/ O
Words O
390 O
M O
Text O
size O
2.4 O
GB O
Table O
2 O
: O
Summary O
of O
the O
pre O
- O
training O
corpus O
. O

3.1 O
Pre O
- O
training O
details O
KinyaBERT B-MethodName
model O
was O
implemented O
using O
Pytorch O
version O
1.9 O
. O

The O
morphological O
analyzer O
and O
POS O
tagger O
were O
implemented O
in O
a O
shared O
library O
using O
POSIX O
C. O
Morphological O
parsing O
of O
the O
corpus O
was O
performed O
as O
a O
pre O
- O
processing O
step O
, O
taking O
20 O
hours O
to O
segment O
the O
390M O
- O
token O
corpus O
on O
an O
12 O
- O
core O
desktop O
machine O
. O

Pre O
- O
training O
was O
performed O
using O
RTX O
3090 O
and O
RTX O
2080Ti O
desktop O
GPUs O
. O

Each O
KinyaBERT B-MethodName
model O
takes O
on O
average O
22 O
hours O
to O
train O
for O
1000 O
steps O
on O
one O
RTX O
3090 O
GPU O
or O
29 O
hours O
on O
one O
RTX O
2080Ti O
GPU O
. O

Baseline O
models O
( O
BERT B-MethodName
BPE O
and O
BERT B-MethodName
MORPHO O
) O
were O
pre O
- O
trained O
on O
cloud O
tensor O
processing O
units O
( O
TPU O
v3 O
- O
8 O
devices O
each O
with O
128 O
GB O
memory O
) O
us-5351ing O
PyTorch O
/ O
XLA3package O
and O
a O
TPU O
- O
optimized O
fairseq O
toolkit O
( O
Ott O
et O
al O
. O
, O
2019 O
) O
. O

Pre O
- O
training O
on O
TPU O
took O
2.3 O
hours O
per O
1000 O
steps O
. O

The O
baselines O
were O
trained O
on O
TPU O
because O
there O
were O
no O
major O
changes O
needed O
to O
the O
existing O
RoBERTA O
( O
base O
) O
architecture O
implemented O
in O
fairseq O
and O
the O
TPU O
resources O
were O
available O
and O
efﬁcient O
. O

In O
all O
cases O
, O
pre O
- O
training O
batch B-HyperparameterName
size I-HyperparameterName
was O
set O
to O
2560 B-HyperparameterValue
sequences O
, O
with O
maximum O
512 O
tokens O
in O
each O
sequence O
. O

The O
maximum O
learning B-HyperparameterName
rates I-HyperparameterName
was O
set O
to O
410 4which O
is O
achieved O
after O
2000 O
steps O
and O
then O
linearly O
decays O
. O

Our O
main O
results O
and O
ablation O
results O
were O
obtained O
from O
models O
pre O
- O
trained O
for O
32 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
in O
all O
cases O
. O

Other O
pre O
- O
training O
details O
, O
model O
architectural O
dimensions O
and O
other O
hyper O
- O
parameters O
are O
given O
in O
the O
Appendix O
. O

3.2 O
Evaluation O
tasks O
Machine O
translated O
GLUE B-DatasetName
benchmark O
– O
The O
General B-DatasetName
Language I-DatasetName
Understanding I-DatasetName
Evaluation I-DatasetName
( O
GLUE B-DatasetName
) O
benchmark O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
has O
been O
widely O
used O
to O
evaluate O
pre O
- O
trained O
language O
models O
. O

In O
order O
to O
assess O
KinyaBERT B-MethodName
performance O
on O
such O
high O
level O
language O
tasks O
, O
we O
used O
Google O
Translate O
API O
to O
translate O
a O
subset O
of O
the O
GLUE B-DatasetName
benchmark O
( O
MRPC B-DatasetName
, O
QNLI B-DatasetName
, O
RTE B-DatasetName
, O
SST-2 B-DatasetName
, O
STS B-DatasetName
- I-DatasetName
B I-DatasetName
and O
WNLI B-DatasetName
tasks O
) O
into O
Kinyarwanda O
. O

CoLA O
task O
was O
left O
because O
it O
is O
English O
- O
speciﬁc O
. O

MNLI O
and O
QQP O
tasks O
were O
also O
not O
translated O
because O
they O
were O
too O
expensive O
to O
translate O
with O
Google O
’s O
commercial O
API O
. O

While O
machine O
translation O
adds O
more O
noise O
to O
the O
data O
, O
evaluating O
on O
this O
dataset O
is O
still O
relevant O
because O
all O
models O
compared O
have O
to O
cope O
with O
the O
same O
noise O
. O

To O
understand O
this O
translation O
noise O
, O
we O
also O
run O
user O
evaluation O
experiments O
, O
whereby O
four O
volunteers O
proﬁcient O
in O
both O
English O
and O
Kinyarwanda O
evaluated O
a O
random O
sample O
of O
6000 O
translated O
GLUE B-DatasetName
examples O
, O
and O
assigned O
a O
score O
to O
each O
example O
on O
a O
scale O
from O
1 O
to O
4 O
( O
See O
Table O
11 O
in O
Appendix O
) O
. O

These O
scores O
help O
us O
characterize O
the O
noise O
in O
the O
data O
and O
contextualize O
our O
results O
with O
regards O
to O
other O
GLUE O
evaluations O
. O

Results O
on O
these O
GLUE B-DatasetName
tasks O
are O
shown O
in O
Table O
3 O
. O
Named B-TaskName
entity I-TaskName
recognition I-TaskName
( O
NER B-TaskName
) O
– O
We O
use O
the O
Kinyarwanda O
subset O
of O
the O
MasakhaNER B-DatasetName
dataset O
( O
Adelani O
et O
al O
. O
, O
2021 O
) O
for O
NER B-TaskName
task O
. O

This O
is O
a O
high O
quality O
NER B-TaskName
dataset O
annotated O
by O
native O
speakers O
for O
major O
African O
languages O
including O
Kinyarwanda O
. O

The O
task O
requires O
predicting O
four O
entity O
types O
: O
Persons O
( O
PER O
) O
, O
Locations O
( O
LOC O
) O
, O
Or3https://github.com/pytorch/xla/ganizations O
( O
ORG O
) O
, O
and O
date O
and O
time O
( O
DATE O
) O
. O

Results O
on O
this O
NER B-TaskName
task O
are O
presented O
in O
Table O
4 O
. O

News B-TaskName
Categorization I-TaskName
Task O
( O
NEWS B-TaskName
) O
– O

For O
a O
document O
classiﬁcation O
experiment O
, O
we O
collected O
a O
set O
of O
categorized O
news O
articles O
from O
seven O
major O
news O
websites O
that O
regularly O
publish O
in O
Kinyarwanda O
. O

The O
articles O
were O
already O
categorized O
, O
so O
no O
more O
manual O
labeling O
was O
needed O
. O

This O
dataset O
is O
similar O
to O
Niyongabo O
et O

al O
. O
( O
2020 O
) O
, O
but O
in O
our O
case O
, O
we O
limited O
the O
number O
collected O
articles O
per O
category O
to O
3000 O
in O
order O
to O
have O
a O
more O
balanced O
label O
distribution O
( O
See O
Table O
10 O
in O
the O
Appendix O
) O
. O

The O
ﬁnal O
dataset O
contains O
a O
total O
of O
25.7 O
K O
articles O
spanning O
12 O
categories O
and O
has O
been O
split O
into O
training O
, O
validation O
and O
test O
sets O
in O
the O
ratios O
of O
70 O
% O
, O
5 O
% O
and O
25 O
% O
respectively O
. O

Results O
on O
this O
NEWS B-TaskName
task O
are O
presented O
in O
Table O
5 O
. O

For O
each O
evaluation O
task O
, O
we O
use O
a O
two O
- O
layer O
feedforward O
network O
on O
top O
of O
the O
sentence O
encoder O
as O
it O
is O
typically O
done O
in O
other O
BERT B-MethodName
models O
. O

The O
ﬁnetuning O
hyper O
- O
parameters O
are O
presented O
in O
Table O
14 O
in O
the O
Appendix O
. O

3.3 O
Main O
results O
The O
main O
results O
are O
presented O
in O
Table O
3 O
, O
Table O
4 O
, O
and O
Table O
5 O
. O

Each O
result O
is O
the O
average O
of O
10 O
independent O
ﬁne O
- O
tuning O
runs O
. O

Each O
average O
result O
is O
shown O
with O
the O
standard O
deviation O
of O
the O
10 O
runs O
. O

Except O
for O
XLM B-MethodName
- I-MethodName
R I-MethodName
, O
all O
other O
models O
are O
pre O
- O
trained O
on O
the O
same O
corpus O
( O
See O
Table O
2 O
) O
for O
32 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
using O
the O
same O
hyper O
- O
parameters O
. O

On O
the O
GLUE B-DatasetName
task O
, O
KinyaBERT B-MethodName
ASC I-MethodName
achieves O
4.3 B-MetricValue
% I-MetricValue
better O
average O
score O
than O
the O
strongest O
baseline O
. O

KinyaBERT B-MethodName
ASC I-MethodName
also O
leads O
to O
more O
robust O
results O
on O
multiple O
tasks O
. O

It O
is O
also O
shown O
that O
having O
just O
a O
morphological O
analyzer O
is O
not O
enough O
: O
BERT B-MethodName
MORPHO I-MethodName
still O
under O
- O
performs O
even O
though O
it O
uses O
morphological O
tokenization O
. O

Multilingual O
XLM B-MethodName
- I-MethodName
R I-MethodName
achieves O
least O
performance O
in O
most O
cases O
, O
possibly O
because O
it O
was O
not O
pre O
- O
trained O
on O
Kinyarwanda O
text O
and O
uses O
inadequate O
tokenization O
. O

On O
the O
NER B-TaskName
task O
, O
KinyaBERT B-MethodName
ADR O
achieves O
best O
performance O
, O
about O
3.2 B-MetricValue
% I-MetricValue
better O
average O
F1 B-MetricName
score O
than O
the O
strongest O
baseline O
. O

One O
of O
the O
architectural O
differences O
between O
KinyaBERT B-MethodName
ADR I-MethodName
and O
KinyaBERT B-MethodName
ASC I-MethodName
is O
that O
KinyaBERT B-MethodName
ADR I-MethodName
uses O
three O
POS O
tag O
embeddings O
while O
KinyaBERT B-MethodName
ASC I-MethodName
uses O
two O
. O

Assuming O
that O
POS O
tagging O
facilitates O
named O
entity O
recognition O
, O
this O
empirical O
result O
suggests O
that O
increasing O
the O
amount O
of O
POS O
tag O
information5352Task O
: O

MRPC O
QNLI O

RTE O
SST-2 O
STS O
- O
B O
WNLI O
# O
Train O
examples O
: O
3.4 O
K O
104.7 O
K O
2.5 O
K O
67.4 O
K O
5.8 O
K O
0.6 O
K O
Translation O
score O
: O
2.7/4.0 O
2.9/4.0 O
3.0/4.0 O
2.7/4.0 O
3.1/4.0 O
2.9/4.0 O
Model O
Validation O
Set O
XLM O
- O
R O
84.2/78.3 O
0:8=1:079.00:358.43:278.70:677.7/77.8 O

0:7=0:655.42:0 O
BERT B-MethodName
BPE O
83.3/76.6 O
0:8=1:481.90:259.21:580.10:475.6/75.7 O

7:8=7:355.41:9 O
BERT B-MethodName
MORPHO O
84.3/77.4 O
0:6=1:181.60:259.21:581.60:576.8/77.0 O

0:8=0:754.22:5 O
KinyaBERT B-MethodName
ADR O
87.1/82.1 O
0:5=0:781.60:161.81:481.80:679.6/79.5 O

0:4=0:354.52:2 O
KinyaBERT B-MethodName
ASC O
86.6/81.3 O
0:5=0:782.30:364.31:482.40:580.0/79.9 O
0:5=0:556.20:8 O
Model O
Test O
Set O
XLM O
- O
R O
82.6/76.0 O
0:6=0:678.10:356.43:276.30:469.5/68.9 O

1:0=1:163.73:9 O
BERT B-MethodName
BPE O
82.8/76.2 O
0:6=0:881.10:355.62:879.10:468.9/67.8 O
1:8=1:763.44:1 O
BERT B-MethodName
MORPHO O
82.7/75.4 O
0:8=1:380.80:456.71:080.70:568.9/67.8 O
1:5=1:365.00:3 O
KinyaBERT B-MethodName
ADR O
84.4/ O
78.70:5=0:681.20:358.11:180.90:573.2/72.0 O
0:4=0:365.10:0 O
KinyaBERT B-MethodName
ASC O
84.6/78.40:2=0:382.20:658.80:781.40:674.5/73.5 O
0:2=0:265.00:2 O
Table O
3 O
: O
Performance O
results O
on O
the O
machine O
translated O
GLUE B-DatasetName
benchmark O
( O
Wang O
et O
al O
. O
, O
2019 O
) O
. O

The O
translation O
score O
is O
the O
sample O
average O
translation O
quality O
score O
assigned O
by O
volunteers O
. O

For O
MRPC B-DatasetName
, O
we O
report O
accuracy B-MetricName
and O
F1 B-MetricName
. O

For O
STS B-DatasetName
- I-DatasetName
B I-DatasetName
, O
we O
report O
Pearson B-MetricName
and O
Spearman B-MetricName
correlations I-MetricName
. O

For O
all O
others O
, O
we O
report O
accuracy B-MetricName
. O

The O
best O
results O
are O
shown O
in O
bold O
while O
equal O
top O
results O
are O
underlined O
. O

Task O
: O
NER O
# O
Train O
examples O
: O
2.1 O
K O
Model O
Validation O
Set O
Test O
Set O
XLM O
- O
R O
80.3 O
1:0 O
71.81:5 O
BERT B-MethodName
BPE O
83.40:9 O

74.80:8 O
BERT B-MethodName
MORPHO O
83.20:9 O

72.80:9 O
KinyaBERT B-MethodName
ADR O
87.10:8 O
77.21:0 O
KinyaBERT B-MethodName
ASC O
86.20:4 O
76.30:5 O
Table O
4 O
: O
Micro B-MetricName
average I-MetricName
F1 I-MetricName
scores O
on O
Kinyarwanda O
NER B-TaskName
task O
( O
Adelani O
et O
al O
. O
, O
2021 O
) O
. O
Task O
: O
NEWS O
# O
Train O
examples O
: O
18.0 O
K O
Model O
Validation O
Set O
Test O
Set O
XLM O
- O
R O
83.8 O
0:3 O
84.00:2 O
BERT B-MethodName
BPE O
87.60:4 O
88.30:3 O
BERT B-MethodName
MORPHO O
86.90:4 O

86.90:3 O
KinyaBERT B-MethodName
ADR O
88.80:3 O
88.00:3 O
KinyaBERT B-MethodName
ASC O
88.40:3 O
88.00:2 O
Table O
5 O
: O
Accuracy B-MetricName
results O
on O
Kinyarwanda O
NEWS B-TaskName
categorization I-TaskName
task O
. O

in O
the O
model O
, O
possibly O
through O
diversiﬁcation O
( O
i.e. O
multiple O
POS O
tag O
embedding O
vectors O
per O
word O
) O
, O
can O
lead O
to O
better O
NER B-TaskName
performance O
. O

The O
NEWS B-TaskName
categorization I-TaskName
task O
resulted O
in O
differing O
performances O
between O
validation O
and O
testsets O
. O

This O
may O
be O
a O
result O
that O
solving O
such O
task O
does O
not O
require O
high O
level O
language O
modeling O
but O
rather O
depends O
on O
spotting O
few O
keywords O
. O

Previous O
research O
on O
a O
similar O
task O
( O
Niyongabo O
et O
al O
. O
, O
2020 O
) O
has O
shown O
that O
simple O
classiﬁers O
based O
on O
TF O
- O
IDF O
features O
sufﬁce O
to O
achieve O
best O
performance O
. O

The O
morphological O
analyzer O
and O
POS O
tagger O
inherently O
have O
some O
level O
of O
noise O
because O
they O
do O
not O
always O
perform O
with O
perfect O
accuracy O
. O

While O
we O
did O
not O
have O
a O
simple O
way O
of O
assessing O
the O
impact O
of O
this O
noise O
in O
this O
work O
, O
we O
can O
logically O
expect O
that O
the O
lower O
the O
noise O
the O
better O
the O
results O
could O
be O
. O

Improving O
the O
morphological O
analyzer O
and O
POS O
tagger O
and O
quantitatively O
evaluating O
its O
accuracy O
is O
part O
of O
future O
work O
. O

Even O
though O
our O
POS O
tagger O
uses O
heuristic O
methods O
and O
was O
evaluated O
mainly O
through O
qualitative O
exploration O
, O
we O
can O
still O
see O
its O
positive O
impact O
on O
the O
pre O
- O
trained O
language O
model O
. O

We O
did O
not O
use O
previous O
work O
on O
Kinyarwanda O
POS O
tagging O
because O
it O
is O
largely O
different O
from O
this O
work O
in O
terms O
of O
scale O
, O
tag O
dictionary O
and O
dataset O
size O
and O
availability O
. O

We O
plot O
the O
learning O
curves O
during O
ﬁne O
- O
tuning O
process O
of O
KinyaBERT B-MethodName
and O
the O
baselines O
. O

The O
results O
in O
Figure O
2 O
indicate O
that O
KinyaBERT B-MethodName
ﬁnetuning O
has O
better O
convergence O
across O
all O
tasks O
. O

Additional O
results O
also O
show O
that O
positional O
attention O
( O
Ke O
et O
al O
. O
, O
2020 O
) O
learned O
by O
KinyaBERT B-MethodName
has O
more O
uniform O
and O
smoother O
relative O
bias O
while O
BERT B-MethodName
BPE I-MethodName
and O
BERT B-MethodName
MORPHO I-MethodName
have O
more O
noisy5353Figure O
2 O
: O
Comparison O
of O
ﬁne O
- O
tuning O
loss O
curves O
between O
KinyaBERT B-MethodName
and O
baselines O
on O
the O
evaluation O
tasks O
. O

KinyaBERT B-MethodName
ASC I-MethodName
achieves O
the O
best O
convergence O
in O
most O
cases O
, O
indicating O
better O
effectiveness O
of O
its O
model O
architecture O
and O
pre O
- O
training O
objective O
. O
relative O
positional O
bias O
( O
See O
Figure O
3 O
in O
Appendix O
) O
. O

This O
is O
possibly O
an O
indication O
that O
KinyaBERT B-MethodName
allows O
learning O
better O
word O
- O
relative O
syntactic O
regularities O
. O

However O
, O
this O
aspect O
needs O
to O
be O
investigated O
more O
systematically O
in O
future O
research O
. O

While O
the O
main O
sentence O
/ O
document O
encoder O
of O
KinyaBERT B-MethodName
is O
equivalent O
to O
a O
standard O
BERT B-MethodName
“ O
BASE O
” O
conﬁguration O
on O
top O
of O
a O
small O
morphology O
encoder O
, O
overall O
, O
the O
model O
actually O
decreases O
the O
number O
of O
parameters O
by O
more O
than O
12 O
% O
through O
embedding O
layer O
savings O
. O

This O
is O
because O
using O
morphological O
representation O
reduces O
the O
vocabulary O
size O
. O

Using O
smaller O
embedding O
vectors O
at O
the O
morphology O
encoder O
level O
also O
signiﬁcantly O
reduces O
the O
overall O
number O
of O
parameters O
. O

Table O
8 O
in O
Appendix O
shows O
the O
vocabulary O
sizes O
and O
parameter O
count O
of O
KinyaBERT B-MethodName
in O
comparison O
to O
the O
baselines O
. O

While O
the O
sizing O
of O
the O
embeddings O
was O
done O
essentially O
to O
match O
BERT B-MethodName
“ O
BASE O
” O
conﬁguration O
, O
future O
studies O
can O
shed O
more O
light O
on O
how O
different O
model O
sizes O
affect O
performance O
. O

3.4 O
Ablation O
study O
We O
conducted O
an O
ablation O
study O
to O
clarify O
some O
of O
the O
design O
choices O
made O
for O
KinyaBERT B-MethodName
architecture O
. O

We O
make O
variations O
along O
two O
axes O
: O
( O
i O
) O
morphology O
input O
and O
( O
ii O
) O
pre O
- O
training O
task O
, O
which O
gave O
us O
four O
variants O
that O
we O
pre O
- O
trained O
for O
32 O
K O
steps O
and O
evaluated O
on O
the O
same O
downstream O
tasks O
. O

•AFS!STEM+ASC O
: O
Morphological O
features O
are O
captured O
by O
two O
POS O
tag O
and O
one O
afﬁx O
set O
vectors O
. O

We O
predict O
both O
the O
stem O
and O
afﬁx O
set O
. O

This O
corresponds O
to O
KinyaBERT B-MethodName
ASC O
presented O
in O
the O
main O
results.•POS!STEM+ADR O
: O
Morphological O
features O
are O
carried O
by O
three O
POS O
tag O
vectors O
and O
we O
predict O
the O
stem O
and O
afﬁx O
probability O
vector O
. O

This O
corresponds O
to O
KinyaBERT B-MethodName
ADR O
. O

•A O
VG!STEM+ADR O
: O
Morphological O
features O
are O
captured O
by O
two O
POS O
tag O
vectors O
and O
the O
pointwise O
average O
of O
afﬁx O
hidden O
vectors O
from O
the O
morphology O
encoder O
. O

We O
predict O
the O
stem O
and O
afﬁx O
probability O
vector O
. O
•STEM!STEM O
: O
We O
omit O
the O
morphology O
encoder O
and O
train O
a O
model O
with O
only O
the O
stem O
parts O
without O
afﬁxes O
and O
only O
predict O
the O
stem O
. O

Ablation O
results O
presented O
in O
Table O
6 O
indicate O
that O
using O
afﬁx O
sets O
for O
both O
morphology O
encoding O
and O
prediction O
gives O
better O
results O
for O
many O
GLUE O
tasks O
. O

The O
under O
- O
performance O
of O
“ O
STEM O
! O
STEM O
” O
on O
high O
resource O
tasks O
( O
QNLI B-DatasetName
and O
SST-2 B-DatasetName
) O
is O
an O
indication O
that O
morphological O
information O
from O
afﬁxes O
is O
important O
. O

However O
, O
the O
utility O
of O
this O
information O
depends O
on O
the O
task O
as O
we O
see O
mixed O
results O
on O
other O
tasks O
. O

Due O
to O
a O
large O
design O
space O
for O
a O
morphologyaware O
language O
model O
, O
there O
are O
still O
a O
number O
of O
other O
design O
choices O
that O
can O
be O
explored O
in O
future O
studies O
. O

One O
may O
vary O
the O
amount O
of O
POS O
tag O
embeddings O
used O
, O
vary O
the O
size O
afﬁx O
set O
vocabulary O
or O
the O
dimension O
of O
the O
morphology O
encoder O
embeddings O
. O

One O
may O
also O
investigate O
the O
potential O
of O
other O
architectures O
for O
the O
morphology O
encoder O
, O
such O
as O
convolutional O
networks O
. O

Our O
early O
attempt O
of O
using O
recurrent O
neural O
networks O
( O
RNNs O
) O
for O
the O
morphology O
encoder O
was O
abandoned O
because O
it O
was O
too O
slow O
to O
train.5354Task O
: O

MRPC O
QNLI O
RTE O
SST-2 O
STS O
- O
B O
WNLI O
NER O
NEWS O
Morphology!Prediction O

Validation O
Set O
AFS!STEM+ASC O
86.6/81.3 O
82.3 O
64.3 O
82.4 O
80.0/79.9 O
56.2 O
86.2 O
88.4 O
POS!STEM+ADR O
87.1/82.1 O
81.6 O
61.8 O
81.8 O
79.6/79.5 O
54.5 O
87.1 O
88.8 O
A O
VG!STEM+ADR O
85.5/80.3 O
81.4 O
63.0 O
82.1 O
79.6/79.5 O
55.8 O
86.6 O
88.3 O
STEM!STEM O
86.4/81.5 O
80.4 O
63.4 O
77.5 O
79.7/79.5 O
50.4 O
86.6 O
88.0 O
Morphology!Prediction O

Test O
Set O
AFS!STEM+ASC O
84.6/78.4 O

82.2 O
58.8 O
81.4 O
74.5/73.5 O
65.0 O
76.3 O
88.0 O
POS!STEM+ADR O
84.4/ O
78.7 O
81.2 O
58.1 O
80.9 O
73.2/72.0 O
65.1 O
77.2 O
88.0 O
A O
VG!STEM+ADR O

84.0/78.2 O

81.7 O
59.4 O
80.7 O
73.6/72.6 O
65.0 O
76.9 O
88.2 O
STEM!STEM O
84.2/78.6 O
80.3 O
59.8 O
77.5 O
73.3/72.0 O
59.6 O
76.4 O
88.4 O
Table O
6 O
: O
Ablation O
results O
: O
each O
result O
is O
an O
average O
of O
10 O
independent O
ﬁne O
- O
tuning O
runs O
. O

Metrics O
, O
dataset O
sizes O
and O
noise O
statistics O
are O
the O
same O
as O
for O
the O
main O
results O
in O
Table O
3 O
, O
Table O
4 O
and O
Table O
5 O
. O
4 O
Related O
Work O
BERT B-MethodName
- O
variant O
pre O
- O
trained O
language O
models O
( O
PLMs O
) O
were O
initially O
pre O
- O
trained O
on O
monolingual O
highresource O
languages O
. O

Multilingual O
PLMs O
that O
include O
both O
high O
- O
resource O
and O
low O
- O
resource O
languages O
have O
also O
been O
introduced O
( O
Devlin O
et O
al O
. O
, O
2019 O
; O
Conneau O
et O
al O
. O
, O
2020 O
; O

Xue O
et O
al O
. O
, O
2021 O
; O
Chung O
et O

al O
. O
, O
2020 O
) O
. O

However O
, O
it O
has O
been O
found O
that O
these O
multilingual O
models O
are O
biased O
towards O
high O
- O
resource O
languages O
and O
use O
fewer O
low O
quality O
and O
uncleaned O
low O
- O
resource O
data O
( O
Kreutzer O
et O
al O
. O
, O
2022 O
) O
. O

The O
included O
low O
- O
resource O
languages O
are O
also O
very O
limited O
because O
they O
are O
mainly O
sourced O
from O
Wikipedia O
articles O
, O
where O
languages O
with O
few O
articles O
like O
Kinyarwanda O
are O
often O
left O
behind O
( O
Joshi O
et O
al O
. O
, O
2020 O
; O
Nekoto O
et O
al O
. O
, O
2020 O
) O
. O

Joshi O
et O
al O
. O

( O
2020 O
) O
classify O
the O
state O
of O
NLP O
for O
Kinyarwanda O
as O
“ O
Scraping O
- O
By O
” O
, O
meaning O
it O
has O
been O
mostly O
excluded O
from O
previous O
NLP O
research O
, O
and O
require O
the O
creation O
of O
dedicated O
resources O
and O
models O
. O

Kinyarwanda O
has O
been O
studied O
mostly O
in O
descriptive O
linguistics O
( O
Kimenyi O
, O
1976 O
, O
1978a O
, O
b O
, O
1988 O
; O
Jerro O
, O
2016 O
) O
. O

Few O
recent O
NLP O
works O
on O
Kinyarwanda O
include O
Morphological O
Analysis O
( O
Muhirwe O
, O
2009 O
; O
Nzeyimana O
, O
2020 O
) O
, O
Text O
Classiﬁcation O
( O
Niyongabo O
et O
al O
. O
, O
2020 O
) O
, O
Named O
Entity O
Recognition O
( O
Rijhwani O
et O
al O
. O
, O
2020 O
; O

Adelani O
et O

al O
. O
, O
2021 O
; O
Sälevä O
and O
Lignos O
, O
2021 O
) O
, O
POS O
tagging O
( O
Garrette O
and O
Baldridge O
, O
2013 O
; O
Garrette O
et O
al O
. O
, O
2013 O
; O
Duong O
et O

al O
. O
, O
2014 O

; O
Fang O
and O
Cohn O
, O
2016 O
; O
Cardenas O
et O
al O
. O
, O
2019 O
) O
, O
and O
Parsing O
( O
Sun O
et O
al O
. O
, O
2014 O
; O
Mielens O
et O
al O
. O
, O
2015 O
) O
. O

There O
is O
no O
prior O
study O
on O
pre O
- O
trained O
language O
modeling O
for O
Kinyarwanda O
. O

There O
are O
very O
few O
works O
on O
monolingual O
PLMsfor O
African O
languages O
. O

To O
the O
best O
of O
our O
knowledge O
there O
is O
currently O
only O
AfriBERT B-MethodName
( O
Ralethe O
, O
2020 O
) O
that O
has O
been O
pre O
- O
trained O
on O
Afrikaans O
, O
a O
language O
spoken O
in O
South O
Africa O
. O

In O
this O
paper O
, O
we O
aim O
to O
increase O
the O
inclusion O
of O
African O
languages O
in O
NLP O
community O
by O
introducing O
a O
PLM O
for O
Kinyarwanda O
. O

Differently O
to O
the O
previous O
works O
( O
see O
Table O
15 O
in O
Appendix O
) O
which O
solely O
pretrained O
unmodiﬁed O
BERT B-MethodName
models O
, O
we O
propose O
an O
improved O
BERT B-MethodName
architecture O
for O
morphologically O
rich O
languages O
. O

Recently O
, O
there O
has O
been O
a O
research O
push O
to O
improve O
sub O
- O
word O
tokenization O
by O
adopting O
characterbased O
models O
( O
Ma O
et O
al O
. O
, O
2020 O
; O
Clark O
et O
al O
. O
, O
2022 O
) O
. O

While O
these O
methods O
are O
promising O
for O
the O
“ O
language O
- O
agnostic O
” O
case O
, O
they O
are O
still O
solely O
based O
on O
the O
surface O
form O
of O
words O
, O
and O
thus O
have O
the O
same O
limitations O
as O
BPE O
when O
processing O
morphologically O
rich O
languages O
. O

We O
leave O
it O
to O
future O
research O
to O
empirically O
explore O
how O
these O
characterbased O
methods O
compare O
to O
morphology O
- O
aware O
models O
. O

5 O
Conclusion O
This O
work O
demonstrates O
the O
effectiveness O
of O
explicitly O
incorporating O
morphological O
information O
in O
language O
model O
pre O
- O
training O
. O

The O
proposed O
twotier O
Transformer O
architecture O
allows O
the O
model O
to O
represent O
morphological O
compositionality O
. O

Experiments O
conducted O
on O
Kinyarwanda O
, O
a O
low O
resource O
morphologically O
rich O
language O
, O
reveal O
signiﬁcant O
performance O
improvement O
on O
several O
downstream O
NLP O
tasks O
when O
using O
the O
proposed O
architecture O
. O

These O
ﬁndings O
should O
motivate O
more O
research O
into O
morphology O
- O
aware O
language O
models.5355Acknowledgements O
This O
work O
was O
supported O
with O
Cloud O
TPUs O
from O
Google O
’s O
TPU O
Research O
Cloud O
( O
TRC O
) O
program O
and O
Google O
Cloud O
Research O
Credits O
with O
the O
award O
GCP19980904 O
. O

We O
also O
thank O
the O
anonymous O
reviewers O
for O
their O
insightful O
feedback O
. O

References O
David O
Ifeoluwa O
Adelani O
, O
Jade O
Abbott O
, O
Graham O
Neubig O
, O
Daniel O
D’souza O
, O
Julia O
Kreutzer O
, O
Constantine O
Lignos O
, O
Chester O
Palen O
- O
Michel O
, O
Happy O
Buzaaba O
, O
Shruti O
Rijhwani O
, O
Sebastian O
Ruder O
, O
Stephen O
Mayhew O
, O
Israel O
Abebe O
Azime O
, O
Shamsuddeen O
H. O
Muhammad O
, O
Chris O
Chinenye O
Emezue O
, O
Joyce O
NakatumbaNabende O
, O
Perez O
Ogayo O
, O
Aremu O
Anuoluwapo O
, O
Catherine O
Gitau O
, O
Derguene O
Mbaye O
, O
Jesujoba O
Alabi O
, O
Seid O
Muhie O
Yimam O
, O
Tajuddeen O
Rabiu O
Gwadabe O
, O
Ignatius O
Ezeani O
, O
Rubungo O
Andre O
Niyongabo O
, O
Jonathan O
Mukiibi O
, O
Verrah O
Otiende O
, O
Iroro O
Orife O
, O
Davis O
David O
, O
Samba O
Ngom O
, O
Tosin O
Adewumi O
, O
Paul O
Rayson O
, O
Mofetoluwa O
Adeyemi O
, O
Gerald O
Muriuki O
, O
Emmanuel O
Anebi O
, O
Chiamaka O
Chukwuneke O
, O
Nkiruka O
Odu O
, O
Eric O
Peter O
Wairagala O
, O
Samuel O
Oyerinde O
, O
Clemencia O
Siro O
, O
Tobius O
Saul O
Bateesa O
, O
Temilola O
Oloyede O
, O
Yvonne O
Wambui O
, O
Victor O
Akinode O
, O
Deborah O
Nabagereka O
, O
Maurice O
Katusiime O
, O
Ayodele O
Awokoya O
, O
Mouhamadane O
MBOUP O
, O
Dibora O
Gebreyohannes O
, O
Henok O
Tilaye O
, O
Kelechi O
Nwaike O
, O
Degaga O
Wolde O
, O
Abdoulaye O
Faye O
, O
Blessing O
Sibanda O
, O
Orevaoghene O
Ahia O
, O
Bonaventure O
F. O
P. O
Dossou O
, O
Kelechi O
Ogueji O
, O
Thierno O
Ibrahima O
DIOP O
, O
Abdoulaye O
Diallo O
, O
Adewale O
Akinfaderin O
, O
Tendai O
Marengereke O
, O
and O
Salomey O
Osei O
. O

2021 O
. O

MasakhaNER O
: O
Named O
entity O
recognition O
for O
African O
languages O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
9:1116–1131 O
. O

Fady O
Baly O
, O
Hazem O
Hajj O
, O
et O
al O
. O
2020 O
. O

Arabert O
: O
Transformer O
- O
based O
model O
for O
arabic O
language O
understanding O
. O

In O
Proceedings O
of O
the O
4th O
Workshop O
on O
Open O
- O
Source O
Arabic O
Corpora O
and O
Processing O
Tools O
, O
with O
a O
Shared O
Task O
on O
Offensive O
Language O
Detection O
, O
pages O
9–15 O
. O

Kenneth O
R O
Beesley O
and O
Lauri O
Karttunen O
. O

2000 O
. O

Finitestate O
non O
- O
concatenative O
morphotactics O
. O

In O
Proceedings O
of O
the O
38th O
Annual O
Meeting O
on O
Association O
for O
Computational O
Linguistics O
, O
pages O
191–198 O
. O

Kenneth O
R O
Beesley O
and O
Lauri O
Karttunen O
. O

2003 O
. O

Finitestate O
morphology O
: O
Xerox O
tools O
and O
techniques O
. O

CSLI O
, O
Stanford O
. O

Emily O
M O
Bender O
. O

2019 O
. O

The O
# O
benderrule O
: O
On O
naming O
the O
languages O
we O
study O
and O
why O
it O
matters O
. O

The O
Gradient O
, O
14 O
. O

Yoshua O
Bengio O
, O
Réjean O
Ducharme O
, O
Pascal O
Vincent O
, O
and O
Christian O
Janvin O
. O
2003 O
. O

A O
neural O
probabilistic O
language O
model O
. O

The O
journal O
of O
machine O
learning O
research O
, O
3:1137–1155.Piotr O
Bojanowski O
, O
Edouard O
Grave O
, O
Armand O
Joulin O
, O
and O
Tomas O
Mikolov O
. O

2017 O
. O

Enriching O
word O
vectors O
with O
subword O
information O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
5:135–146 O
. O

José O
Canete O
, O
Gabriel O
Chaperon O
, O
Rodrigo O
Fuentes O
, O
and O
Jorge O
Pérez O
. O

2020 O
. O

Spanish O
pre O
- O
trained O
bert O
model O
and O
evaluation O
data O
. O

PML4DC O
at O
ICLR O
, O
2020 O
. O

Ronald O
Cardenas O
, O
Ying O
Lin O
, O
Heng O
Ji O
, O
and O
Jonathan O
May O
. O

2019 O
. O

A O
grounded O
unsupervised O
universal O
partof O
- O
speech O
tagger O
for O
low O
- O
resource O
languages O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
2428–2439 O
. O

Branden O
Chan O
, O
Stefan O
Schweter O
, O
and O
Timo O
Möller O
. O
2020 O
. O

German O
’s O
next O
language O
model O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
6788–6796 O
, O
Barcelona O
, O
Spain O
( O
Online O
) O
. O

International O
Committee O
on O
Computational O
Linguistics O
. O

Hyung O
Won O
Chung O
, O
Thibault O
Fevry O
, O
Henry O
Tsai O
, O
Melvin O
Johnson O
, O
and O
Sebastian O
Ruder O
. O
2020 O
. O

Rethinking O
embedding O
coupling O
in O
pre O
- O
trained O
language O
models O
. O

In O
International O
Conference O
on O
Learning O
Representations O
. O

Jonathan O
H O
Clark O
, O
Dan O
Garrette O
, O
Iulia O
Turc O
, O
and O
John O
Wieting O
. O
2022 O
. O

Canine O
: O
Pre O
- O
training O
an O
efﬁcient O
tokenization O
- O
free O
encoder O
for O
language O
representation O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
10:73–91 O
. O
Alexis O
Conneau O
, O
Kartikay O
Khandelwal O
, O
Naman O
Goyal O
, O
Vishrav O
Chaudhary O
, O
Guillaume O
Wenzek O
, O
Francisco O
Guzmán O
, O
Edouard O
Grave O
, O
Myle O
Ott O
, O
Luke O
Zettlemoyer O
, O
and O
Veselin O
Stoyanov O
. O

2020 O
. O

Unsupervised O
cross O
- O
lingual O
representation O
learning O
at O
scale O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
8440 O
– O
8451 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Pieter O
Delobelle O
, O
Thomas O
Winters O
, O
and O
Bettina O
Berendt O
. O
2020 O
. O

RobBERT O
: O
a O
Dutch O
RoBERTa O
- O
based O
Language O
Model O
. O

In O
Findings O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
EMNLP O
2020 O
, O
pages O
3255–3265 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

Bert O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
. O

Long O
Duong O
, O
Trevor O
Cohn O
, O
Karin O
Verspoor O
, O
Steven O
Bird O
, O
and O
Paul O
Cook O
. O

2014 O
. O

What O
can O
we O
get O
from O
1000 O
tokens O
? O

a O
case O
study O
of O
multilingual O
pos O
tagging O
for O
resource O
- O
poor O
languages O
. O

In O
Proceedings O
of5356the O
2014 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
( O
EMNLP O
) O
, O
pages O
886–897 O
. O
Meng O
Fang O
and O
Trevor O
Cohn O
. O
2016 O
. O

Learning O
when O
to O
trust O
distant O
supervision O
: O
An O
application O
to O
lowresource O
pos O
tagging O
using O
cross O
- O
lingual O
projection O
. O

InProceedings O
of O
The O
20th O
SIGNLL O
Conference O
on O
Computational O
Natural O
Language O
Learning O
, O
pages O
178–186 O
. O

G O
David O
Forney O
. O

1973 O
. O

The O
viterbi O
algorithm O
. O

Proceedings O
of O
the O
IEEE O
, O
61(3):268–278 O
. O

Dan O
Garrette O
and O
Jason O
Baldridge O
. O

2013 O
. O

Learning O
a O
part O
- O
of O
- O
speech O
tagger O
from O
two O
hours O
of O
annotation O
. O

In O
Proceedings O
of O
the O
2013 O
conference O
of O
the O
North O
American O
chapter O
of O
the O
association O
for O
computational O
linguistics O
: O
Human O
language O
technologies O
, O
pages O
138–147 O
. O

Dan O
Garrette O
, O
Jason O
Mielens O
, O
and O
Jason O
Baldridge O
. O
2013 O
. O

Real O
- O
world O
semi O
- O
supervised O
learning O
of O
postaggers O
for O
low O
- O
resource O
languages O
. O

In O
Proceedings O
of O
the O
51st O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
583–592 O
. O

Kyle O
Jerro O
. O

2016 O
. O

The O
locative O
applicative O
and O
the O
semantics O
of O
verb O
class O
in O
kinyarwanda O
. O

Diversity O
in O
African O
languages O
, O
page O
289 O
. O

Pratik O
Joshi O
, O
Sebastin O
Santy O
, O
Amar O
Budhiraja O
, O
Kalika O
Bali O
, O
and O
Monojit O
Choudhury O
. O

2020 O
. O

The O
state O
and O
fate O
of O
linguistic O
diversity O
and O
inclusion O
in O
the O
nlp O
world O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
6282–6293 O
. O

Guolin O
Ke O
, O
Di O
He O
, O
and O
Tie O
- O
Yan O
Liu O
. O
2020 O
. O

Rethinking O
positional O
encoding O
in O
language O
pre O
- O
training O
. O

In O
International O
Conference O
on O
Learning O
Representations O
. O

Alexandre O
Kimenyi O
. O

1976 O
. O

Subjectivization O
rules O
in O
kinyarwanda O
. O

In O
Annual O
Meeting O
of O
the O
Berkeley O
Linguistics O
Society O
, O
volume O
2 O
, O
pages O
258–268 O
. O

Alexandre O
Kimenyi O
. O

1978a O
. O

Aspects O
of O
naming O
in O
kinyarwanda O
. O

Anthropological O
linguistics O
, O
20(6):258–271 O
. O

Alexandre O
Kimenyi O
. O

1978b O
. O

A O
relational O
grammar O
of O
kinyarwanda O
. O

University O
of O
California O
, O
Publications O
in O
Linguistics O
Berkeley O
, O
Cal O
, O
91:1–248 O
. O

Alexandre O
Kimenyi O
. O

1988 O
. O

Passiveness O
in O
kinyarwanda O
. O

In O
Passive O
and O
Voice O
, O
page O
355 O
. O

John O
Benjamins O
. O

Stav O
Klein O
and O
Reut O
Tsarfaty O
. O

2020 O
. O

Getting O
the O
# O
# O
life O
out O
of O
living O
: O
How O
adequate O
are O
word O
- O
pieces O
for O
modelling O
complex O
morphology O
? O

In O
Proceedings O
of O
the O
17th O
SIGMORPHON O
Workshop O
on O
Computational O
Research O
in O
Phonetics O
, O
Phonology O
, O
and O
Morphology O
, O
pages O
204–209 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Kimmo O
Koskenniemi O
. O

1983 O
. O

Two O
- O
level O
model O
for O
morphological O
analysis O
. O

In O
IJCAI O
, O
volume O
83 O
, O
pages O
683–685 O
. O

Fajri O
Koto O
, O
Afshin O
Rahimi O
, O
Jey O
Han O
Lau O
, O
and O
Timothy O
Baldwin O
. O

2020 O
. O

Indolem O
and O
indobert O
: O
A O
benchmark O
dataset O
and O
pre O
- O
trained O
language O
model O
for O
indonesian O
nlp O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
757–770 O
. O
John O
Koutsikakis O
, O
Ilias O
Chalkidis O
, O
Prodromos O
Malakasiotis O
, O
and O
Ion O
Androutsopoulos O
. O

2020 O
. O

Greek O
- O
bert O
: O
The O
greeks O
visiting O
sesame O
street O
. O

In O
11th O
Hellenic O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
pages O
110 O
– O
117 O
. O

Julia O
Kreutzer O
, O
Isaac O
Caswell O
, O
Lisa O
Wang O
, O
Ahsan O
Wahab O
, O
Daan O
van O
Esch O
, O
Nasanbayar O
Ulzii O
- O
Orshikh O
, O
Allahsera O
Tapo O
, O
Nishant O
Subramani O
, O
Artem O
Sokolov O
, O
Claytone O
Sikasote O
, O
et O
al O
. O
2022 O
. O

Quality O
at O
a O
glance O
: O
An O
audit O
of O
web O
- O
crawled O
multilingual O
datasets O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
10:50–72 O
. O

Y O
Kuratov O
and O
M O
Arkhipov O
. O

2019 O
. O

Adaptation O
of O
deep O
bidirectional O
multilingual O
transformers O
for O
russian O
language O
. O

In O
Komp’juternaja O
Lingvistika O
i O
Intellektual’nye O
Tehnologii O
, O
pages O
333–339 O
. O
Hang O
Le O
, O
Loïc O
Vial O
, O
Jibril O
Frej O
, O
Vincent O
Segonne O
, O
Maximin O
Coavoux O
, O
Benjamin O
Lecouteux O
, O
Alexandre O
Allauzen O
, O
Benoit O
Crabbe O
, O
Laurent O
Besacier O
, O
and O
Didier O
Schwab O
. O

2020 O
. O

Flaubert O
: O

Unsupervised O
language O
model O
pre O
- O
training O
for O
french O
. O

In O
LREC O
. O

Wentao O
Ma O
, O
Yiming O
Cui O
, O
Chenglei O
Si O
, O
Ting O
Liu O
, O
Shijin O
Wang O
, O
and O
Guoping O
Hu O
. O
2020 O
. O

CharBERT O
: O
Character O
- O
aware O
pre O
- O
trained O
language O
model O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
39–50 O
, O
Barcelona O
, O
Spain O
( O
Online O
) O
. O

International O
Committee O
on O
Computational O
Linguistics O
. O

Louis O
Martin O
, O
Benjamin O
Muller O
, O
Pedro O
Javier O
Ortiz O
Suárez O
, O
Yoann O
Dupont O
, O
Laurent O
Romary O
, O
Éric O
de O
la O
Clergerie O
, O
Djamé O
Seddah O
, O
and O
Benoît O
Sagot O
. O
2020 O
. O

CamemBERT O
: O
a O
tasty O
French O
language O
model O
. O

InProceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
7203–7219 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Mihai O
Masala O
, O
Stefan O
Ruseti O
, O
and O
Mihai O
Dascalu O
. O

2020 O
. O

Robert O
– O
a O
romanian O
bert O
model O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
6626–6637 O
. O

John O
J O
McCarthy O
. O

1981 O
. O

A O
prosodic O
theory O
of O
nonconcatenative O
morphology O
. O

Linguistic O
inquiry O
, O
12(3):373–418 O
. O

Jason O
Mielens O
, O
Liang O
Sun O
, O
and O
Jason O
Baldridge O
. O
2015 O
. O

Parse O
imputation O
for O
dependency O
annotations O
. O

In5357Proceedings O
of O
the O
53rd O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
7th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1385 O
– O
1394 O
. O

Tomas O
Mikolov O
, O
Ilya O
Sutskever O
, O
Kai O
Chen O
, O
Greg O
S O

Corrado O
, O
and O
Jeff O
Dean O
. O

2013 O
. O

Distributed O
representations O
of O
words O
and O
phrases O
and O
their O
compositionality O
. O

Advances O
in O
neural O
information O
processing O
systems O
, O
26 O
. O

Mahdi O
Mohseni O
and O
Amirhossein O
Tebbifakhr O
. O

2019 O
. O

MorphoBERT O
: O
a O
Persian O
NER O
system O
with O
BERT B-MethodName
and O
morphological O
analysis O
. O

In O
Proceedings O
of O
The O
First O
International O
Workshop O
on O
NLP O
Solutions O
for O
Under O
Resourced O
Languages O
( O
NSURL O
2019 O
) O
colocated O
with O
ICNLSP O
2019 O
- O
Short O
Papers O
, O
pages O
23 O
– O
30 O
, O
Trento O
, O
Italy O
. O

Association O
for O
Computational O
Linguistics O
. O

Jackson O
Muhirwe O
. O

2007 O
. O

Computational O
analysis O
of O
kinyarwanda O
morphology O
: O

The O
morphological O
alternations O
. O

International O
Journal O
of O
computing O
and O
ICT O
Research O
, O
1(1):85–92 O
. O

Jackson O
Muhirwe O
. O

2009 O
. O

Morphological O
analysis O
of O
tone O
marked O
kinyarwanda O
text O
. O

In O
International O
Workshop O
on O
Finite O
- O
State O
Methods O
and O
Natural O
Language O
Processing O
, O
pages O
48–55 O
. O

Springer O
. O

Wilhelmina O
Nekoto O
, O
Vukosi O
Marivate O
, O
Tshinondiwa O
Matsila O
, O
Timi O
Fasubaa O
, O
Taiwo O
Fagbohungbe O
, O
Solomon O
Oluwole O
Akinola O
, O
Shamsuddeen O
Muhammad O
, O
Salomon O
Kabongo O
Kabenamualu O
, O
Salomey O
Osei O
, O
Freshia O
Sackey O
, O
Rubungo O
Andre O
Niyongabo O
, O
Ricky O
Macharm O
, O
Perez O
Ogayo O
, O
Orevaoghene O
Ahia O
, O
Musie O
Meressa O
Berhe O
, O
Mofetoluwa O
Adeyemi O
, O
Masabata O
Mokgesi O
- O
Selinga O
, O
Lawrence O
Okegbemi O
, O
Laura O
Martinus O
, O
Kolawole O
Tajudeen O
, O
Kevin O
Degila O
, O
Kelechi O
Ogueji O
, O
Kathleen O
Siminyu O
, O
Julia O
Kreutzer O
, O
Jason O
Webster O
, O
Jamiil O
Toure O
Ali O
, O
Jade O
Abbott O
, O
Iroro O
Orife O
, O
Ignatius O
Ezeani O
, O
Idris O
Abdulkadir O
Dangana O
, O
Herman O
Kamper O
, O
Hady O
Elsahar O
, O
Goodness O
Duru O
, O
Ghollah O
Kioko O
, O
Murhabazi O
Espoir O
, O
Elan O
van O
Biljon O
, O
Daniel O
Whitenack O
, O
Christopher O
Onyefuluchi O
, O
Chris O
Chinenye O
Emezue O
, O
Bonaventure O
F. O
P. O
Dossou O
, O
Blessing O
Sibanda O
, O
Blessing O
Bassey O
, O
Ayodele O
Olabiyi O
, O
Arshath O
Ramkilowan O
, O
Alp O
Öktem O
, O
Adewale O
Akinfaderin O
, O
and O
Abdallah O
Bashir O
. O

2020 O
. O

Participatory O
research O
for O
low O
- O
resourced O
machine O
translation O
: O
A O
case O
study O
in O
African O
languages O
. O

InFindings O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
EMNLP O
2020 O
, O
pages O
2144–2160 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Dat O
Quoc O
Nguyen O
and O
Anh O
Tuan O
Nguyen O
. O

2020 O
. O

PhoBERT O
: O
Pre O
- O
trained O
language O
models O
for O
Vietnamese O
. O

In O
Findings O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
EMNLP O
2020 O
, O
pages O
1037–1042 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Rubungo O
Andre O
Niyongabo O
, O
Qu O
Hong O
, O
Julia O
Kreutzer O
, O
and O
Li O
Huang O
. O

2020 O
. O

Kinnews O
and O
kirnews O
: O
Benchmarking O
cross O
- O
lingual O
text O
classiﬁcation O
forkinyarwanda O
and O
kirundi O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
5507–5521 O
. O

Derek O
Nurse O
and O
Gérard O
Philippson O
. O

2006 O
. O

The O
bantu O
languages O
. O

Routledge O
. O

Antoine O
Nzeyimana O
. O

2020 O
. O

Morphological O
disambiguation O
from O
stemming O
data O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
4649–4660 O
, O
Barcelona O
, O
Spain O
( O
Online O
) O
. O

International O
Committee O
on O
Computational O
Linguistics O
. O

Myle O
Ott O
, O
Sergey O
Edunov O
, O
Alexei O
Baevski O
, O
Angela O
Fan O
, O
Sam O
Gross O
, O
Nathan O
Ng O
, O
David O
Grangier O
, O
and O
Michael O
Auli O
. O

2019 O
. O

fairseq O
: O
A O
fast O
, O
extensible O
toolkit O
for O
sequence O
modeling O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Demonstrations O
) O
, O
pages O
48–53 O
, O
Minneapolis O
, O
Minnesota O
. O

Association O
for O
Computational O
Linguistics O
. O

Jeffrey O
Pennington O
, O
Richard O
Socher O
, O
and O
Christopher O
D O
Manning O
. O

2014 O
. O

Glove O
: O
Global O
vectors O
for O
word O
representation O
. O

In O
Proceedings O
of O
the O
2014 O
conference O
on O
empirical O
methods O
in O
natural O
language O
processing O
( O
EMNLP O
) O
, O
pages O
1532–1543 O
. O

Matthew O
E. O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O

2018 O
. O

Deep O
contextualized O
word O
representations O
. O

In O
Proceedings O
of O
the O
2018 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
Papers O
) O
, O
pages O
2227–2237 O
, O
New O
Orleans O
, O
Louisiana O
. O

Association O
for O
Computational O
Linguistics O
. O

Sello O
Ralethe O
. O

2020 O
. O

Adaptation O
of O
deep O
bidirectional O
transformers O
for O
afrikaans O
language O
. O

In O
Proceedings O
of O
The O
12th O
Language O
Resources O
and O
Evaluation O
Conference O
, O
pages O
2475–2478 O
. O

Shruti O
Rijhwani O
, O
Shuyan O
Zhou O
, O
Graham O
Neubig O
, O
and O
Jaime O
G O
Carbonell O
. O

2020 O
. O

Soft O
gazetteers O
for O
lowresource O
named O
entity O
recognition O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
8118–8123 O
. O
Piotr O
Rybak O
, O
Robert O
Mroczkowski O
, O
Janusz O
Tracz O
, O
and O
Ireneusz O
Gawlik O
. O

2020 O
. O

Klej O
: O
Comprehensive O
benchmark O
for O
polish O
language O
understanding O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
1191 O
– O
1201 O
. O

Jonne O
Sälevä O
and O
Constantine O
Lignos O
. O

2021 O
. O

Mining O
wikidata O
for O
name O
resources O
for O
african O
languages O
. O

arXiv O
preprint O
arXiv:2104.00558 O
. O

Raphael O
Scheible O
, O
Fabian O
Thomczyk O
, O
Patric O
Tippmann O
, O
Victor O
Jaravine O
, O
and O
Martin O
Boeker O
. O

2020 O
. O

Gottbert O
: O
a O
pure O
german O
language O
model O
. O

arXiv O
preprint O
arXiv:2012.02110 O
.5358Rico O

Sennrich O
, O
Barry O
Haddow O
, O
and O
Alexandra O
Birch O
. O
2016 O
. O

Neural O
machine O
translation O
of O
rare O
words O
with O
subword O
units O
. O

In O
Proceedings O
of O
the O
54th O

Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1715 O
– O
1725 O
, O
Berlin O
, O
Germany O
. O

Association O
for O
Computational O
Linguistics O
. O

Fábio O
Souza O
, O
Rodrigo O
Nogueira O
, O
and O
Roberto O
Lotufo O
. O
2020 O
. O

Bertimbau O
: O
Pretrained O
bert O
models O
for O
brazilian O
portuguese O
. O

In O
Brazilian O
Conference O
on O
Intelligent O
Systems O
, O
pages O
403–417 O
. O

Springer O
. O

Liang O
Sun O
, O
Jason O
Mielens O
, O
and O
Jason O
Baldridge O
. O

2014 O
. O

Parsing O
low O
- O
resource O
languages O
using O
gibbs O
sampling O
for O
pcfgs O
with O
latent O
annotations O
. O

In O
Proceedings O
of O
the O
2014 O
conference O
on O
empirical O
methods O
in O
natural O
language O
processing O
( O
EMNLP O
) O
, O
pages O
290 O
– O
300 O
. O

Yoshimasa O
Tsuruoka O
and O
Jun’ichi O
Tsujii O
. O

2005 O
. O

Bidirectional O
inference O
with O
the O
easiest-ﬁrst O
strategy O
for O
tagging O
sequence O
data O
. O

In O
Proceedings O
of O
Human O
Language O
Technology O
Conference O
and O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
467–474 O
. O

Ashish O
Vaswani O
, O
Noam O
Shazeer O
, O
Niki O
Parmar O
, O
Jakob O
Uszkoreit O
, O
Llion O
Jones O
, O
Aidan O
N O
Gomez O
, O
Lukasz O
Kaiser O
, O
and O
Illia O
Polosukhin O
. O
2017 O
. O

Attention O
is O
all O
you O
need O
. O

In O
NIPS O
. O
Antti O
Virtanen O
, O
Jenna O
Kanerva O
, O
Rami O
Ilo O
, O
Jouni O
Luoma O
, O
Juhani O
Luotolahti O
, O
Tapio O
Salakoski O
, O
Filip O
Ginter O
, O
and O
Sampo O
Pyysalo O
. O

2019 O
. O

Multilingual O
is O
not O
enough O
: O
Bert O
for O
ﬁnnish O
. O

arXiv O
preprint O
arXiv:1912.07076 O
. O

Alex O
Wang O
, O
Amanpreet O
Singh O
, O
Julian O
Michael O
, O
Felix O
Hill O
, O
Omer O
Levy O
, O
and O
Samuel O
R O
Bowman O
. O

2019 O
. O

Glue B-DatasetName
: O
A O
multi O
- O
task O
benchmark O
and O
analysis O
platform O
for O
natural O
language O
understanding O
. O

In O
7th O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2019 O
. O

Linting O
Xue O
, O
Noah O
Constant O
, O
Adam O
Roberts O
, O
Mihir O
Kale O
, O
Rami O
Al O
- O
Rfou O
, O
Aditya O
Siddhant O
, O
Aditya O
Barua O
, O
and O
Colin O
Raffel O
. O
2021 O
. O

mT5 O
: O

A O
massively O
multilingual O
pre O
- O
trained O
text O
- O
to O
- O
text O
transformer O
. O

In O
Proceedings O
of O
the O
2021 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
pages O
483–498 O
, O
Online O
. O

Association O
for O
Computational O
Linguistics O
. O

Manzil O
Zaheer O
, O
Guru O
Guruganesh O
, O
Kumar O
Avinava O
Dubey O
, O
Joshua O
Ainslie O
, O
Chris O
Alberti O
, O
Santiago O
Ontanon O
, O
Philip O
Pham O
, O
Anirudh O
Ravula O
, O
Qifan O
Wang O
, O
Li O
Yang O
, O
et O
al O
. O
2020 O
. O

Big O
bird O
: O

Transformers O
for O
longer O
sequences O
. O

Advances O
in O
Neural O
Information O
Processing O
Systems O
, O
33:17283–17297.5359Appendix O
A O
Data O
Tables O
, O
Hyper O
- O
parameters O
& O
Additional O
results O
Module O
Values O
Morphology O
Encoder O
: O
Number B-HyperparameterName
of I-HyperparameterName
Layers I-HyperparameterName
4 B-HyperparameterValue
Attention B-HyperparameterName
heads I-HyperparameterName
4 B-HyperparameterValue
Hidden B-HyperparameterName
Size I-HyperparameterName
128 B-HyperparameterValue
Attention B-HyperparameterName
head I-HyperparameterName
size I-HyperparameterName
32 B-HyperparameterValue
FFN B-HyperparameterName
inner I-HyperparameterName
hidden I-HyperparameterName
size I-HyperparameterName
512 B-HyperparameterValue
Morphological B-HyperparameterName
embedding I-HyperparameterName
size I-HyperparameterName
128 B-HyperparameterValue
Sentence O
/ O
Document O
Encoder O
: O
Number B-HyperparameterName
of I-HyperparameterName
Layers I-HyperparameterName
12 B-HyperparameterValue
Attention B-HyperparameterName
heads I-HyperparameterName
12 B-HyperparameterValue
Hidden B-HyperparameterName
Size I-HyperparameterName
768 B-HyperparameterValue
Attention B-HyperparameterName
head I-HyperparameterName
size I-HyperparameterName
64 B-HyperparameterValue
FFN B-HyperparameterName
inner I-HyperparameterName
hidden I-HyperparameterName
size I-HyperparameterName
3072 B-HyperparameterValue
Stem B-HyperparameterName
embedding I-HyperparameterName
size I-HyperparameterName
256 B-HyperparameterValue
Table O
7 O
: O
KinyaBERT B-MethodName
Architectural O
dimensions O
. O

Model O
( O
# O
Params O
) O
Vocab O
. O

Size O
XLM O
- O
R O
( O
270 O
M O
): O
Sentence O
- O
Piece O
tokens O
250 O
K O
BERT B-MethodName
BPE O
( O
120 O
M O
): O
BPE O
Tokens O
43 O
K O
BERT B-MethodName
MORPHO O
( O
127 O
M O
): O
Morphemes O
& O
BPE O
Tokens O
51 O
K O
KinyaBERT B-MethodName
ADR O
( O
101 O
M O
): O
Stems O
& O
BPE O
Tokens O
34 O
K O
Afﬁxes O

0.3 O
K O
POS O

Tags O
0.2 O
K O
KinyaBERT B-MethodName
ASC O
( O
105 O
M O
): O
Stems O
& O
BPE O
Tokens O
34 O
K O
Afﬁx O
sets O
34 O
K O
Afﬁxes O
0.3 O
K O
POS O

Tags O
0.2 O
K O
Table O
8 O
: O
V O
ocabulary O
sizes O
for O
embedding O
layers O
. O

Hyper O
- O
parameter O
Values O
Dropout B-HyperparameterName
0.1 B-HyperparameterValue
Attention B-HyperparameterName
Dropout I-HyperparameterName

0.1 B-HyperparameterValue
Warmup B-HyperparameterName
Steps I-HyperparameterName
2 B-HyperparameterValue
K I-HyperparameterValue
Max B-HyperparameterName
Steps I-HyperparameterName
200 B-HyperparameterValue
K I-HyperparameterValue
Weight B-HyperparameterName
Decay I-HyperparameterName
0.01 B-HyperparameterValue
Learning B-HyperparameterName
Rate I-HyperparameterName
Decay I-HyperparameterName
Linear B-HyperparameterValue
Peak B-HyperparameterName
Learning I-HyperparameterName
Rate I-HyperparameterName
4e-4 B-HyperparameterValue
Batch B-HyperparameterName
Size I-HyperparameterName
2560 B-HyperparameterValue
Optimizer B-HyperparameterName
LAMB B-HyperparameterValue
Adam B-HyperparameterName
1e-6 B-HyperparameterValue
Adam B-HyperparameterName
 I-HyperparameterName
1 I-HyperparameterName
0.90 B-HyperparameterValue
Adam B-HyperparameterName
 I-HyperparameterName
2 I-HyperparameterName
0.98 B-HyperparameterValue
Gradient B-HyperparameterName
Clipping I-HyperparameterName
0 B-HyperparameterValue
Table O
9 O
: O
Pre O
- O
training O
hyper O
- O
parameters O
Category O
# O
Articles O
entertainment O
3000 O
sports O
3000 O
security O
3000 O
economy O
3000 O
health O
3000 O
politics O
3000 O
religion O
2020 O
development O
1813 O
technology O
1105 O
culture O
994 O
relationships O
940 O
people O
852 O
Total O
25724 O
Table O
10 O
: O
NEWS O
categorization O
dataset O
label O
distribution O
. O

Score O
Translation O
quality O
1 O
Invalid O
or O
meaningless O
translation O
2 O
Invalid O
but O
not O
totally O
wrong O
3 O
Almost O
valid O
, O
but O
not O
totally O
correct O
4 O
Valid O
and O
correct O
translation O
Table O
11 O
: O
Machine O
- O
translated O
GLUE B-DatasetName
benchmark O
scoring O
prompt O
levels.5360POS O
Tag O
~Ppweight O
Description O
Example O
V#000 O
1.8 O
Inﬁnitive O
Verb O
kuvuga O
‘ O
to O
say O
’ O
V#001 O
1 O
Gerund O
or O
verbal O
noun O
uwavuze O
‘ O
the O
one O
who O
said O
’ O
V#002 O
1.5 O
Imperative O
verb O
vuga O
‘ O
say O
’ O
V#004 O
1.5 O
Continuous O
present O
verb O
aracyavuga O
‘ O
she O
is O
still O
saying O
’ O
V#005 O
1.5 O
Past O
tense O
verb O
yaravuze O

‘ O
she O
said O
’ O
V#006 O
1.5 O
Future O
tense O
verb O
azavuga O
‘ O
she O
will O
say O
’ O
V#010 O
1.5 O
Verb O
without O
tense O
mark O
avuga O
‘ O
saying O
’ O
N#011 O
1 O
Noun O
without O
augmment O
( O
wa)muntu O
‘ O
person O
’ O
N#012 O
2 O
Noun O
with O
augment O
umuntu O
‘ O
a O
person O
’ O
DE#013 O
2 O
Demonstrative O
ng- O
nguyu O
‘ O
this O
is O
her O
’ O
DE#020 O
3 O
Personal O
demonstrative O
wowe O
‘ O
you O
’ O
DE#021 O
2 O
Demonstrative O
with O
augment O
uwo O
‘ O
this O
( O
person O
) O
’ O
PO#025 O
2 O
Possessive O
+ O
augment O
+ O
owner O
uwawe O
‘ O
yours O
’ O
QA#026 O
0.5 O
Qualiﬁcative O
adjective O
+ O
augment O
+ O
bu O
ubuto O
‘ O
littleness O
’ O
QA#027 O
1 O
Qualiﬁcative O
adjective O
+ O
augment O
-bu O
umuto O
‘ O
the O
little O
one O
’ O
QA#028 O
2.5 O
Qualiﬁcative O
adjective O
-augment O
muto O
‘ O
little O
’ O
QA#029 O
3 O
Qualiﬁcative O
adjective O
-augment O
+ O
reduplication O
mutomuto O
‘ O
( O
kind O
of O
) O
little O
’ O
NU#030 O
2.5 O
Numeral O
babiri O
‘ O
two O
( O
people O
) O
’ O
OT#033 O
2.5 O
Quoting O
-ti O
bati O
: O
‘ O
they O
said O
: O
’ O
NP#035 O
2 O
Proper O
names O
Yohana O
‘ O
John O
’ O
DI#036 O
3 O
Digits O
84 O
AD#037 O
2.5 O
Adverb O
bucece O
‘ O
silently O
’ O
VC#038 O
2.5 O
Conjunctive O
adverbs O
hanyuma O
‘ O
and O
then O
’ O
CO#039 O
2.5 O
Commanding O
expressions O
cyono O
‘ O
please O
’ O
CA#040 O
2.5 O
Calling O
expressions O
yewe O
‘ O
you O
’ O
QU#044 O
3 O
Questioning O
adverb O

he O
he O
‘ O

where O
’ O
SP#054 O
2.5 O
Spatial O
hakurya O
‘ O
over O
there O
’ O
TE#055 O
2.5 O
Temporal O
kare O
‘ O
early O
’ O
RL#056 O
3 O
Relatives O
masenge O
‘ O
my O
aunt O
’ O
PR#057 O
3 O
Prepositions O
ku O
‘ O
on O
’ O
OR#064 O
2.5 O
Orientations O
amajyaruguru O
‘ O
north O
’ O
AJ#065 O
2.5 O
Adjectives O
rusange O
‘ O
common O
’ O
NN#066 O
2.5 O
Nominal O
loanwords O
kopi O
‘ O
copy O
’ O
HR#067 O
3 O
Hours O
( O
saa O
) O
mbiri O
‘ O
eight O
o’clock O
’ O
DT#068 O
2.5 O
Date O
taliki O
‘ O
date O
’ O
EN#069 O
3 O
Common O
English O
terms O
live O
, O
like O
, O
share O
IJ#070 O
2.5 O
Interjections O
dorere O
‘ O
see O
! O
’ O

CJ#071 O
3 O
Conjunctions O
ko O
‘ O
that O
’ O
CP#078 O
3 O
Copula O
ni O

‘ O
it O
is O
’ O
RE#079 O
3 O
Responses O
yego O
‘ O
yes O
’ O
UN#083 O
3 O
Measuring O
units O
metero O
‘ O
meter O
’ O
MO#084 O
4 O
Months O
Mutarama O
‘ O
January O
’ O
PT#085 O
3 O
Punctuations O
. O

Table O
12 O
: O
Examples O
of O
POS O
tags O
used O
in O
KinyaBERT B-MethodName
along O
with O
precedence O
weights O
~Pp(xtjyt)in O
Equation O
2.5361Afﬁx O
Set O
Example O
Surface O
form O
V:2 O
: O
ku O
- O
V:18 O
: O
a O
ku O
- O
gend O
- O
a O
kugenda O
‘ O
to O
walk O
’ O
N:0 O
: O
u O
- O
N:1 O
: O
mu O
u O
- O
mu O
- O
ntu O
umuntu O
‘ O
a O
person O
’ O
PO:1 O
: O
i O
i O
- O
a O
- O
cu O
yacu O
‘ O
our O
’ O
N:0 O
: O
i O
- O
N:1 O
: O
n O
i O
- O
n O
- O
kiko O
inkiko O
‘ O
courts O
’ O
PO:1 O
: O
u O
u O
- O
a O
- O
bo O
wabo O
‘ O
their O
’ O
V:2 O
: O
a O
- O
V:4 O
: O
a O
- O
V:18 O
: O
ye O
a O
- O
a O
- O
bon O
- O
ye O
yabonye O
‘ O
she O
saw O
’ O
DE:1 O
: O
u O
- O
DE:2 O
: O
u O
u O
- O
u O
- O
o O
uwo O
‘ O
that O
’ O
V:2 O
: O
u O
- O
V:4 O
: O
a O
- O
V:17 O
: O
w O
- O
V:18 O
: O
ye O
u O
- O
a O
- O
vug O
- O
w O
- O
ye O
wavuzwe O
‘ O
who O
was O
talked O
about O
’ O
QA:1 O
: O
ki O
- O
QA:3 O
: O
ki O
- O
QA:4 O
: O
re O
ki O
- O
re O
- O
ki O
- O
re O
kirekire O
‘ O
tall O
’ O
Table O
13 O
: O
Examples O
of O
afﬁx O
sets O
used O
by O
KinyaBERT B-MethodName
ASC O
; O
there O
are O
34 O
K O
sets O
in O
total O
. O

Hyperparameter O
MRPC O
QNLI O

RTE O
SST-2 O
STS O
- O
B O
WNLI O
NER O
NEWS O
Peak O
Learning O
Rate O

1e-5 O

1e-5 O
2e-5 O

1e-5 O
2e-5 O

1e-5 O
5e-5 O
1e-5 O
Batch O
Size O
16 O
32 O
16 O
32 O
16 O
16 O
32 O
32 O

Learning O
Rate O
Decay O
Linear O

Linear O

Linear O

Linear O
Linear O
Linear O
Linear O
Linear O
Weight O
Decay O
0.1 O
0.1 O
0.1 O
0.1 O
0.1 O
0.1 O
0.1 O
0.1 O
Max O
Epochs O
15 O
15 O
15 O
15 O
15 O
15 O
30 O
15 O
Warmup O
Steps O
proportion O
6 O
% O
6 O
% O
6 O
% O
6 O
% O
6 O
% O
6 O
% O
6 O
% O
6 O
% O
Optimizer O
AdamW O

AdamW O
AdamW O
AdamW O
AdamW O
AdamW O
AdamW O
AdamW O
Table O
14 O
: O
Downstream O
task O
ﬁne O
- O
tuning O
hyper O
- O
parameters O
. O

Paper O
LanguagePre O
- O
training O
Positional O
Input O
Tasks O
Embedding O
Representation O
Mohseni O
and O
Tebbifakhr O
( O
2019 O
) O
Persian O
MLM+NSP O
Absolute O
Morphemes O
Kuratov O
and O
Arkhipov O
( O
2019 O
) O
Russian O
MLM+NSP O
Absolute O
BPE O
Masala O
et O
al O
. O

( O
2020 O
) O

Romanian O
MLM+NSP O
Absolute O
BPE O
Baly O
et O
al O
. O

( O
2020 O
) O

Arabic O
WWM+NSP O
Absolute O
BPE O
Koto O
et O

al O
. O

( O
2020 O
) O

Indonesian O
MLM+NSP O
Absolute O
BPE O
Chan O
et O

al O
. O

( O
2020 O
) O

German O
WWM O
Absolute O
BPE O
Delobelle O

et O
al O
. O

( O
2020 O
) O

Dutch O
MLM O
Absolute O
BPE O
Nguyen O
and O
Tuan O
Nguyen O
( O
2020 O
) O

Vietnamese O
MLM O
Absolute O
BPE O
Canete O
et O

al O
. O

( O
2020 O
) O

Spanish O
WWM O
Absolute O
BPE O
Rybak O
et O
al O
. O

( O
2020 O
) O

Polish O
MLM O
Absolute O
BPE O
Martin O
et O
al O
. O

( O
2020 O
) O

French O
MLM O
Absolute O
BPE O
Le O
et O
al O
. O

( O
2020 O
) O

French O
MLM O
Absolute O
BPE O
Koutsikakis O

et O

al O
. O

( O
2020 O
) O

Greek O
MLM+NSP O
Absolute O
BPE O
Souza O
et O
al O
. O

( O
2020 O
) O

Portuguese O
MLM O
Absolute O
BPE O
Ralethe O
( O
2020 O
) O
Afrikaans O
MLM+NSP O
Absolute O
BPE O
This O
work O
Kinyarwanda O
MLM O
: O
STEM+AFFIXES O
TUPE O
- O
R O
Morphemes+BPE O
Table O
15 O
: O
Comparison O
between O
KinyaBERT B-MethodName
and O
other O
monolingual O
BERT B-MethodName
- O
variant O
PLMs O
. O

We O
only O
compare O
with O
previous O
works O
that O
have O
been O
published O
in O
either O
journals O
or O
conferences O
as O
of O
August O
2021 O
. O

We O
excluded O
some O
extremely O
high O
- O
resource O
languages O
such O
as O
English O
and O
Chinese O
. O

MLM O
: O
Masked O
language O
model O
; O
NSP O
: O
Next O
Sentence O
Prediction O
; O
WWM O
: O
Whole O
Word O
Masked.5362BERT O
BPE O
; O
Average O
non O
- O
adjacent O
diagonal O
STDEV O
= O
0.81 O
for O
ji jj2[2;10 O
] O
BERT B-MethodName
MORPHO O
; O
Average O
non O
- O
adjacent O
diagonal O
STDEV O
= O
0.80 O
for O
ji jj2[2;10 O
] O
KinyaBERT B-MethodName
ADR O
; O
Average O
non O
- O
adjacent O
diagonal O
STDEV O
= O
0.75 O
for O
ji jj2[2;10 O
] O
KinyaBERT B-MethodName
ASC O
; O
Average O
non O
- O
adjacent O
diagonal O
STDEV O
= O
0.75 O
for O
ji jj2[2;10 O
] O
Figure O
3 O
: O
Visualization O
of O
the O
positional O
attention O
bias O
( O
normalized O
) O
of O
the O
12 O
attention O
heads O
. O

Each O
( O
i;j)attention O
bias O
( O
Ke O
et O
al O
. O
, O
2020 O
) O
indicates O
the O
positional O
correlations O
between O
the O
ithandjthwords O
/ O
tokens O
in O
a O
sentence.5363 O


Proceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
pages O
3682–3692 O
August O
1–6 O
, O
2021 O
. O

© O
2021 O
Association O
for O
Computational O
Linguistics3682MPC O
- O
BERT B-MethodName
: O
A O
Pre O
- O
Trained O
Language O
Model O
for O
Multi O
- O
Party O
Conversation O
Understanding O
Jia O
- O
Chen O
Gu1 O
, O
Chongyang O
Tao2 O
, O
Zhen O
- O
Hua O
Ling1 O
, O
Can O
Xu2 O
, O
Xiubo O
Geng2 O
, O
Daxin O
Jiang2y O
1National O
Engineering O
Laboratory O
for O
Speech O
and O
Language O
Information O
Processing O
, O
University O
of O
Science O
and O
Technology O
of O
China O
, O
Hefei O
, O
China O
2Microsoft O
, O
Beijing O
, O
China O
gujc@mail.ustc.edu.cn O
, O
zhling@ustc.edu.cn O
, O
fchotao O
, O
caxu O
, O
xigeng O
, O
djiang O
g@microsoft.com O
Abstract O
Recently O
, O
various O
neural O
models O
for O
multiparty O
conversation O
( O
MPC O
) O
have O
achieved O
impressive O
improvements O
on O
a O
variety O
of O
tasks O
such O
as O
addressee O
recognition O
, O
speaker O
identiﬁcation O
and O
response O
prediction O
. O

However O
, O
these O
existing O
methods O
on O
MPC O
usually O
represent O
interlocutors O
and O
utterances O
individually O
and O
ignore O
the O
inherent O
complicated O
structure O
in O
MPC O
which O
may O
provide O
crucial O
interlocutor O
and O
utterance O
semantics O
and O
would O
enhance O
the O
conversation O
understanding O
process O
. O

To O
this O
end O
, O
we O
present O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
, O
a O
pre O
- O
trained O
model O
for O
MPC O
understanding O
that O
considers O
learning O
who O
says O
what O
towhom O
in O
a O
uniﬁed O
model O
with O
several O
elaborated O
self O
- O
supervised O
tasks O
. O

Particularly O
, O
these O
tasks O
can O
be O
generally O
categorized O
into O
( O
1 O
) O
interlocutor O
structure O
modeling O
including O
reply O
- O
to O
utterance O
recognition O
, O
identical O
speaker O
searching O
and O
pointer O
consistency O
distinction O
, O
and O
( O
2 O
) O
utterance O
semantics O
modeling O
including O
masked O
shared O
utterance O
restoration O
and O
shared O
node O
detection O
. O

We O
evaluate O
MPCBERT B-MethodName
on O
three O
downstream O
tasks O
including O
addressee O
recognition O
, O
speaker O
identiﬁcation O
and O
response O
selection O
. O

Experimental O
results O
show O
that O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
outperforms O
previous O
methods O
by O
large O
margins O
and O
achieves O
new O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
all O
three O
downstream O
tasks O
at O
two O
benchmarks O
. O

1 O
Introduction O
Building O
a O
conversational O
agent O
with O
intelligence O
has O
drawn O
signiﬁcant O
attention O
from O
both O
academia O
and O
industry O
. O

Most O
of O
existing O
methods O
have O
studied O
understanding O
conversations O
between O
two O
participants O
, O
aiming O
to O
return O
an O
appropriate O
response O
either O
in O
a O
generation O
- O
based O
( O
Shang O
et O
al O
. O
, O
Work O
done O
during O
the O
internship O
at O
Microsoft O
. O

yCorresponding O
author O
. O

Speaker O
Utterance O
Addressee O
I.1How O
can O
I O
setup O
if O
I O
want O
add O
new O
- O
server O
at O
xchat O
? O

I.2From O
places O
, O
network O
servers O
, O
work O
I.1 O
group O
, O
his O
computer O
, O
and O
then O
I O
clicked O
on O
the O
shared O
folder O
. O

I.3 O
It O
did O
not O
allow O
you O
to O
see O
the O
ﬁles O
? O

I.2 O
I.2It O
prompts O
for O
authentication O
and O
I O
I.3 O
do O
n’t O
know O
what O
to O
put O
. O

I O
tried O
guest O
with O
no O
password O
. O

I.4 O
Put O
proper O
authentication O
in O
, O
then O
? O

I.2 O
I.3 O
I O
think O
you O
had O
kde O
on O
suse O
? O

I.2 O
Table O
1 O
: O
An O
MPC O
example O
in O
Ubuntu O
IRC O
channel O
. O

Here O
, O
“ O
I. O
” O
is O
the O
abbreviation O
of O
“ O
interlocutor O
” O
. O
2015 O
; O
Serban O
et O
al O
. O
, O
2016 O
, O
2017 O
; O
Zhang O
et O
al O
. O
, O
2018b O
, O
2020 O
) O
or O
retrieval O
- O
based O
manner O
( O
Lowe O
et O
al O
. O
, O
2015 O
; O
Wu O
et O
al O
. O
, O
2017 O
; O
Zhou O
et O

al O
. O
, O
2018 O
; O
Tao O
et O
al O
. O
, O
2019a O
, O
b O
; O
Gu O
et O
al O
. O
, O
2019a O
, O
b O
, O
2020 O
) O
. O

Recently O
, O
researchers O
have O
paid O
more O
attention O
to O
a O
more O
practical O
and O
challenging O
scenario O
involving O
more O
than O
two O
participants O
, O
which O
is O
well O
known O
as O
multiparty O
conversation O
( O
MPC O
) O
( O
Ouchi O
and O
Tsuboi O
, O
2016 O
; O
Zhang O
et O
al O
. O
, O
2018a O
; O
Le O
et O
al O
. O
, O
2019 O
; O
Hu O
et O
al O
. O
, O
2019 O
) O
. O

Table O
1 O
shows O
an O
MPC O
example O
in O
the O
Ubuntu O
Internet O
Relay O
Chat O
( O
IRC O
) O
channel O
, O
which O
is O
composed O
of O
a O
sequence O
of O
( O
speaker O
, O
utterance O
, O
addressee O
) O
triples O
. O

In O
addition O
to O
returning O
an O
appropriate O
response O
, O
predicting O
who O
will O
be O
the O
next O
speaker O
( O
Meng O
et O
al O
. O
, O
2018 O
) O
and O
who O
is O
the O
addressee O
of O
an O
utterance O
( O
Ouchi O
and O
Tsuboi O
, O
2016 O
; O
Zhang O
et O
al O
. O
, O
2018a O
; O
Le O
et O
al O
. O
, O
2019 O
) O
are O
unique O
and O
important O
issues O
in O
MPC O
. O

An O
instance O
of O
MPC O
always O
contains O
complicated O
interactions O
between O
interlocutors O
, O
between O
utterances O
and O
between O
an O
interlocutor O
and O
an O
utterance O
. O

Therefore O
, O
it O
is O
challenging O
to O
model O
the O
conversation O
ﬂow O
and O
fully O
understand O
the O
dialogue O
content O
. O

Existing O
studies O
on O
MPC O
learn O
the O
representations O
of O
interlocutors O
and O
utterances O
with O
neural O
networks O
, O
and O
their O
representation3683spaces O
are O
either O
separate O
( O
Ouchi O
and O
Tsuboi O
, O
2016 O
) O
or O
interactive O
( O
Zhang O
et O
al O
. O
, O
2018a O
) O
. O

However O
, O
the O
semantics O
contained O
in O
the O
interlocutor O
and O
utterance O
representations O
may O
not O
be O
effectively O
captured O
as O
they O
are O
from O
two O
different O
representation O
spaces O
. O

Recently O
, O
to O
take O
advantage O
of O
the O
breakthrough O
in O
pre O
- O
training O
language O
models O
( O
PLMs O
) O
for O
natural O
language O
understanding O
, O
some O
studies O
proposed O
to O
integrate O
the O
speaker O
( O
Gu O
et O
al O
. O
, O
2020 O
) O
or O
topic O
( O
Wang O
et O
al O
. O
, O
2020 O
) O
information O
into O
PLMs O
. O

Despite O
of O
the O
performance O
improvement O
on O
response O
selection O
, O
these O
models O
still O
overlook O
the O
inherent O
relationships O
between O
utterances O
and O
interlocutors O
, O
such O
as O
“ O
address O
- O
to O
” O
. O

Furthermore O
, O
most O
existing O
studies O
design O
models O
for O
each O
individual O
task O
in O
MPC O
( O
e.g. O
, O
addressee O
recognition O
, O
speaker O
identiﬁcation O
and O
response O
prediction O
) O
separately O
. O

Intuitively O
, O
these O
tasks O
are O
complementary O
among O
each O
other O
. O

Making O
use O
of O
these O
tasks O
simultaneously O
may O
produce O
better O
contextualized O
representations O
of O
interlocutors O
and O
utterances O
, O
and O
would O
enhance O
the O
conversation O
understanding O
, O
but O
is O
neglected O
in O
previous O
studies O
. O

On O
account O
of O
above O
issues O
, O
we O
propose O
MPCBERT B-MethodName
which O
jointly O
learns O
who O
says O
what O
towhom O
in O
MPC O
by O
designing O
self O
- O
supervised O
tasks O
for O
PLMs O
, O
so O
as O
to O
improve O
the O
ability O
of O
PLMs O
on O
MPC O
understanding O
. O

Speciﬁcally O
, O
the O
ﬁve O
designed O
tasks O
includes O
reply O
- O
to O
utterance O
recognition O
, O
identical O
speaker O
searching O
, O
pointer O
consistency O
distinction O
, O
masked O
shared O
utterance O
restoration O
and O
shared O
node O
detection O
. O

The O
ﬁrst O
three O
tasks O
are O
designed O
to O
model O
the O
interlocutor O
structure O
in O
MPC O
in O
a O
semantics O
- O
to O
- O
structure O
manner O
. O

In O
the O
output O
of O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
, O
an O
interlocutor O
is O
described O
through O
the O
encoded O
representations O
of O
the O
utterances O
it O
says O
. O

Thus O
, O
the O
representations O
of O
utterance O
semantics O
are O
utilized O
to O
construct O
the O
conversation O
structure O
in O
these O
three O
tasks O
. O

On O
the O
other O
hand O
, O
the O
last O
two O
tasks O
are O
designed O
to O
model O
the O
utterance O
semantics O
in O
a O
structure O
- O
to O
- O
semantics O
manner O
. O

Intuitively O
, O
the O
conversation O
structure O
inﬂuences O
the O
information O
ﬂow O
in O
MPC O
. O

Thus O
, O
the O
structure O
information O
can O
also O
be O
used O
to O
strengthen O
the O
representations O
of O
utterance O
semantics O
in O
return O
. O

In O
general O
, O
these O
ﬁve O
self O
- O
supervised O
tasks O
are O
employed O
to O
jointly O
train O
the O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
in O
a O
multi O
- O
task O
learning O
framework O
, O
which O
helps O
the O
model O
to O
learn O
the O
complementary O
information O
among O
interlocutors O
and O
utterances O
, O
and O
that O
between O
structure O
and O
semantics O
. O

By O
this O
means O
, O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
can O
produce O
better O
interlocutor O
and O
utterance O
representations O
which O
can O
be O
effectively O
generalized O
to O
multiple O
downstream O
tasks O
of O
MPC O
. O

To O
measure O
the O
effectiveness O
of O
these O
selfsupervised O
tasks O
and O
to O
test O
the O
generalization O
ability O
of O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
, O
we O
evaluate O
it O
on O
three O
downstream O
tasks O
including O
addressee B-TaskName
recognition I-TaskName
, O
speaker B-TaskName
identiﬁcation I-TaskName
and O
response B-TaskName
selection I-TaskName
, O
which O
are O
three O
core O
research O
issues O
of O
MPC O
. O

Two O
benchmarks O
based O
on O
Ubuntu O
IRC O
channel O
are O
employed O
for O
evaluation O
. O

One O
was O
released O
by O
Hu O
et O
al O
. O

( O
2019 O
) O
. O

The O
other O
was O
released O
by O
Ouchi O
and O
Tsuboi O
( O
2016 O
) O
and O
has O
three O
experimental O
settings O
according O
to O
session O
lengths O
. O

Experimental O
results O
show O
that O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
outperforms O
the O
current O
state O
- O
of O
- O
the O
- O
art O
models O
by O
margins O
of O
3.51 O
% O
, O
2.86 O
% O
, O
3.28 O
% O
and O
5.36 O
% O
on O
the O
test O
sets O
of O
these O
two O
benchmarks O
respectively O
in O
terms O
of O
the O
session O
accuracy O
of O
addressee O
recognition O
, O
by O
margins O
of O
7.66 O
% O
, O
2.60 O
% O
, O
3.38 O
% O
and O
4.24 O
% O
respectively O
in O
terms O
of O
the O
utterance O
precision O
of O
speaker O
identiﬁcation O
, O
and O
by O
margins O
of O
3.82 O
% O
, O
2.71 O
% O
, O
2.55 O
% O
and O
3.22 O
% O
respectively O
in O
terms O
of O
the O
response O
recall O
of O
response O
selection O
. O

In O
summary O
, O
our O
contributions O
in O
this O
paper O
are O
three O
- O
fold O
: O
( O
1 O
) O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
, O
a O
PLM O
for O
MPC O
understanding O
, O
is O
proposed O
by O
designing O
ﬁve O
selfsupervised O
tasks O
based O
on O
the O
interactions O
among O
utterances O
and O
interlocutors O
. O

( O
2 O
) O
Three O
downstream O
tasks O
are O
employed O
to O
comprehensively O
evaluate O
the O
effectiveness O
of O
our O
designed O
self O
- O
supervised O
tasks O
and O
the O
generalization O
ability O
of O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
. O

( O
3 O
) O
Our O
proposed O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
achieves O
new O
state O
- O
ofthe O
- O
art O
performance O
on O
all O
three O
downstream O
tasks O
at O
two O
benchmarks O
. O

2 O
Related O
Work O
Existing O
methods O
on O
building O
dialogue O
systems O
can O
be O
generally O
categorized O
into O
studying O
twoparty O
conversations O
and O
multi O
- O
party O
conversations O
( O
MPC O
) O
. O

In O
this O
paper O
, O
we O
study O
MPC O
. O

In O
addition O
to O
predicting O
utterances O
, O
identifying O
the O
speaker O
and O
recognizing O
the O
addressee O
of O
an O
utterance O
are O
also O
important O
tasks O
for O
MPC O
. O

Ouchi O
and O
Tsuboi O
( O
2016 O
) O
ﬁrst O
proposed O
the O
task O
of O
addressee O
and O
response O
selection O
and O
created O
an O
MPC O
corpus O
for O
studying O
this O
task O
. O

Zhang O
et O
al O
. O

( O
2018a O
) O
proposed O
SI B-MethodName
- I-MethodName
RNN I-MethodName
, O
which O
updated O
speaker O
embeddings O
role O
- O
sensitively O
for O
addressee O
and O
response O
selection O
. O

Meng O
et O

al O
. O
( O
2018 O
) O
proposed O
a O
task O
of O
speaker O
classiﬁcation O
as O
a O
surrogate O
task O
for O
speaker O
modeling O
. O

Le O
et O
al.3684(2019 O
) O
proposed O
a O
who B-MethodName
- I-MethodName
to I-MethodName
- I-MethodName
whom I-MethodName
( O
W2W B-MethodName
) O
model O
to O
recognize O
the O
addressees O
of O
all O
utterances O
. O

Hu O
et O
al O
. O

( O
2019 O
) O
proposed O
a O
graph B-MethodName
- I-MethodName
structured I-MethodName
network I-MethodName
( O
GSN B-MethodName
) O
to O
model O
the O
graphical O
information O
ﬂow O
for O
response O
generation O
. O

Wang O
et O
al O
. O

( O
2020 O
) O
proposed O
to O
track O
the O
dynamic O
topic O
for O
response O
selection O
. O

Generally O
speaking O
, O
previous O
studies O
on O
MPC O
can O
not O
unify O
the O
representations O
of O
interlocutors O
and O
utterances O
effectively O
. O

Also O
, O
they O
are O
limited O
to O
each O
individual O
task O
, O
ignoring O
the O
complementary O
information O
among O
different O
tasks O
. O

To O
the O
best O
of O
our O
knowledge O
, O
this O
paper O
makes O
the O
ﬁrst O
attempt O
to O
design O
various O
self O
- O
supervised O
tasks O
for O
building O
PLMs O
aiming O
at O
MPC O
understanding O
, O
and O
to O
evaluate O
the O
performance O
of O
PLMs O
on O
three O
downstream O
tasks O
as O
comprehensively O
as O
possible O
. O

3 O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
and O
Self O
- O
Supervised O
Tasks O
An O
MPC O
instance O
is O
composed O
of O
a O
sequence O
of O
( O
speaker O
, O
utterance O
, O
addressee O
) O
triples O
, O
denoted O
asf(sn O
; O
un O
; O
an)gN O
n=1 O
, O
where O
Nis O
the O
number O
of O
turns O
in O
the O
conversation O
. O

Our O
goal O
is O
to O
build O
a O
pre O
- O
trained O
language O
model O
for O
universal O
MPC O
understanding O
. O

Given O
a O
conversation O
, O
this O
model O
is O
expected O
to O
produce O
embedding O
vectors O
for O
all O
utterances O
which O
contain O
not O
only O
the O
semantic O
information O
of O
each O
utterance O
, O
but O
also O
the O
speaker O
and O
addressee O
structure O
of O
the O
whole O
conversation O
. O

Thus O
, O
it O
can O
be O
effectively O
adapted O
to O
various O
downstream O
tasks O
by O
ﬁne O
- O
tuning O
model O
parameters O
. O

3.1 O
Model O
Overview O
In O
this O
paper O
, O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
is O
chosen O
as O
the O
backbone O
of O
our O
PLM O
for O
MPC O
. O

Thus O
, O
we O
name O
it O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
. O

It O
is O
worth O
noting O
that O
our O
proposed O
self O
- O
supervised O
tasks O
for O
training O
MPCBERT B-MethodName
can O
also O
be O
applied O
to O
other O
types O
of O
PLMs O
. O

We O
ﬁrst O
give O
an O
overview O
of O
the O
input O
representations O
and O
the O
overall O
architectures O
of O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
. O

When O
constructing O
the O
input O
representations O
, O
in O
order O
to O
consider O
the O
speaker O
information O
of O
each O
utterance O
, O
speaker O
embeddings O
( O
Gu O
et O
al O
. O
, O
2020 O
) O
are O
introduced O
as O
shown O
in O
Figure O
1 O
. O

Considering O
that O
the O
set O
of O
interlocutors O
are O
inconsistent O
in O
different O
conversations O
, O
a O
position O
- O
based O
interlocutor O
embedding O
table O
is O
initialized O
randomly O
at O
ﬁrst O
and O
updated O
during O
pre O
- O
training O
, O
which O
means O
each O
interlocutor O
in O
a O
conversation O
is O
assigned O
with O
an O
embedding O
vector O
according O
to O
the O
order O
it O
appears O
in O
the O
conversation O
. O

Then O
, O
the O
speaker O
embeddings O
for O
each O
utterance O
can O
be O
derived O
bylooking O
up O
this O
embedding O
table O
. O

The O
speaker O
embeddings O
are O
combined O
with O
standard O
token O
, O
position O
and O
segmentation O
embeddings O
and O
are O
then O
encoded O
by O
BERT B-MethodName
. O

The O
output O
embeddings O
of O
BERT B-MethodName
corresponding O
to O
different O
input O
tokens O
are O
utilized O
by O
different O
self O
- O
supervised O
tasks O
for O
further O
calculation O
. O

3.2 O
Tasks O
of O
Interlocutor O
Structure O
Modeling O
The O
ﬁrst O
three O
tasks O
follow O
the O
semantics O
- O
tostructure O
manner O
. O

In O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
, O
each O
interlocutor O
is O
described O
through O
the O
encoded O
representations O
of O
the O
utterances O
it O
says O
. O

Thus O
, O
the O
representations O
of O
utterance O
semantics O
are O
utilized O
to O
construct O
the O
conversation O
structure O
. O

Figure O
1 O
shows O
the O
input O
representations O
and O
the O
model O
architectures O
of O
these O
three O
tasks O
. O

A O
[ O
CLS O
] O
token O
is O
inserted O
at O
the O
start O
of O
each O
utterance O
, O
denoting O
its O
utterancelevel O
representation O
. O

Then O
, O
all O
utterances O
in O
a O
conversation O
are O
concatenated O
and O
a O
[ O
SEP O
] O
token O
is O
inserted O
at O
the O
end O
of O
the O
whole O
sequence O
. O

It O
is O
notable O
that O
these O
three O
tasks O
share O
the O
same O
form O
of O
input O
data O
. O

Thus O
, O
the O
input O
only O
needs O
to O
be O
encoded O
once O
by O
BERT B-MethodName
while O
the O
output O
can O
be O
fed O
into O
three O
tasks O
, O
which O
is O
computation O
- O
efﬁcient O
. O

As O
shown O
in O
Figure O
1 O
, O
a O
task O
- O
dependent O
non O
- O
linear O
transformation O
layer O
is O
placed O
on O
top O
of O
BERT B-MethodName
in O
order O
to O
adapt O
the O
output O
of O
BERT B-MethodName
to O
different O
tasks O
. O

We O
will O
describe O
the O
details O
of O
these O
tasks O
as O
follows O
. O

3.2.1 O
Reply O
- O
to O
Utterance O
Recognition O
To O
enable O
the O
model O
to O
recognize O
the O
addressee O
of O
each O
utterance O
, O
a O
self O
- O
supervised O
task O
named O
replyto O
utterance O
recognition O
( O
RUR B-TaskName
) O
is O
proposed O
to O
learn O
which O
preceding O
utterance O
the O
current O
utterance O
replies O
to O
. O

After O
encoded O
by O
BERT B-MethodName
, O
we O
extract O
the O
contextualized O
representations O
for O
each O
[ O
CLS O
] O
token O
representing O
individual O
utterances O
. O

Next O
, O
a O
non O
- O
linear O
transformation O
followed O
by O
a O
layer O
normalization O
are O
performed O
to O
derive O
the O
utterance O
representations O
for O
this O
speciﬁc O
task O
fuRUR B-TaskName
igN O
i=1 O
, O
where O
uRUR B-TaskName
i2Rdandd= O
768 O
. O

Then O
, O
for O
a O
speciﬁc O
utterance O
U O
i O
, O
its O
matching O
scores O
with O
all O
its O
preceding O
utterances O
are O
calculated O
as O
mij O
= O
softmax O
( O
uRUR B-TaskName
> O
iAruruRUR B-TaskName
j);(1 O
) O
where O
Arur2Rddis O
a O
linear O
transformation O
, O
mij O
denotes O
the O
matching O
degree O
of O
U O
jbeing O
the O
replyto O
utterance O
of O
U O
i O
, O
and O
1j O
< O

i O
. O

We O
construct O
a O
setSby O
sampling O
a O
certain O
number O
of O
utterances3685 O
Ui O
’ O
  O
Ui O
  O
UN O
  O
[ O
SEP O
] O
InputToken O
Embeddings O
Segment O
Embeddings O
Position O
Embeddings O
Speaker O
Embeddings O
... O
...... O
... O
... O
... O
... O
... O
Pre O
- O
trained O
Language O
Model O
( O
BERT B-MethodName
) O
E[CLS O
] O
  O
EU_i O
’ O
  O
E[CLS O
] O
  O
EU_i O
  O
E[CLS O
] O
  O
EU_N O
  O
E[SEP O
] O
Output O
... O
... O

[ O
CLS O
] O
  O

[ O
CLS O
] O
  O

[ O
CLS O
] O
( O
a O
) O
Reply O
-to O

Utterance O
Recognition O
Non O
- O
linear O
Transformation O
+ O
Layer O
Normalization O
ui'RUR B-TaskName
  O
uiRUR B-TaskName
  O
uNRUR B-TaskName
... O
... O

mij O
...... O
... O
......... O

Uj O
’ O
  O
Uj O
... O
  O

[ O
CLS O
] O
  O

[ O
CLS O
] O
... O
... O
... O
... O
... O
... O
... O
E[CLS O
] O
  O
EU_j O
’ O
  O
E[CLS O
] O
  O
EU_j O
... O
... O
uj'RUR B-TaskName
  O
ujRUR B-TaskName
... O

... O
( O
b O
) O
Identical O
Speaker O
Searching O
Non O
- O
linear O
Transformation O
+ O
Layer O
Normalization O
ui'iss O
  O
uiiss O
  O
uNiss O
... O
... O
... O
  O
uj'iss O
  O
ujiss O
... O
... O
( O
c O
) O
Pointer O
Consistency O
Distinction O
Non O
- O
linear O
Transformation O
+ O
Layer O
Normalization O
ui'pcd O
  O
uipcd O
  O
uNpcd O
... O
... O

... O
  O
uj'pcd O
  O
ujpcd O
... O

... O
Pointer O
   O
Pointer O
  O
Similarity O
Classifier O
... O

Figure O
1 O
: O
Input O
representations O
and O
model O
architectures O
of O
the O
three O
self O
- O
supervised O
tasks O
for O
interlocutor O
structure O
modeling O
, O
including O
( O
a O
) O
reply O
- O
to O
utterance O
recognition O
, O
( O
b O
) O
identical O
speaker O
searching O
and O
( O
c O
) O
pointer O
consistency O
distinction O
. O

in O
a O
conversation O
and O
this O
recognition O
operation O
is O
performed O
for O
each O
utterance O
in O
S. O
Meanwhile O
, O
a O
dynamic O
sampling O
strategy O
is O
adopted O
so O
that O
models O
can O
see O
more O
samples O
. O

Finally O
, O
the O
pretraining O
objective O
of O
this O
self O
- O
supervised O
task O
is O
to O
minimize O
the O
cross O
- O
entropy O
loss O
as O
Lrur= X O
i2Si 1X O
j=1yijlog(mij O
) O
; O
( O
2 O
) O
where O
yij= O
1if O

Ujis O
the O
reply O
- O
to O
utterance O
of O
U O
i O
andyij= O
0otherwise O
. O

3.2.2 O
Identical O
Speaker O
Searching O
Having O
knowledge O
of O
who O
is O
the O
speaker O
of O
an O
utterance O
is O
also O
important O
for O
MPC O
. O

The O
task O
ofidentical O
speaker O
searching O
( O
ISS O
) O
is O
designed O
by O
masking O
the O
speaker O
embedding O
of O
a O
speciﬁc O
utterance O
in O
the O
input O
representation O
, O
and O
aims O
to O
predict O
its O
speaker O
given O
the O
conversation O
. O

Since O
the O
set O
of O
interlocutors O
vary O
across O
conversations O
, O
the O
task O
of O
predicting O
the O
speaker O
of O
an O
utterance O
is O
reformulated O
as O
searching O
for O
the O
utterances O
sharing O
the O
identical O
speaker O
. O

First O
, O
for O
a O
speciﬁc O
utterance O
, O
its O
speaker O
embedding O
is O
masked O
with O
a O
special O
[ O
Mask O
] O
interlocutor O
embedding O
to O
avoid O
information O
leakage O
. O

Given O
the O
utterance O
representations O
for O
this O
speciﬁc O
task O
fuiss O
igN O
i=1where O
uiss O
i2Rd O
, O
the O
matching O
scores O
of O
Uiwith O
all O
its O
preceding O
utterances O
are O
calculated O
similarly O
with O
Eq O
. O

( O
1 O
) O
. O

Here O
, O
mijdenotes O
thematching O
degree O
of O
U O
jsharing O
the O
same O
speaker O
with O
U O
i. O

For O
each O
instance O
in O
the O
dynamic O
sampling O
setS O
, O
there O
must O
be O
an O
utterance O
in O
previous O
turns O
sharing O
the O
same O
speaker O
. O

Otherwise O
, O
it O
is O
removed O
out O
of O
the O
set O
. O

Finally O
, O
the O
pre O
- O
training O
objective O
of O
this O
task O
is O
to O
minimize O
the O
cross O
- O
entropy O
loss O
similarly O
with O
Eq O
. O

( O
2 O
) O
. O

Here O
, O
yij= O
1if O
Ujshares O
the O
same O
speaker O
with O
U O
iandyij= O
0otherwise O
. O

3.2.3 O
Pointer O
Consistency O
Distinction O
We O
design O
a O
task O
named O
pointer O
consistency O
distinction O
( O
PCD O
) O
to O
jointly O
model O
speakers O
and O
addressees O
in O
MPC O
. O

In O
this O
task O
, O
a O
pair O
of O
utterances O
representing O
the O
“ O
reply O
- O
to O
” O
relationship O
is O
deﬁned O
as O
a O
speaker O
- O
to O
- O
addressee O
pointer O
. O

Here O
, O
we O
assume O
that O
the O
representations O
of O
two O
pointers O
directing O
from O
the O
same O
speaker O
to O
the O
same O
addressee O
should O
be O
consistent O
. O

As O
illustrated O
in O
Figure O
2 O
( O
a O
) O
, O
speaker O
S O
mspeaks O
U O
iand O
U O
jwhich O
reply O
to O
U O

i0and O

U O
j0from O
speaker O
S O
nrespectively O
. O

Thus O
, O
the O
utterance O
tuples O
( O
U O
i O
, O
Ui0 O
) O
and O
( O
U O
j O
, O
Uj0 O
) O
both O
represent O
the O
pointer O
of O
S O
m O
- O
to O
- O
S O
nand O
their O
pointer O
representations O
should O
be O
consistent O
.. O

Given O
the O
utterance O
representations O
for O
this O
speciﬁc O
taskfupcd O
igN O
i=1where O
upcd O
i2Rd O
, O
we O
ﬁrst O
capture O
the O
pointer O
information O
contained O
in O
each O
utterance O
tuple O
. O

The O
element O
- O
wise O
difference O
and O
multiplication O
between O
an O
utterance O
tuple O
( O
U O
i O
, O
Ui0 O
) O
are O
computed O
and O
are O
concatenated O
as O
pii0= O

[ O
upcd O
i upcd O
i0;upcd O
i O
 O
upcd O
i0 O
] O
; O
( O
3)3686 O
Ui O
   O
Ui O
  O
... O

   O

Uj O
   O
Uj O
Sn O
Sm O
... O
... O
: O
Speaker O
: O
Utterance O
: O
Utterance O
-to O
- O
utterance O
: O
Speaker O
-to O
- O
utterance O
( O
a O
) O
Pointer O
consistency O
distinction O
U1 O
  O
U2 O
  O
U3 O
  O
U5 O
  O
U8 O
U4 O
  O
U6 O
U7 O
U9(b O
) O

Shared O
node O
detection O
Figure O
2 O
: O
Illustrations O
of O
the O
self O
- O
supervised O
tasks O
of O
( O
a O
) O
pointer O
consistency O
distinction O
and O
( O
b O
) O
shared O
node O
detection O
. O

Rectangles O
denote O
utterances O
, O
circles O
denote O
interlocutors O
, O
a O
solid O
line O
denotes O
an O
utterance O
replying O
to O
an O
utterance O
, O
and O
a O
dashed O
line O
denotes O
an O
utterance O
from O
an O
interlocutor O
. O

where O
pii02R2d O
. O

Then O
, O
we O
compress O
pii0and O
obtain O
the O
pointer O
representation O
pii0as O
pii0 O
= O
ReLU O
( O
pii0Wpcd+bpcd O
) O
; O
( O
4 O
) O
where O
Wpcd2R2ddandbpcd2Rdare O
parameters O
. O

Identically O
, O
a O
consistent O
pointer O
representations O
pjj0and O
an O
inconsistent O
one O
pkk0sampled O
from O
this O
conversation O
are O
obtained O
. O

The O
similarities O
between O
every O
two O
pointers O
are O
calculated O
as O
mij O
= O
sigmoid O
( O
p O
> O
ii0Apcdpjj0 O
) O
; O
( O
5 O
) O
where O
mijdenotes O
the O
matching O
degree O
of O
pointer O
pii0being O
consistent O
with O
pointer O
pjj0.mikcan O
be O
derived O
accordingly O
. O

Finally O
, O
the O
pre O
- O
training O
objective O
of O
this O
task O
is O
to O
minimize O
the O
hinge O
loss O
which O
enforces O
mijto O
be O
larger O
than O
mikby O
at O
least O
a O
margin O
as O
Lpcd O
= O
maxf0; mij+mikg O
: O
( O
6 O
) O
3.3 O
Tasks O
of O
Utterance O
Semantics O
Modeling O
Intuitively O
, O
the O
conversation O
structure O
might O
inﬂuence O
the O
information O
ﬂow O
, O
so O
that O
it O
can O
be O
used O
to O
strengthen O
the O
representations O
of O
utterance O
semantics O
. O

Thus O
, O
two O
self O
- O
supervised O
tasks O
following O
the O
structure O
- O
to O
- O
semantics O
manner O
are O
designed O
. O

3.3.1 O
Masked O
Shared O
Utterance O
Restoration O
There O
are O
usually O
several O
utterances O
replying O
- O
to O
a O
shared O
utterance O
in O
MPC O
. O

Intuitively O
, O
a O
shared O
utterance O
is O
semantically O
relevant O
to O
more O
utterances O
in O
the O
context O
than O
non O
- O
shared O
ones O
. O

Based O
on O
this O
characteristic O
, O
we O
design O
a O
task O
named O
masked O
shared O
utterance O
restoration O
( O
MSUR O
) O
. O

We O
ﬁrst O
randomly O
sample O
an O
utterance O
from O
all O
shared O
utterances O
in O
a O
conversation O
and O
all O
tokens O
in O
this O
sampled O
utterance O
are O
masked O
with O
a O
[ O
MASK]token O
. O

Then O
the O
model O
is O
enforced O
to O
restore O
the O
masked O
utterance O
given O
the O
rest O
conversation O
. O

Formally O
, O
assuming O
U O
ias O
the O
masked O
shared O
utterance O
and O
lias O
the O
number O
of O
tokens O
in O
U O
i. O
Given O
the O
token O
representations O
for O
this O
task O
fumsur O
i;tgli O
t=1 O
where O
umsur O
i;t2Rd O
, O
the O
probability O
distribution O
of O
each O
masked O
token O
can O
be O
calculated O
as O
pui;t O
= O
softmax O
( O
umsur O
i;tWmsur O
+ O
bmsur);(7 O
) O
where O
Wmsur2RdVis O
the O
token O
embedding O
table O
, O
Vdenotes O
the O
vocabulary O
size O
, O
and O
bmsur2 O
RVis O
a O
bias O
vector O
. O

Finally O
, O
the O
pre O
- O
training O
objective O
of O
this O
self O
- O
supervised O
task O
is O
to O
minimize O
the O
negative O
log O
- O
likelihood O
loss O
as O
Lmsur O
= O
 1 O

liliX O
t=1log O
p O
ui;t O
; O
( O
8) O
where O
pui;tis O
the O
element O
in O
pui;tcorresponding O
to O
the O
original O
token O
. O

3.3.2 O
Shared O
Node O
Detection O
A O
full O
MPC O
instance O
can O
be O
divided O
into O
several O
sub O
- O
conversations O
and O
we O
assume O
that O
the O
representations O
of O
sub O
- O
conversations O
under O
the O
same O
parent O
node O
tend O
to O
be O
similar O
. O

As O
illustrated O
in O
Figure O
2 O
( O
b O
) O
, O
two O
sub O
- O
conversations O
fU3 O
, O
U5 O
, O
U7 O
, O
U8gandfU4 O
, O
U6 O
, O
U9gshare O
the O
same O
parent O
node O
U2 O
. O

Thus O
, O
they O
should O
be O
semantically O
relevant O
. O

Under O
this O
assumption O
, O
we O
design O
a O
self O
- O
supervised O
task O
named O
shared O
node O
detection O
( O
SND O
) O
, O
which O
utilizes O
the O
conversation O
structure O
to O
strengthen O
the O
capability O
of O
models O
on O
measuring O
the O
semantic O
relevance O
of O
two O
sub O
- O
conversations O
. O

We O
ﬁrst O
construct O
the O
pre O
- O
training O
samples O
for O
this O
task O
. O

Empirically O
, O
only O
the O
sub O
- O
conversations O
under O
the O
top O
shared O
node O
in O
a O
conversation O
are O
collected O
in O
order O
to O
ﬁlter O
out O
the O
sub O
- O
conversations O
with O
few O
utterances O
. O

Given O
a O
full O
MPC O
, O
the O
two O
sub O
- O
conversations O
with O
the O
most O
utterances O
form O
a O
positive O
pair O
. O

For O
each O
positive O
pair O
, O
we O
replace O
one O
of O
its O
elements O
with O
another O
sub O
- O
conversation O
randomly O
sampled O
from O
the O
training O
corpus O
to O
form O
a O
negative O
pair O
. O

Formally O
, O
given O
two O
sub O
- O
conversations O
ciand O
cj O
, O
utterances O
in O
each O
sub O
- O
conversation O
are O
ﬁrst O
concatenated O
respectively O
to O
form O
two O
segments O
. O

Then O
, O
the O
two O
segments O
are O
concatenated O
with O
a O
[ O
SEP O
] O
token O
and O
a O
[ O
CLS O
] O
token O
is O
inserted O
at O
the O
beginning O
of O
the O
whole O
sequence O
. O

This O
sequence O
are O
encoded O
by O
BERT B-MethodName
to O
derive O
the O
contextualized3687representation O
for O
the O
[ O
CLS O
] O
token O
. O

A O
non O
- O
linear O
transformation O
with O
sigmoid O
activation O
is O
further O
applied O
to O
this O
representation O
for O
calculating O
the O
matching O
score O
mij O
, O
i.e. O
, O
the O
probability O
of O
ciand O
cjsharing O
the O
same O
parent O
node O
. O

Finally O
, O
the O
pretraining O
objective O
of O
this O
task O
is O
to O
minimize O
the O
cross O
- O
entropy O
loss O
as O
Lsnd= [yijlog(mij O
) O
+ O
( O
1 yij)log(1 mij O
) O
] O
; O
( O
9 O
) O
where O
yij= O
1 O
ifciandcjshare O
the O
same O
parent O
node O
and O
yij= O
0otherwise O
. O

3.4 O
Multi O
- O
task O
Learning O
In O
addition O
, O
we O
also O
adopt O
the O
tasks O
of O
masked O
language O
model O
( O
MLM O
) O
and O
next O
sentence O
prediction O
( O
NSP O
) O
in O
original O
BERT B-MethodName
pre O
- O
training O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
which O
have O
been O
proven O
effective O
for O
incorporating O
domain O
knowledge O
( O
Gu O
et O
al O
. O
, O
2020 O
; O
Gururangan O
et O
al O
. O
, O
2020 O
) O
. O

Finally O
, O
MPCBERT B-MethodName
is O
trained O
by O
performing O
multi O
- O
task O
learning O
that O
minimizes O
the O
sum O
of O
all O
loss O
functions O
as O
L O
= O
Lrur+Liss+Lpcd+Lmsur O
+ O
Lsnd+Lmlm+Lnsp:(10 O
) O
4 O
Downstream O
Tasks O
4.1 O
Addressee B-TaskName
Recognition I-TaskName
Given O
a O
multi O
- O
party O
conversation O
where O
part O
of O
the O
addressees O
are O
unknown O
, O
Ouchi O
and O
Tsuboi O
( O
2016 O
) O
and O
Zhang O
et O
al O
. O

( O
2018a O
) O
recognized O
an O
addressee O
of O
the O
last O
utterance O
. O

Le O
et O
al O
. O

( O
2019 O
) O
recognized O
addressees O
of O
all O
utterances O
in O
a O
conversation O
. O

In O
this O
paper O
, O
we O
follow O
the O
more O
challenging O
setting O
in O
Le O
et O
al O
. O

( O
2019 O
) O
. O

Formally O
, O
models O
are O
asked O
to O
predict O
f^angN O
n=1 O
givenf(sn O
; O
un O
; O
an)gN O
n=1nfangN O
n=1 O
, O
where O
^anis O
selected O
from O
the O
interlocutor O
set O
in O
this O
conversation O
andndenotes O
exclusion O
. O

When O
applying O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
, O
this O
task O
is O
reformulated O
as O
ﬁnding O
a O
preceding O
utterance O
from O
the O
same O
addressee O
. O

Its O
RUR B-TaskName
matching O
scores O
with O
all O
preceding O
utterances O
are O
calculated O
following O
Eq O
. O

( O
1 O
) O
. O

Then O
, O
the O
utterance O
with O
the O
highest O
score O
is O
selected O
and O
the O
speaker O
of O
the O
selected O
utterance O
is O
considered O
as O
the O
recognized O
addressee O
. O

Finally O
, O
the O
ﬁne O
- O
tuning O
objective O
of O
this O
task O
is O
to O
minimize O
the O
crossentropy O
loss O
as O
Lar= NX O
i=2i 1X O
j=1yijlog(mij O
) O
; O
( O
11)where O
mijis O
deﬁned O
in O
Eq O
. O

( O
1 O
) O
, O
yij= O
1 O
if O
the O
speaker O
of O
U O
jis O
the O
addressee O
of O
U O
iandyij= O
0 O
otherwise O
. O

4.2 O
Speaker B-TaskName
Identiﬁcation I-TaskName
This O
task O
aims O
to O
identify O
the O
speaker O
of O
the O
last O
utterance O
in O
a O
conversation O
. O

Formally O
, O
models O
are O
asked O
to O
predict O
^sNgivenf(sn O
; O
un O
; O
an)gN O
n=1nsN O
, O
where O
^sNis O
selected O
from O
the O
interlocutor O
set O
in O
this O
conversation O
. O

When O
applying O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
, O
this O
task O
is O
reformulated O
as O
identifying O
the O
utterances O
sharing O
the O
same O
speaker O
. O

For O
the O
last O
utterance O
UN O
, O
its O
speaker O
embedding O
is O
masked O
and O
its O
ISS O
matching O
scores O
mNjwith O
all O
preceding O
utterances O
are O
calculated O
following O
Section O
3.2.2 O
. O

The O
ﬁnetuning O
objective O
of O
this O
task O
is O
to O
minimize O
the O
cross O
- O
entropy O
loss O
as O
Lsi= N 1X O
j=1yNjlog(mNj O
) O
; O
( O
12 O
) O
where O
yNj= O
1if O
Ujshares O
the O
same O
speaker O
with O
UNandyNj= O
0otherwise O
. O

4.3 O
Response B-TaskName
Selection I-TaskName
This O
task O
asks O
models O
to O
select O
^uNfrom O
a O
set O
of O
response O
candidates O
given O
the O
conversation O
context O
f(sn O
; O
un O
; O
an)gN O
n=1nuN. O

The O
key O
is O
to O
measure O
the O
similarity O
between O
two O
segments O
of O
context O
and O
response O
. O

We O
concatenate O
each O
response O
candidate O
with O
the O
context O
and O
extract O
the O
contextualized O
representation O
e[CLS O
] O
for O
the O
ﬁrst O
[ O
CLS O
] O
token O
using O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
. O

Then O
, O
e[CLS O
] O
is O
fed O
into O
a O
nonlinear O
transformation O
with O
sigmoid O
activation O
to O
obtain O
the O
matching O
score O
between O
the O
context O
and O
the O
response O
. O

Finally O
, O
the O
ﬁne O
- O
tuning O
objective O
of O
this O
task O
is O
to O
minimize O
the O
cross O
- O
entropy O
loss O
according O
to O
the O
true O
/ O
false O
labels O
of O
responses O
in O
the O
training O
set O
as O
Lrs= [ylog(mcr)+(1 y)log(1 mcr)];(13 O
) O
where O
y= O
1if O
the O
response O
ris O
a O
proper O
one O
for O
the O
context O
c O
; O
otherwise O
y= O
0 O
. O
5 O
Experiments O
5.1 O
Datasets O
We O
evaluated O
our O
proposed O
methods O
on O
two O
Ubuntu B-DatasetName
IRC I-DatasetName
benchmarks O
. O

One O
was O
released O
by O
Hu O
et O
al O
. O
( O
2019 O
) O
, O
in O
which O
both O
speaker O
and O
addressee O
labels O
was O
provided O
for O
each O
utterance O
. O

The O
other O
benchmark O
was O
released O
by O
Ouchi O
and O
Tsuboi3688Datasets O
Train O
Valid O
Test O
Hu O
et O
al O
. O

( O
2019 O
) O
311,725 O
5,000 O
5,000 O
Ouchi O
and O
Tsuboi O
( O
2016)Len-5 O
461,120 O
28,570 O
32,668 O
Len-10 O
495,226 O
30,974 O
35,638 O
Len-15 O
489,812 O
30,815 O
35,385 O
Table O
2 O
: O
Statistics O
of O
the O
two O
benchmarks O
evaluated O
in O
this O
paper O
. O
( O
2016 O
) O
. O

Here O
, O
we O
adopted O
the O
version O
shared O
in O
Le O
et O
al O
. O

( O
2019 O
) O
for O
fair O
comparison O
. O

The O
conversation O
sessions O
were O
separated O
into O
three O
categories O
according O
to O
the O
session O
length O
( O
Len5 O
, O
Len-10 O
and O
Len-15 O
) O
following O
the O
splitting O
strategy O
of O
previous O
studies O
( O
Ouchi O
and O
Tsuboi O
, O
2016 O
; O
Zhang O
et O
al O
. O
, O
2018a O
; O
Le O
et O
al O
. O
, O
2019 O
) O
. O

Table O
2 O
presents O
the O
statistics O
of O
the O
two O
benchmarks O
evaluated O
in O
our O
experiments O
. O

5.2 O
Baseline O
Models O
Non O
- O
pre O
- O
training O
- O
based O
models O
Ouchi O
and O
Tsuboi O
( O
2016 O
) O
proposed O
a O
dynamic O
model O
DRNN B-MethodName
which O
updated O
speaker O
embeddings O
with O
the O
conversation O
ﬂow O
. O

Zhang O

et O
al O
. O

( O
2018a O
) O
improved O
DRNN B-MethodName
to O
SI O
- O
RNN B-MethodName
which O
updated O
speaker O
embeddings O
role O
- O
sensitively O
. O

Le O
et O

al O
. O
( O
2019 O
) O
proposed O
W2W B-MethodName
which O
jointly O
modeled O
interlocutors O
and O
utterances O
in O
a O
uniform O
framework O
, O
and O
predicted O
all O
addressees O
. O

Pre O
- O
training O
- O
based O
models O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
was O
pre O

-trained O
to O
learn O
general O
language O
representations O
with O
MLM O
and O
NSP O
tasks O
. O

SABERT B-MethodName
( O
Gu O
et O
al O
. O
, O
2020 O
) O
added O
speaker O
embeddings O
and O
further O
pre O
- O
trained O
BERT B-MethodName
on O
a O
domain O
- O
speciﬁc O
corpus O
to O
incorporate O
domain O
knowledge O
. O

We O
re O
- O
implemented O
SA B-MethodName
- I-MethodName
BERT I-MethodName
with O
the O
pre O
- O
training O
corpus O
used O
in O
this O
paper O
to O
ensure O
fair O
comparison O
. O

5.3 O
Implementation O
Details O
The O
version O
of O
BERT B-MethodName
- I-MethodName
base I-MethodName
- I-MethodName
uncased I-MethodName
was O
adopted O
for O
all O
our O
experiments O
. O

For O
pre O
- O
training O
, O
GELU O
( O
Hendrycks O
and O
Gimpel O
, O
2016 O
) O
was O
employed O
as O
the O
activation O
for O
all O
non O
- O
linear O
transformations O
. O

The O
Adam B-HyperparameterValue
method O
( O
Kingma O
and O
Ba O
, O
2015 O
) O
was O
employed O
for O
optimization O
. O

The O
learning B-MetricName
rate I-MetricName
was O
initialized O
as O
0.00005 B-HyperparameterValue
and O
the O
warmup B-MetricName
proportion I-MetricName
was O
set O
to O
0.1 B-HyperparameterValue
. O

We O
pre O
- O
trained O
BERT B-MethodName
for O
10 B-HyperparameterValue
epochs B-HyperparameterName
. O

The O
training O
set O
of O
the O
dateset O
used O
in O
Hu O
et O
al O
. O

( O
2019 O
) O
was O
employed O
for O
pre O
- O
training O
. O

The O
maximum B-HyperparameterName
utterance I-HyperparameterName
number I-HyperparameterName
was O
set O
to O
7 B-HyperparameterValue
. O

The O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
was O
set O
to O
230 B-HyperparameterValue
. O

The O
maximum B-HyperparameterName
sampling I-HyperparameterName
numbers I-HyperparameterName
for O
each O
examplewere O
set O
to O
4 B-HyperparameterValue
for O
RUR B-TaskName
, O
2 B-HyperparameterValue
for O
ISS O
and O
2 B-HyperparameterValue
for O
PCD O
. O
in O
Eq O
. O

( O
6 O
) O
was O
set O
to O
0.4 B-HyperparameterValue
, O
achieving O
the O
best O
performance O
out O
of O
f0.2 B-MetricValue
, O
0.4 B-MetricValue
, O
0.6 B-MetricValue
, O
0.8gon B-MetricValue
the O
validation O
set O
. O

The O
pre O
- O
training O
was O
performed O
using O
a O
GeForce O
RTX O
2080 O

Ti O
GPU O
and O
the O
batch B-HyperparameterName
size I-HyperparameterName
was O
set O
to O
4 B-HyperparameterValue
. O

For O
ﬁne O
- O
tuning O
, O
some O
conﬁgurations O
were O
different O
according O
to O
the O
characteristics O
of O
these O
datasets O
. O

For O
Hu O
et O
al O
. O

( O
2019 O
) O
, O
the O
maximum B-HyperparameterName
utterance I-HyperparameterName
number I-HyperparameterName
was O
set O
to O
7 B-HyperparameterValue
and O
the O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
was O
set O
to O
230 B-HyperparameterValue
. O

For O
the O
three O
experimental O
settings O
in O
Ouchi O
and O
Tsuboi O
( O
2016 O
) O
, O
the O
maximum B-HyperparameterName
utterance I-HyperparameterName
numbers I-HyperparameterName
were O
set O
to O
5 B-HyperparameterValue
, O
10 B-HyperparameterValue
and O
15 B-HyperparameterValue
, O
and O
the O
maximum B-HyperparameterName
sequence I-HyperparameterName
lengths I-HyperparameterName
were O
set O
to O
120 B-HyperparameterValue
, O
220 B-HyperparameterValue
and O
320 B-HyperparameterValue
. O

All O
parameters O
in O
PLMs O
were O
updated O
. O

The O
learning B-HyperparameterName
rate I-HyperparameterName
was O
initialized O
as O
0.00002 B-HyperparameterValue
and O
the O
warmup B-HyperparameterName
proportion I-HyperparameterName
was O
set O
to O
0.1 B-HyperparameterValue
. O

For O
Hu O
et O
al O
. O

( O
2019 O
) O
, O
the O
ﬁne O
- O
tuning O
process O
was O
performed O
for O
10 B-HyperparameterValue
epochs B-HyperparameterName
for O
addressee B-TaskName
recognition I-TaskName
, O
10 B-HyperparameterValue
epochs B-HyperparameterName
for O
speaker B-TaskName
identiﬁcation I-TaskName
, O
and O
5 B-HyperparameterValue
epochs B-HyperparameterName
for O
response B-TaskName
selection I-TaskName
. O

For O
Ouchi O
and O
Tsuboi O
( O
2016 O
) O
, O
the O
ﬁne O
- O
tuning O
epochs B-HyperparameterName
were O
set O
to O
5 B-HyperparameterValue
, O
5 B-HyperparameterValue
and O
3 B-HyperparameterValue
respectively O
. O

The O
ﬁne O
- O
tuning O
was O
also O
performed O
using O
a O
GeForce O
RTX O
2080 O

Ti O
GPU O
. O

The O
batch B-HyperparameterName
sizes I-HyperparameterName
were O
set O
to O
16 B-HyperparameterValue
for O
Hu O
et O
al O
. O

( O
2019 O
) O
, O
and O
40 B-HyperparameterValue
, O
20 B-HyperparameterValue
, O
and O
12 B-HyperparameterValue
for O
the O
three O
experimental O
settings O
in O
Ouchi O
and O
Tsuboi O
( O
2016 O
) O
respectively O
. O

The O
validation O
set O
was O
used O
to O
select O
the O
best O
model O
for O
testing O
. O

All O
codes O
were O
implemented O
in O
the O
TensorFlow O
framework O
( O
Abadi O
et O
al O
. O
, O
2016 O
) O
and O
are O
published O
to O
help O
replicate O
our O
results.1 O
5.4 O
Metrics O
and O
Results O
Addressee B-TaskName
recognition I-TaskName
We O
followed O
the O
metrics O
of O
previous O
work O
( O
Le O
et O
al O
. O
, O
2019 O
) O
by O
employing O
precision@1 B-MetricName
( O
P@1 B-MetricName
) O
to O
evaluate O
each O
utterance O
with O
ground O
truth O
. O

Also O
, O
a O
session O
is O
marked O
as O
positive O
if O
the O
addressees O
of O
all O
its O
utterances O
are O
correctly O
recognized O
, O
which O
is O
calculated O
as O
accuracy B-MetricName
( O
Acc B-MetricName
. O
) O
. O

Table O
3 O
presents O
the O
results O
of O
addressee B-TaskName
recognition I-TaskName
. O

It O
shows O
that O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
outperforms O
the O
best O
performing O
model O
, O
i.e. O
, O
SA B-MethodName
- I-MethodName
BERT I-MethodName
, O
by O
margins O
of O
3.51 B-MetricValue
% I-MetricValue
, O
2.86 B-MetricValue
% I-MetricValue
, O
3.28 B-MetricValue
% I-MetricValue
and O
5.36 B-MetricValue
% I-MetricValue
on O
these O
test O
sets O
respectively O
in O
terms O
of O
Acc B-MetricName
. O
, O
verifying O
the O
effectiveness O
of O
the O
proposed O
ﬁve O
selfsupervised O
tasks O
as O
a O
whole O
. O

To O
further O
illustrate O
the O
effectiveness O
of O
each O
task O
, O
ablation O
tests O
were O
performed O
as O
shown O
in O
the O
last O
ﬁve O
rows O
of O
Table O
3 O
. O

We O
can O
observe O
that O
all O
self O
- O
supervised O
tasks O
are O
useful O
as O
removing O
any O
of O
them O
causes O
performance O
1https://github.com/JasonForJoy/MPC-BERT3689Hu O
et O
al O
. O

( O
2019 O
) O
Ouchi O
and O
Tsuboi O
( O
2016 O
) O

Len-5 O
Len-10 O
Len-15 O
P@1 O
Acc O
. O
P@1 O

Acc O
. O
P@1 O

Acc O
. O
P@1 O

Acc O
. O

Preceding O
( O
Le O
et O
al O
. O
, O
2019 O
) O
- O
- O
63.50 O
40.46 O
56.84 O
21.06 O
54.97 O
13.08 O
Subsequent O
( O
Le O
et O
al O
. O
, O
2019 O
) O
- O
- O
61.03 O
40.25 O
54.57 O
20.26 O
53.07 O
12.79 O
DRNN O
( O
Ouchi O
and O
Tsuboi O
, O
2016 O
) O
- O
- O
72.75 O
58.18 O
65.58 O
34.47 O
62.60 O
22.58 O
SIRNN O
( O
Zhang O
et O
al O
. O
, O
2018a O
) O
- O
- O
75.98 O
62.06 O
70.88 O
40.66 O
68.13 O
28.05 O
W2W O

( O

Le O
et O
al O
. O
, O
2019 O
) O
- O
- O
77.55 O
63.81 O
73.52 O
44.14 O
73.42 O
34.23 O
BERT B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
96.16 O
83.50 O
85.95 O
75.99 O
83.41 O
58.22 O
81.09 O
44.94 O
SA O
- O
BERT B-MethodName
( O
Gu O
et O
al O
. O
, O
2020 O
) O
97.12 O
88.91 O
86.81 O
77.45 O
84.46 O
60.30 O
82.84 O
47.23 O
MPC O
- O
BERT B-MethodName
98.31 O
92.42 O
88.73 O
80.31 O
86.23 O
63.58 O
85.55 O
52.59 O
MPC O
- O
BERT B-MethodName
w/o O
. O

RUR B-TaskName
97.75 O
89.98 O
87.51 O
78.42 O
85.63 O
62.26 O
84.78 O
50.83 O
MPC O
- O
BERT B-MethodName
w/o O
. O

ISS O
98.20 O
91.96 O
88.67 O
80.25 O
86.14 O
63.40 O
85.02 O
51.12 O
MPC O
- O
BERT B-MethodName
w/o O
. O

PCD O
98.20 O
91.90 O
88.51 O
80.06 O
85.92 O
62.84 O
85.21 O
51.17 O
MPC O
- O
BERT B-MethodName
w/o O
. O

MSUR O
98.08 O
91.32 O
88.70 O
80.26 O
86.21 O
63.46 O
85.28 O
51.23 O
MPC O
- O
BERT B-MethodName
w/o O
. O

SND O
98.25 O
92.18 O
88.68 O
80.25 O
86.14 O
63.41 O
85.29 O
51.39 O
Table O
3 O
: O
Evaluation O
results O
of O
addressee O
recognition O
on O
the O
test O
sets O
. O

Results O
except O
ours O
are O
cited O
from O
Le O
et O
al O
. O
( O
2019 O
) O
. O

Numbers O
in O
bold O
denote O
that O
the O
improvement O
over O
the O
best O
performing O
baseline O
is O
statistically O
signiﬁcant O
( O
t O
- O
test O
with O
p O
- O
value O
< O
0.05 O
) O
. O

Hu O
et O

al O
. O

( O
2019 O
) O
Ouchi O
and O
Tsuboi O
( O
2016 O
) O

Len-5 O
Len-10 O
Len-15 O
BERT B-MethodName

( O
Devlin O
et O
al O
. O
, O
2019 O
) O
71.81 O
62.24 O
53.17 O
51.58 O
SA O
- O
BERT B-MethodName
( O
Gu O
et O
al O
. O
, O
2020 O
) O
75.88 O
64.96 O
57.62 O
54.28 O
MPC O
- O
BERT B-MethodName
83.54 O
67.56 O
61.00 O
58.52 O
MPC O
- O
BERT B-MethodName
w/o O
. O

RUR B-TaskName
82.48 O
66.88 O
60.12 O
57.33 O
MPC O
- O
BERT B-MethodName
w/o O
. O

ISS O
77.95 O
66.77 O
60.03 O
56.73 O
MPC O
- O
BERT B-MethodName
w/o O
. O

PCD O
83.39 O
67.12 O
60.62 O
58.00 O
MPC O
- O
BERT B-MethodName
w/o O
. O

MSUR O
83.51 O
67.21 O
60.76 O
58.03 O
MPC O
- O
BERT B-MethodName
w/o O
. O

SND O
83.47 O
67.04 O
60.44 O
58.12 O
Table O
4 O
: O
Evaluation O
results O
of O
speaker O
identiﬁcation O
on O
the O
test O
sets O
in O
terms O
of O
P@1 B-MetricName
. O

Numbers O
in O
bold O
denote O
that O
the O
improvement O
over O
the O
best O
performing O
baseline O
is O
statistically O
signiﬁcant O
( O
t O
- O
test O
with O
p O
- O
value O
< O
0.05 O
) O
. O
drop O
. O

Among O
the O
ﬁve O
tasks O
, O
RUR B-TaskName
plays O
the O
most O
important O
role O
, O
and O
the O
tasks O
focusing O
on O
modeling O
interlocutor O
structure O
contribute O
more O
than O
those O
for O
utterance O
semantics O
. O

Speaker B-TaskName
identiﬁcation I-TaskName
Similarly O
, O
P@1 B-MetricName
was O
employed O
as O
the O
evaluation O
metric O
of O
speaker B-TaskName
identiﬁcation I-TaskName
for O
the O
last O
utterance O
of O
a O
conversation O
and O
the O
results O
are O
shown O
in O
Table O
4 O
. O

It O
shows O
that O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
outperforms O
SA B-MethodName
- I-MethodName
BERT I-MethodName
by O
margins O
of O
7.66 B-MetricValue
% I-MetricValue
, O
2.60 B-MetricValue
% I-MetricValue
, O
3.38 B-MetricValue
% I-MetricValue
and O
4.24 B-MetricValue
% I-MetricValue
respectively O
in O
terms O
of O
P@1 B-MetricName
. O

Besides O
, O
from O
the O
ablation O
results O
we O
ﬁnd O
that O
all O
tasks O
are O
useful O
for O
improving O
the O
performance O
of O
speaker B-TaskName
identiﬁcation I-TaskName
and O
ISS O
and O
RUR B-TaskName
contribute O
the O
most O
. O

In O
particular O
, O
removing O
PCD O
, O
MSUR O
and O
SND O
only O
leads O
to O
slight O
performance O
drop O
. O

The O
reason O
might O
bethat O
the O
information O
conveyed O
by O
these O
tasks O
is O
redundant O
. O

Response B-TaskName
selection I-TaskName
The O
R B-MetricName
n@kmetrics I-MetricName
adopted O
by O
previous O
studies O
( O
Ouchi O
and O
Tsuboi O
, O
2016 O
; O
Zhang O
et O
al O
. O
, O
2018a O
) O
were O
used O
here O
. O

Each O
model O
was O
tasked O
with O
selecting O
kbest O
- O
matched O
responses O
fromnavailable O
candidates O
, O
and O
we O
calculated O
the O
recall O
as O
R B-MetricName
n@k I-MetricName
. O

Two O
settings O
were O
followed O
in O
which O
kwas O
set O
to O
1 B-MetricValue
and O
nwas O
set O
to O
2 B-MetricValue
or O
10 B-MetricValue
. O

Table O
5 O
presents O
the O
results O
of O
response B-TaskName
selection I-TaskName
. O

It O
shows O
that O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
outperforms O
SABERT B-MethodName
by O
margins O
of O
3.82 B-MetricValue
% I-MetricValue
, O
2.71 B-MetricValue
% I-MetricValue
, O
2.55 B-MetricValue
% I-MetricValue
and O
3.22 B-MetricValue
% I-MetricValue
respectively O
in O
terms O
of O
R B-MetricName
10@1 I-MetricName
. O

Ablation O
tests O
show O
that O
SND B-TaskName
is O
the O
most O
useful O
task O
for O
response B-TaskName
selection I-TaskName
and O
the O
two O
tasks O
focusing O
on O
the O
utterance O
semantics O
contribute O
more O
than O
those3690Hu O
et O
al O
. O

( O
2019 O
) O
Ouchi O
and O
Tsuboi O
( O
2016 O
) O

Len-5 O
Len-10 O
Len-15 O
R2@1 O
R10@1 O
R2@1 O

R10@1 O

R2@1 O

R10@1 O

R2@1 O

R10@1 O
DRNN O
( O
Ouchi O
and O
Tsuboi O
, O
2016 O
) O
- O
- O
76.07 O
33.62 O
78.16 O
36.14 O
78.64 O
36.93 O
SIRNN O
( O
Zhang O
et O
al O
. O
, O
2018a O
) O
- O
- O
78.14 O
36.45 O
80.34 O
39.20 O
80.91 O
40.83 O
BERT B-MethodName
( O
Devlin O
et O

al O
. O
, O
2019 O
) O
92.48 O
73.42 O
85.52 O
53.95 O
86.93 O
57.41 O
87.19 O
58.92 O
SA O
- O
BERT B-MethodName
( O
Gu O
et O
al O
. O
, O
2020 O
) O
92.98 O
75.16 O
86.53 O
55.24 O
87.98 O
59.27 O
88.34 O
60.42 O
MPC O
- O
BERT B-MethodName
94.90 O
78.98 O
87.63 O
57.95 O
89.14 O
61.82 O
89.70 O
63.64 O
MPC O
- O
BERT B-MethodName
w/o O
. O

RUR B-TaskName
94.48 O
78.16 O
87.20 O
57.56 O
88.96 O
61.47 O
89.07 O
63.24 O
MPC O
- O
BERT B-MethodName
w/o O
. O

ISS O
94.58 O
78.82 O
87.54 O
57.77 O
88.98 O
61.76 O
89.58 O
63.51 O
MPC O
- O
BERT B-MethodName
w/o O
. O

PCD O
94.66 O
78.70 O
87.50 O
57.51 O
88.75 O
61.62 O
89.45 O
63.46 O
MPC O
- O
BERT B-MethodName
w/o O
. O

MSUR O
94.36 O
78.22 O
87.11 O
57.58 O
88.59 O
61.05 O
89.25 O
63.20 O
MPC O
- O
BERT B-MethodName
w/o O
. O

SND O
93.92 O
76.96 O
87.30 O
57.54 O
88.77 O
61.54 O
89.27 O
63.34 O
Table O
5 O
: O
Evaluation O
results O
of O
response O
selection O
on O
the O
test O
sets O
. O

Results O
except O
ours O
are O
cited O
from O
Ouchi O
and O
Tsuboi O
( O
2016 O
) O
and O
Zhang O
et O
al O
. O
( O
2018a O
) O
. O

Numbers O
in O
bold O
denote O
that O
the O
improvement O
over O
the O
best O
performing O
baseline O
is O
statistically O
signiﬁcant O
( O
t O
- O
test O
with O
p O
- O
value O
< O
0.05 O
) O
. O

5 O
10 O
15 O
Length50607080Session O
Accuracy B-MetricName
BERT B-MethodName
SA B-MethodName
- I-MethodName
BERT I-MethodName
MPC B-MethodName
- I-MethodName
BERT I-MethodName
( O
a O
) O
Addressee B-TaskName
recognition I-TaskName
5 O
10 O
15 O
Length556065Utterance O
Preision O
BERT B-MethodName
SA B-MethodName
- I-MethodName
BERT I-MethodName
MPC B-MethodName
- I-MethodName
BERT I-MethodName
( O
b O
) O
Speaker B-TaskName
identiﬁcation I-TaskName
5 O
10 O
15 O
Length545658606264Response O
Recall O
BERT B-MethodName
SA B-MethodName
- I-MethodName
BERT I-MethodName
MPC B-MethodName
- I-MethodName
BERT I-MethodName
( O
c O
) O
Response B-TaskName
selection I-TaskName
Figure O
3 O
: O
Performance O
of O
models O
under O
different O
session O
lengths O
on O
the O
test O
sets O
of O
Ouchi O
and O
Tsuboi O
( O
2016 O
) O
on O
the O
tasks O
of O
( O
a O
) O
addressee B-TaskName
recognition I-TaskName
, O
( O
b O
) O
speaker B-TaskName
identiﬁcation I-TaskName
and O
( O
c O
) O
response B-TaskName
selection I-TaskName
. O

focusing O
on O
the O
interlocutor O
structures O
. O

5.5 O
Discussions O
Figure O
3 O
illustrates O
how O
the O
performance O
of O
BERT B-MethodName
, O
SA B-MethodName
- I-MethodName
BERT I-MethodName
and O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
changed O
with O
respect O
to O
different O
session O
lengths O
on O
the O
test O
sets O
of O
Ouchi O
and O
Tsuboi O
( O
2016 O
) O
. O

It O
can O
be O
seen O
that O
the O
performance O
of O
addressee B-TaskName
recognition I-TaskName
and O
speaker B-TaskName
identiﬁcation I-TaskName
dropped O
as O
the O
session O
length O
increased O
. O

The O
reason O
might O
be O
that O
longer O
sessions O
always O
contain O
more O
interlocutors O
which O
increase O
the O
difﬁculties O
of O
predicting O
interlocutors O
. O

Meanwhile O
, O
the O
performance O
of O
response B-TaskName
selection I-TaskName
was O
signiﬁcantly O
improved O
as O
the O
session O
length O
increased O
. O

It O
can O
be O
attributed O
to O
that O
longer O
sessions O
enrich O
the O
representations O
of O
contexts O
with O
more O
details O
which O
beneﬁt O
response O
selection O
. O

Furthermore O
, O
as O
the O
session O
length O
increased O
, O
the O
performance O
of O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
dropped O
more O
slightly O
than O
that O
of O
SA B-MethodName
- I-MethodName
BERT I-MethodName
on O
addressee B-TaskName
recognition I-TaskName
andspeaker B-TaskName
identiﬁcation I-TaskName
, O
and O
the O
R B-MetricName
10@1gap I-MetricName
between O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
and O
SA B-MethodName
- I-MethodName
BERT I-MethodName
on O
response B-TaskName
selection I-TaskName
enlarged O
from O
2.71 B-MetricValue
% I-MetricValue
to O
3.22 B-MetricValue
% I-MetricValue
. O

These O
results O
imply O
the O
superiority O
of O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
over O
SA B-MethodName
- I-MethodName
BERT I-MethodName
on O
modeling O
long O
MPCs O
with O
complicated O
structures O
. O

6 O
Conclusion O
In O
this O
paper O
, O
we O
present O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
, O
a O
pre O
- O
trained O
language O
model O
with O
ﬁve O
self O
- O
supervised O
tasks O
for O
MPC O
understanding O
. O

These O
tasks O
jointly O
learn O
who O
says O
what O
towhom O
in O
MPCs O
. O

Experimental O
results O
on O
three O
downstream O
tasks O
show O
that O
MPC B-MethodName
- I-MethodName
BERT I-MethodName
outperforms O
previous O
methods O
by O
large O
margins O
and O
achieves O
new O
state O
- O
of O
- O
the O
- O
art O
performance O
on O
two O
benchmarks O
. O

Acknowledgments O
We O
thank O
anonymous O
reviewers O
for O
their O
valuable O
comments.3691References O
Mart O
´ O
ın O
Abadi O
, O
Paul O
Barham O
, O
Jianmin O
Chen O
, O
Zhifeng O
Chen O
, O
Andy O
Davis O
, O
Jeffrey O
Dean O
, O
Matthieu O
Devin O
, O
Sanjay O
Ghemawat O
, O
Geoffrey O
Irving O
, O
Michael O
Isard O
, O
Manjunath O
Kudlur O
, O
Josh O
Levenberg O
, O
Rajat O
Monga O
, O
Sherry O
Moore O
, O
Derek O
Gordon O
Murray O
, O
Benoit O
Steiner O
, O
Paul O
A. O
Tucker O
, O
Vijay O
Vasudevan O
, O
Pete O
Warden O
, O
Martin O
Wicke O
, O
Yuan O
Yu O
, O
and O
Xiaoqiang O
Zheng O
. O
2016 O
. O

Tensorﬂow O
: O
A O
system O
for O
large O
- O
scale O
machine O
learning O
. O

In O
12th O
USENIX O
Symposium O
on O
Operating O
Systems O
Design O
and O
Implementation O
, O
OSDI O
2016 O
, O
Savannah O
, O
GA O
, O
USA O
, O
November O
2 O
- O
4 O
, O
2016 O
. O
, O
pages O
265–283 O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

BERT B-MethodName
: O
pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
NAACL O
- O
HLT O
2019 O
, O
Minneapolis O
, O
MN O
, O
USA O
, O
June O
2 O
- O
7 O
, O
2019 O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
. O
Jia O
- O
Chen O
Gu O
, O
Tianda O
Li O
, O
Quan O
Liu O
, O
Zhen O
- O
Hua O
Ling O
, O
Zhiming O
Su O
, O
Si O
Wei O
, O
and O
Xiaodan O
Zhu O
. O
2020 O
. O

Speaker O
- O
aware O
BERT B-MethodName
for O
multi O
- O
turn O
response O
selection O
in O
retrieval O
- O
based O
chatbots O
. O

In O
CIKM O
’ O
20 O
: O
The O
29th O
ACM O
International O
Conference O
on O
Information O
and O
Knowledge O
Management O
, O
Virtual O
Event O
, O
Ireland O
, O
October O
19 O
- O
23 O
, O
2020 O
, O
pages O
2041 O
– O
2044 O
. O

Jia O
- O
Chen O
Gu O
, O
Zhen O
- O
Hua O
Ling O
, O
and O
Quan O
Liu O
. O
2019a O
. O

Interactive O
matching O
network O
for O
multi O
- O
turn O
response O
selection O
in O
retrieval O
- O
based O
chatbots O
. O

In O
Proceedings O
of O
the O
28th O
ACM O
International O
Conference O
on O
Information O
and O
Knowledge O
Management O
, O
CIKM O
2019 O
, O
Beijing O
, O
China O
, O
November O
3 O
- O
7 O
, O
2019 O
, O
pages O
2321–2324 O
. O
Jia O
- O
Chen O
Gu O
, O
Zhen O
- O
Hua O
Ling O
, O
Xiaodan O
Zhu O
, O
and O
Quan O
Liu O
. O

2019b O
. O

Dually O
interactive O
matching O
network O
for O
personalized O
response O
selection O
in O
retrieval O
- O
based O
chatbots O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
EMNLP O
- O
IJCNLP O
2019 O
, O
Hong O
Kong O
, O
China O
, O
November O
3 O
- O
7 O
, O
2019 O
, O
pages O
1845–1854 O
. O

Association O
for O
Computational O
Linguistics O
. O

Suchin O
Gururangan O
, O
Ana O
Marasovic O
, O
Swabha O
Swayamdipta O
, O
Kyle O
Lo O
, O
Iz O
Beltagy O
, O
Doug O
Downey O
, O
and O
Noah O
A. O
Smith O
. O

2020 O
. O

Do O
n’t O
stop O
pretraining O
: O
Adapt O
language O
models O
to O
domains O
and O
tasks O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
ACL O
2020 O
, O
Online O
, O
July O
5 O
- O
10 O
, O
2020 O
, O
pages O
8342–8360 O
. O

Dan O
Hendrycks O
and O
Kevin O
Gimpel O
. O

2016 O
. O

Bridging O
nonlinearities O
and O
stochastic O
regularizers O
with O
gaussian O
error O
linear O
units O
. O

CoRR O
, O
abs/1606.08415.Wenpeng O
Hu O
, O
Zhangming O
Chan O
, O
Bing O
Liu O
, O
Dongyan O
Zhao O
, O
Jinwen O
Ma O
, O
and O
Rui O
Yan O
. O
2019 O
. O

GSN O
: O
A O
graph O
- O
structured O
network O
for O
multi O
- O
party O
dialogues O
. O

InProceedings O
of O
the O
Twenty O
- O
Eighth O
International O
Joint O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
IJCAI O
2019 O
, O
Macao O
, O
China O
, O
August O
10 O
- O
16 O
, O
2019 O
, O
pages O
5010–5016 O
. O

Diederik O
P. O
Kingma O
and O
Jimmy O
Ba O
. O
2015 O
. O

Adam O
: O
A O
method O
for O
stochastic O
optimization O
. O

In O
3rd O
International O
Conference O
on O
Learning O
Representations O
, O
ICLR O
2015 O
, O
San O
Diego O
, O
CA O
, O
USA O
, O
May O
7 O
- O
9 O
, O
2015 O
, O
Conference O
Track O
Proceedings O
. O

Ran O
Le O
, O
Wenpeng O
Hu O
, O
Mingyue O
Shang O
, O
Zhenjun O
You O
, O
Lidong O
Bing O
, O
Dongyan O
Zhao O
, O
and O
Rui O
Yan O
. O
2019 O
. O

Who O
is O
speaking O
to O
whom O
? O

learning O
to O
identify O
utterance O
addressee O
in O
multi O
- O
party O
conversations O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
and O
the O
9th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
, O
EMNLP O
- O
IJCNLP O
2019 O
, O
Hong O
Kong O
, O
China O
, O
November O
3 O
- O
7 O
, O
2019 O
, O
pages O
1909 O
– O
1919 O
. O

Ryan O
Lowe O
, O
Nissan O
Pow O
, O
Iulian O
Serban O
, O
and O
Joelle O
Pineau O
. O
2015 O
. O

The O
ubuntu O
dialogue O
corpus O
: O
A O
large O
dataset O
for O
research O
in O
unstructured O
multi O
- O
turn O
dialogue O
systems O
. O

In O
Proceedings O
of O
the O
SIGDIAL O
2015 O
Conference O
, O
The O
16th O
Annual O
Meeting O
of O
the O
Special O
Interest O
Group O
on O
Discourse O
and O
Dialogue O
, O
2 O
- O
4 O
September O
2015 O
, O
Prague O
, O
Czech O
Republic O
, O
pages O
285–294 O
. O

Zhao O
Meng O
, O
Lili O
Mou O
, O
and O
Zhi O
Jin O
. O

2018 O
. O

Towards O
neural O
speaker O
modeling O
in O
multi O
- O
party O
conversation O
: O
The O
task O
, O
dataset O
, O
and O
models O
. O

In O
Proceedings O
of O
the O
Eleventh O
International O
Conference O
on O
Language O
Resources O
and O
Evaluation O
, O
LREC O
2018 O
, O
Miyazaki O
, O
Japan O
, O
May O
7 O
- O
12 O
, O
2018 O
. O

European O
Language O
Resources O
Association O
( O
ELRA O
) O
. O

Hiroki O
Ouchi O
and O
Yuta O
Tsuboi O
. O

2016 O
. O

Addressee O
and O
response O
selection O
for O
multi O
- O
party O
conversation O
. O

In O
Proceedings O
of O
the O
2016 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
EMNLP O
2016 O
, O
Austin O
, O
Texas O
, O
USA O
, O
November O
1 O
- O
4 O
, O
2016 O
, O
pages O
2133–2143 O
. O
Iulian O
Vlad O
Serban O
, O
Alessandro O
Sordoni O
, O
Yoshua O
Bengio O
, O
Aaron O
C. O
Courville O
, O
and O
Joelle O
Pineau O
. O
2016 O
. O

Building O
end O
- O
to O
- O
end O
dialogue O
systems O
using O
generative O
hierarchical O
neural O
network O
models O
. O

In O
Proceedings O
of O
the O
Thirtieth O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
February O
12 O
- O
17 O
, O
2016 O
, O
Phoenix O
, O
Arizona O
, O
USA O
, O
pages O
3776–3784 O
. O
Iulian O
Vlad O
Serban O
, O
Alessandro O
Sordoni O
, O
Ryan O
Lowe O
, O
Laurent O
Charlin O
, O
Joelle O
Pineau O
, O
Aaron O
C. O
Courville O
, O
and O
Yoshua O
Bengio O
. O
2017 O
. O

A O
hierarchical O
latent O
variable O
encoder O
- O
decoder O
model O
for O
generating O
dialogues O
. O

In O
Proceedings O
of O
the O
Thirty O
- O
First O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
February O
4 O
- O
9 O
, O
2017 O
, O
San O
Francisco O
, O
California O
, O
USA O
, O
pages O
3295 O
– O
3301 O
. O

AAAI O
Press.3692Lifeng O
Shang O
, O
Zhengdong O
Lu O
, O
and O
Hang O
Li O
. O
2015 O
. O

Neural O
responding O
machine O
for O
short O
- O
text O
conversation O
. O

In O
Proceedings O
of O
the O
53rd O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
7th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
of O
the O
Asian O
Federation O
of O
Natural O
Language O
Processing O
, O
ACL O
2015 O
, O
July O
2631 O
, O
2015 O
, O
Beijing O
, O
China O
, O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
1577–1586 O
. O

Chongyang O
Tao O
, O
Wei O
Wu O
, O
Can O
Xu O
, O
Wenpeng O
Hu O
, O
Dongyan O
Zhao O
, O
and O
Rui O
Yan O
. O
2019a O
. O

Multirepresentation O
fusion O
network O
for O
multi O
- O
turn O
response O
selection O
in O
retrieval O
- O
based O
chatbots O
. O

In O
Proceedings O
of O
the O
Twelfth O
ACM O
International O
Conference O
on O
Web O
Search O
and O
Data O
Mining O
, O
WSDM O
2019 O
, O
Melbourne O
, O
VIC O
, O
Australia O
, O
February O
11 O
- O
15 O
, O
2019 O
, O
pages O
267–275 O
. O
ACM O
. O

Chongyang O
Tao O
, O
Wei O
Wu O
, O
Can O
Xu O
, O
Wenpeng O
Hu O
, O
Dongyan O
Zhao O
, O
and O
Rui O
Yan O
. O
2019b O
. O

One O
time O
of O
interaction O
may O
not O
be O
enough O
: O
Go O
deep O
with O
an O
interaction O
- O
over O
- O
interaction O
network O
for O
response O
selection O
in O
dialogues O
. O

In O
Proceedings O
of O
the O
57th O
Conference O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
ACL O
2019 O
, O
Florence O
, O
Italy O
, O
July O
28August O
2 O
, O
2019 O
, O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
1 O
– O
11 O
. O

Weishi O
Wang O
, O
Steven O
C. O
H. O
Hoi O
, O
and O
Shaﬁq O
R. O
Joty O
. O

2020 O
. O

Response O
selection O
for O
multi O
- O
party O
conversations O
with O
dynamic O
topic O
tracking O
. O

In O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
EMNLP O
2020 O
, O
Online O
, O
November O
16 O
- O
20 O
, O
2020 O
, O
pages O
6581 O
– O
6591 O
. O

Yu O
Wu O
, O
Wei O
Wu O
, O
Chen O
Xing O
, O
Ming O
Zhou O
, O
and O
Zhoujun O
Li O
. O
2017 O
. O

Sequential O
matching O
network O
: O
A O
new O
architecture O
for O
multi O
- O
turn O
response O
selection O
in O
retrieval O
- O
based O
chatbots O
. O

In O
Proceedings O
of O
the O
55th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
ACL O
2017 O
, O
Vancouver O
, O
Canada O
, O
July O
30 O
- O
August O
4 O
, O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
496–505 O
. O

Rui O
Zhang O
, O
Honglak O
Lee O
, O
Lazaros O
Polymenakos O
, O
and O
Dragomir O
R. O
Radev O
. O

2018a O
. O

Addressee O
and O
response O
selection O
in O
multi O
- O
party O
conversations O
with O
speaker O
interaction O
rnns O
. O

In O
Proceedings O
of O
the O
ThirtySecond O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
( O
AAAI-18 O
) O
, O
the O
30th O
innovative O
Applications O
of O
Artiﬁcial O
Intelligence O
( O
IAAI-18 O
) O
, O
and O
the O
8th O
AAAI O
Symposium O
on O
Educational O
Advances O
in O
Artiﬁcial O
Intelligence O
( O
EAAI-18 O
) O
, O
New O
Orleans O
, O
Louisiana O
, O
USA O
, O
February O
2 O
- O
7 O
, O
2018 O
, O
pages O
5690–5697 O
. O

Yizhe O
Zhang O
, O
Michel O
Galley O
, O
Jianfeng O
Gao O
, O
Zhe O
Gan O
, O
Xiujun O
Li O
, O
Chris O
Brockett O
, O
and O
Bill O
Dolan O
. O

2018b O
. O

Generating O
informative O
and O
diverse O
conversational O
responses O
via O
adversarial O
information O
maximization O
. O

InAdvances O
in O
Neural O
Information O
Processing O
Systems O
31 O
: O
Annual O
Conference O
on O
Neural O
Information O
Processing O
Systems O
2018 O
, O
NeurIPS O
2018 O
, O
December O
3 O
- O
8 O
, O
2018 O
, O
Montr O
´ O
eal O
, O
Canada O
, O
pages O
1815–1825.Yizhe O
Zhang O
, O
Siqi O
Sun O
, O
Michel O
Galley O
, O
Yen O
- O
Chun O
Chen O
, O
Chris O
Brockett O
, O
Xiang O
Gao O
, O
Jianfeng O
Gao O
, O
Jingjing O
Liu O
, O
and O
Bill O
Dolan O
. O

2020 O
. O

DIALOGPT O
: O
Large O
- O
scale O
generative O
pre O
- O
training O
for O
conversational O
response O
generation O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
System O
Demonstrations O
, O
ACL O
2020 O
, O
Online O
, O
July O
5 O
- O
10 O
, O
2020 O
, O
pages O
270–278 O
. O

Association O
for O
Computational O
Linguistics O
. O

Xiangyang O
Zhou O
, O
Lu O
Li O
, O
Daxiang O
Dong O
, O
Yi O
Liu O
, O
Ying O
Chen O
, O
Wayne O
Xin O
Zhao O
, O
Dianhai O
Yu O
, O
and O
Hua O
Wu O
. O

2018 O
. O

Multi O
- O
turn O
response O
selection O
for O
chatbots O
with O
deep O
attention O
matching O
network O
. O

In O
Proceedings O
of O
the O
56th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
ACL O
2018 O
, O
Melbourne O
, O
Australia O
, O
July O
15 O
- O
20 O
, O
2018 O
, O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
1118–1127 O
. O


Proceedings O
of O
the O
60th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
921 O
- O
931 O
May O
22 O
- O
27 O
, O
2022 O
c O

 O
2022 O
Association O
for O
Computational O
Linguistics O
RoCBert B-MethodName
: O
Robust B-MethodName
Chinese I-MethodName
Bert I-MethodName
with O
Multimodal O
Contrastive O
Pretraining O
Hui O
Su1 O
, O
Weiwei O
Shi1 O
, O
Xiaoyu O
Shen2 O
Xiao O
Zhou1 O
, O
Tuo O
Ji1 O
, O
Jiarui O
Fang1 O
, O
and O
Jie O
Zhou1 O
1Pattern O
Recognition O
Center O
, O
Wechat O
AI O
, O
Tencent O
Inc O
, O
China O
2Saarland O
Informatics O
Campus O
aaronsu@tencent.com O
Abstract O
Large O
- O
scale O
pretrained O
language O
models O
have O
achieved O
SOTA O
results O
on O
NLP O
tasks O
. O

However O
, O
they O
have O
been O
shown O
vulnerable O
to O
adversarial O
attacks O
especially O
for O
logographic O
languages O
like O
Chinese O
. O

In O
this O
work O
, O
we O
propose O
ROCBERT B-MethodName
: O
a O
pretrained O
Chinese O
Bert O
that O
is O
robust O
to O
various O
forms O
of O
adversarial O
attacks O
like O
word O
perturbation O
, O
synonyms O
, O
typos O
, O
etc O
. O

It O
is O
pretrained O
with O
the O
contrastive O
learning O
objective O
which O
maximizes O
the O
label O
consistency O
under O
different O
synthesized O
adversarial O
examples O
. O

The O
model O
takes O
as O
input O
multimodal O
information O
including O
the O
semantic O
, O
phonetic O
and O
visual O
features O
. O

We O
show O
all O
these O
features O
are O
important O
to O
the O
model O
robustness O
since O
the O
attack O
can O
be O
performed O
in O
all O
the O
three O
forms O
. O

Across O
5 O
Chinese O
NLU B-TaskName
tasks O
, O
R O
OCBERT O
outperforms O
strong O
baselines O
under O
three O
blackbox O
adversarial O
algorithms O
without O
sacriﬁcing O
the O
performance O
on O
clean O
testset O
. O

It O
also O
performs O
the O
best O
in O
the O
toxic B-TaskName
content I-TaskName
detection I-TaskName
task O
under O
human O
- O
made O
attacks O
. O

1 O
Introduction O
Large O
- O
scale O
pretrained O
models O
, O
by O
ﬁnetuning O
on O
sufﬁcient O
annotated O
data O
, O
have O
been O
able O
to O
approach O
or O
even O
surpass O
human O
performance O
on O
many O
benchmark O
testsets O
( O
Peters O
et O
al O
. O
, O
2018 O
; O
Radford O
et O
al O
. O
; O
Devlin O
et O
al O
. O
, O
2019 O
; O
Liu O
et O
al O
. O
, O
2019 O
; O
Brown O
et O
al O
. O
, O
2020 O
) O
. O

However O
, O
even O
pretrained O
with O
huge O
amounts O
of O
text O
, O
the O
models O
are O
still O
vulnerable O
under O
adversarial O
attacks O
like O
synonyms O
, O
word O
deletion O
/ O
swapping O
, O
misspelling O
, O
etc O
( O
Li O
et O
al O
. O
, O
2019a O
; O

Jin O
et O
al O
. O
, O
2020 O
; O
Sun O
et O
al O
. O
, O
2020a O
; O
Eger O
and O
Benz O
, O
2020 O
) O
. O

These O
adversarial O
examples O
occur O
frequently O
in O
the O
real O
- O
world O
scenario O
and O
can O
be O
made O
either O
naturally O
( O
e.g. O
, O
typos O
) O
or O
maliciously O
( O
e.g. O
, O
to O
avoid O
auto O
detection O
of O
toxic O
content)1 O
. O

Equal O
contribution O
. O

1The O
concept O
of O
adversarial O
examples O
is O
quite O
wide O
. O

In O
this O
paper O
, O
we O
focus O
on O
adversarial O
examples O
that O
do O
NOT O
change O
the O
original O
semantics O
( O
Mozes O
et O
al O
. O
, O
2021)Attacker O
Text O
phonetic O
克(kè)比的精神值得永远学习 O
visual O
科此的精神值得永远学习 O
character O
split O
禾斗匕匕的精神值得永远学习 O
synonym O
我(wˇo)科(k¯e)的精神值得永远学习 O
synonym O
+ O
phonetic O
蜗(w¯o)壳(ké)的精神值得永远学习 O
to O
pinyin O
kebi O
的精神值得永远学习 O
to O
pinyin O
+ O
unicode O
keb1 O
的精神值得学习永远 O
swap O
科比的精神值得永远习学 O
insertion O
科比的精神九值得永远学习 O
deletion O
科比的神值得永远学习 O
Original O
: O
科(k¯e)比(bˇı)的精神值得永远学习 O
Translation O
: O
Kobe O
’s O
spirit O
is O
worth O
studying O
forever O
. O
Table O
1 O
: O
Examples O
of O
various O
attackers O
. O

Contents O
in O
the O
brackets O
are O
corresponding O
pinyins O
of O
Chinese O
characters O
. O

The O
lack O
of O
robustness O
with O
them O
can O
easily O
lead O
to O
large O
performance O
drop O
when O
testing O
in O
the O
noisy O
real O
- O
world O
trafﬁc O
. O

The O
issue O
is O
particularly O
outstanding O
for O
logographic O
languages O
like O
Chinese O
since O
the O
attack O
can O
be O
either O
with O
the O
glyph O
character O
, O
pinyin O
( O
the O
romanized O
phonetic O
representations O
) O
or O
a O
combination O
of O
them O
( O
Wang O
et O
al O
. O
, O
2020 O
; O

Li O
et O
al O
. O
, O
2020d O
; O
Zhang O
et O
al O
. O
, O
2020 O
; O
Nuo O
et O
al O
. O
, O
2020 O
) O
. O

We O
show O
some O
examples O
in O
Table O
1 O
. O

The O
word O
“ O
科 O
比(Kobe O
) O
” O
can O
be O
replaced O
with O
synonyms O
, O
phonetically O
or O
visually O
similar O
words O
. O

The O
attacker O
can O
also O
replace O
the O
character O
with O
its O
pinyin O
then O
continue O
the O
attack O
in O
the O
alphabet O
- O
level O
( O
“ O
keb1 O
” O
in O
the O
table O
) O
. O

The O
isolation O
of O
semantics O
and O
phonetics O
, O
and O
the O
rich O
set O
of O
glyph O
characters O
in O
written O
Chinese O
makes O
the O
attacking O
forms O
much O
more O
diverse O
than O
in O
alphabetic O
languages O
like O
English O
. O

Current O
research O
works O
usually O
adopt O
two O
ways O
to O
defend O
adversarial O
attacks O
: O
( O
1 O
) O
Run O
spell O
checking O
to O
correct O
the O
written O
errors O
before O
feeding O
to O
the O
prediction O
model O
( O
Pruthi O
et O
al O
. O
, O
2019 O
; O
Li O
et O

al O
. O
, O
2020b O
; O
Mozes O
et O
al O
. O
, O
2021 O
) O
, O
and O
( O
2 O
) O
Adversarial O
training O
, O
which O
adds O
adversarial O
example O
to O
the O
training O
data O
( O
Zang O
et O
al O
. O
, O
2020 O
; O
Li O
et O
al O
. O
, O
2020a O
; O
Liu O
et O
al O
. O
, O
2020 O
) O
. O

For O
the O
former O
, O
Chinese O
spell O
checking O
itself O
is O
even O
a O
more O
difﬁcult O
task O
because O
it O
requires O
the O
model O
to O
accurately O
recover O
the O
original O
text O
. O

Any O
tiny O
errors O
of O
the O
spell O
checking O
can921lead O
to O
unpredicted O
model O
behaviors O
. O

For O
the O
latter O
, O
it O
is O
hard O
for O
the O
model O
to O
adapt O
to O
all O
adversarial O
variants O
only O
in O
the O
ﬁnetuning O
stage O
, O
especially O
when O
the O
training O
data O
is O
sparse O
( O
Meng O
et O
al O
. O
, O
2021 O
) O
. O

To O
address O
the O
above O
challenges O
, O
we O
propose O
ROCBERT B-MethodName
, O
aRobust B-MethodName
Chinese I-MethodName
BERT I-MethodName
pretrained O
with O
the O
contrastive O
learning O
objective O
by O
maximizing O
the O
label O
consistency O
under O
various O
adversarial O
examples O
. O

The O
adversarial O
examples O
are O
synthesized O
from O
an O
algorithm O
that O
encapsulates O
common O
types O
of O
attacks O
. O

We O
also O
consider O
combinatorial O
attacks O
where O
multiple O
types O
of O
attacks O
can O
be O
added O
on O
top O
of O
each O
other O
, O
which O
has O
never O
been O
considered O
in O
previous O
research O
. O

To O
defend O
attacks O
in O
all O
levels O
, O
we O
incorporate O
multimodal O
information O
into O
the O
encoder O
. O

The O
phonetic O
and O
visual O
features O
are O
inserted O
into O
one O
self O
- O
attention O
layer O
then O
dynamically O
fused O
in O
later O
layers O
. O

Across O
5 O
standard O
NLU B-TaskName
tasks O
and O
one O
toxic B-TaskName
content I-TaskName
detection I-TaskName
task O
, O
we O
show O
the O
pretrained O
model O
achieves O
new O
SOTAs O
under O
various O
adversarial O
attackers O
. O

In O
short O
, O
our O
contribution O
are O
( O
1 O
) O
We O
propose O
pretraining O
a O
robust O
Chinese O
Bert B-MethodName
with O
adversarial O
contrastive O
learning O
, O
such O
that O
the O
model O
can O
perform O
well O
on O
not O
only O
clean O
testbeds O
, O
but O
also O
adversarial O
examples O
. O

( O
2 O
) O
The O
model O
is O
pretrained O
with O
synthesized O
adversarial O
examples O
covering O
combinations O
of O
semantic O
, O
phonetic O
and O
visual O
attacks O
. O

It O
takes O
as O
input O
multimodal O
features O
to O
handle O
all O
levels O
of O
possible O
attacks O
. O

( O
3 O
) O
The O
pretrained O
model O
outperforms O
strong O
baselines O
across O
5 O
NLU B-TaskName
tasks O
and O
1 O
toxic B-TaskName
content I-TaskName
detection I-TaskName
task O
under O
various O
adversarial O
attackers O
. O

( O
4 O
) O
We O
perform O
an O
extensive O
ablation O
studies O
for O
pretraining O
options O
and O
have O
a O
wide O
comparison O
with O
popular O
defending O
methods O
, O
which O
we O
hope O
will O
beneﬁt O
future O
research O
. O

2 O
Related O
Work O
Adversarial O
attack O
There O
have O
been O
a O
lot O
of O
works O
showing O
the O
vulnerability O
of O
NLP O
models O
under O
adversarial O
examples O
( O
Li O
et O
al O
. O
, O
2020c O
; O
Garg O
and O
Ramakrishnan O
, O
2020 O
; O
Zang O
et O
al O
. O
, O
2020 O
) O
, O
which O
are O
understandable O
by O
humans O
yet O
lead O
to O
significant O
model O
prediction O
drops O
. O

There O
are O
usually O
two O
types O
of O
attacks O
: O
( O
1 O
) O
semantic O
equivalent O
replacement O
, O
which O
can O
be O
synthesized O
by O
replacing O
words O
based O
on O
vector O
similarity O
( O
Jin O
et O
al O
. O
, O
2020 O
; O
Wang O
et O

al O
. O
, O
2020 O
) O
, O
WordNet O
synonyms O
( O
Zang O
et O
al O
. O
, O
2020 O
) O
, O
masked O
prediction O
from O
pretrained O
models O
( O
Li O
et O

al O
. O
, O
2020c O
; O
Garg O
and O
Ramakrishnan O
, O
2020 O
; O
Li O
et O
al O
. O
, O
2020d O
) O
, O
etc O
. O
( O
2 O
) O
noise O
injection O
, O
whichcan O
be O
synthesized O
by O
adding O
/ O
deleting O
/ O
swapping O
words O
( O
Li O
et O
al O
. O
, O
2019a O
; O
Gil O
et O
al O
. O
, O
2019 O
; O
Sun O
et O
al O
. O
, O
2020a O
) O
, O
replacing O
words O
with O
phonetically O
or O
visually O
similar O
ones O
( O
Eger O
et O
al O
. O
, O
2019 O
; O
Eger O
and O
Benz O
, O
2020 O
) O
. O

For O
logographic O
languages O
like O
Chinese O
, O
the O
noise O
can O
be O
much O
more O
complex O
as O
it O
can O
be O
injected O
on O
both O
the O
glyph O
characters O
or O
romanized O
pinyins O
( O
Zhang O
et O
al O
. O
, O
2020 O
; O

Nuo O
et O
al O
. O
, O
2020 O
) O
. O

Adversarial O
defense O

The O
most O
common O
way O
of O
adversarial O
defense O
is O
adversarial O
training O
, O
which O
simply O
appends O
synthesized O
adversarial O
examples O
into O
the O
training O
data O
( O
Zang O
et O
al O
. O
, O
2020 O
; O
Li O
et O

al O
. O
, O
2020a O
) O

. O

Nonetheless O
, O
it O
relies O
only O
on O
the O
limited O
labeled O
training O
data O
. O

In O
contrast O
, O
the O
proposed O
ROCBERTis O
pretrained O
on O
billions O
of O
text O
and O
can O
better O
adapted O
to O
diverse O
adversarial O
variants O
. O

Another O
popular O
way O
is O
to O
ﬁrst O
remove O
the O
noise O
with O
off O
- O
the O
- O
shelf O
spell O
checkers O
, O
then O
feed O
the O
corrected O
text O
into O
the O
model O
( O
Li O
et O
al O
. O
, O
2020b O
) O
. O

However O
, O
Chinese O
spell O
checking O
requires O
fully O
recovering O
the O
correct O
text O
and O
current O
model O
performances O
are O
far O
from O
satisfactory O
( O
Liu O
et O
al O
. O
, O
2021 O
; O
Xu O
et O

al O
. O
, O
2021 O
; O
Wang O
et O

al O
. O
, O
2021a O
) O
. O

Any O
tiny O
error O
in O
the O
spell O
checking O
process O
can O
lead O
to O
unpredicted O
model O
behaviors O
. O

It O
also O
incurs O
signiﬁcant O
latency O
to O
model O
prediction O
. O

ROCBERTdoes O
not O
add O
additional O
latency O
and O
can O
perform O
well O
even O
if O
fully O
recovery O
is O
difﬁcult O
due O
to O
its O
consistency O
- O
maximization O
pretraining O
objective O
. O

There O
have O
also O
been O
works O
on O
pretraining O
more O
robust O
models O
through O
virtual O
adversarial O
training O
and O
noise O
regularization O
( O
Yoo O
and O
Qi O
, O
2021 O
; O
Wang O
et O
al O
. O
, O
2021b O
; O
Meng O
et O
al O
. O
, O
2021 O
) O
, O
but O
they O
perform O
poorly O
on O
man O
- O
made O
attacks O
. O

3 O
Adversarial O
Example O
Synthesis O
3.1 O
Attacking O
Chinese O
Characters O
As O
we O
focus O
on O
Chinese O
in O
this O
paper O
and O
Chinese O
characters O
are O
much O
more O
diverse O
than O
in O
alphabetical O
languages O
, O
we O
design O
the O
following O
5 O
Chinesespeciﬁc O
attacking O
algorithms O
ﬁrst O
. O

phonetic O
: O
Replace O
a O
Chinese O
character O
with O
a O
random O
homonym O
( O
ignoring O
diacritics O
) O
. O

For O
polyphones O
, O
we O
consider O
the O
2 O
most O
common O
pinyins2 O
. O

Visual O
: O
Replace O
Chinese O
characters O
with O
their O
visually O
similar O
characters O
( O
with O
the O
similarity O
table O
in O
the O
Kanji O
Database O
Project)3 O
. O

Character O
Split O
: O
Split O
one O
character O
into O
two O
parts O
with O
every O
part O
still O
being O
( O
or O
visually O
similar O
to O
) O
a O
valid O
Chinese O
character O
. O

We O
follow O
the O
Chinese O
2https://unicode.org/charts/unihan.html O
3http://kanji-database.sourceforge.net/922Figure O
1 O
: O
Adversarial O
example O
synthesis O
process O
. O

splitting O
dictionary4 O
, O
which O
contains O
17,803 O
splitting O
ways O
for O
Chinese O
characters O
in O
total O
. O
Synonym O
: O
Segment O
Chinese O
characters O
into O
words O
with O
the O
jieba O
tokenizer5 O
, O
then O
randomly O
replace O
the O
word O
with O
one O
of O
its O
synonyms O
. O

Two O
words O
are O
treated O
as O
synonym O
if O
they O
share O
a O
similarity O
score O
of O
over O
0.756 O
. O

We O
only O
replace O
adjectives O
or O
nouns O
as O
we O
ﬁnd O
other O
words O
can O
be O
hardly O
replaced O
without O
changing O
the O
semantics O
. O

Character O
to O
Pinyin O
: O
Replaces O
the O
character O
into O
its O
pinyin O
representation O
( O
without O
diacritics O
) O
. O

3.2 O
Attacking O
Other O
Characters O
Apart O
from O
Chinese O
characters O
, O
there O
are O
often O
other O
characters O
like O
the O
pinyin O
, O
numbers O
, O
punctuations O
and O
foreign O
words O
in O
the O
Chinese O
corpus O
. O

The O
following O
4 O
types O
of O
attacks O
apply O
to O
not O
only O
Chinese O
characters O
, O
but O
also O
all O
other O
characters O
. O

Unicode O
: O
Randomly O
sample O
one O
of O
the O
visually O
similar O
unicodes O
as O
a O
replacement7 O
. O

Random O
Insertion O
: O
Sample O
one O
character O
from O
the O
vocabulary O
set O
, O
then O
randomly O
insert O
the O
character O
to O
the O
left O
or O
right O
of O
the O
current O
character O
. O

Swap O
: O

Swap O
the O
character O
with O
its O
neighbor O
. O

Deletion O
: O

Delete O
the O
character O
directly O
. O

Examples O
of O
all O
types O
of O
attacks O
are O
in O
Table O
1 O
. O
3.3 O
Synthesis O
Process O
The O
synthesis O
process O
of O
adversarial O
examples O
is O
as O
follow O
: O
Given O
one O
sentence O
, O
we O
ﬁrst O
select O
several O
4https://github.com/kfcd/chaizi O
5https://github.com/fxsjy/jieba O
6https://github.com/chatopera/Synonyms O
7http://www.unicode.org/Public/security/revision03/confusablesSummary.txtcharacters O
to O
attack O
. O

For O
each O
selected O
character O
, O
we O
then O
combine O
the O
above O
mentioned O
characterlevel O
attacking O
algorithms8to O
get O
its O
attacked O
form O
. O

Attack O
Ratio O
: O

The O
attack O
ratio O

 O
decides O
how O
many O
characters O
we O
will O
attack O
. O

Let O
ncbe O
the O
number O
of O
characters O
in O
the O
sentence O
, O
we O
deﬁne O

 O
as O
: O

 O
= O
min(max O
( O
int();1);nc O
) O
N(max(1;0:15nc);1)(1 O
) O
where O
the O
intfunction O
rounds O
into O
the O
closest O
integer O
. O

The O
intuition O
is O
that O
we O
want O
to O
attack O
15 O
% O
of O
the O
characters O
on O
average9 O
. O

If O
the O
sentence O
is O
short O
, O
we O
will O
make O
sure O
to O
attack O
at O
least O
one O
character O
. O

We O
insert O
normal O
Gaussian O
noise O
on O
top O
of O
the O
average O
ratio O
to O
add O
some O
randomness O
. O

Character O
Selection O
: O
There O
have O
been O
many O
research O
works O
showing O
that O
attacking O
informative O
words O
is O
more O
effective O
than O
random O
words O
( O
Li O
et O
al O
. O
, O
2019a O
; O
Sun O
et O
al O
. O
, O
2020a O
) O
. O

Therefore O
, O
we O
decide O
the O
chance O
of O
one O
character O
cibeing O
selected O
based O
on O
its O
informativeness O
in O
the O
sentence O
. O

Let O
w(ci)denote O
the O
word O
cibelongs O
to O
, O
the O
informative O
score O
for O
ciis O
counted O
as O
the O
difference O
of O
the O
language O
model O
loss O
after O
deleting O
w(ci)(denoted O
asL(Ow(ci O
) O
) O

( O
Li O
et O
al O
. O
, O
2016)10 O
. O

The O
chance O
that O
ciwill O
be O
selected O
to O
be O
attacked O
is O
: O
p(ci O
) O
= O
eL(Ow(ci O
) O
) O
jw(ci)jPnw O
j=1eL(Owj)(2 O
) O

wherenwis O
the O
number O
of O
words O
in O
the O
sentence O
. O

jw(ci)jmeans O
the O
number O
of O
characters O
in O
w(ci O
) O
such O
that O
characters O
in O
the O
same O
word O
have O
equal O
chances O
to O
be O
selected O
. O

Attack O
Combination O
: O
There O
can O
be O
combinations O
of O
attacks O
for O
one O
character O
. O

For O
example O
, O
we O
can O
transfer O
one O
Chinese O
character O
into O
its O
pinyin O
then O
continue O
to O
attack O
it O
in O
the O
alphabet O
level O
( O
“ O
to O
pinyin O
+ O
unicode O
” O
in O
Table O
1 O
) O
. O

We O
deﬁne O
it O
as O
a O
sequential O
process O
where O
a O
new O
attack O
can O
be O
added O
on O
top O
at O
each O
step O
. O

Speciﬁcally O
, O
the O
new O
character O
~cafter O
all O
the O
attack O
combinations O
applied O
to O
cis O
: O
~c O
= O
AS(c)A2A1(c O
) O
p(S(c O
) O
= O
k O
) O
= O
q(1 q)k 1(3 O
) O
8For O
synonym O
replacement O
which O
applies O
in O
the O
word O
level O
, O
we O
apply O
it O
on O
the O
word O
that O
the O
selected O
character O
belongs O
to O
. O

9The O
ratio O
is O
chosen O
by O
manual O
annotation O
. O

15 O
% O
is O
the O
highest O
ratio O
we O
can O
attack O
without O
hurting O
human O
reading O
. O

10We O
use O
ChineseGPT O
( O
Zhang O
et O
al O
. O
, O
2021 O
) O
as O
the O
language O
model O
, O
so O
“ O
word O
” O
here O
means O
the O
subword O
token O
deﬁned O
in O
the O
vocabulary O
of O
ChineseGPT.923whermeans O
applying O
a O
new O
attacking O
algorithm O
Ato O
the O
output O
of O
the O
last O
step O
. O

At O
each O
step O
i O
, O
the O
attacking O
algorithm O
Aiis O
randomly O
selected O
from O
all O
algorithms O
that O
are O
applicable O
to O
the O
output O
from O
stepi 1.S(c)is O
the O
number O
of O
attacking O
steps O
applied O
to O
c O
, O
which O
follows O
an O
exponentially O
decay O
function O
. O

We O
set O
q= O
0:7empirically O
. O

The O
full O
process O
of O
adversarial O
example O
synthesis O
is O
illustreated O
in O
Figure O
1 O
. O
4 O
Multimodal O
Contrastive O
Pretraining O
With O
the O
above O
- O
mentioned O
algorithm O
to O
sample O
adversarial O
examples O
, O
we O
can O
pretrain O
the O
model O
with O
the O
multimodal O
contrastive O
learning O
objective O
. O

4.1 O
Multimodal O
Features O
We O
follow O
the O
standard O
Bert B-MethodName
architecture O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
as O
our O
backbone O
, O
based O
on O
which O
we O
integrate O
phonetic O
and O
visual O
features O
for O
input O
text O
. O

Feature O
Representation O
: O
For O
every O
character O
c O
in O
our O
vocabulary O
, O
apart O
from O
the O
standard O
semantic O
embeddingSe(c O
) O
, O
we O
include O
two O
more O
vectors O
Ph(c)andVi(c)to O
encode O
its O
phonetic O
and O
visual O
features O
respectively O
. O

If O
cis O
not O
a O
Chinese O
character O
, O
it O
has O
its O
own O
phonetic O
vector O
. O

Otherwise O
, O
Ph(c O
) O

= O
P O
k2pinyin O
( O
c)Ph(k)wherepinyin O
( O
c)is O
its O
pinyin O
sequence O
. O
Vi(c)is O

extracted O
from O
its O
3232image O
I(c O
) O
. O

The O
image O
is O
in O
simsun O
( O
宋体 O
) O
for O
Chinese O
characters O
and O
arial O
for O
others O
, O
the O
default O
fonts O
for O
most O
online O
text O
. O

Vi(c)is O

deﬁned O
as O
: O
Vi(c O
) O

= O
LayerNorm O
( O
MTResNet O
18(I(c O
) O
) O
) O

( O
4 O
) O
M O
is O
a O
learnable O
matrix O
and O
we O
utilize O
Resnet18 O
( O
He O
et O
al O
. O
, O
2016 O
) O
to O
map O
I(c)into O
a O
onedimentional O
vector O
( O
freezed O
during O
training O
) O
. O

Visual O
Representation O
Pretrain O
: O
To O
get O
an O
reasonable O
initialization O
, O
we O
add O
another O
pretraining O
stage O
only O
for O
the O
visual O
representation O
. O

Phonetic O
representations O
are O
randomly O
initialized11.Min O
Eq O
4 O
is O
pretrained O
with O
the O
same O
contrastive O
loss O
as O
in O
Eq O
5 O
. O

The O
positive O
sample O
for O
the O
charactercis O
its O
visually O
adversarial O
form O
~c O
= O
A(c O
) O
. O

A O
 O
U(visual O
, O
character O
split O
, O
unicode O
) O
, O
which O
means O
uniform O
sampling O
from O
the O
three O
visual O
attacking O
algorithms O
mentioned O
in O
§ O
3 O
. O

If O
cis O
split O
into O
two O
characters O
c1andc2 O
, O
we O
sum O
the O
visual O
representation O
of O
the O
two O
split O
characters O
Vi(~c O
) O
= O
Vi(c1 O
) O

+ O
Vi(c2 O
) O
. O

The O
negative O
samples O
11We O
show O
in O
Section O
5.3 O
that O
pretraining O
is O
necessary O
for O
visual O
features O
not O
but O
for O
phonetic O
features.are O
all O
other O
characters O
in O
the O
same O
batch O
. O

After O
training O
, O
visually O
similar O
characters O
will O
be O
close O
in O
their O
representation O
space O
. O

Feature O
Integration O
: O
A O
straightforward O
way O
to O
integrate O
these O
multimodal O
features O
is O
to O
fuse O
them O
before O
fed O
to O
the O
encoder O
( O
Sun O
et O
al O
. O
, O
2021 O
; O

Liu O
et O
al O
. O
, O
2021 O
) O
. O

However O
, O
three O
features O
will O
be O
given O
equal O
weights O
and O
the O
model O
can O
not O
dynamically O
attend O
to O
only O
useful O
features O
. O

Another O
way O
is O
a O
twostep O
encoding O
which O
ﬁrst O
decides O
the O
weight O
, O
then O
encode O
with O
selective O
attention O
( O
Xu O
et O
al O
. O
, O
2021 O
) O
, O
but O
it O
will O
signiﬁcantly O
slow O
down O
the O
system O
. O

We O
propose O
a O
lightweight O
fusion O
method O
layer O
- O
insert O
, O
which O
insert O
multimodal O
features O
in O
only O
one O
encoder O
layer O
. O

Let O
Hk(i)denote O
the O
representation O
of O
theith O
word O
in O
the O
kth O
layer O
, O
we O
insert O
by O
: O
W1 O
= O
KT O
1Hk(i)Hk(i)V1 O
W2 O
= O
KT O
2Hk(i)Ph(i)V2 O
W3 O
= O
KT O
3Hk(i)Vi(i)V3 O
Hk(i O
) O
= O
W1Hk(i O
) O
+ O
W2Ph(i O
) O

+ O
W3Vi(i O
) O

W1+W2+W3 O
wherePh(i)andVi(i)are O
the O
phonetic O
and O
visual O
representations O
and O
Kj O
= O
Vjare O
learnable O
matrices O
. O

Intuitively O
we O
can O
use O
the O
layer O
0tok 1to O

decide O
the O
weights O
of O
three O
multimodal O
representations O
and O
use O
the O
rest O
layers O
for O
sentence O
representation O
learning O
. O

It O
allows O
dynamic O
fusion O
according O
to O
sentence O
context O
yet O
adds O
marginal O
complexity O
. O

4.2 O
Model O
Loss O
The O
model O
loss O
has O
two O
components O
: O
the O
contrastive O
learning O
loss O
and O
the O
standard O
masked O
language O
model O
( O
MLM O
) O
loss O
. O

Contrastive O
Learning O
: O
The O
idea O
of O
contrastive O
learning O
( O
Chen O
et O
al O
. O
, O
2020 O
; O
Kim O
et O
al O
. O
, O
2021 O
) O
is O
that O
the O
representation O
space O
should O
be O
made O
closer O
for O
similar O
( O
positive O
) O
samples O
and O
farther O
for O
dissimilar O
( O
negative O
) O
samples O
. O

For O
each O
sentence O
, O
we O
treat O
its O
adversarial O
form O
( O
obtained O
from O
the O
algorithm O
in O
§ O
3 O
) O
as O
positive O
and O
all O
the O
other O
sentences O
in O
the O
same O
batch O
as O
negative O
. O

Given O
a O
batch O
with O
N O
sentences O
, O
the O
loss O
to O
the O
ith O
sentencesiis O
: O
Lc(i O
) O

= O
 logesim(si;~si)= O
  O
PN O
j=1esim(si;sj)= O
 O
; O
( O
5 O
) O
where O
 O
is O
a O
temperature O
hyperparameter O
and O
~siis O
the O
adversarial O
example O
synthesized O
from O
si O
. O

We O
set O
 O
= O
0:01based O
on O
our O
pilot O
experiments O
and924deﬁnesim(si;~si)ash O
> O
i O
~ O
hi O
khikk O
~ O
hik O
, O
which O
is O
the O
cosine O
similarity O
in O
their O
representation O
space O
hiand O
~ O
hi O
. O

Mix O
with O
MLM O
: O
We O
mix O
the O
contrastive O
learning O
loss O
with O
the O
standard O
masked O
language O
model O
( O
MLM O
) O
loss O
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
to O
enable O
both O
sentence O
and O
word O
level O
representation O
learning O
. O

We O
use O
a O
character O
- O
based O
tokenizer O
because O
( O
1 O
) O
Chinese O
characters O
as O
themselves O
stand O
for O
individual O
semantic O
units O
( O
Li O
et O
al O
. O
, O
2019b O
) O
and O
( O
2 O
) O
char O
- O
based O
models O
are O
much O
more O
robust O
under O
noisy O
and O
adversarial O
scenarios O
( O
El O
Boukkouri O
et O
al O
. O
, O
2020 O
) O
. O

For O
Chinese O
characters O
, O
we O
use O
two O
masking O
strategies O
– O
Whole O
Word O
Masking O
( O
WWM O
) O
and O
Char O
Masking O
( O
CM O
) O
because O
a O
large O
number O
of O
words O
in O
Chinese O
consist O
of O
multiple O
characters O
( O
Cui O
et O
al O
. O
, O
2019 O
; O
Sun O
et O
al O
. O
, O
2021 O
) O
. O

The O
contrastive O
learning O
loss O
and O
the O
MLM O
loss O
are O
equally O
weighted O
. O

5 O
Experiments O
5.1 O

Experiment O
Setup O
Model O
Details O
We O
use O
a O
vocabulary O
size O
of O
16224 O
, O
out O
of O
which O
14642 O
are O
Chinese O
characters O
. O

We O
provide O
two O
versions O
of O
ROCBERT B-MethodName
: O
base O
and O
large O
. O

The O
base O
version O
has O
12 B-HyperparameterValue
layers B-HyperparameterName
/ O
heads O
with O
768 B-HyperparameterValue
hidden B-HyperparameterName
neurons I-HyperparameterName
. O

It O
is O
trained O
for O
600k B-HyperparameterValue
steps B-HyperparameterName
with O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
4k B-HyperparameterValue
, O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-4 B-HyperparameterValue
and O
warmup B-HyperparameterName
rate I-HyperparameterName
of O
25k B-HyperparameterValue
steps O
. O

The O
large O
version O
has O
48 B-HyperparameterValue
layers B-HyperparameterName
and O
24 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
with O
1024 B-HyperparameterValue
hidden B-HyperparameterName
neurons I-HyperparameterName
. O

It O
is O
trained O
for O
500 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
3e-4 B-HyperparameterValue
, O
warmup B-HyperparameterName
of O
70 B-HyperparameterValue
K I-HyperparameterValue
steps O
and O
batch B-HyperparameterName
size I-HyperparameterName
of O
8k B-HyperparameterValue
. O

Pretraining O
Details O
Following O
the O
common O
practice O
, O
we O
pretrain O
our O
model O
on O
2 O
TB O
text O
extracted O
from O
a O
mixture O
of O
THUCTC12 B-DatasetName
, O
Chinese B-DatasetName
Wikipedia I-DatasetName
and O
Common B-DatasetName
Crawl I-DatasetName
. O

Models O
are O
trained O
on O
64 O
NVIDIA O
V100 O
( O
32 O
GB O
) O
GPUs O
with O
FP16 O
and O
ZERO O
- O
stage-1 O
optimization O
( O
Rasley O
et O
al O
. O
, O
2020 O
) O
. O

To O
make O
better O
use O
of O
the O
GPU O
, O
we O
train O
our O
model O
with O
PatricStar13which O
applies O
a O
dynamic O
memory O
scheduling O
with O
a O
chunk O
- O
based O
memory O
management O
module O
( O
Fang O
et O
al O
. O
, O
2021 O
) O
. O

The O
memory O
management O
ofﬂoads O
everything O
but O
the O
current O
computing O
part O
of O
the O
model O
to O
CPUs O
. O

This O
results O
in O
training O
a O
much O
larger O
model O
within O
the O
same O
hardware O
environment O
. O

The O
chunk O
- O
based O
memory O
management O
takes O
advantage O
of O
the O
linear O
structure O
of O
the O
transformer O
- O
based O
model O
, O
so O
that O
it O
will O
inherently O
prefetch O
the O
upcoming O
layers O
to O
GPUs O
. O

12https://github.com/thunlp/THUCTC O
13https://github.com/Tencent/PatrickStarBaseline O
Models O
We O
compare O
our O
model O
with O
SOTA O
pretrained O
Chinese O
models O
: O
( O
1 O
) O
MBertChinese B-MethodName
( O
Devlin O
et O
al O
. O
, O
2019 O
) O
, O
( O
2 O
) O
Bert B-MethodName
- I-MethodName
wwm I-MethodName
( O
Cui O
et O
al O
. O
, O
2019 O
) O
, O
( O
3 O
) O
MacBert B-MethodName
( O
Cui O
et O
al O
. O
, O
2020 O
) O
, O
( O
4 O
) O
Ernie B-MethodName
- I-MethodName
gram I-MethodName
( O
Sun O
et O
al O
. O
, O
2019 O
, O
2020b O
) O
and O
( O
5 O
) O
ChineseBert B-MethodName
( O
Sun O
et O
al O
. O
, O
2021 O
) O
. O

BERT B-MethodName
- I-MethodName
wwm I-MethodName
continues O
pretraining O
from O
MBert B-MethodName
- I-MethodName
Chinese I-MethodName
with O
the O
Whole O
Word O
Masking O
pretraining O
strategy O
. O

MacBERT B-MethodName
applies O
the O
MLM O
- O
As O
- O
Correlation O
( O
MAC O
) O
pretraining O
strategy O
as O
well O
as O
the O
sentence O
- O
order O
prediction O
( O
SOP O
) O
task O
. O

ERNIE B-MethodName
- I-MethodName
gram I-MethodName
adopts O
various O
masking O
strategies O
including O
token O
- O
level O
, O
phraselevel O
and O
entity O
- O
level O
masking O
to O
pretrain O
BERT B-MethodName
on O
largescale O
heterogeneous O
data O
. O

Chinese B-MethodName
- I-MethodName
Bert I-MethodName
is O
pretrained O
with O
the O
glyph O
and O
phonetic O
features O
. O

Tasks O
We O
test O
our O
model O
on O
5 O
standard O
Chinese O
NLU B-TaskName
tasks O
and O
one O
toxic B-TaskName
detection I-TaskName
tasks O
. O

The O
5 O
NLU B-TaskName
tasks O
are O
: O
( O
1 O
) O
ChnSentiCorp B-DatasetName
, O
Chinese B-TaskName
sentiment I-TaskName
classiﬁcation I-TaskName
with O
2k O
training O
data14 O
, O
( O
2 O
) O

TNEWS B-DatasetName
: O
news B-TaskName
title I-TaskName
classiﬁcation I-TaskName
with O
50k O
training O
data O
, O
( O
3 O
) O
AFQMC B-DatasetName
: O
question B-TaskName
matching I-TaskName
with O
34k O
training O
data O
, O
( O
4 O
) O
CSL B-DatasetName
, O
keyword B-TaskName
recognition I-TaskName
from O
paper O
abstracts O
with O
20k O
training O
data O
ChnSentiCorp B-DatasetName
: O
2k O
( O
Xu O
et O

al O
. O
, O
2020 O
) O
and O
( O
5 O
) O
CMNLI B-DatasetName
, O
Chinese B-TaskName
Multi I-TaskName
- I-TaskName
Genre I-TaskName
NLI I-TaskName
with O
390k O
data O
( O
Conneau O
et O
al O
. O
, O
2018 O
) O
. O

Toxic B-TaskName
detection I-TaskName
can O
server O
as O
a O
task O
with O
“ O
human O
- O
made O
" O
attacks O
in O
contrast O
with O
the O
synthesized O
ones O
. O

It O
is O
collected O
from O
user O
interactions O
( O
written O
) O
with O
a O
popular O
online O
conversational O
platform O
, O
where O
users O
sometimes O
use O
various O
manmade O
attacks O
to O
avoid O
automatic O
system O
ﬁltering O
of O
junk O
ads O
, O
porn O
and O
abusive O
information O
. O

We O
manually O
annotate O
50k O
user O
inputs O
and O
identify O
2k O
toxic O
contents O
( O
positive O
) O
, O
out O
of O
which O
90 O
% O
are O
in O
adversarial O
forms O
. O

We O
randomly O
sample O
2k O
negative O
text O
then O
split O
the O
whole O
into O
train O
/ O
dev O
/ O
test O
with O
8:1:1 O
. O

Attacker O
We O
test O
the O
model O
performance O
under O
three O
different O
attackers O
( O
all O
untargeted O
as O
we O
do O
not O
need O
restrictions O
to O
the O
target O
class O
): O
( O
1 O
) O
ADV O
, O
our O
own O
attacking O
algorithm O
, O
( O
2 O
) O
TextFooler O
( O
Jin O
et O
al O
. O
, O
2020 O
) O
, O
a O
black O
- O
box O
algorithm O
replacing O
important O
words O
with O
semantically O
similar O
ones O
and O
( O
3 O
) O
Argot(Zhang O
et O
al O
. O
, O
2020 O
) O
, O
a O
black O
- O
box O
attacking O
algorithm O
considering O
Chinese O
- O
speciﬁc O
features O
. O

We O
set O
the O
maximum O
attacking O
ratio O
for O
all O
the O
three O
algorithms O
as O
20 O
% O
. O

TextFooler O
is O
originally O
designed O
for O
English O
, O
we O
reimplement O
it O
with O
corresponding O
pretrained O
Chinese O
- O
version O
models O
. O

14We O
use O
the O
small O
version O
of O
training O
data O
to O
test O
the O
fewshot O
capability O
of O
models.925Model O
Clean O
ADV O
TextFooler O
Argot O
Base O
MBert O
91.16 O
58.57 O
62.29 O
46.65 O
Bert O
- O
wwm O
91.27 O
59.28 O
63.22 O
44.52 O
MacBert O
91.33 O
59.72 O
63.18 O
44.34 O
Ernie O
- O
gram O
90.76 O
57.81 O
60.20 O
42.71 O
ChineseBert O
91.01 O
60.07 O
65.73 O
47.78 O
RoCBert O
91.45 O
81.62 O
83.11 O
68.40 O
Large O
MacBert O
92.05 O
55.92 O
45.75 O
41.83 O
RoCBert O
92.58 O
83.17 O
85.74 O
69.40 O
Table O
2 O
: O
Performance O
on O
ChnSentiCorp O
5.2 O
Experiment O
Results O
Chinese O
NLU B-TaskName
Results O
We O
show O
the O
results O
on O
5 O
Chinese O
NLU B-TaskName
tasks O
in O
tables O
2 O
to O
6 O
. O

For O
every O
task O
, O
we O
report O
the O
model O
accuracy B-MetricName
measured O
in O
the O
clean O
testset O
and O
the O
adversarial O
testsets O
under O
3 O
adversarial O
algorithms O
ADV O
, O
TextFooler O
andArgot O
. O

We O
report O
the O
performance O
of O
all O
base O
- O
version O
models O
for O
a O
fair O
comparison O
. O

We O
select O
the O
best O
- O
performed O
base O
- O
version O
model O
to O
test O
its O
large O
- O
version O
performance O
and O
compare O
it O
with O
ROCBERT B-MethodName
. O

As O
can O
be O
seen O
, O
our O
attacking O
algorithm O
ADV O
do O
not O
affect O
much O
on O
TNEWS B-DatasetName
, O
AFQMC B-DatasetName
and O
CSL B-DatasetName
because O
they O
rely O
more O
on O
the O
global O
sentence O
structure O
instead O
of O
individual O
words O
. O

On O
tasks O
like O
sentiment B-TaskName
classiﬁcation I-TaskName
and O
NLI B-TaskName
, O
single O
words O
contribute O
mostly O
to O
the O
model O
decision O
and O
therefore O
the O
attacking O
can O
lead O
to O
signiﬁcant O
performance O
drop O
. O

Argot O
and O
TextFooler O
lead O
to O
more O
drop O
compared O
with O
ADV O
because O
they O
explicitly O
select O
words O
that O
affect O
the O
model O
decisions O
most O
while O
ADV O
selects O
words O
to O
attack O
based O
on O
the O
general O
language O
model O
scores O
. O

Argot O
is O
more O
effective O
than O
TextFooler O
because O
it O
tailors O
its O
character O
replacement O
to O
consider O
Chinese O
- O
speciﬁc O
features O
. O

Overall O
ROCBERT B-MethodName
outperforms O
other O
models O
over O
all O
attacking O
algorithms O
on O
all O
the O
5 O
tasks O
. O

Even O
in O
the O
clean O
dataset O
, O
it O
performs O
the O
best O
on O
4 O
out O
of O
the O
5 O
tasks O
. O

ChineseBert B-MethodName
performs O
the O
second O
under O
various O
attacks O
because O
it O
also O
considers O
multimodal O
features O
during O
its O
pretraining O
same O
as O
ROCBERT B-MethodName
, O
which O
further O
conﬁrms O
the O
importance O
of O
using O
mulimodal O
features O
in O
Chinese O
language O
pretraining O
. O

Toxic B-TaskName
Content I-TaskName
Detection I-TaskName
Results O

We O
train O
all O
models O
in O
the O
toxic B-TaskName
content I-TaskName
detection I-TaskName
task O
. O

As O
can O
be O
seen O
in O
Table O
7 O
, O
ROCBERT B-MethodName
outperforms O
all O
other O
models O
over O
4 O
metrics O
. O

This O
conﬁrms O
the O
its O
effectiveness O
at O
capturing O
the O
true O
semantics O
regardless O
of O
its O
adversarial O
form O
. O

The O
differenceModel O
Clean O
ADV O
TextFooler O
Argot O
Base O
MBert O
56.84 O
53.76 O
42.05 O
40.18 O
Bert O
- O
wwm O
57.44 O
54.12 O
45.25 O
40.76 O
MacBert O
57.53 O
54.41 O
45.10 O
41.94 O
Ernie O
- O
gram O
57.30 O
52.58 O
43.02 O
41.16 O
ChineseBert O
57.65 O
55.74 O
51.01 O
50.27 O
RoCBert O
58.64 O
57.14 O
52.05 O
52.21 O
Large O
ChineseBert O
59.65 O
55.92 O
50.75 O
51.83 O
RoCBert O
59.98 O
59.17 O
54.74 O
54.46 O
Table O
3 O
: O
Performance O
on O
TNEWS O
Model O
Clean O
ADV O
TextFooler O
Argot O
Base O
MBert O
74.07 O
72.04 O
57.69 O
51.24 O
Bert O
- O
wwm O
75.07 O
72.40 O
57.58 O
51.05 O
MacBert O
74.79 O
72.08 O
57.37 O
50.78 O
Ernie O
- O
gram O
75.42 O
71.07 O
56.81 O
50.34 O
ChineseBert O
73.77 O
72.59 O
57.92 O
52.41 O
RoCBert O
75.48 O
74.11 O
62.95 O
62.16 O
Large O
Ernie O
- O
gram O
76.35 O
70.92 O
58.04 O
50.64 O
RoCBert O
77.48 O
76.43 O
65.85 O
64.97 O
Table O
4 O
: O
Performance O
on O
AFQMC O
among O
models O
is O
smaller O
because O
they O
have O
all O
been O
ﬁnetuned O
on O
this O
task O
. O

All O
models O
can O
get O
adapted O
to O
different O
forms O
of O
attacks O
in O
the O
training O
process O
while O
the O
tables O
2 O
to O
6 O
are O
testing O
the O
zeroshot O
generalization O
to O
unknown O
attacks O
. O

T O
A O
CL O
CI O
CP0:60:70:8 O
ADVAccuracy O
T O

A O
CL O
CI O
CP0:50:60:70:8 O
TextfoolerT O
A O
CL O
CI O
CP0:40:50:6 O
Argotbest O
- O
other O
+ O
spell O
- O
checker O
RoCBert O
A O
CL O

CI O
CP O
T0:60:70:8 O
ADVAccuracy O
A O
CL O
CI O
CP O
T0:60:8 O
TextfoolerA O
CL O
CI O
CP O
T0:40:50:60:7 O
Argotbest O
- O
other O
+ O
advtrain O
RoCBert O
+ O
advtrain O
Figure O
2 O
: O
Defending O
Method O
Comparison O
on O
CI(CMNLI O
) O
CP(ChnSentiCorp O
) O
, O
T(TNEWS O
) O
, O
A(AFQMC O
) O
and O
CL(CSL O
) O
. O

Defending O
Method O
Comparison O
We O
further O
compare O
ROCBERTwith O
two O
other O
popular O
ways O
of O
defending O
adversarial O
attack O
: O
( O
1 O
) O
run O
a O
spellchecker O
before O
fed O
to O
the O
model O
and O
( O
2 O
) O
adversarial O
training O
( O
advtrain O
) O
which O
augments O
training O
data O
with O
adversarial O
examples O
. O

We O
add O
these O
two O
de-926Model O
Clean O
ADV O
TextFooler O
Argot O
Base O
MBert O
81.83 O
78.28 O
61.06 O
52.40 O
Bert O
- O
wwm O
81.50 O
79.08 O
61.68 O
53.41 O
MacBert O
81.97 O
78.34 O
61.75 O
52.35 O
Ernie O
- O
gram O
82.70 O
79.53 O
63.54 O
53.66 O
ChineseBert O
81.77 O
78.69 O
61.27 O
53.79 O
RoCBert O
83.83 O
82.56 O
69.29 O
63.07 O
Large O
Ernie O
- O
gram O
83.05 O
79.42 O
61.85 O
57.43 O
RoCBert O
85.28 O
83.59 O
70.13 O
66.38 O
Table O
5 O
: O
Performance O
on O
CSL O
Model O
Clean O
ADV O
TextFooler O
Argot O
Base O
MBert O
80.53 O
69.57 O
50.21 O
45.52 O
Bert O
- O
wwm O
80.79 O
68.54 O
50.46 O
44.26 O
MacBert O
81.01 O
69.94 O
49.86 O
42.07 O
Ernie O
- O
gram O
82.22 O
68.83 O
50.77 O
44.69 O
ChineseBert O
81.42 O
72.27 O
52.85 O
47.15 O
RoCBert O
81.27 O
74.14 O
59.95 O
55.17 O
Large O
Ernie O
- O
gram O
82.36 O
70.11 O
52.45 O
45.82 O
RoCBert O
82.38 O
76.83 O
60.26 O
56.64 O
Table O
6 O
: O
Performance O
on O
CMNLI O
fending O
methods O
on O
top O
of O
the O
best O
- O
performed O
base O
model O
( O
on O
clean O
testsets O
) O
in O
different O
tasks O
: O
ChineseBert B-MethodName
for O
TNEWS B-DatasetName
, O
Ernie B-MethodName
- I-MethodName
gram I-MethodName
for O
AFQMC B-DatasetName
, O
CSL B-DatasetName
and O
CMNLI B-DatasetName
, O
MacBert B-MethodName
for O
ChnSentiCorp B-DatasetName
. O

We O
apply O
the O
spell O
- O
checker O
in O
Cheng O
et O
al O
. O

( O
2020 O
) O
. O

The O
results O
are O
visualized O
in O
Figure O
2 O
. O

We O
can O
see O
that O
spell O
checking O
improves O
the O
performance O
only O
marginally O
and O
sometimes O
even O
hurt O
the O
performance O
( O
best O
- O
other O
under O
ADV O
in O
CI O
) O
. O

The O
reason O
could O
be O
that O
the O
spell O
checker O
performs O
poorly O
for O
out O
- O
of O
- O
domain O
adversarial O
examples O
. O

The O
errors O
could O
be O
propagated O
and O
further O
reduce O
the O
performance O
. O

advtrain O
can O
signiﬁcantly O
beneﬁt O
the O
performance O
, O
but O
note O
that O
it O
explicitly O
“ O
peeps O
" O
at O
the O
adversarial O
algorithm O
applied O
in O
the O
testset O
while O
ROCBERTis O
not O
aware O
of O
the O
testing O
adversarial O
algorithm O
. O

Nevertheless O
, O
it O
is O
still O
comparable O
and O
in O
some O
cases O
even O
outperforms O
advtrain O
. O

By O
combiningROCBERTand O
advtrain O
, O
the O
model O
robustness O
can O
be O
further O
improved O
. O

5.3 O
Ablation O
Study O
We O
perform O
a O
set O
of O
ablation O
studies O
to O
understand O
the O
choice O
of O
different O
components O
in O
ROCBERT B-MethodName
. O

All O
models O
in O
this O
section O
are O
pretrained O
with O
the O
same O
base O
architecture O
and O
hyperparameters O
for O
one O
epoch O
on O
1 O
M O
sampled O
training O
text O
then O
tested O
in O
TNEWS B-DatasetName
. O

The O
results O
are O
shown O
in O
Table O
8.Model O

Acc O
Precision O
Recall O
F1 O
Base O
MBert O
85.11 O
87.12 O
81.35 O
84.13 O
Bert O
- O
wwm O
85.70 O
87.30 O
81.37 O
84.23 O
MacBert O
85.26 O
87.24 O
81.35 O
84.19 O
Ernie O
- O
gram O
85.94 O
87.43 O
81.38 O
84.29 O
ChineseBert O
85.52 O
87.29 O
81.36 O
84.22 O
RoCBert O
87.10 O
89.26 O
83.14 O
86.42 O
Large O
Ernie O
- O
gram O
87.30 O
88.96 O
82.57 O
85.64 O
RoCBert O
88.49 O
90.36 O
84.25 O
87.20 O
Table O
7 O
: O
Performance O
on O
Toxic O
Detection O
Setting O
Clean O
ADV O
Textfooler O

Argot O
Best O
55.38 O
52.23 O
47.72 O
44.59 O
Model O
Loss O
MLM O
54.63 O
48.58 O
38.63 O
33.75 O
Contrastive O
54.97 O
50.73 O
41.80 O
39.25 O
Tokenization O
bpe O
55.40 O
48.64 O
38.19 O
35.67 O
char O
- O
cnn O
53.23 O
49.45 O
44.37 O
41.44 O
Multimodal O
- O
vis O
- O
pretrain O
53.29 O
51.18 O
44.42 O
40.08 O
- O
vis O
53.35 O
51.20 O
45.45 O
41.86 O
- O
pho O
54.71 O
51.18 O
46.02 O
42.08 O
+ O
pho O
- O
pretrain O
54.96 O
51.95 O
47.03 O
43.56 O
Architecture O
Sum O
54.63 O
52.06 O
46.57 O
43.27 O
Concatenate O
55.13 O
52.14 O
46.69 O
43.84 O
Two O
- O
step O
55.09 O
51.81 O
45.39 O
42.67 O
Insert O
Layer O
Layer O
0 O
55.10 O
52.02 O
47.46 O
44.27 O
Layer O
4 O
54.63 O
51.95 O
47.35 O
44.33 O
Layer O
7 O
54.43 O
51.76 O
46.83 O
44.08 O
Layer O
10 O
54.25 O
50.98 O
46.20 O
43.56 O
Table O
8 O
: O
Ablation O
studies O
on O
TNEWS O
with O
different O
settings O
. O

Best O
indicates O
the O
best O
setting O
used O
in O
R O
OCBERT O
. O

Loss O
To O
study O
the O
effects O
of O
the O
loss O
function O
used O
in O
the O
pretraining O
stage O
. O

We O
tried O
two O
other O
settings O
: O
( O
1)contrastive O
only O
, O
where O
the O
model O
is O
pretrained O
only O
with O
the O
contrastive O
learning O
loss O
in O
Eq O
5 O
and O
( O
2)MLM O
- O
only O
, O
where O
the O
model O
is O
pretrained O
only O
with O
the O
MLM O
objective O
as O
in O
standard O
Bert O
. O

We O
can O
see O
that O
both O
options O
lowers O
down O
the O
model O
performance O
. O

By O
combining O
both O
loss O
, O
the O
model O
can O
be O
robust O
under O
adversarial O
attacks O
without O
affecting O
the O
performance O
in O
clean O
data O
. O

Tokenization O
It O
has O
been O
widely O
demonstrated O
that O
char O
- O
based O
tokenization O
is O
preferred O
for O
Chinese O
characters O
( O
Li O
et O
al O
. O
, O
2019b O
) O
, O
but O
it O
is O
rather O
unclear O
how O
we O
should O
model O
pinyins O
and O
nonChinese O
words O
. O

We O
try O
different O
tokenization O
methods O
for O
non O
- O
Chinese O
characters O
: O
( O
1 O
) O
bpe(Sennrich O
et O
al O
. O
, O
2016 O
) O
. O

We O
set O
the O
vocabulary O
as O
20k O
and O
train O
the O
split O
on O
the O
training O
data O
( O
after O
convert-9275 O
% O
10 O
% O
15 O
% O
20 O
% O
25%0:890:90:91 O
CleanAccuracy O
5 O
% O
10 O
% O
15 O
% O
20 O
% O
25%0:60:650:70:75 O
ADV O
5 O
% O
10 O
% O
15 O
% O
20 O
% O
25%0:650:70:750:8 O
ArgotAccuracy O
5 O
% O
10 O
% O
15 O
% O
20 O
% O
25%0:450:50:550:60:65 O
TextfoolerRoCBert O
w/o O
noise O
w/o O
CS O
SimCSE O
Figure O
3 O
: O
Ablation O
study O
with O
varying O
attacking O
ratio O
, O
w/o O
Gaussian O
noise O
, O
w/o O
character O
selection O
( O
CS O
) O
and O
SimCSE O
. O

ing O
all O
Chinese O
characters O
into O
pinyin O
) O
. O

( O
2 O
) O
charcnn(Zhang O
et O
al O
. O
, O
2015 O
) O
, O
which O
process O
each O
character O
individually O
but O
get O
the O
pinyin O
embedding O
with O
a O
char O
- O
cnn O
. O

The O
best O
setting O
in O
ROCBERT O
used O
char O
- O
sum O
which O
processes O
each O
character O
individually O
and O
set O
the O
pinyin O
embedding O
as O
the O
sum O
of O
its O
character O
embeddings O
. O

We O
can O
see O
that O
bpe O
hurt O
the O
performance O
. O

This O
might O
be O
because O
the O
bpe O
split O
is O
trained O
on O
clean O
data O
only O
. O

For O
adversarial O
examples O
, O
the O
letters O
in O
pinyins O
can O
be O
easily O
perturbed O
and O
break O
its O
vocabulary O
. O

Char O
- O
based O
tokenization O
is O
more O
robust O
under O
adversarial O
attacks O
. O

Char O
- O
cnn O
does O
not O
lead O
to O
improvement O
here O
, O
probably O
because O
there O
are O
a O
limited O
combination O
of O
letters O
in O
Chinese O
pinyins O
( O
400 O
) O
, O
each O
pinyin O
can O
usually O
be O
uniquely O
identiﬁed O
by O
its O
bag O
of O
characters O
without O
the O
need O
of O
order O
information O
. O

Multimodal O
feature O
We O
tried O
removing O
the O
visual O
feature O
pretraining O
as O
mentioned O
in O
§ O
4 O
and O
observe O
the O
performance O
drop O
( O
-vis O
- O
pretrain O
) O
. O

It O
is O
even O
worse O
than O
removing O
the O
visual O
feature O
completely O
( O
-vis O
) O
, O
suggesting O
the O
pretraining O
for O
visual O
features O
is O
essential O
, O
without O
which O
the O
model O
can O
be O
hard O
to O
learn O
meaningful O
visual O
features O
. O

The O
phonetic O
feature O
is O
less O
crucial O
than O
visual O
features O
but O
also O
brings O
positive O
improvement O
. O

By O
adding O
a O
pretraining O
stage O
for O
the O
phonetic O
features O
too O
( O
+ O
pho O
- O
pretrain O
) O
, O
the O
improvement O
is O
very O
marginal O
. O

As O
the O
phonetic O
features O
are O
also O
based O
on O
character O
embeddings O
, O
it O
might O
be O
easier O
for O
the O
model O
to O
automatically O
learn O
the O
phonetic O
features O
compared O
with O
the O
visual O
features O
. O

Multimodal O
integration O
We O
compare O
our O
proposed O
layer O
- O
insert O
with O
three O
other O
ways O
of O
integrating O
multimodal O
features O
: O
( O
1 O
) O
sum O
( O
Liu O
et O
al O
. O
, O
2021 O
) O
, O
which O
sums O
the O
multimodal O
embeddings O
, O
( O
2)concatenation O
, O
which O
concatenate(Sun O
et O

al O
. O
, O
2021 O
) O
, O
which O
concatenate O
the O
multimodal O
embeddings O
then O
fuse O
with O
an O
MLP O
layer O
, O
( O
3 O
) O
two O
- O
step O
( O
Xu O
et O
al O
. O
, O
2021 O
) O
, O
which O
ﬁrst O
determine O
the O
weight O
of O
different O
embeddings O
then O
fuse O
to O
the O
encoder O
. O

We O
can O
see O
that O
ROCBERT B-MethodName
performs O
best O
with O
only O
marginal O
computational O
overhead O
by O
updating O
the O
encoder O
representation O
in O
one O
layer O
. O

Insert O
Layer O
We O
further O
analyze O
the O
effects O
of O
the O
insertion O
layer O
. O

Our O
best O
setting O
inserts O
the O
multimodal O
features O
in O
layer O
1 O
for O
the O
base O
model O
and O
layer O
3 O
for O
the O
large O
model O
. O

From O
Table O
8 O
, O
we O
can O
see O
that O
when O
inserting O
them O
in O
the O
upper O
layer O
4,7 O
and O
10 O
, O
the O
performance O
gradually O
drops O
, O
suggesting O
an O
earlier O
insert O
is O
helpful O
for O
the O
model O
to O
incorporate O
these O
features O
in O
- O
depth O
. O

However O
, O
inserting O
them O
in O
layer O
0 O
is O
also O
worse O
since O
the O
model O
can O
only O
learn O
weight O
among O
multimodal O
features O
solely O
from O
bag O
of O
words O
. O

Attacking O
Algorithm O
We O
change O
the O
settings O
in O
our O
attacking O
algorithm O
to O
see O
the O
effects O
in O
Figure O
3 O
. O

We O
can O
see O
the O
attacking O
ratio O
can O
neither O
be O
too O
small O
nor O
too O
large O
. O

15 O
% O
is O
a O
sweet O
spot O
for O
pretraining O
. O

The O
Gaussian O
noise O
added O
in O
Eq O
1 O
also O
brings O
positive O
effects O
consistently O
, O
suggesting O
we O
should O
not O
use O
a O
ﬁxed O
attacking O
ratio O
in O
the O
pretraining O
stage O
. O

The O
character O
selection O
is O
also O
crucial O
and O
removing O
it O
signiﬁcantly O
reduces O
the O
performance O
. O

To O
show O
whether O
it O
is O
necessary O
to O
adopt O
our O
attacking O
algorithm O
with O
complex O
combinations O
of O
attacking O
forms O
. O

We O
further O
compare O
with O
pretraining O
the O
model O
with O
SimCSE O
( O
Gao O
et O
al O
. O
, O
2021 O
) O
, O
an O
algorithm O
which O
uses O
drop O
out O
as O
the O
noise O
instead O
of O
our O
adversarial O
examples O
. O

We O
can O
see O
that O
SimCSE O
is O
rarely O
helpful O
under O
different O
attacks O
. O

This O
suggests O
it O
is O
important O
to O
deﬁne O
rulebased O
attacking O
algorithms O
to O
better O
ﬁt O
the O
realworld O
attacks O
. O

General O
drop O
- O
out O
regularizations O
can O
not O
adapt O
well O
to O
complex O
real O
- O
world O
attacks O
. O

6 O
Conclusion O
We O
present O
ROCBERT B-MethodName
: O
the O
ﬁrst O
pretrained O
Chinese O
language O
model O
that O
is O
robust O
under O
various O
forms O
of O
adversarial O
attacks O
. O

It O
is O
pretrained O
with O
the O
multimodal O
contrastive O
learning O
objective O
and O
achieves O
the O
best O
performance O
on O
5 O
Chinese O
NLU B-TaskName
tasks O
un-928der O
three O
different O
attacking O
algorithms O
without O
negative O
effects O
on O
clean O
testsets O
. O

It O
also O
signiﬁcantly O
outperforms O
the O
others O
in O
the O
toxic O
content O
detection O
task O
. O

Extensive O
ablation O
studies O
are O
provided O
to O
beneﬁt O
future O
research O
. O

References O
Tom O
B O
Brown O
, O
Benjamin O
Mann O
, O
Nick O
Ryder O
, O
Melanie O
Subbiah O
, O
Jared O
Kaplan O
, O
Prafulla O
Dhariwal O
, O
Arvind O
Neelakantan O
, O
Pranav O
Shyam O
, O
Girish O
Sastry O
, O
Amanda O
Askell O
, O
et O
al O
. O
2020 O
. O

Language O
models O
are O
few O
- O
shot O
learners O
. O

arXiv O
preprint O
arXiv:2005.14165 O
. O

Ting O
Chen O
, O
Simon O
Kornblith O
, O
Mohammad O
Norouzi O
, O
and O
Geoffrey O
Hinton O
. O

2020 O
. O

A O
simple O
framework O
for O
contrastive O
learning O
of O
visual O
representations O
. O

In O
International O
conference O
on O
machine O
learning O
, O
pages O
1597–1607 O
. O

PMLR O
. O

Xingyi O
Cheng O
, O
Weidi O
Xu O
, O
Kunlong O
Chen O
, O
Shaohua O
Jiang O
, O
Feng O
Wang O
, O
Taifeng O
Wang O
, O
Wei O
Chu O
, O
and O
Yuan O
Qi O
. O
2020 O
. O

Spellgcn O
: O
Incorporating O
phonological O
and O
visual O
similarities O
into O
language O
models O
for O
chinese O
spelling O
check O
. O

In O
Proceedings O
of O
the O
58th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
871–881 O
. O

Alexis O
Conneau O
, O
Ruty O
Rinott O
, O
Guillaume O
Lample O
, O
Adina O
Williams O
, O
Samuel O
Bowman O
, O
Holger O
Schwenk O
, O
and O
Veselin O
Stoyanov O
. O

2018 O
. O

Xnli O
: O
Evaluating O
crosslingual O
sentence O
representations O
. O

In O
Proceedings O
of O
the O
2018 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
2475–2485 O
. O
Yiming O
Cui O
, O
Wanxiang O
Che O
, O
Ting O
Liu O
, O
Bing O
Qin O
, O
Shijin O
Wang O
, O
and O
Guoping O
Hu O
. O
2020 O
. O

Revisiting O
pretrained O
models O
for O
chinese O
natural O
language O
processing O
. O

In O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
: O
Findings O
, O
pages O
657–668 O
. O

Yiming O
Cui O
, O
Wanxiang O
Che O
, O
Ting O
Liu O
, O
Bing O
Qin O
, O
Ziqing O
Yang O
, O
Shijin O
Wang O
, O
and O
Guoping O
Hu O
. O
2019 O
. O

Pre O
- O
training O
with O
whole O
word O
masking O
for O
chinese O
bert O
. O

arXiv O
preprint O
arXiv:1906.08101 O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

Bert B-MethodName
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O

In O
Proceedings O
of O
the O
2019 O
Conference O
of O
the O
North O
American O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Human O
Language O
Technologies O
, O
Volume O
1 O
( O
Long O
and O
Short O
Papers O
) O
, O
pages O
4171–4186 O
. O

Steffen O
Eger O
and O
Yannik O
Benz O
. O
2020 O
. O

From O
hero O
to O
zn’eroe O
: O
A O
benchmark O
of O
low O
- O
level O
adversarial O
attacks O
. O

arXiv O
preprint O
arXiv:2010.05648 O
. O

Steffen O
Eger O
, O
Gözde O
Gül O
¸ O
Sahin O
, O
Andreas O
Rücklé O
, O
JiUng O
Lee O
, O
Claudia O
Schulz O
, O
Mohsen O
Mesgar O
, O
Krishnkant O
Swarnkar O
, O
Edwin O
Simpson O
, O
and O
IrynaGurevych O
. O

2019 O
. O

Text O
processing O
like O
humans O
do O
: O
Visually O
attacking O
and O
shielding O
nlp O
systems O
. O

arXiv O
preprint O
arXiv:1903.11508 O
. O

Hicham O
El O
Boukkouri O
, O
Olivier O
Ferret O
, O
Thomas O
Lavergne O
, O
Hiroshi O
Noji O
, O
Pierre O
Zweigenbaum O
, O
and O
Jun’ichi O
Tsujii O
. O

2020 O
. O

Characterbert B-MethodName
: O
Reconciling O
elmo O
and O
bert O
for O
word O
- O
level O
open O
- O
vocabulary O
representations O
from O
characters O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
6903–6915 O
. O
Jiarui O
Fang O
, O
Yang O
Yu O
, O
Zilin O
Zhu O
, O
Shenggui O
Li O
, O
Yang O
You O
, O
and O
Jie O
Zhou O
. O

2021 O
. O

Patrickstar O
: O
Parallel O
training O
of O
pre O
- O
trained O
models O
via O
a O
chunk O
- O
based O
memory O
management O
. O

arXiv O
preprint O
arXiv:2108.05818 O
. O

Tianyu O
Gao O
, O
Xingcheng O
Yao O
, O
and O
Danqi O
Chen O
. O
2021 O
. O
Simcse O
: O
Simple O
contrastive O
learning O
of O
sentence O
embeddings O
. O

arXiv O
preprint O
arXiv:2104.08821 O
. O

Siddhant O
Garg O
and O
Goutham O
Ramakrishnan O
. O

2020 O
. O

Bae O
: O
Bert O
- O
based O
adversarial O
examples O
for O
text O
classiﬁcation O
. O

arXiv O
preprint O
arXiv:2004.01970 O
. O

Yotam O
Gil O
, O
Yoav O
Chai O
, O
Or O
Gorodissky O
, O
and O
Jonathan O
Berant O
. O

2019 O
. O

White O
- O
to O
- O
black O
: O
Efﬁcient O
distillation O
of O
black O
- O
box O
adversarial O
attacks O
. O

arXiv O
preprint O
arXiv:1904.02405 O
. O

Kaiming O
He O
, O
Xiangyu O
Zhang O
, O
Shaoqing O
Ren O
, O
and O
Jian O
Sun O
. O

2016 O
. O

Deep O
residual O
learning O
for O
image O
recognition O
. O

In O
Proceedings O
of O
the O
IEEE O
conference O
on O
computer O
vision O
and O
pattern O
recognition O
, O
pages O
770 O
– O
778 O
. O

Di O
Jin O
, O
Zhijing O
Jin O
, O
Joey O
Tianyi O
Zhou O
, O
and O
Peter O
Szolovits O
. O
2020 O
. O

Is O
bert O
really O
robust O
? O

a O
strong O
baseline O
for O
natural O
language O
attack O
on O
text O
classiﬁcation O
and O
entailment O
. O

In O
Proceedings O
of O
the O
AAAI O
conference O
on O
artiﬁcial O
intelligence O
, O
volume O
34 O
, O
pages O
8018–8025 O
. O

Taeuk O
Kim O
, O
Kang O
Min O
Yoo O
, O
and O
Sang O
- O
goo O
Lee O
. O
2021 O
. O

Self O
- O
guided O
contrastive O
learning O
for O
bert O
sentence O
representations O
. O

arXiv O
preprint O
arXiv:2106.07345 O
. O

Dianqi O
Li O
, O
Yizhe O
Zhang O
, O
Hao O
Peng O
, O
Liqun O
Chen O
, O
Chris O
Brockett O
, O
Ming O
- O
Ting O
Sun O
, O
and O
Bill O
Dolan O
. O

2020a O
. O

Contextualized O
perturbation O
for O
textual O
adversarial O
attack O
. O

arXiv O
preprint O
arXiv:2009.07502 O
. O

Jinfeng O
Li O
, O
Tianyu O
Du O
, O
Shouling O
Ji O
, O
Rong O
Zhang O
, O
Quan O
Lu O
, O
Min O
Yang O
, O
and O
Ting O
Wang O
. O

2020b O
. O

Textshield O
: O
Robust O
text O
classiﬁcation O
based O
on O
multimodal O
embedding O
and O
neural O
machine O
translation O
. O

In O
29th O
fUSENIXgSecurity O
Symposium O
( O
fUSENIXgSecurity O
20 O
) O
, O
pages O
1381–1398 O
. O

Jinfeng O
Li O
, O
Shouling O
Ji O
, O
Tianyu O
Du O
, O
Bo O
Li O
, O
and O
Ting O
Wang O
. O

2019a O
. O

Textbugger O
: O
Generating O
adversarial O
text O
against O
real O
- O
world O
applications O
. O

arXiv O
preprint O
arXiv:1812.05271 O
. O

Jiwei O
Li O
, O
Will O
Monroe O
, O
and O
Dan O
Jurafsky O
. O

2016 O
. O

Understanding O
neural O
networks O
through O
representation O
erasure O
. O

arXiv O
preprint O
arXiv:1612.08220 O
.929Linyang O

Li O
, O
Ruotian O
Ma O
, O
Qipeng O
Guo O
, O
Xiangyang O
Xue O
, O
and O
Xipeng O
Qiu O
. O
2020c O
. O

Bert O
- O
attack O
: O
Adversarial O
attack O
against O
bert O
using O
bert O
. O

arXiv O
preprint O
arXiv:2004.09984 O
. O

Linyang O
Li O
, O
Yunfan O
Shao O
, O
Demin O
Song O
, O
Xipeng O
Qiu O
, O
and O
Xuanjing O
Huang O
. O

2020d O
. O

Generating O
adversarial O
examples O
in O
chinese O
texts O
using O
sentence O
- O
pieces O
. O

arXiv O
preprint O
arXiv:2012.14769 O
. O

Xiaoya O
Li O
, O
Yuxian O
Meng O
, O
Xiaofei O
Sun O
, O
Qinghong O
Han O
, O
Arianna O
Yuan O
, O
and O
Jiwei O
Li O
. O

2019b O
. O

Is O
word O
segmentation O
necessary O
for O
deep O
learning O
of O
chinese O
representations O
? O

In O
Proceedings O
of O
the O
57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
3242–3252 O
. O

Hui O
Liu O
, O
Yongzheng O
Zhang O
, O
Yipeng O
Wang O
, O
Zheng O
Lin O
, O
and O
Yige O
Chen O
. O

2020 O
. O

Joint O
character O
- O
level O
word O
embedding O
and O
adversarial O
stability O
training O
to O
defend O
adversarial O
text O
. O

In O
Proceedings O
of O
the O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
volume O
34 O
, O
pages O
8384–8391 O
. O

Shulin O
Liu O
, O
Tao O
Yang O
, O
Tianchi O
Yue O
, O
Feng O
Zhang O
, O
and O
Di O
Wang O
. O
2021 O
. O

Plome O
: O

Pre O
- O
training O
with O
misspelled O
knowledge O
for O
chinese O
spelling O
correction O
. O

InProceedings O
of O
the O
59th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
and O
the O
11th O
International O
Joint O
Conference O
on O
Natural O
Language O
Processing O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
2991–3000 O
. O

Yinhan O
Liu O
, O
Myle O
Ott O
, O
Naman O
Goyal O
, O
Jingfei O
Du O
, O
Mandar O
Joshi O
, O
Danqi O
Chen O
, O
Omer O
Levy O
, O
Mike O
Lewis O
, O
Luke O
Zettlemoyer O
, O
and O
Veselin O
Stoyanov O
. O

2019 O
. O

Roberta O
: O
A O
robustly O
optimized O
bert O
pretraining O
approach O
. O

Zhao O
Meng O
, O
Yihan O
Dong O
, O
Mrinmaya O
Sachan O
, O
and O
Roger O
Wattenhofer O
. O
2021 O
. O

Self O
- O
supervised O
contrastive O
learning O
with O
adversarial O
perturbations O
for O
robust O
pretrained O
language O
models O
. O

arXiv O
preprint O
arXiv:2107.07610 O
. O

Maximilian O
Mozes O
, O
Pontus O
Stenetorp O
, O
Bennett O
Kleinberg O
, O
and O
Lewis O
Grifﬁn O
. O

2021 O
. O

Frequency O
- O
guided O
word O
substitutions O
for O
detecting O
textual O
adversarial O
examples O
. O

In O
Proceedings O
of O
the O
16th O
Conference O
of O
the O
European O
Chapter O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
Main O
Volume O
, O
pages O
171–186 O
. O

Cheng O
Nuo O
, O
Guo O
- O
Qin O
Chang O
, O
Haichang O
Gao O
, O
Ge O
Pei O
, O
and O
Yang O
Zhang O
. O

2020 O
. O

Wordchange O
: O

Adversarial O
examples O
generation O
approach O
for O
chinese O
text O
classiﬁcation O
. O

IEEE O
Access O
, O
8:79561–79572 O
. O

Matthew O
E O
Peters O
, O
Mark O
Neumann O
, O
Mohit O
Iyyer O
, O
Matt O
Gardner O
, O
Christopher O
Clark O
, O
Kenton O
Lee O
, O
and O
Luke O
Zettlemoyer O
. O

2018 O
. O

Deep O
contextualized O
word O
representations O
. O

In O
Proceedings O
of O
NAACL O
- O
HLT O
, O
pages O
2227–2237 O
. O

Danish O
Pruthi O
, O
Bhuwan O
Dhingra O
, O
and O
Zachary O
C O
Lipton O
. O
2019 O
. O

Combating O
adversarial O
misspellings O
with O
robust O
word O
recognition O
. O

In O
Proceedings O
of O
the57th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
pages O
5582–5591 O
. O

Alec O
Radford O
, O
Karthik O
Narasimhan O
, O
Tim O
Salimans O
, O
and O
Ilya O
Sutskever O
. O

Improving O
language O
understanding O
by O
generative O
pre O
- O
training O
. O

Jeff O
Rasley O
, O
Samyam O
Rajbhandari O
, O
Olatunji O
Ruwase O
, O
and O
Yuxiong O
He O
. O
2020 O
. O

Deepspeed O
: O
System O
optimizations O
enable O
training O
deep O
learning O
models O
with O
over O
100 O
billion O
parameters O
. O

In O
Proceedings O
of O
the O
26th O
ACM O
SIGKDD O
International O
Conference O
on O
Knowledge O
Discovery O
& O
Data O
Mining O
, O
pages O
3505 O
– O
3506 O
. O

Rico O
Sennrich O
, O
Barry O
Haddow O
, O
and O
Alexandra O
Birch O
. O
2016 O
. O

Neural O
machine O
translation O
of O
rare O
words O
with O
subword O
units O
. O

In O
Proceedings O
of O
the O
54th O

Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
( O
Volume O
1 O
: O
Long O
Papers O
) O
, O
pages O
1715 O
– O
1725 O
. O

Lichao O
Sun O
, O
Kazuma O
Hashimoto O
, O
Wenpeng O
Yin O
, O
Akari O
Asai O
, O
Jia O
Li O
, O
Philip O
Yu O
, O
and O
Caiming O
Xiong O
. O
2020a O
. O

Adv O
- O
bert O
: O
Bert O
is O
not O
robust O
on O
misspellings O
! O

generating O
nature O
adversarial O
samples O
on O
bert O
. O

arXiv O
preprint O
arXiv:2003.04985 O
. O

Yu O
Sun O
, O
Shuohuan O
Wang O
, O
Yukun O
Li O
, O
Shikun O
Feng O
, O
Xuyi O
Chen O
, O
Han O
Zhang O
, O
Xin O
Tian O
, O
Danxiang O
Zhu O
, O
Hao O
Tian O
, O
and O
Hua O
Wu O
. O
2019 O
. O

Ernie B-MethodName
: O
Enhanced O
representation O
through O
knowledge O
integration O
. O

arXiv O
preprint O
arXiv:1904.09223 O
. O

Yu O
Sun O
, O
Shuohuan O
Wang O
, O
Yukun O
Li O
, O
Shikun O
Feng O
, O
Hao O
Tian O
, O
Hua O
Wu O
, O
and O
Haifeng O
Wang O
. O

2020b O
. O

Ernie B-MethodName
2.0 I-MethodName
: O
A O
continual O
pre O
- O
training O
framework O
for O
language O
understanding O
. O

In O
Proceedings O
of O
the O
AAAI O
Conference O
on O
Artiﬁcial O
Intelligence O
, O
volume O
34 O
, O
pages O
8968 O
– O
8975 O
. O

Zijun O
Sun O
, O
Xiaoya O
Li O
, O
Xiaofei O
Sun O
, O
Yuxian O
Meng O
, O
Xiang O
Ao O
, O
Qing O
He O
, O
Fei O
Wu O
, O
and O
Jiwei O
Li O
. O
2021 O
. O

Chinesebert B-MethodName
: O
Chinese O
pretraining O
enhanced O
by O
glyph O
and O
pinyin O
information O
. O

arXiv O
preprint O
arXiv:2106.16038 O
. O

Baoxin O
Wang O
, O
Wanxiang O
Che O
, O
Dayong O
Wu O
, O
Shijin O
Wang O
, O
Guoping O
Hu O
, O
and O
Ting O
Liu O
. O

2021a O
. O

Dynamic O
connected O
networks O
for O
chinese O
spelling O
check O
. O

In O
Findings O
of O
the O
Association O
for O
Computational O
Linguistics O
: O
ACL O
- O
IJCNLP O
2021 O
, O
pages O
2437–2446 O
. O

Boxin O
Wang O
, O
Boyuan O
Pan O
, O
Xin O
Li O
, O
and O
Bo O
Li O
. O
2020 O
. O

Towards O
evaluating O
the O
robustness O
of O
chinese O
bert O
classiﬁers O
. O

arXiv O
preprint O
arXiv:2004.03742 O
. O

Dong O
Wang O
, O
Ning O
Ding O
, O
Piji O
Li O
, O
and O
Hai O
- O
Tao O
Zheng O
. O
2021b O
. O

Cline O
: O
Contrastive O
learning O
with O
semantic O
negative O
examples O
for O
natural O
language O
understanding O
. O

arXiv O
preprint O
arXiv:2107.00440 O
. O

Heng O
- O
Da O
Xu O
, O
Zhongli O
Li O
, O
Qingyu O
Zhou O
, O
Chao O
Li O
, O
Zizhen O
Wang O
, O
Yunbo O
Cao O
, O
Heyan O
Huang O
, O
and O
XianLing O
Mao O
. O
2021 O
. O

Read O
, O
listen O
, O
and O
see O
: O
Leveraging O
multimodal O
information O
helps O
chinese O
spell O
checking O
. O

arXiv O
preprint O
arXiv:2105.12306 O
.930Liang O

Xu O
, O
Hai O
Hu O
, O
Xuanwei O
Zhang O
, O
Lu O
Li O
, O
Chenjie O
Cao O
, O
Yudong O
Li O
, O
Yechen O
Xu O
, O
Kai O
Sun O
, O
Dian O
Yu O
, O
Cong O
Yu O
, O
et O
al O
. O
2020 O
. O

Clue O
: O
A O
chinese O
language O
understanding O
evaluation O
benchmark O
. O

In O
Proceedings O
of O
the O
28th O
International O
Conference O
on O
Computational O
Linguistics O
, O
pages O
4762–4772 O
. O

Jin O
Yong O
Yoo O
and O
Yanjun O
Qi O
. O
2021 O
. O

Towards O
improving O
adversarial O
training O
of O
nlp O
models O
. O

arXiv O
preprint O
arXiv:2109.00544 O
. O

Yuan O
Zang O
, O
Fanchao O
Qi O
, O
Chenghao O
Yang O
, O
Zhiyuan O
Liu O
, O
Meng O
Zhang O
, O
Qun O
Liu O
, O
and O
Maosong O
Sun O
. O
2020 O
. O

Word O
- O
level O
textual O
adversarial O
attacking O
as O
combinatorial O
optimization O
. O

arXiv O
preprint O
arXiv:1910.12196 O
. O

Xiang O
Zhang O
, O
Junbo O
Zhao O
, O
and O
Yann O
LeCun O
. O
2015 O
. O

Character O
- O
level O
convolutional O
networks O
for O
text O
classiﬁcation O
. O

Advances O
in O
neural O
information O
processing O
systems O
, O
28:649–657 O
. O

Zhengyan O
Zhang O
, O
Xu O
Han O
, O
Hao O
Zhou O
, O
Pei O
Ke O
, O
Yuxian O
Gu O
, O
Deming O
Ye O
, O
Yujia O
Qin O
, O
Yusheng O
Su O
, O
Haozhe O
Ji O
, O
Jian O
Guan O
, O
et O
al O
. O
2021 O
. O

Cpm B-MethodName
: O
A O
large O
- O
scale O
generative O
chinese O
pre O
- O
trained O
language O
model O
. O

AI O
Open O
, O
2:93–99 O
. O

Zihan O
Zhang O
, O
Mingxuan O
Liu O
, O
Chao O
Zhang O
, O
Yiming O
Zhang O
, O
Zhou O
Li O
, O
Qi O
Li O
, O
Haixin O
Duan O
, O
and O
Donghong O
Sun O
. O
2020 O
. O

Argot O
: O
Generating O
adversarial O
readable O
chinese O
texts.931 O


