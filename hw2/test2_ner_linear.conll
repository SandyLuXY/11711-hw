Proceedings	O
of	O
the	O
60th	O
Annual	O
Meeting	O
of	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
Volume	O
1	O
:	O
Long	O
Papers	O
,	O
pages	O
46	O
-	O
56	O
May	O
22	O
-	O
27	O
,	O
2022	O
c	O

	O
2022	O
Association	O
for	O
Computational	O
Linguistics	O
AlephBERT	B-MethodName
:	O
Language	O
Model	O
Pre	O
-	O
training	O
and	O
Evaluation	O
from	O
Sub	O
-	O
Word	O
to	O
Sentence	O
Level	O
Amit	O
Seker	O
,	O
Elron	O
Bandel	O
,	O
Dan	O
Bareket	O
,	O
Idan	O
Brusilovsky	O
,	O
Refael	O
Shaked	O
Greenfeld	O
,	O
Reut	O
Tsarfaty	O
Department	O
of	O
Computer	O
Science	O
,	O
Bar	O
Ilan	O
University	O
,	O
Ramat	O
-	O
Gan	O
,	O
Israel	O
{	O
aseker00,elronbandel	O
,	O
dbareket	O
,	O
brusli1	O
,	O
shakedgreenfeld,reut.tsarfaty}@gmail.com	O
Abstract	O
Large	O
Pre	O
-	O
trained	O
Language	O
Models	O
(	O
PLMs	O
)	O
have	O
become	O
ubiquitous	O
in	O
the	O
development	O
of	O
language	O
understanding	O
technology	O
and	O
lie	O
at	O
the	O
heart	O
of	O
many	O
artificial	O
intelligence	O
advances	O
.	O

While	O
advances	O
reported	O
for	O
English	O
using	O
PLMs	O
are	O
unprecedented	O
,	O
reported	O
advances	O
using	O
PLMs	O
for	O
Hebrew	O
are	O
few	O
and	O
far	O
between	O
.	O

The	O
problem	O
is	O
twofold	O
.	O

First	O
,	O
so	O
far	O
,	O
Hebrew	O
resources	O
for	O
training	O
large	O
language	O
models	O
are	O
not	O
of	O
the	O
same	O
magnitude	O
as	O
their	O
English	O
counterparts	O
.	O

Second	O
,	O
most	O
benchmarks	O
available	O
to	O
evaluate	O
progress	O
in	O
Hebrew	O
NLP	B-TaskName
require	O
morphological	O
boundaries	O
which	O
are	O
not	O
available	O
in	O
the	O
output	O
of	O
PLMs	O
.	O

In	O
this	O
work	O
we	O
remedy	O
both	O
aspects	O
.	O

We	O
present	O
AlephBERT	B-MethodName
,	O
a	O
large	O
PLM	O
for	O
Modern	O
Hebrew	O
,	O
trained	O
on	O
larger	O
vocabulary	O
and	O
a	O
larger	O
dataset	O
than	O
any	O
Hebrew	O
PLM	O
before	O
.	O

Moreover	O
,	O
we	O
introduce	O
a	O
novel	O
neural	O
architecture	O
that	O
recovers	O
the	O
morphological	O
segments	O
encoded	O
in	O
contextualized	O
embedding	O
vectors	O
.	O

Based	O
on	O
this	O
new	O
morphological	O
component	O
we	O
offer	O
an	O
evaluation	O
suite	O
consisting	O
of	O
multiple	O
tasks	O
and	O
benchmarks	O
that	O
cover	O
sentencelevel	O
,	O
word	O
-	O
level	O
andsub	O
-	O
word	O
level	O
analyses	O
.	O

On	O
all	O
tasks	O
,	O
AlephBERT	B-MethodName
obtains	O
state	O
-	O
of	O
-	O
theart	O
results	O
beyond	O
contemporary	O
Hebrew	O
stateof	O
-	O
the	O
-	O
art	O
models	O
.	O

We	O
make	O
our	O
AlephBERT	O
model	O
,	O
the	O
morphological	O
extraction	O
component	O
,	O
and	O
the	O
Hebrew	O
evaluation	O
suite	O
publicly	O
available	O
,	O
for	O
future	O
investigations	O
and	O
evaluations	O
of	O
Hebrew	O
PLMs	O
.	O

1	O
Introduction	O
Contextualized	O
word	O
representations	O
provided	O
by	O
models	O
such	O
as	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
GPT3	B-MethodName
(	O
Brown	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
T5	B-MethodName
(	O
Raffel	O
et	O
al	O
.	O
,	O
2020	O
)	O
and	O
more	O
,	O
were	O
shown	O
in	O
recent	O
years	O
to	O
be	O
a	O
critical	O
component	O
for	O
obtaining	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
performance	O
on	O
a	O
wide	O
range	O
of	O
Natural	O
Language	O
Processing	B-DatasetName
(	O
NLP	O
)	O
tasks	O
,	O
from	O
surface	O
syntactic	O
tasks	O
as	O
tagging	O
and	O
parsing	O
,	O
to	O
downstream	O
semantic	O
tasks	O
as	O
question	O
answering	O
,	O
information	O
extraction	O
and	O
text	O
summarization	O
.	O

While	O
advances	O
reported	O
for	O
English	O
using	O
such	O
models	O
are	O
unprecedented	O
,	O
previously	O
reported	O
results	O
using	O
PLMs	O
in	O
Modern	O
Hebrew	O
are	O
far	O
from	O
satisfactory	O
.	O

Specifically	O
,	O
the	O
BERT	B-MethodName
-	O
based	O
Hebrew	O
section	O
of	O
multilingual	O
-	O
BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
(	O
henceforth	O
,	O
mBERT	B-MethodName
)	O
,	O
did	O
not	O
provide	O
a	O
similar	O
boost	O
in	O
performance	O
as	O
observed	O
by	O
the	O
English	O
section	O
of	O
mBERT	B-MethodName
.	O

In	O
fact	O
,	O
for	O
several	O
reported	O
tasks	O
,	O
the	O
results	O
of	O
the	O
mBERT	B-MethodName
model	O
are	O
on	O
a	O
par	O
with	O
pre	O
-	O
neural	O
models	O
or	O
neural	O
models	O
based	O
on	O
non	O
-	O
contextual	O
embeddings	O
(	O
Tsarfaty	O
et	O
al	O
.	O
,	O
2020	O
;	O
Klein	O
and	O
Tsarfaty	O
,	O
2020	O
)	O
.	O

An	O
additional	O
Hebrew	O
BERT	B-MethodName
-	O
based	O
model	O
,	O
HeBERT	B-MethodName
(	O
Chriqui	O
and	O
Yahav	O
,	O
2021	O
)	O
,	O
has	O
been	O
recently	O
released	O
,	O
yet	O
without	O
empirical	O
evidence	O
of	O
performance	O
improvements	O
on	O
key	O
components	O
of	O
the	O
Hebrew	O
NLP	B-TaskName
pipeline	O
.	O

The	O
challenge	O
of	O
developing	O
PLMs	O
for	O
morphologically	O
-	O
rich	O
andmedium	O
-	O
resourced	O
languages	O
such	O
as	O
Modern	O
Hebrew	O
is	O
twofold	O
.	O

First	O
,	O
contextualized	O
word	O
representations	O
are	O
obtained	O
by	O
pre	O
-	O
training	O
a	O
large	O
language	O
model	O
on	O
massive	O
quantities	O
of	O
unlabeled	O
texts	O
.	O

In	O
Hebrew	O
,	O
the	O
size	O
of	O
published	O
texts	O
available	O
for	O
training	O
is	O
relatively	O
small	O
.	O

To	O
wit	O
,	O
Hebrew	B-DatasetName
Wikipedia	I-DatasetName
(	O
300	O
K	O
articles	O
)	O
used	O
for	O
training	O
mBERT	B-MethodName
is	O
orders	O
of	O
magnitude	O
smaller	O
compared	O
to	O
English	B-DatasetName
Wikipedia	I-DatasetName
(	O
6	O
M	O
articles	O
)	O
.	O

Second	O
,	O
commonly	O
accepted	O
benchmarks	O
for	O
evaluating	O
Hebrew	O
models	O
,	O
via	O
Morpho	B-DatasetName
-	O
Syntactic	B-MethodName
Tagging	O
and	O
Parsing	O
(	O
Sadde	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
or	O
Named	O
Entity	O
Recognition	O
(	O
Bareket	O
and	O
Tsarfaty	O
,	O
2020	O
)	O
require	O
decomposition	O
of	O
words	O
into	O
morphemes	O
,	O
1	O
which	O
are	O
distinct	O
of	O
the	O
sub	O
-	O
words	O
(	O
a.k.a	O
.	O
wordpieces	O
)	O
provided	O
by	O
standard	O
PLMs	O
.	O

Such	O
morphemes	O
are	O
as	O
of	O
yet	O
not	O
readily	O
available	O
in	O
the	O
PLMs	O
’	O
output	O
embeddings	O
.	O

Evaluating	O
BERT	B-MethodName
-	O
based	O
models	O
on	O
morphemelevel	O
tasks	O
is	O
thus	O
non	O
-	O
trivial	O
due	O
to	O
the	O
mismatch	O
between	O
the	O
sub	O
-	O
word	O
tokens	O
used	O
as	O
sub	O
-	O
word	O
1These	O
morphemes	O
are	O
affixes	O
and	O
clitics	O
bearing	O
their	O
own	O
POS	O
.	O

They	O
are	O
termed	O
syntactic	O
words	O
in	O
UD	O
(	O
Zeman	O
et	O
al	O
.	O
,	O
2018	O
)	O
,	O
or	O
segments	O
in	O
previous	O
literature	O
on	O
Hebrew	O
NLP.46Figure	B-TaskName
1	O
:	O
PLM	O
Morphological	O
Extraction	O
Pipeline	O
.	O

The	O
two	O
-	O
word	O
phrase	O
“	O
/ֹדנֵfנ	O
/	B-MethodName
דרֹוuנfלביתהלבלַנִ	O
,	O
”	O
transliterated	O
as	O
“	O
lbit	O
hlbn	O
”	O
,	O
mapped	O
to	O
word	O
-	O
pieces	O
which	O
are	O
consumed	O
by	O
a	O
PLM	O
to	O
generate	O
contextualized	O
vectors	O
and	O
extract	O
the	O
sub	O
-	O
word	O
morphological	O
units	O
.	O

In	O
this	O
example	O
the	O
WordPiece	O
Tokenizer	O
splits	O
the	O
first	O
word	O
,	O
“	O
lbit	O
”	O
,	O
into	O
two	O
pieces	O
while	O
leaving	O
the	O
second	O
word	O
,	O
“	O
hlbn	O
”	O
,	O
intact	O
.	O

Consequently	O
,	O
AlephBERT	B-MethodName
generates	O
3	O
embedded	O
vectors	O
-	O
the	O
vectors	O
associated	O
with	O
the	O
split	O
word	O
pieces	O
are	O
averaged	O
to	O
form	O
a	O
single	O
contextualized	O
vector	O
.	O

Finally	O
,	O
the	O
resulting	O
two	O
word	O
vectors	O
are	O
used	O
by	O
the	O
Morphological	B-DatasetName
Extraction	I-DatasetName
Model	O
that	O
generates	O
the	O
disambiguated	O
morphological	O
segments	O
.	O

input	O
units	O
used	O
by	O
the	O
PLMs	O
and	O
the	O
sub	O
-	O
word	O
morphological	O
units	O
needed	O
for	O
evaluation	O
.	O

PLMs	O
employ	O
sub	O
-	O
word	O
tokenization	O
mechanisms	O
such	O
as	O
WordPiece	O
or	O
Byte	O
-	O
Pair	B-DatasetName
Encoding	O
(	O
BPE	O
)	O
for	O
the	O
purposes	O
of	O
minimizing	O
Out	O
-	O
Of	O
-	O
V	O
ocabulary	O
words	O
(	O
Sennrich	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

These	O
sub	O
-	O
word	O
tokens	O
are	O
generated	O
in	O
a	O
pre	O
-	O
processing	O
step	O
,	O
without	O
utilization	O
of	O
any	O
linguistic	O
information	O
,	O
and	O
passed	O
as	O
input	O
to	O
the	O
PLM	O
.	O

Crucially	O
,	O
such	O
word	O
-	O
pieces	O
do	O
not	O
reflect	O
morphological	O
units	O
.	O

Extracting	O
morphological	O
units	O
from	O
contextualized	O
vectors	O
provided	O
by	O
PLMs	O
is	O
challenging	O
yet	O
necessary	O
in	O
order	O
to	O
enable	O
morphological	O
-	O
level	O
evaluation	O
of	O
Hebrew	O
PLMs	O
on	O
standard	O
benchmarks	O
.	O

In	O
this	O
paper	O
we	O
introduce	O
AlephBERT	B-MethodName
,	O
a	O
Hebrew	O
PLM	O
trained	O
on	O
more	O
data	O
and	O
a	O
larger	O
vocabulary	O
than	O
any	O
Hebrew	O
PLM	O
before.2Moreover	O
,	O
we	O
propose	O
a	O
novel	O
architecture	O
that	O
extracts	O
the	O
morphological	O
sub	O
-	O
word	O
units	O
implicitly	O
encoded	O
in	O
the	O
contextualized	O
vectors	O
outputted	O
by	O
PLMs	O
.	O

Using	O
AlephBERT	B-MethodName
and	O
the	O
proposed	O
morphological	O
extraction	O
model	O
we	O
enable	O
evaluation	O
on	O
allexisting	O
Hebrew	O
benchmarks	O
.	O

We	O
thus	O
present	O
a	O
processing	O
and	O
evaluation	O
pipeline	O
tailored	O
to	O
fit	O
Morphologically	B-DatasetName
Rich	I-DatasetName
Languages	I-DatasetName
(	O
MRLs	B-DatasetName
)	O
,	O
i.e.	O
,	O
covering	O
2We	O
make	O
our	O
PLM	O
https://huggingface.co/	O
onlplab	O
/	O
alephbert	O
-	O
base	O
and	O
demo	O
https://nlp	O
.	O

biu.ac.il/~amitse/alephbert/	O
publicly	O
available	O
,	O
to	O
qualitatively	O
assess	O
present	O
and	O
future	O
Hebrew	O
PLMs.sentence	O
-	O
level	O
,	O
word	O
-	O
level	O
and	O
most	O
importantly	O
sub	O
-	O
word	O
morphological	O
-	O
level	O
tasks	O
(	O
Segmentation	O
,	O
Part	O
-	O
of	O
-	O
Speech	O
Tagging	O
,	O
full	O
Morphological	O
Tagging	B-DatasetName
,	O
Dependency	O
Parsing	O
,	O
Named	O
Entity	B-DatasetName
Recognition	I-DatasetName
(	O
NER	B-TaskName
)	O
andSentiment	O
Analysis	O
)	O
,	O
and	O
present	O
new	O
and	O
improved	O
SOTA	O
for	O
Modern	O
Hebrew	O
on	O
all	O
of	O
these	O
tasks	O
.	O

2	O
Previous	O
Work	O
Contextualized	O
word	O
embedding	O
vectors	O
are	O
a	O
major	O
driver	O
for	O
improved	O
performance	O
of	O
deep	O
learning	O
models	O
on	O
many	O
Natural	O
Language	B-DatasetName
Understanding	I-DatasetName
(	O
NLU	B-DatasetName
)	O
tasks	O
.	O

Initially	O
,	O
ELMo	O
(	O
Peters	O
et	O
al	O
.	O
,	O
2018	O
)	O
and	O
ULMFit	B-MethodName
(	O
Howard	O
and	O
Ruder	O
,	O
2018	O
)	O
introduced	O
contextualized	O
word	O
embedding	O
frameworks	O
by	O
training	O
LSTM	B-MethodName
-	O
based	O
models	O
on	O
massive	O
amounts	O
of	O
texts	O
.	O

The	O
linguistic	O
quality	O
encoded	O
in	O
these	O
models	O
was	O
demonstrated	O
over	O
6	O
tasks	O
:	O
Question	O
Answering	O
,	O
Textual	O
Entailment	O
,	O
Semantic	O
Role	O
labeling	O
,	O
Coreference	O
Resolution	O
,	O
Name	O
Entity	O
Extraction	O
,	O
and	O
Sentiment	O
Analysis	O
.	O

The	O
next	O
big	O
leap	O
was	O
obtained	O
with	O
the	O
introduction	O
of	O
the	O
GPT-1	B-MethodName
framework	O
by	O
Radford	O
and	O
Sutskever	O
(	O
2018	O
)	O
.	O

Instead	O
of	O
using	O
LSTM	O
layers	O
,	O
GPT	B-MethodName
is	O
based	O
on	O
12	O
layers	O
of	O
Transformer	O
decoders	O
with	O
each	O
decoder	O
layer	O
composed	O
of	O
a	O
768	O
-	O
dimensional	O
feed	O
-	O
forward	O
layer	O
and	O
12	O
self	O
-	O
attention	O
heads	O
.	O

Devlin	O
et	O
al	O
.	O

(	O
2019	O
)	O
followed	O
along	O
the	O
same	O
lines	O
and	O
implemented	O
Bidirectional	O
Encoder	O
Representations	O
from	O
Transformers	O
,	O
or	O
BERT	B-MethodName
in	O
short	O
.	O

BERT	B-MethodName
attends	O
to	O
the	O
input	O
tokens	O
in	O
both	O
forward	O
and	O
backward	O
directions	O
while	O
optimizing	O
a	O
Masked	O
Language	O
Model	O
and	O
a	O
Next	O
Sentence	O
Prediction	O
objective	O
objectives	O
.	O

BERT	B-MethodName
Benchmarks	O
An	O
integral	O
part	O
involved	O
in	O
developing	O
various	O
PLMs	O
is	O
providing	O
NLU	O
multitask	O
benchmarks	O
used	O
to	O
demonstrate	O
the	O
linguistic	O
abilities	O
of	O
new	O
models	O
and	O
approaches	O
.	O

English	O
BERT	B-MethodName
models	O
are	O
evaluated	O
on	O
3	O
standard	O
major	O
benchmarks	O
.	O

The	O
Stanford	O
Question	B-DatasetName
Answering	B-MethodName
Dataset	B-DatasetName
(	O
SQuAD	B-DatasetName
)	O
(	O
Rajpurkar	O
et	O
al	O
.	O
,	O
2016	O
)	O
is	O
used	O
for	O
testing	O
paragraph	O
-	O
level	O
reading	O
comprehension	O
abilities	O
.	O

Wang	O
et	O
al	O
.	O

(	O
2018	O
)	O
selected	O
a	O
diverse	O
and	O
relatively	O
hard	O
set	O
of	O
sentence	O
and	O
sentence	O
-	O
pair	O
tasks	O
which	O
comprise	O
the	O
General	B-DatasetName
Language	I-DatasetName
Understanding	I-DatasetName
Evaluation	I-DatasetName
(	O
GLUE	B-DatasetName
)	O
benchmark	O
.	O

The	O
SWAG	B-MethodName
(	O
Situations	O
With	O
Adversarial	B-MethodName
Generations	O
)	O
dataset	O
(	O
Zellers	O
et	O
al	O
.	O
,	O
2018	O
)	O
presents	O
models	O
with	O
partial	O
description	O
of	O
grounded	O
situations	O
to	O
see	O
if	O
they	O
can	O
consistently	O
predict	O
subsequent	O
scenarios	O
,	O
thus	O
indicating	O
abilities	O
of	O
commonsense	O
reasoning.47When	O
evaluating	O
Hebrew	O
PLMs	O
,	O
one	O
of	O
the	O
key	O
pitfalls	O
is	O
that	O
there	O
are	O
no	O
Hebrew	O
versions	O
for	O
these	O
benchmarks	O
.	O

Furthermore	O
,	O
none	O
of	O
the	O
suggested	O
benchmarks	O
account	O
for	O
examining	O
the	O
capacity	O
of	O
PLMs	O
for	O
encoding	O
the	O
word	O
-	O
internal	O
morphological	O
structures	O
which	O
are	O
inherent	O
in	O
MRLs	B-DatasetName
.	O

In	O
this	O
work	O
we	O
enable	O
a	O
generic	O
morphological	O
-	O
level	O
evaluation	O
pipeline	O
that	O
is	O
suited	O
for	O
PLMs	O
of	O
MRLs	B-DatasetName
.	O

Multilingual	O
vs.	O
Monolingual	O
BERT	B-MethodName
Devlin	O
et	O
al	O
.	O

(	O
2019	O
)	O
produced	O
2	O
BERT	B-MethodName
models	O
,	O
for	O
English	O
and	O
Chinese	O
.	O

To	O
support	O
other	O
languages	O
,	O
they	O
trained	O
a	O
multilingual	O
BERT	B-MethodName
(	O
mBERT	O
)	O
model	O
combining	O
texts	O
covering	O
over	O
100	O
languages	O
,	O
in	O
the	O
hoped	O
to	O
benefit	O
low	O
-	O
resource	O
languages	O
with	O
the	O
linguistic	O
information	O
obtained	O
from	O
languages	O
with	O
larger	O
datasets	O
.	O

In	O
reality	O
,	O
however	O
,	O
mBERT	B-MethodName
performance	O
on	O
specific	O
languages	O
has	O
not	O
been	O
as	O
successful	O
as	O
English	O
.	O

Consequently	O
,	O
several	O
research	O
efforts	O
focused	O
on	O
building	O
monolingual	O
BERT	B-MethodName
models	O
as	O
well	O
as	O
providing	O
languagespecific	O
evaluation	O
benchmarks	O
.	O

Liu	O
et	O
al	O
.	O

(	O
2019	O
)	O
trained	O
CamemBERT	O
,	O
a	O
French	O
BERT	B-MethodName
model	O
evaluated	O
on	O
syntactic	O
and	O
semantic	O
tasks	O
in	O
addition	O
to	O
natural	O
language	O
inference	O
tasks	O
.	O

Rybak	O
et	O

al	O
.	O
(	O
2020	O
)	O
trained	O
HerBERT	B-MethodName
,	O
a	O
BERT	B-MethodName
PLM	O
for	O
Polish	O
.	O

They	O
evaluated	O
it	O
on	O
a	O
diverse	O
set	O
of	O
existing	O
NLU	B-TaskName
benchmarks	O
as	O
well	O
as	O
a	O
new	O
dataset	O
for	O
sentiment	O
analysis	O
for	O
the	O
e	O
-	O
commerce	O
domain	O
.	O

Polignano	O
et	O
al	O
.	O

(	O
2019	O
)	O
created	O
Alberto	B-MethodName
,	O
a	O
BERT	B-MethodName
model	O
for	O
Italian	O
,	O
using	O
a	O
massive	O
tweet	O
collection	O
.	O

They	O
tested	O
it	O
on	O
several	O
NLU	B-TaskName
tasks	O
—	O
subjectivity	O
,	O
polarity	O
(	O
sentiment	O
)	O
and	O
irony	O
detection	O
in	O
tweets	O
.	O

In	O
order	O
to	O
obtain	O
a	O
large	O
enough	O
training	O
corpus	O
in	O
low	O
-	O
resources	O
languages	O
,	O
such	O
as	O
Finnish	O
(	O
Virtanen	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
Persian	O
(	O
Farahani	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
a	O
great	O
deal	O
of	O
effort	O
went	O
into	O
filtering	O
and	O
cleaning	O
text	O
samples	O
obtained	O
from	O
web	O
crawls	O
.	O

BERT	B-MethodName
for	O
MRLs	B-DatasetName
Languages	O
with	O
rich	O
morphology	O
introduce	O
another	O
challenge	O
involving	O
the	O
identification	O
and	O
extraction	O
of	O
sub	O
-	O
word	O
morphological	O
information	O
.	O

In	O
many	O
MRLs	B-DatasetName
words	O
are	O
composed	O
of	O
sub	O
-	O
word	O
morphological	O
units	O
,	O
with	O
each	O
unit	O
acting	O
as	O
a	O
single	O
syntactic	O
unit	O
bearing	O
as	O
single	O
POS	O
tag	O
(	O
mimicking	O
‘	O
words	O
’	O
in	O
English	O
)	O
.	O

Antoun	O
et	O
al	O
.	O

(	O
2020	O
)	O
addressed	O
this	O
for	O
Arabic	O
,	O
a	O
Semitic	O
MRLs	O
,	O
by	O
pre	O
-	O
processing	O
the	O
training	O
data	O
using	O
a	O
morphological	O
segmenter	O
,	O
producing	O
morphological	O
segments	O
to	O
be	O
used	O
for	O
training	O
AraBERT	O
instead	O
of	O
the	O
actual	O
words	O
.	O

By	O
doing	O
so	O
,	O
they	O
were	O
able	O
to	O
produce	O
output	O
vectors	O
that	O
corre	O
-	B-MethodName
Language	O
Oscar	O
(	O
duped	O
)	O

Size	O
Wikipedia	O
Articles	O
English	O
2.3	O
T	B-MethodName
6,282,774	O
Russian	O
1.2	O
T	O
1,713,164	O
Chinese	O
508	O
G	O
1,188,715	O
French	O
282	O
G	O
2,316,002	O
Arabic	O
82	O
G	O
1,109,879	O
Hebrew	O
20	O
G	O
292,201	O
Table	O
1	O
:	O
Corpora	B-MethodName
Size	O
Comparison	O
:	O
Resource	O
-	O
savvy	O
languages	O
vs.	O
Hebrew	O
.	O

spond	O
to	O
morphological	O
segments	O
rather	O
than	O
the	O
original	O
space	O
-	O
delimited	O
word	O
-	O
tokens	O
.	O

However	O
,	O
this	O
approach	O
requires	O
the	O
application	O
of	O
the	O
same	O
segmenter	O
at	O
inference	O
time	O
as	O
well	O
,	O
and	O
like	O
any	O
pipeline	O
approach	O
,	O
this	O
setup	O
is	O
susceptible	O
to	O
error	O
propagation	O
.	O

This	O
risk	O
is	O
magnified	O
as	O
words	O
in	O
MRLs	B-DatasetName
may	O
be	O
morphologically	O
ambiguous	O
,	O
and	O
the	O
predicted	O
segments	O
might	O
not	O
represent	O
the	O
correct	O
interpretation	O
of	O
the	O
words	O
.	O

As	O
a	O
result	O
,	O
the	O
quality	O
of	O
the	O
PLM	O
depends	O
on	O
the	O
accuracy	O
achieved	O
by	O
the	O
segmenting	O
component	O
.	O

A	O
particular	O
novelty	O
of	O
this	O
work	O
is	O
notmaking	O
any	O
changes	O
to	O
the	O
input	O
,	O
letting	O
the	O
PLM	O
encode	O
morphological	O
information	O
associated	O
with	O
complete	O
Hebrew	O
tokens	O
.	O

Instead	O
,	O
transforming	O
the	O
resulting	O
contextualized	O
word	O
vectors	O
into	O
morphological	O
-	O
level	O
segments	O
via	O
a	O
novel	O
neural	O
architecture	O
which	O
we	O
discuss	O
shortly	O
.	O

Evaluating	O
PLMs	O
for	O
MRLs	B-DatasetName
Across	O
all	O
of	O
the	O
above	O
-	O
mentioned	O
language	O
-	O
specific	O
PLMs	O
,	O
evaluation	O
was	O
performed	O
on	O
the	O
word-,sentence-	O
or	O
paragraph	O
-	O
level	O
.	O

Non	O
examined	O
the	O
capacity	O
of	O
PLMs	O
to	O
encode	O
sub	O
-	O
word	O
morphological	O
-	O
level	O
information	O
which	O
we	O
focus	O
on	O
in	O
this	O
work	O
.	O

¸	O

Sahin	O
et	O
al	O
.	O

(	O
2019	O
)	O
probed	O
various	O
information	O
types	O
encoded	O
in	O
embedded	O
word	O
vectors	O
.	O

Similarly	O
to	O
us	O
,	O
they	O
focused	O
on	O
languages	O
with	O
rich	O
morphology	O
where	O
linguistic	O
signals	O
are	O
encoded	O
at	O
the	O
morphological	O
,	O
subword	O
level	O
.	O

Their	O
work	O
is	O
more	O
about	O
explainability	O
—	O
showing	O
high	O
positive	O
correlation	O
of	O
probing	O
tasks	O
to	O
the	O
downstream	O
tasks	O
,	O
especially	O
for	O
morphologically	O
rich	O
languages	O
.	O

Unlike	O
us	O
,	O
they	O
assume	O
a	O
single	O
POS	O
tag	O
and	O
set	O
of	O
features	O
per	O
word	O
in	O
their	O
probing	O
tasks	O
.	O

In	O
Hebrew	O
,	O
Arabic	O
and	O
other	O
MRLs	O
,	O
tokens	O
may	O
carry	O
multiple	O
POS	O
per	O
word	O
,	O
and	O
are	O
required	O
to	O
be	O
segmented	O
for	O
further	O
processing	O
.	O

We	O
provide	O
a	O
framework	O
that	O
extracts	O
subword	O
morphological	O
units	O
given	O
contextualized	O
word	O
vectors	O
,	O
that	O
enables	O
to	O
evaluate	O
PLMs	O
on	O
morphologically	O
-	O
aware	O
datasets	O
where	O
words	O
can	O
have	O
multiple	O
POS	O
tags	O
and	O
feature	O
-	O
bundles.48Corpus	O
File	O
Size	O
Sentences	O
Words	O
Oscar	O
(	O
deduped	O
)	O
9.8	O
GB	O
20.9	O
M	O
1,043	O
M	O
Twitter	O
6.9	O
GB	O
71.5	O
M	O
774	O
M	O
Wikipedia	O
1.1	O
GB	O
6.3	O
M	O
127	O
M	O
Total	O
17.9	O
GB	O
98.7	O
M	O
1.9B	O
Table	O
2	O
:	O
AlephBERT	B-MethodName
’s	O
Training	O
Data	O
.	O

3	O
AlephBERT	O
Pre	O
-	O
Training	O
Data	O
The	O
PLM	O
termed	O
AlephBERT	O
that	O
we	O
provide	O
herein	O
is	O
trained	O
on	O
a	O
larger	O
dataset	O
and	O
a	O
larger	O
vocabulary	O
than	O
any	O
Hebrew	O
BERT	B-MethodName
instantiation	O
before	O
.	O

The	O
data	O
we	O
train	O
on	O
is	O
listed	O
in	O
Table	O
2	O
.	O

Concretely	O
,	O
we	O
employ	O
the	O
following	O
datasets	O
for	O
pre	O
-	O
training	O
:	O
(	O
i	O
)	O
Oscar	B-MethodName
:	O

Deduplicated	O
Hebrew	O
portion	O
extracted	O
from	O
Common	B-DatasetName
Crawl	I-DatasetName
via	O
language	O
classification	O
,	O
filtering	O
and	O
cleaning	O
(	O
Ortiz	O
Suárez	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

(	O
ii	O
)	O
Wikipedia	O
:	O

Texts	O
from	O
all	O
of	O
Hebrew	B-DatasetName
Wikipedia	I-DatasetName
,	O
extracted	O
using	O
Attardi	O
(	O
2015	O
)	O
.	O

(	O
iii	O
)	O
Twitter	O
:	O
Hebrew	O
tweets	O
collected	O
between	O
2014	O
-	O
09	O
-	O
28	O
and	O
2018	O
-	O
03	O
-	O
07	O
.	O

We	O
removed	O
markers	O
(	O
“	O
RT	O
:	O
”	O
,	O
“	O
@	O
”	O
user	O
mentions	O
and	O
URLs	O
)	O
,	O
and	O
eliminated	O
duplicates	O
.	O

For	O
data	O
statistics	O
,	O
see	O
Table	O
2	O
.	O

The	O
Hebrew	O
portions	O
of	O
Oscar	O
andWikipedia	O
provide	O
us	O
with	O
a	O
training	O
-	O
set	O
size	O
orders	O
-	O
ofmagnitude	O
smaller	O
compared	O
with	O
resource	O
-	O
savvy	O
languages	O
,	O
as	O
shown	O
in	O
Table	O
1	O
.	O

In	O
order	O
to	O
build	O
a	O
strong	O
PLM	O
we	O
need	O
a	O
considerable	O
boost	O
in	O
the	O
amount	O
of	O
sentences	O
the	O
PLM	O
can	O
learn	O
from	O
,	O
which	O
in	O
our	O
case	O
comes	O
form	O
massive	O
amounts	O
of	O
tweets	O
added	O
to	O
the	O
training	O
set	O
.	O

We	O
acknowledge	O
the	O
potential	O
inherent	O
concerns	O
associated	O
with	O
this	O
data	O
source	O
(	O
population	O
bias	O
,	O
behavior	O
patterns	O
,	O
bot	O
masquerading	O
as	O
humans	O
etc	O
.	O
)	O
and	O
note	O
that	O
we	O
have	O
not	O
made	O
any	O
explicit	O
attempt	O
to	O
identify	O
these	O
cases	O
.	O

Honoring	O
ethical	O
and	O
legal	O
constraints	O
we	O
have	O
not	O
manually	O
analyzed	O
nor	O
published	O
this	O
data	O
source	O
.	O

While	O
the	O
free	O
form	O
language	O
expressed	O
in	O
tweets	O
might	O
differ	O
significantly	O
from	O
the	O
text	O
found	O
in	O
Oscar	O
and	O
Wikipedia	O
,	O
the	O
sheer	O
volume	O
of	O
tweets	O
helps	O
us	O
close	O
the	O
resource	O
gap	O
substantially	O
with	O
minimal	O
effort.3	O
Model	O
We	O
used	O
the	O
Transformers	O
training	O
framework	O
of	O
Huggingface	O
(	O
Wolf	O
et	O
al	O
.	O
,	O
2020	O
)	O
and	O
trained	O
two	O
different	O
models	O
—	O
a	O
small	O
model	O
with	O
6	O
hidden	O
layers	O
learned	O
from	O
the	O
Oscar	O
portion	O
of	O
our	O
dataset	O
,	O
and	O
a	O
base	O
model	O
with	O
12	O
hidden	O
layers	O
which	O
was	O
trained	O
on	O
the	O
entire	O
dataset	O
.	O

The	O
processing	O
units	O
used	O
are	O
wordpieces	O
generated	O
by	O
training	O
BERT	B-MethodName
tokenizers	O
over	O
the	O
respective	O
3For	O
more	O
details	O
and	O
an	O
ethical	O
discussion	O
,	O
see	O
Section	O
8.datasets	O
with	O
a	O
vocabulary	O
size	O
of	O
52	O
K	O
in	O
both	O
cases	O
.	O

Following	O
the	O
work	O
on	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O
2019	O
)	O
we	O
optimize	O
AlephBERT	B-MethodName
with	O
a	O
masked	O
-	O
token	O
prediction	O
loss	O
.	O

We	O
deploy	O
the	O
default	O
masking	O
configuration	O
where	O
15	O
%	O
of	O
word	O
piece	O
tokens	O
are	O
masked	O
.	O

In	O
80	O
%	O
of	O
the	O
cases	O
,	O
they	O
are	O
replaced	O
by	O
[	O
MASK	O
]	O
,	O
in	O
10	O
%	O
of	O
the	O
cases	O
,	O
they	O
are	O
replaced	O
by	O
a	O
random	O
token	O
and	O
in	O
the	O
remaining	O
cases	O
,	O
the	O
masked	O
tokens	O
are	O
left	O
as	O
is	O
.	O

Operation	O
To	O
optimize	O
GPU	O
utilization	O
and	O
decrease	O
training	O
time	O
we	O
split	O
the	O
dataset	O
into	O
4	O
chunks	O
based	O
on	O
the	O
number	O
of	O
tokens	O
in	O
a	O
sentence	O
and	O
consequently	O
we	O
are	O
able	O
to	O
increase	O
batch	O
sizes	O
and	O
dramatically	O
shorten	O
training	O
time	O
.	O

chunk1	O
chunk2	O
chunk3	O
chunk4	O
max	O
tokens	O
0>32	O
32>64	O
64>128	O
128>512	O
num	O
sentences	O

70	O
M	O
20	O
M	O
5	O
M	O
2	O
M	O
We	O
trained	O
for	O
5	O
epochs	O
with	O
learning	O
rate	O
1e4	O
followed	O
by	O
an	O
additional	O
5	O
epochs	O
with	O
learning	O
rate	O
at	O
5e-5	O
for	O
a	O
total	O
of	O
10	O
epochs	O
.	O

We	O
trained	O
AlephBERT	O
baseover	O
the	O
entire	O
dataset	O
on	O
an	O
NVidia	O
DGX	B-DatasetName
server	O
with	O
8	O
V100	O
GPUs	O
which	O
took	O
8	O
days	O
.	O

AlephBERT	B-MethodName
small	O
was	O
trained	O
over	O
the	O
Oscar	O
portion	O
only	O
,	O
using	O
4	O
GTX	B-MethodName
2080ti	O
GPUs	O
taking	O
5	O
days	O
in	O
total	O
.	O

4	O
The	O
Morphological	O
Extraction	O
Model	O
Modern	O
Hebrew	O
is	O
a	O
Semitic	O
language	O
with	O
rich	O
morphology	O
and	O
complex	O
orthography	O
.	O

As	O
a	O
result	O
,	O
the	O
basic	O
processing	O
units	O
in	O
the	O
language	O
are	O
typically	O
smaller	O
than	O
raw	O
space	O
-	O
delimited	O
tokens	O
.	O

Subsequently	O
,	O
most	O
standard	O
evaluation	O
tasks	O
require	O
knowledge	O
of	O
the	O
internal	O
morphological	O
boundaries	O
within	O
the	O
raw	O
tokens	O
.	O

To	O
accommodate	O
this	O
granularity	O
requirement	O
we	O
developed	O
a	O
neural	O
model	O
designed	O
to	O
produce	O
the	O
disambiguated	O
morphological	O
segments	O
for	O
each	O
token	O
in	O
context	O
.	O

These	O
linguistic	O
segmentations	O
are	O
distinct	O
of	O
the	O
word	O
-	O
pieces	O
employed	O
by	O
the	O
PLM	O
.	O

In	O
the	O
morphological	O
extraction	O
neural	O
model	O
,	O
each	O
input	O
token	O
is	O
represented	O
by	O
(	O
one	O
or	O
more	O
)	O
contextualized	O
word	O
-	O
vectors	O
produced	O
by	O
the	O
PLM	O
.	O

Each	O
word	O
-	O
piece	O
token	O
is	O
associated	O
with	O
a	O
vector	O
,	O
and	O
for	O
each	O
space	O
-	O
delimited	O
token	O
,	O
we	O
average	O
the	O
word	O
-	O
piece	O
vectors	O
.	O

We	O
feed	O
the	O
resulting	O
vector	O
into	O
a	O
seq2seq	O
model	O
and	O
encode	O
the	O
surface	O
token	O
as	O
a	O
sequence	O
of	O
characters	O
using	O
a	O
BiLSTM	O
,	O
followed	O
by	O
a	O
decoder	O
that	O
generates	O
an	O
output	O
sequence	O
of	O
characters	O
,	O
using	O
space	O
as	O
a	O
special	O
symbol	O
signaling	O
morphological	O
boundaries.49Raw	O
input	O
/ֹדנֵfנ	O
/	O
דרֹוuנfלביתהלבלַנִ	O
(	O
lbit	O
hlbn	O
)	O
Space	O
-	O
delimited	O
words	O
/ֹדנֵfנ	O
/	O
דרֹוuנfהלבלַנִ	O
(	O
hlbn	O
)	O
/ֹדנֵfלביתדרֹו	O

(	O
lbit	O
)	O
Index	O
5	O
4	O
3	O
2	O
1	O
Segmentation	O
/ֹדנֵfנ	O
/	O
דרֹוuנfלבלַנִ(lbn	O
)	O
white	O
/ֹדנֵfהדרֹו(h	O
)	O
the	O
/ֹדנֵfביתדרֹו(bit	O
)	O
house	O
/ֹדנֵfהדרֹו(h	O
)	O

the	O
/ֹדנֵfלדרֹו(l	O
)	O
to	O
POS	B-MethodName

ADJ	B-DatasetName
DET	B-MethodName
NOUN	I-MethodName

DET	O
ADP	O
Morphology	O
Gender	O
=	O
Masc	O
|Number	O
=	O
Sing	O
PronType	O
=	O
Art	O
Gender	O
=	O
Masc	O
|Number	O
=	O
Sing	O
PronType	O
=	O
Art	O
Dependencies	O
3	O
/	O
amod	O
5	O
/	O
det	O
1	O
/	O
obj	O
3	O
/	O
def	O
0	O
/	O
ROOT	O
Word	O
-	O
level	O
NER	O
E	O
-	O
ORG	O
B	B-MethodName
-	O
ORG	O
Morpheme	O
-	O
level	O
NER	O
E	O
-	O
ORG	O
I	O
-	O
ORG	O
I	O
-	O
ORG	O
B	O
-	O
ORG	O
O	O
Table	O
3	O
:	O
Illustration	O
of	O
Evaluated	O
Word	O
-	O
Based	O
and	O
Morpheme	O
-	O
Based	O
Downstream	O
Tasks	O
.	O

The	O
two	O
-	O
word	O
input	O
phrase	O
“	O
/ֹדנֵfנ	O
/	O
דרֹוuנfלביתהלבלַנִ	O
,	O
”	O
transliterated	O
as	O
“	O
lbit	O
hlbn	O
”	O
(	O
to	O
the	O
White	O
House	O
)	O
,	O
decompose	O
into	O
five	O
morphological	O
segments	O
(	O
‘	O
to	O
-	O
the	O
-	O
house	O
the	O
-	O
white	O
’	O
)	O
.	O

The	O
Hebrew	O
text	O
goes	O
from	O
right	O
to	O
left	O
.	O

Figure	O
2	O
:	O
Illustration	O
of	O
the	O
Morphological	O
Extraction	O
Model	O
.	O

The	O
embedded	O
vectors	O
associated	O
with	O
the	O
wordpieces	O
(	O
v1	O
and	O
v2	O
representing	O
word	O
-	O
piece	O
vectors	O
generated	O
in	O
Figure	O
1	O
)	O
are	O
combined	O
(	O
averaged	O
)	O
to	O
produce	O
a	O
single	O
word	O
context	O
vector	O
.	O

This	O
context	O
vector	O
initializes	O
the	O
hidden	O
(	O
forward	O
and	O
backward	O
)	O
state	O
of	O
a	O
BiLSTM	O
that	O
encodes	O
the	O
characters	O
of	O
the	O
origin	O
word	O
.	O

The	O
decoder	O
LSTM	O
outputs	O
a	O
sequence	O
of	O
characters	O
,	O
where	O
a	O
special	O
empty	O
symbol	O
indicates	O
a	O
morphological	O
segment	O
boundary	O
.	O

In	O
multi	O
-	O
task	O
setup	O
,	O
a	O
fully	O
connected	O
linear	O
layer	O
is	O
used	O
to	O
predict	O
a	O
label	O
whenever	O
a	O
segment	O
boundary	O
is	O
detected	O
.	O

For	O
tasks	O
involving	O
both	O
segments	O
and	O
labels	O
(	O
Part	O
-	O
of	O
-	O
Speech	O
Tagging	O
,	O
Morphological	O
-	O
Features	O
Tagging	O
,	O
Named	O
-	O
Entity	O
Recognition	O
)	O
we	O
expand	O
this	O
network	O
in	O
a	O
multi	O
-	O
task	O
learning	O
setup	O
;	O
when	O
generating	O
an	O
end	O
-	O
of	O
-	O
segment	O
(	O
space	O
)	O
symbol	O
,	O
the	O
model	O
also	O
predicts	O
task	O
label	O
,	O
and	O
we	O
combine	O
the	O
segment	O
-	O
label	O
losses	O
.	O

The	O
complete	O
morphological	O
extraction	O
architecture	O
is	O
illustrated	O
in	O
Figure	O
2	O
.	O
5	O
Experimental	O
Setup	O
Goal	O

In	O
order	O
to	O
empirically	O
gauge	O
the	O
effect	O
of	O
model	O
size	O
and	O
data	O
quantity	O
on	O
the	O
quality	O
of	O
the	O
language	O
model	O
,	O
we	O
compare	O
the	O
performance	O
of	O
AlephBERT	B-MethodName
(	O
both	O
small	O
andbase	O
)	O
with	O
all	O
existing	O
Hebrew	B-DatasetName
BERT	I-DatasetName
instantiations	O
.	O

In	O
this	O
Section	O
,	O
we	O
detail	O
the	O
tasks	O
and	O
evaluation	O
metrics	O
.	O

In	O
the	O
nextSection	O
,	O
we	O
present	O
and	O
analyze	O
the	O
results	O
.	O

5.1	O
Sentence	O
-	O
Based	O
Modeling	O
Sentiment	O
Analysis	O
We	O
first	O
report	O
on	O
a	O
sentence	O
classification	O
task	O
,	O
assigning	O
a	O
sentence	O
with	O
one	O
of	O
three	O
sentiment	O
values	O
:	O
negative	O
,	O
positive	O
,	O
neutral	O
.	O

Sentence	O
-	O
level	O
predictions	O
are	O
achieved	O
by	O
directly	O
fine	O
-	O
tuning	O
the	O
PLM	O
using	O
an	O
additional	O
sentenceclassification	O
head	O
The	O
sentence	O
-	O
level	O
embedding	O
vector	O
representation	O
is	O
the	O
one	O
associated	O
with	O
the	O
special	O

[	O
CLS	O
]	O
BERT	B-MethodName
token	O
.	O

We	O
used	O
a	O
version	O
of	O
the	O
Hebrew	O
Facebook	B-DatasetName
Sentiment	O
dataset	O
(	O
henceforth	O
FB	O
)	O
of	O
Amram	O
et	O
al	O
.	O
(	O
2018	O
)	O
which	O
we	O
corrected	O
by	O
removing	O
leaked	O
samples.4We	O
fine	O
-	O
tuned	O
all	O
models	O
for	O
15	O
epochs	O
with	O
5	O
different	O
seeds	O
,	O
and	O
report	O
mean	O
accuracy	O
.	O

5.2	O
Word	O
-	O
Based	O
Modeling	O
Named	O
Entity	O
Recognition	O

In	O
this	O
setup	O
we	O
assume	O
a	O
sequence	O
labeling	O
task	O
based	O
on	O
spacedelimited	O
word	O
-	O
tokens	O
.	O

The	O
input	O
comprises	O
of	O
the	O
sequence	O
of	O
words	O
in	O
the	O
sentence	O
,	O
and	O
the	O
output	O
contains	O
BIOES	O
tags	O
indicating	O
entity	O
spans	O
.	O

Word	O
-	O
level	O
NER	B-TaskName
predictions	O
are	O
achieved	O
by	O
directly	O
fine	O
-	O
tuning	O
the	O
PLMs	O
using	O
an	O
additional	O
token	O
-	O
classification	O
head	O
In	O
cases	O
where	O
a	O
word	O
is	O
split	O
into	O
multiple	O
word	O
pieces	O
by	O
the	O
PLM	O
tokenizer	O
,	O
we	O
employ	O
common	O
practice	O
and	O
use	O
the	O
first	O
word	O
-	O
piece	O
vector	O
.	O

We	O
evaluate	O
this	O
model	O
on	O
two	O
corpora	O
.	O

(	O
i	O
)	O
The	O
Ben	B-MethodName
-	O
Mordecai	O
(	O
BMC	B-DatasetName
)	O
corpus	O
(	O
Ben	O
Mordecai	O
and	O
Elhadad	O
,	O
2005	O
)	O
,	O
which	O
contains	O
3294	O
sentences	O
with	O
4600	O
entities	O
and	O
seven	O
different	O
entity	O
categories	O
(	O
Date	O
,	O
Location	O
,	O
Money	O
,	O
Organization	O
,	O
Person	O
,	O
Percent	O
,	O
Time	O
)	O
.	O

To	O
remain	O
compatible	O
with	O
the	O
original	O
work	O
we	O
train	O
and	O
test	O
the	O
models	O
on	O
3	O
4This	O
version	O
has	O
a	O
total	O
of	O
8,465	O
samples	O
and	O
is	O
publicly	O
available	O
here	O
:	O
https://github.com/OnlpLab/	O
Hebrew	O
-	O
Sentiment	O
-	O
Data50different	O
splits	O
as	O
in	O
Bareket	O
and	O
Tsarfaty	O
(	O
2020	O
)	O
.	O

(	O
ii	O
)	O
The	O
Named	O
Entities	O
and	O
MOrphology	O
(	O
NEMO	O
)	O
corpus5(Bareket	O
and	O
Tsarfaty	O
,	O
2020	O
)	O
which	O
is	O
an	O
extension	O
of	O
the	O
SPMRL	O
dataset	O
with	O
Named	O
Entities	O
.	O

The	O
NEMO	B-TaskName
corpus	O
contains	O
6220	O
sentences	O
with	O
7713	O
entities	O
of	O
nine	O
entity	O
types	O
(	O
Language	O
,	O
Product	O
,	O
Event	O
,	O
Facility	O
,	O
Geo	O
-	O
Political	O
Entity	O
,	O
Location	O
,	O
Organization	O
,	O
Person	O
,	O
Work	O
-	O
Of	O
-	O
Art	O
)	O
.	O

We	O
trained	O
both	O
models	O
for	O
15	O
epochs	O
with	O
5	O
different	O
seeds	O
and	O
report	O
mean	O
F1	B-MetricName
scores	O
on	O
entity	O
spans	O
.	O

5.3	O
Morpheme	B-MethodName
-	O
Based	O
Modeling	O
Finally	O
,	O
to	O
probe	O
the	O
PLM	O
capacity	O
to	O
accurately	O
predict	O
word	O
-	O
internal	O
structure	O
,	O
we	O
test	O
all	O
models	O
on	O
five	O
tasks	O
that	O
require	O
knowledge	O
of	O
the	O
internal	O
morphology	O
of	O
raw	O
words	O
.	O

The	O
input	O
to	O
all	O
these	O
tasks	O
is	O
a	O
Hebrew	O
sentence	O
represented	O
as	O
a	O
raw	O
sequence	O
of	O
space	O
-	O
delimited	O
words	O
:	O
(	O
i	O
)	O
Segmentation	O
:	O

Generating	O
a	O
sequence	O
of	O
morphological	O
segments	O
representing	O
the	O
basic	O
processing	O
units	O
.	O

These	O
units	O
comply	O
with	O
the	O
2	O
-	O
level	O
representation	O
of	O
tokens	O
defined	O
by	O
UD	O
,	O
each	O
unit	O
with	O
a	O
single	O
POS	O
tag.6	O
(	O
ii	O
)	O
Part	O
-	O
of	O
-	O
Speech	O
(	O
POS	O
)	O
Tagging	O
:	O
Tagging	O
each	O
segment	O
with	O
a	O
single	O
POS	O
.	O

(	O
iii	O
)	O
Morphological	O
Tagging	O
:	O
Tagging	O
each	O
segment	O
with	O
a	O
single	O
POS	O
and	O
a	O
set	O
of	O
features	O
.	O

Equivalent	O
to	O
the	O
AllTags	O
evaluation	O
defined	O
in	O
the	O
CoNLL18	B-DatasetName
shared	O
task.7	O
(	O
iv	O
)	O
Morpheme	O
-	O
Based	O
NER	B-TaskName
:	O
Tagging	O
each	O
segment	O
with	O
a	O
BIOES	O
and	O
its	O
entity	O
-	O
type	O
.	O

(	O
v	O
)	O
Dependency	O
Parsing	O
:	O
Use	O
each	O
segment	O
as	O
a	O
node	O
in	O
the	O
predicted	O
dependency	O
tree	O
.	O

We	O
train	O
and	O
test	O
all	O
morphologically	O
-	O
aware	O
models	O
using	O
two	O
available	O
morphologically	O
-	O
aware	O
Hebrew	O
resources	O
:	O
•The	O
Hebrew	O
Section	O
of	O
the	O
SPMRL	B-MethodName
Task	O
(	O
Seddah	O
et	O

al	O
.	O
,	O
2013	O
)	O
.	O

•The	O
Hebrew	O
Section	O
of	O
the	O
UD	O
treebanks	O
collection	O
(	O
Sadde	O
et	O
al	O
.	O
,	O
2018	O
)	O

All	O
models	O
were	O
trained	O
for	O
15	O
epochs	O
with	O
5	O
different	O
seeds	O
and	O
we	O
report	O
two	O
variants	O
of	O
mean	O
F1	B-MetricName
scores	O
as	O
described	O
next	O
.	O

5Available	O
here	O
:	O
https://github.com/OnlpLab/	O
NEMO	O
-	O
Corpus	O
6https://universaldependencies.org/u/	B-TaskName
overview	O
/	O
tokenization.html	O
7https://universaldependencies.org/	O
conll18	O
/	O
results	O
-	O
alltags.htmlFor	O
tasks	O
(	O
i)–(iv	O
)	O
we	O
use	O
the	O
morphological	O
extraction	O
model	O
(	O
Section	O
4	O
)	O
to	O
extract	O
the	O
morphological	O
segments	O
of	O
each	O
word	O
in	O
context	O
and	O
also	O
predict	O
the	O
labels	O
via	O
Multitask	O
training	O
.	O

For	O
task	O
(	O
iv	O
)	O
the	O
NER	B-TaskName
task	O
,	O
we	O
use	O
the	O
morphologically	O
-	O
annotated	O
data	O
files	O
of	O
the	O
aforementioned	O
SPMRL	B-DatasetName
-	O
based	O
NEMO	B-TaskName
corpus	O
(	O
Bareket	O
and	O
Tsarfaty	O
,	O
2020	O
)	O
.	O

In	O
addition	O
to	O
the	O
multi	O
-	O
task	O
setup	O
described	O
earlier	O
,	O
we	O
design	O
another	O
setup	O
in	O
which	O
we	O
first	O
only	O
segment	O
the	O
text	O
,	O
and	O
then	O
perform	O
fine	O
-	O
tuning	O
with	O
a	O
token	O
classification	O
attention	O
head	O
directly	O
applied	O
to	O
the	O
PLM	O
output	O
for	O
the	O
segmented	O
tokens	O
(	O
similar	O
to	O
the	O
way	O
we	O
fine	O
-	O
tune	O
the	O
PLM	O
for	O
the	O
word	O
-	O
based	O
NER	B-TaskName
task	O
described	O
in	O
the	O
previous	O
section	O
)	O
.	O

We	O
acknowledge	O
that	O
we	O
are	O
fine	O
-	O
tuning	O
the	O
PLM	O
on	O
morphological	O
segments	O
the	O
model	O
was	O
not	O
originally	O
pre	O
-	O
trained	O
on	O
,	O
however	O
,	O
as	O
we	O
shall	O
see	O
shortly	O
,	O
this	O
seemingly	O
unintuitive	O
strategy	O
performs	O
surprisingly	O
well	O
.	O

For	O
task	O
(	O
v	O
)	O
we	O
set	O
up	O
a	O
dependency	O
parsing	O
evaluation	O
pipeline	O
using	O
the	O
standalone	O
Hebrew	O
parser	O
offered	O
by	O
More	O
et	O

al	O
.	O

(	O
2019	O
)	O
(	O
a.k.a	O
YAP	O
)	O
which	O
was	O
trained	O
to	O
produce	O
SPMRL	O
dependency	O
labels	O
.	O

The	O
morphological	O
information	O
for	O
each	O
word	O
(	O
namely	O
the	O
segments	O
and	O
POS	O
tags	O
)	O
is	O
recovered	O
by	O
our	O
morphological	O
extraction	O
model	O
,	O
and	O
is	O
used	O
as	O
input	O
features	O
for	O
the	O
YAP	O
standalone	O
dependency	O
parser	O
.	O

5.4	O
Morpheme	O
-	O
Based	O
Evaluation	O
Metrics	O
Aligned	O
Segment	O
The	O
CoNLL18	O
Shared	B-MethodName
Task	O
evaluation	O
campaign8reports	O
scores	O
for	O
segmentation	O
and	O
POS	O
tagging9for	O
all	O
participating	O
languages	O
.	O

For	O
multi	O
-	O
segment	O
words	O
,	O
the	O
gold	O
and	O
predicted	O
segments	O
are	O
aligned	O
by	O
their	O
Longest	O
Common	B-MethodName
Sub	I-MethodName
-	O
sequence	O
,	O
and	O
only	O
matching	O
segments	O
are	O
counted	O
as	O
true	O
positives	O
.	O

We	O
use	O
the	O
script	O
to	O
compare	O
aligned	O
segment	O
and	O
tagging	O
scores	O
between	O
oracle	O
(	O
gold	O
)	O
segmentation	O
and	O
realistic	O
(	O
predicted	O
)	O
segmentation	O
.	O

Aligned	O
Multi	O
-	O
Set	O
In	O
addition	O
to	O
the	O
CoNLL18	B-DatasetName
metrics	O
,	O
we	O
compute	O
F1	B-MetricName
scores	O
,	O
with	O
a	O
slight	O
but	O
important	O
difference	O
from	O
the	O
shared	O
task	O
,	O
as	O
defined	O
by	O
More	O
et	O

al	O
.	O

(	O
2019	O
)	O
and	O
Seker	O
and	O
Tsarfaty	O
(	O
2020	O
)	O
.	O

For	O
each	O
word	O
,	O
counts	O
are	O
based	O
on	O
multiset	O
intersections	O
of	O
the	O
gold	O
and	O
predicted	O
labels	O
ignoring	O
the	O
order	O
of	O
the	O
segments	O
while	O
account8https://universaldependencies.org/	O
conll18	O
/	O
results.html	O
9respectively	O
referred	O
to	O
as	O
’	O
Segmented	O
Words	O
’	O
and	O
’	O
UPOS	O
’	O
in	O
the	O
CoNLL18	B-DatasetName
evaluation	O
script51Task	O
NER	O
(	O
Word	O
)	O
Sentiment	O

Corpus	O
NEMO	O
BMC	O
FB	B-DatasetName
Prev	O
.	O

SOTA	O
77.75	O
85.22	O
NA	O
mBERT	O
79.07	O
87.77	O
79.07	O
HeBERT	B-MethodName
81.48	O
89.41	O
81.48	O
AlephBERT	O
small	O
78.69	O
89.07	O
78.69	O
AlephBERT	O
base	O
84.91	O
91.12	O
84.91	O
Table	O
4	O
:	O
Word	O
-	O
based	O
NER	O
F1	O
.	O

Previous	O
SOTA	B-TaskName
on	O
both	O
corpora	O
reported	O
by	O
the	O
NEMO	B-DatasetName
models	O
of	O
Bareket	O
and	O
Tsarfaty	O
(	O
2020	O
)	O
.	O

Sentiment	O
Analysis	B-DatasetName
accuracy	O
on	O
the	O
corrected	O
version	O
of	O
the	O
corpus	O
of	O
Amram	O
et	O
al	O
.	O

(	O
2018	O
)	O
.	O

ing	O
for	O
the	O
number	O
of	O
each	O
segment	O
.	O

Aligned	O
mset	O
is	O
based	O
on	O
set	O
difference	O
which	O
acknowledges	O
the	O
possible	O
undercover	O
of	O
covert	O
morphemes	O
which	O
is	O
an	O
appropriate	O
measure	O
of	O
morphological	O
accuracy	O
.	O

Discussion	O
To	O
illustrate	O
the	O
difference	O
between	O
aligned	O
segment	O
andaligned	O
mset	O
,	O
let	O
us	O
take	O
for	O
example	O
the	O
gold	O
segmented	O
tag	O
sequence	O
:	O
b	O
/	O
IN	O
,	O
h	O
/	O
DET	O
,	O
bit	O
/	O
NOUN	O
and	O
the	O
predicted	O
segmented	O
tag	O
sequence	O
b	O
/	O
IN	O
,	O
bit	O
/	O
NOUN	O
.	O

According	O
to	O
aligned	O
segment	O
,	O
the	O
first	O
segment	O
(	O
b	O
/	O
IN	O
)	O
is	O
aligned	O
and	O
counted	O
as	O
a	O
true	O
positive	O
,	O
the	O
second	O
segment	O
however	O
is	O
considered	O
as	O
a	O
false	O
positive	O
(	O
bit	O
/	O
NOUN	O
)	O
and	O
false	O
negative	O
(	O
h	O
/	O
DET	O
)	O
while	O
the	O
third	O
gold	O
segment	O
is	O
also	O
counted	O
as	O
a	O
false	O
negative	O
(	O
bit	O
/	O
NOUN	O
)	O
.	O

On	O
the	O
other	O
hand	O
with	O
aligned	O
multi	O
-	O
set	O
both	O
b	O
/	O
IN	O
andbit	O
/	O
NOUN	O
exist	O
in	O
the	O
gold	O
and	O
predicted	O
sets	O
and	O
counted	O
as	O
true	O
positives	O
,	O
while	O
h	O
/	O
DET	O
is	O
mismatched	O
and	O
counted	O
as	O
a	O
false	O
negative	O
.	O

In	O
both	O
cased	O
the	O
total	O
counts	O
across	O
words	O
in	O
the	O
entire	O
datasets	O
are	O
incremented	O
accordingly	O
and	O
finally	O
used	O
for	O
computing	O
Precision	O
,	O
Recall	O
and	O
F1	B-MetricName
.	O

6	O
Results	O
Sentence	O
-	O
Level	O
Task	O
Sentiment	O
analysis	O
accuracy	O
results	O
are	O
provided	O
in	O
Table	O
4	O
.	O

All	O
BERTbased	B-MethodName
models	O
substantially	O
outperform	O
the	O
original	O
CNN	B-TaskName
Baseline	O
reported	O
by	O
Amram	O
et	O
al	O
.	O

(	O
2018	O
)	O
.	O

AlephBERT	B-MethodName
baseis	O
setting	O
a	O
new	O
SOTA	B-MethodName
.	O

Word	O
-	O
Based	O
Task	O
On	O
our	O
two	O
NER	B-TaskName
benchmarks	O
,	O
we	O
report	O
F1	B-MetricName
scores	O
on	O
the	O
word	O
-	O
based	O
fine	O
-	O
tuned	O
model	O
in	O
Table	O
4	O
.	O

While	O
we	O
see	O
noticeable	O
improvements	O
for	O
the	O
mBERT	B-MethodName
and	O
HeBert	B-MethodName
variants	O
over	O
the	O
current	O
SOTA	O
,	O
the	O
most	O
significant	O
increase	O
is	O
achieved	O
by	O
AlephBERT	O
base	O
,	O
setting	O
a	O
new	O
and	O
improved	O
SOTA	O
on	O
this	O
task	O
.	O

Morpheme	O
-	O
Level	O
Tasks	O
As	O
a	O
particular	O
novelty	O
of	O
this	O
work	O
,	O
we	O
report	O
BERT	B-MethodName
-	O
based	O
results	O
on	O
sub	O
-	O
Task	O
Segment	O
POS	O
Features	O
UAS	O
LAS	O
Prev	O
.	O

SOTA	O
NA	B-MethodName
90.49	O
85.98	O
75.73	O
69.41	O
mBERT	O
97.36	O
93.37	O
89.36	O
80.17	O
74.9	O
HeBERT	B-MethodName
97.97	O
94.61	O
90.93	O
81.86	O
76.54	O
AlephBERT	O
small	O
97.71	O
94.11	O
90.56	O
81.5	O
76.07	O
AlephBERT	B-MethodName
base	O
98.10	O
94.90	O
91.41	O
82.07	O
76.9	O
Table	O
5	O
:	O
Morpheme	O
-	O
Based	O
results	O
on	O
the	O
SPMRL	O
corpus	O
.	O

Aligned	O
MultiSet	O
(	O
mset	O
)	O
F1	O
for	O
Segmentation	O
,	O
POS	O
tags	O
and	O
Morphological	O
Features	O
-	O
previous	O
SOTA	B-DatasetName
reported	O
by	O
Seker	O
and	O
Tsarfaty	O
(	O
2020	O
)	O
(	O
POS	B-DatasetName
)	O
and	O
More	O
et	O
al	O
.	O

(	O
2019	O
)	O
(	O
features	O
)	O
.	O

Labeled	O
and	O
Unlabeled	O
Accuracy	O
Scores	O
for	O
morphological	O
-	O
level	O
Dependency	O
Parsing	O
-	O
previous	O
SOTA	B-DatasetName
reported	O
by	O
More	O
et	O
al	O
.	O

(	O
2019	O
)	O
(	O
uninfused	O
/	O
realistic	O
scenario	O
)	O
Task	O
Segment	O
POS	O
Features	O
Prev	O
.	O

SOTA	O
NA	B-MethodName
94.02	O
NA	O
mBERT	O
97.70	O
94.76	O
90.98	O
HeBERT	O
98.05	O
96.07	O
92.53	O
AlephBERT	O
small	O
97.86	O
95.58	O
92.06	O
AlephBERT	O
base	O
98.20	O
96.20	O
93.05	O
Table	O
6	O
:	O
Morpheme	O
-	O
Based	O
Aligned	O
MultiSet	O
(	O
mset	O
)	O
F1	B-MetricName
results	O
on	O
the	O
UD	O
corpus	O
.	O

Previous	O
SOTA	B-DatasetName
reported	O
by	O
Seker	O
and	O
Tsarfaty	O
(	O
2020	O
)	O
(	O
POS	B-DatasetName
)	O
word	O
(	O
segment	O
-	O
level	O
)	O
information	O
.	O

Specifically	O
,	O
we	O
evaluate	O
word	O
segmentation	O
,	O
POS	O
,	O
Morphological	O
Features	O
,	O
NER	B-TaskName
and	O
dependencies	O
compared	O
against	O
morphologically	O
-	O
labeled	O
test	O
sets	O
.	O

In	O
all	O
cases	O
,	O
we	O
use	O
raw	O
space	O
-	O
delimited	O
tokens	O
as	O
input	O
and	O
produce	O
morphological	O
segments	O
with	O
our	O
morphological	O
extraction	O
model	O
.	O

Table	O
5	O
presents	O
evaluation	O
results	O
for	O
the	O
SPRML	B-DatasetName
dataset	O
,	O
compared	O
against	O
the	O
previous	O
SOTA	O
of	O
More	O
et	O
al	O
.	O

(	O
2019	O
)	O
.	O

For	O
segmentation	O
,	O
POS	O
tagging	O
,	O
and	O
morphological	O
tagging	O
we	O
report	O
aligned	O
multiset	O
F1	B-MetricName
scores	O
.	O

BERT	B-MethodName
-	O
based	O
segmentations	O
are	O
similar	O
,	O
all	O
scoring	O
in	O
the	O
high	O
range	O
of	O
97	O
-	O
98	O
F1	O
,	O
which	O
are	O
hard	O
to	O
improve	O
further.10	O
For	O
POS	O
tagging	O
and	O
morphological	O
features	O
,	O
all	O
BERT	B-MethodName
-	O
based	O
models	O
considerably	O
outperform	O
the	O
previous	O
SOTA	O
.	O

For	O
syntactic	O
dependencies	O
we	O
report	O
labeled	O
and	O
unlabeled	O
accuracy	O
scores	O
of	O
the	O
trees	O
generated	O
by	O
YAP	O
(	O
More	O
et	O
al	O
.	O
,	O
2019	O
)	O
on	O
our	O
predicted	O
segmentation	O
.	O

Here	O
we	O
see	O
impressive	O
improvement	O
compared	O
to	O
the	O
previous	O
SOTA	B-MethodName
of	O
a	O
joint	O
morpho	O
-	O
syntactic	O
framework	O
.	O

It	O
confirms	O
that	O
morphological	O
errors	O
early	O
in	O
the	O
pipeline	O
negatively	O
impact	O
downstream	O
tasks	O
,	O
and	O
highlight	O
the	O
importance	O
of	O
morphologically	O
-	O
driven	O
benchmarks	O
10According	O
to	O
error	O
analysis	O
,	O
most	O
of	O
these	O
errors	O
are	O
annotation	O
errors	O
or	O
truly	O
ambiguous	O
cases.52Task	O
Segment	O
POS	O
Features	O
Prev	O
.	O

SOTA	O
96.03	O
93.75	O
91.24	O
mBERT	O
97.17	O
94.27	O
90.51	O
HeBERT	B-MethodName
97.54	O
95.60	O
92.15	O
AlephBERT	O
small	O
97.31	O
95.13	O
91.65	O
AlephBERT	O
base	O
97.70	O
95.84	O
92.71	O
Table	O
7	O
:	O
Morpheme	O
-	O
Based	O
Aligned	O
(	O
CoNLL	O
shared	O
task	O
)	O
F1	O
on	O
the	O
UD	O
corpus	O
.	O

Previous	O
SOTA	B-MethodName
reported	O
by	O
Minh	O
Van	O
Nguyen	O
and	O
Nguyen	O
(	O
2021	O
)	O
Architecture	O
Pipeline	B-DatasetName
Pipeline	I-DatasetName
MultiTask	B-MethodName
Segmentation	I-MethodName
(	O
Oracle	B-MethodName
)	O
(	O
Predicted	O
)	O
Task	O
Seg	O
NER	B-TaskName
Seg	O
NER	B-TaskName
Seg	O
NER	B-TaskName
Prev	O
.	O

SOTA	O
100.00	O
79.10	O
95.15	O
69.52	O
97.05	O
77.11	O
mBERT	O
100.00	O
77.92	O
97.68	O
72.72	O
97.24	O
72.97	O
HeBERT	B-MethodName
100.00	O
82	O
98.15	O
76.74	O
97.92	O
74.86	O
AlephBERT	O
small	O
100.00	O
79.44	O
97.78	O
73.08	O
97.74	O
72.46	O
AlephBERT	B-MethodName
base	O
100.00	O
83.94	O
98.29	O
80.15	O
98.19	O
79.15	O
Table	O
8	O
:	O
Morpheme	B-MethodName
-	O
Based	B-MethodName
NER	O
F1	O
on	O
the	O
NEMO	B-DatasetName
corpus	O
.	O

Previous	O
SOTA	B-MethodName
reported	O
by	O
Bareket	O
and	O
Tsarfaty	O
(	O
2020	O
)	O
for	O
the	O
Pipeline	B-DatasetName
(	O
Oracle	O
)	O
,	O
Pipeline	B-DatasetName
(	O
Predicted	O
)	O
and	O
a	O
Hybrid	B-MethodName
(	O
almost	O
-	O
joint	O
)	O
scenarios	O
,	O
respectively	O
.	O

as	O
an	O
integral	O
part	O
of	O
PLM	O
evaluation	O
for	O
MRLs	B-DatasetName
.	O

All	O
in	O
all	O
we	O
see	O
a	O
repeating	O
trend	O
placing	O
AlephBERT	B-MethodName
basefirst	O
on	O
all	O
morphological	O
tasks	O
,	O

indicating	O
the	O
depth	O
of	O
the	O
model	O
and	O
a	O
larger	O
pretraining	O
dataset	O
improve	O
the	O
ability	O
of	O
the	O
PLM	O
to	O
capture	O
word	O
-	O
internal	O
structure	O
.	O

These	O
trends	O
are	O
replicated	O
on	O
the	O
UD	O
Hebrew	O
corpus	O
,	O
for	O
two	O
different	O
evaluation	O
metrics	O
—	O
the	O
Aligned	B-DatasetName
MultiSet	O
F1	B-DatasetName
Scores	O
as	O
in	O
previous	O
work	O
on	O
Hebrew	O
(	O
More	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
(	O
Seker	O
and	O
Tsarfaty	O
,	O
2020	O
)	O
,	O
and	O
the	O
Aligned	B-DatasetName
Segment	O
F1	B-DatasetName
scores	O
metrics	O
as	O
described	O
in	O
the	O
UD	O
shared	O
task	O
(	O
Zeman	O
et	O
al	O
.	O
,	O
2018	O
)	O
—	O
reported	O
in	O
Tables	O
6	O
and	O
7	O
respectively	O
.	O

Morpheme	O
-	O
Level	O
NER	B-TaskName
results	O
Earlier	O
in	O
this	O
section	O
we	O
considered	O
NER	B-TaskName
a	O
word	O
-	O
level	O
task	O
that	O
simply	O
requires	O
fine	O
-	O
tuning	O
on	O
the	O
word	O
level	O
.	O

However	O
,	O
this	O
setup	O
is	O
not	O
accurate	O
enough	O
and	O
less	O
useful	O
for	O
downstream	O
tasks	O
,	O
since	O
the	O
exact	O
entity	O
boundaries	O
are	O
often	O
word	O
internal	O
(	O
Bareket	O
and	O
Tsarfaty	O
,	O
2020	O
)	O
.	O

We	O
hence	O
report	O
morpheme	O
-	O
based	O
NER	B-TaskName
evaluation	O
,	O
respecting	O
the	O
exact	O
boundaries	O
of	O
entity	O
mentions	O
.	O

To	O
obtain	O
morpheme	O
-	O
based	O
labeled	O
-	O
span	O
of	O
Named	O
Entities	O
,	O
we	O
could	O
either	O
employ	O
a	O
pipeline	O
,	O
first	O
predicting	O
segmentation	O
and	O
then	O
applying	O
a	O
fine	O
-	O
tuned	O
labeling	O
model	O
directly	O
on	O
the	O
segments	O
,	O
or	O
employ	O
a	O
multi	O
-	O
task	O
model	O
and	O
predict	O
NER	B-TaskName
labels	O
while	O
performing	O
segmentation	O
.	O

Table	O
8	O
presents	O
segmentation	O
and	O
NER	B-TaskName
results	O
for	O
3	O
different	O
scenarios	O
:	O
(	O
i	O
)	O
a	O
pipeline	O
as	O
-	O
suming	O
gold	O
segmentation	O
(	O
ii	O
)	O
a	O
pipeline	O
assuming	O
predicted	O
segmentation	O
(	O
iii	O
)	O
segmentation	O
and	O
NER	B-TaskName
labels	O
obtained	O
jointly	O
in	O
a	O
multi	O
-	O
task	O
setup	O
.	O

AlephBERT	B-MethodName
baseconsistently	O
scores	O
highest	O
in	O
all	O
3	O
.	O

Looking	O
at	O
the	O
Pipeline	B-MethodName
-	O
Predicted	O
scores	O
,	O
there	O
is	O
a	O
clear	O
correlation	O
between	O
a	O
higher	O
segmentation	O
quality	O
of	O
a	O
PLM	O
and	O
its	O
ability	O
to	O
produce	O
better	O
NER	B-TaskName
results	O
.	O

Moreover	O
,	O
the	O
differences	O
in	O
NER	B-TaskName
scores	O
are	O
considerable	O
(	O
unlike	O
the	O
subtle	O
differences	O
in	O
segmentation	O
,	O
POS	O
and	O
morphological	O
features	O
scores	O
)	O
and	O
draw	O
our	O
attention	O
to	O
the	O
relationship	O
between	O
the	O
size	O
of	O
the	O
PLM	O
,	O
the	O
size	O
of	O
the	O
pre	O
-	O
training	O
data	O
and	O
the	O
quality	O
of	O
the	O
final	O
NER	B-TaskName
models	O
.	O

Specifically	O
,	O
HeBERT	B-MethodName
and	O
AlephBERT	B-MethodName
smallwere	O
both	O
pre	O
-	O
trained	O
on	O
similar	O
datasets	O
and	O
comparable	O
vocabulary	O
sizes	O
(	O
heBERT	B-MethodName
with	O
30	O
K	O
and	O
AlephBERT	O
-	O
small	O
with	O
52	O
K	O
)	O
but	O
HeBERT	B-MethodName
,	O
with	O
its	O
12	O
hidden	O
layers	O
,	O
performs	O
better	O
compared	O
to	O
AlephBERT	O
smallwhich	O
is	O
composed	O
of	O
only	O
6	O
hidden	O
layers	O
.	O

It	O
thus	O
appears	O
that	O
semantic	O
information	O
is	O
learned	O
in	O
those	O
deeper	O
layers	O
,	O
helping	O
in	O
both	O
discriminating	O
entities	O
and	O
improving	O
the	O
morphological	O
segmentation	O
capacity	O
.	O

In	O
addition	O
,	O
comparing	O
AlephBERT	B-MethodName
base	O
and	O
HeBERT	B-MethodName
we	O
note	O
that	O
they	O
are	O
both	O
modeled	O
with	O
the	O
same	O
12	O
hidden	O
layer	O
architecture	O
—	O
the	O
only	O
differences	O
between	O
them	O
are	O
in	O
the	O
size	O
of	O
their	O
vocabularies	O
(	O
30	O
K	O
vs	O
52	O
K	O
respectively	O
)	O
and	O
the	O
size	O
of	O
the	O
training	O
data	O
(	O
Oscar	O
-	B-MethodName
Wikipedia	O
vs	O
OscarWikipedia	B-DatasetName
-	O
Tweets	O
)	O
.	O

The	O
improvements	O
exhibited	O
by	O
AlephBERT	B-MethodName
base	O
,	O
compared	O
to	O
HeBERT	B-MethodName
,	O
suggest	O
large	O
amounts	O
of	O
training	O
data	O
and	O
larger	O
vocabulary	O
are	O
invaluable	O
.	O

By	O
exposing	O
AlephBERT	O
baseto	O
a	O
substantially	O
larger	O
amount	O
of	O
text	O
we	O
increased	O
the	O
ability	O
of	O
the	O
PLM	O
to	O
encode	O
syntactic	O
and	O
semantic	O
signals	O
associated	O
with	O
Named	O
Entities	O
.	O

Our	O
NER	B-TaskName
experiments	O
further	O
suggest	O
that	O
a	O
pipeline	O
composed	O
of	O
our	O
accurate	O
morphological	O
segmentation	O
model	O
followed	O
by	O
AlephBERT	B-MethodName
base	O
with	O
a	O
token	O
classification	O
head	O
is	O
the	O
best	O
strategy	O
for	O
generating	O
morphologically	O
-	O
aware	O
NER	B-TaskName
labels	O
.	O

Finally	O
,	O
we	O
observe	O
that	O
while	O
AlephBERT	B-MethodName
excels	O
at	O
morphosyntactic	O
tasks	O
,	O
on	O
tasks	O
with	O
a	O
more	O
semantic	O
flavor	O
there	O
is	O
room	O
for	O
improvement	O
.	O

7	O
Conclusion	O
Modern	O
Hebrew	O
,	O
a	O
morphologically	O
-	O
rich	O
and	O
medium	O
-	O
resourced	O
language	O
,	O
has	O
for	O
long	O
suffered	O
from	O
a	O
gap	O
in	O
the	O
resources	O
available	O
for	O
NLP	O
applications	O
,	O
and	O
lower	O
level	O
of	O
empirical	O
results	O
than	O
observed	O
in	O
other	O
,	O
resource	O
-	O
rich	O
languages	O
.	O

This53work	O
provides	O
the	O
first	O
step	O
in	O
remedying	O
the	O
situation	O
,	O
by	O
making	O
available	O
a	O
large	O
Hebrew	O
PLM	O
,	O
named	O
AlephBERT	B-MethodName
,	O
with	O
larger	O
vocabulary	O
and	O
larger	O
training	O
set	O
than	O
any	O
Hebrew	B-DatasetName
PLM	O
before	O
,	O
and	O
with	O
clear	O
evidence	O
as	O
to	O
its	O
empirical	O
advantages	O
.	O

Crucially	O
,	O
we	O
augment	O
the	O
PLM	O
with	O
a	O
morphological	O
disambiguation	O
component	O
that	O
matches	O
the	O
input	O
granularity	O
of	O
the	O
downstream	O
tasks	O
.	O

Our	O
system	O
does	O
not	O
presuppose	O
Hebrewspecific	B-MethodName
linguistic	O
-	O
rules	O
,	O
and	O
can	O
be	O
transparently	O
applied	O
to	O
any	O
language	O
for	O
which	O
2	O
-	O
level	O
segmentation	O
data	O
(	O
i.e.	O
,	O
the	O
standard	O
UD	O
benchmarks	O
)	O
exists	O
.	O

AlephBERT	B-MethodName
baseobtains	O
state	O
-	O
of	O
-	O
the	O
-	O
art	O
results	O
on	O
morphological	O
segmentation	O
,	O
POS	O
tagging	O
,	O
morphological	O
feature	O
extraction	O
,	O
dependency	O
parsing	O
,	O
named	O
-	O
entity	O
recognition	O
,	O
and	O
sentiment	O
analysis	O
,	O
outperforming	O
all	O
existing	O
Hebrew	O
PLMs	O
.	O

Our	O
proposed	O
morphologically	O
-	O
driven	O
pipeline11serves	O
as	O
a	O
solid	O
foundation	O
for	O
future	O
evaluation	O
of	O
Hebrew	O
PLMs	B-TaskName
and	O
of	O
MRLs	B-DatasetName
in	O
general	O
.	O

8	O
Ethical	O
Statement	O
We	O
follow	O
Bender	O
and	O
Friedman	O
(	O
2018	O
)	O
regarding	O
professional	O
practice	O
for	O
NLP	O
technology	O
and	O
address	O
ethical	O
issues	O
that	O
result	O
from	O
the	O
use	O
of	O
data	O
in	O
the	O
development	O
of	O
the	O
models	O
in	O
our	O
work	O
.	O

Pre	O
-	O
Training	O
Data	O
.	O

The	O
two	O
initial	O
data	O
sources	O
we	O
used	O
to	O
pre	O
-	O
train	O
the	O
language	O
models	O
are	O
Oscar	O
and	O
Wikipedia	O
.	O

In	O
using	O
the	O
Wikipedia	O
and	O
Oscar	O
we	O
followed	O
standard	O
language	O
model	O
training	O
efforts	O
,	O
such	O
as	O
BERT	B-MethodName
and	O
RoBERTa	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

We	O
use	O
the	O
languagespecific	O
Oscar	B-MetricName
data	O
according	O
to	O
the	O
terms	O
specified	O
in	O
Ortiz	O
Suárez	O
et	O
al	O
.	O

(	O
2020	O
)	O

and	O
we	O
extract	O
texts	O
from	O
language	O
-	O
specific	O
Wikipedia	O
dumps	O
.	O

On	O
top	O
of	O
that	O
,	O
a	O
big	O
portion	O
of	O
the	O
data	O
used	O
to	O
train	O
AlephBERT	B-MethodName
originates	O
from	O
the	O
Twitter	O
sample	O
stream.12	O
As	O
shown	O
in	O
Table	O
2	O
,	O
this	O
data	O
set	O
includes	O
70	O
M	O
Hebrew	O
tweets	O
which	O
were	O
collected	O
over	O
a	O
period	O
of	O
4	O
years	O
(	O
2014	O
to	O
2018	O
)	O
.	O

We	O
acknowledge	O
the	O
potential	O
concerns	O
inherently	O
associated	O
with	O
Twitter	O
data	O
(	O
population	O
bias	O
,	O
behavior	O
patterns	O
,	O
bot	O
masquerading	O
as	O
humans	O
etc	O
.	O
)	O
and	O
note	O
that	O
we	O
have	O
not	O
made	O
any	O
explicit	O
attempt	O
to	O
identify	O
these	O
cases	O
.	O

We	O
only	O
used	O
the	O
text	O
field	O
of	O
the	O
tweets	O
and	O
completely	O
discard	O
any	O
other	O
information	O
included	O
11Available	O
at	O
https://github.com/OnlpLab/	O
AlephBERT	O
12https://developer.twitter.com/en/	O
docs	O
/	O
twitter	O
-	O
api	O
/	O
tweets	O
/	O
volume	O
-	O
streams/	O
api	O
-	O
reference	O
/	O
get	O
-	O
tweets	O
-	O
sample	O
-	O
streamin	O
the	O
stream	O
(	O
such	O
as	O
identities	O
,	O
followers	O
,	O
structure	O
of	O
threads	O
,	O
date	O
of	O
publication	O
,	O
etc	O
)	O
.	O

We	O
have	O
not	O
made	O
any	O
effort	O
to	O
identify	O
or	O
filter	O
out	O
any	O
samples	O
based	O
on	O
user	O
properties	O
such	O
as	O
age	O
,	O
gender	O
and	O
location	O
nor	O
have	O
we	O
made	O
any	O
effort	O
to	O
identify	O
content	O
characteristics	O
such	O
as	O
genre	O
or	O
topic	O
.	O

To	O
reduce	O
exposure	O
of	O
private	O
information	O
we	O
cleaned	O
up	O
all	O
user	O
mentions	O
and	O
URLs	O
from	O
the	O
text	O
.	O

Honoring	O
ethical	O
and	O
legal	O
constraints	O
we	O
have	O
not	O
manually	O
analyzed	O
nor	O
published	O
this	O
data	O
source	O
.	O

While	O
the	O
free	O
-	O
form	O
language	O
expressed	O
in	O
tweets	O
might	O
differ	O
significantly	O
from	O
the	O
text	O
found	O
in	O
Oscar	B-DatasetName
/	O
Wikipedia	B-DatasetName
,	O
the	O
sheer	O
volume	O
of	O
tweets	O
helps	O
us	O
close	O
the	O
substantial	O
resource	O
gap	O
.	O

Training	O
and	O
Evaluation	O
Benchmarks	O
.	O

The	O
SPMRL	O
(	O
Seddah	O
et	O
al	O
.	O
,	O
2013	O
)	O
and	O
UD	O
(	O
Sadde	O
et	O
al	O
.	O
,	O
2018	O
)	O
datasets	O
we	O
used	O
for	O
evaluating	O
segmentation	O
,	O
tagging	O
and	O
parsing	O
,	O
were	O
used	O
to	O
both	O
train	O
our	O
morphological	O
extraction	O
model	O
as	O
well	O
as	O
provide	O
us	O
with	O
the	O
test	O
data	O
to	O
evaluate	O
on	O
morphological	O
level	O
tasks	O
.	O

Both	O
datasets	O
are	O
publicly	O
available	O
and	O
widely	O
used	O
in	O
research	O
and	O
industry	O
.	O

The	O
NEMO	B-TaskName
corpus	O
(	O
Bareket	O
and	O
Tsarfaty	O
,	O
2020	O
)	O
used	O
to	O
train	O
and	O
evaluate	O
word	O
and	O
morpheme	O
level	O
NER	B-TaskName
is	O
an	O
extension	O
of	O
the	O
SPMRL	O
dataset	O
augmented	O
with	O
entities	O
and	O
follows	O
the	O
same	O
license	O
terms	O
.	O

The	O
BMC	B-DatasetName
dataset	O
used	O
for	O
training	O
and	O
evaluating	O
word	O
-	O
level	O
NER	B-TaskName
was	O
created	O
and	O
published	O
by	O
Ben	O
Mordecai	O
and	O
Elhadad	O
(	O
2005	O
)	O
and	O
it	O
is	O
publicly	O
available	O
for	O
NER	B-TaskName
evaluation	O
.	O

We	O
used	O
the	O
sentiment	O
analysis	O
dataset	O
of	O
Amram	O
et	O
al	O
.	O

(	O
2018	O
)	O
for	O
training	O
and	O
evaluating	O
AlephBERT	B-MethodName
on	O
a	O
sentence	O
level	O
task	O
,	O
and	O
we	O
follow	O
their	O
terms	O
of	O
use	O
.	O

As	O
mentioned	O
,	O
this	O
dataset	O
had	O
some	O
flows	O
,	O
and	O
we	O
describe	O
carefully	O
the	O
steps	O
we	O
’ve	O
taken	O
to	O
fix	O
them	O
before	O
using	O
this	O
corpus	O
in	O
our	O
experiments	O
for	O
internal	O
evaluation	O
purposes	O
.	O

We	O
make	O
our	O
in	O
-	O
house	O
cleaning	O
scripts	O
and	O
split	O
information	O
publicly	O
available	O
.	O

Acknowledgements	O
This	O
research	O
was	O
funded	O
by	O
the	O
European	O
Research	O
Council	O
(	O
ERC	O
grant	O
agreement	O
no	O
.	O
677352	O
)	O
and	O
by	O
a	O
research	O
grant	O
from	O
the	O
Ministry	O
of	O
Science	O
and	O
Technology	O
(	O
MOST	B-TaskName
)	O
of	O
the	O
Israeli	O
Government	O
,	O
for	O
which	O
we	O
are	O
grateful	O
.	O

References	O
Adam	O
Amram	O
,	O
Anat	O
Ben	O
-	O
David	O
,	O
and	O
Reut	O
Tsarfaty	O
.	O
2018	O
.	O

Representations	O
and	O
architectures	O
in	O
neu-54ral	O
sentiment	O
analysis	O
for	O
morphologically	O
rich	O
languages	O
:	O
A	O
case	O
study	O
from	O
modern	O
hebrew	O
.	O

In	O
Proceedings	O
of	O
the	O
27th	O
International	B-DatasetName
Conference	I-DatasetName
on	O
Computational	B-DatasetName
Linguistics	O
,	O
COLING	O
2018	O
,	O
Santa	O
Fe	O
,	O
New	O
Mexico	O
,	O
USA	O
,	O
August	O
20	O
-	O
26	O
,	O
2018	O
,	O
pages	O
2242	O
–	O
2252	O
.	O

Wissam	O
Antoun	O
,	O
Fady	O
Baly	O
,	O
and	O
Hazem	O
Hajj	O
.	O

2020	O
.	O

AraBERT	B-MethodName
:	O
Transformer	O
-	O
based	O
model	O
for	O
Arabic	O
language	O
understanding	O
.	O

In	O
Proceedings	O
of	O
the	O
4th	O
Workshop	B-DatasetName
on	O
Open	O
-	O
Source	O
Arabic	O
Corpora	O
and	O
Processing	O
Tools	O
,	O
with	O
a	O
Shared	O
Task	O
on	O
Offensive	O
Language	O
Detection	O
,	O
pages	O
9–15	O
,	O
Marseille	O
,	O
France	O
.	O

European	O
Language	O
Resource	O
Association	O
.	O

Giusepppe	O
Attardi	O
.	O

2015	O
.	O

Wikiextractor	O
.	O

https://	O
github.com/attardi/wikiextractor	O
.	O

Dan	O
Bareket	O
and	O
Reut	O
Tsarfaty	O
.	O

2020	O
.	O

Neural	O
modeling	O
for	O
named	O
entities	O
and	O
morphology	O
(	O
nemoˆ2	O
)	O
.	O

CoRR	O
,	O
abs/2007.15620	O
.	O

Naama	O
Ben	O
Mordecai	O
and	O
Michael	O
Elhadad	O
.	O

2005	O
.	O

Hebrew	O
named	O
entity	O
recognition	O
.	O

Emily	O
M.	O
Bender	O
and	O
Batya	O
Friedman	O
.	O

2018	O
.	O

Data	O
statements	O
for	O
natural	O
language	O
processing	O
:	O
Toward	O
mitigating	O
system	O
bias	O
and	O
enabling	O
better	O
science	O
.	O

Transactions	O
of	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
,	O
6:587–604	O
.	O

Tom	O
Brown	O
,	O
Benjamin	O
Mann	O
,	O
Nick	O
Ryder	O
,	O
Melanie	O
Subbiah	O
,	O
Jared	O
D	O
Kaplan	O
,	O
Prafulla	O
Dhariwal	O
,	O
Arvind	O
Neelakantan	O
,	O
Pranav	O
Shyam	O
,	O
Girish	O
Sastry	O
,	O
Amanda	O
Askell	O
,	O
Sandhini	O
Agarwal	O
,	O
Ariel	O
Herbert	O
-	O
V	B-MethodName
oss	O
,	O
Gretchen	O
Krueger	O
,	O
Tom	O
Henighan	O
,	O
Rewon	O
Child	O
,	O
Aditya	O
Ramesh	O
,	O
Daniel	O
Ziegler	O
,	O
Jeffrey	O
Wu	O
,	O
Clemens	O
Winter	O
,	O
Chris	O
Hesse	O
,	O
Mark	O
Chen	O
,	O
Eric	O
Sigler	O
,	O
Mateusz	O
Litwin	O
,	O
Scott	O
Gray	O
,	O
Benjamin	O
Chess	O
,	O
Jack	O
Clark	O
,	O
Christopher	O
Berner	O
,	O
Sam	O
McCandlish	O
,	O
Alec	O
Radford	O
,	O
Ilya	O
Sutskever	O
,	O
and	O
Dario	O
Amodei	O
.	O

2020	O
.	O

Language	O
models	O
are	O
few	O
-	O
shot	O
learners	O
.	O

In	O
Advances	B-DatasetName
in	O
Neural	O
Information	O
Processing	O
Systems	O
,	O
volume	O
33	O
,	O
pages	O
1877–1901	O
.	O

Curran	O
Associates	O
,	O
Inc.	O
Avihay	O
Chriqui	O
and	O
Inbal	O
Yahav	O
.	O
2021	O
.	O

Hebert	O
|	O
&	O
hebemo	O
:	O
a	O
hebrew	O
bert	O
model	O
and	O
a	O
tool	O
for	O
polarity	O
analysis	O
and	O
emotion	O
recognition	O
.	O

Jacob	O
Devlin	O
,	O
Ming	O
-	O
Wei	O
Chang	O
,	O
Kenton	O
Lee	O
,	O
and	O
Kristina	O
Toutanova	O
.	O
2019	O
.	O

BERT	B-MethodName
:	O
Pre	O
-	O
training	O
of	O
deep	O
bidirectional	O
transformers	O
for	O
language	O
understanding	O
.	O

In	O
Proceedings	O
of	O
the	O
2019	O
Conference	B-MetricName
of	O
the	B-MethodName
North	I-MethodName
American	O
Chapter	B-MethodName
of	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
:	O
Human	O
Language	O
Technologies	O
,	O
Volume	O
1	O
(	O
Long	O
and	O
Short	O
Papers	O
)	O
,	O
pages	O
4171–4186	O
,	O
Minneapolis	O
,	O
Minnesota	O
.	O

Association	O
for	O
Computational	O
Linguistics	O
.	O

Mehrdad	O
Farahani	O
,	O
Mohammad	O
Gharachorloo	O
,	O
Marzieh	O
Farahani	O
,	O
and	O
Mohammad	O
Manthouri	O
.	O
2020	O
.	O

Parsbert	O
:	O
Transformer	O
-	O
based	O
model	O
for	O
persian	O
language	O
understanding	O
.	O

Jeremy	O
Howard	O
and	O
Sebastian	O
Ruder	O
.	O

2018	O
.	O

Universal	O
language	O
model	O
fine	O
-	O
tuning	O
for	O
text	O
classification	O
.	O

InProceedings	O
of	O
the	O
56th	O
Annual	O
Meeting	O
of	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
(	O
Volume	O
1	O
:	O
Long	O
Papers	O
)	O
,	O
pages	O
328–339	O
,	O
Melbourne	O
,	O
Australia	O
.	O
Association	O
for	O
Computational	O
Linguistics	O
.	O

Stav	O
Klein	O
and	O
Reut	O
Tsarfaty	O
.	O

2020	O
.	O

Getting	O
the	O
#	O
#	O
life	O
out	O
of	O
living	O
:	O
How	O
adequate	O
are	O
word	O
-	O
pieces	O
for	O
modelling	O
complex	O
morphology	O
?	O

In	O
Proceedings	O
of	O
the	O
17th	O
SIGMORPHON	O
Workshop	O
on	O
Computational	O
Research	O
in	O
Phonetics	O
,	O
Phonology	O
,	O
and	O
Morphology	O
,	O
SIGMORPHON	O
2020	O
,	O
Online	O
,	O
July	O
10	O
,	O
2020	O
,	O
pages	O
204–209	O
.	O

Yinhan	O
Liu	O
,	O
Myle	O
Ott	O
,	O
Naman	O
Goyal	O
,	O
Jingfei	O
Du	O
,	O
Mandar	O
Joshi	O
,	O
Danqi	O
Chen	O
,	O
Omer	O
Levy	O
,	O
Mike	O
Lewis	O
,	O
Luke	O
Zettlemoyer	O
,	O
and	O
Veselin	O
Stoyanov	O
.	O

2019	O
.	O
RoBERTa	B-MethodName
:	O
A	O
Robustly	O
Optimized	O
BERT	B-MethodName
Pretraining	O
Approach	O
.	O

Amir	O
Pouran	O
Ben	O
Veyseh	O
Minh	O
Van	O
Nguyen	O
,	O
Viet	O
Lai	O
and	O
Thien	O
Huu	O
Nguyen	O
.	O
2021	O
.	O

Trankit	B-MethodName
:	O

A	O
lightweight	O
transformer	O
-	O
based	O
toolkit	O
for	O
multilingual	O
natural	O
language	O
processing	O
.	O

In	O
Proceedings	O
of	O
the	O
16th	O
Conference	B-DatasetName
of	O
the	O
European	B-DatasetName
Chapter	B-MethodName
of	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
:	O
System	O
Demonstrations	O
.	O

Amir	O
More	O
,	O
Amit	O
Seker	O
,	O
Victoria	O
Basmova	O
,	O
and	O
Reut	O
Tsarfaty	O
.	O

2019	O
.	O

Joint	O
transition	O
-	O
based	O
models	O
for	O
morpho	O
-	O
syntactic	O
parsing	O
:	O
Parsing	O
strategies	O
for	O
mrls	O
and	O
a	O
case	O
study	O
from	O
modern	O
hebrew	O
.	O

Trans	O
.	O

Assoc	O
.	O

Comput	O
.	O

Linguistics	O
,	O
7:33–48	O
.	O

Pedro	O
Javier	O
Ortiz	O
Suárez	O
,	O
Laurent	O
Romary	O
,	O
and	O
Benoît	O
Sagot	O
.	O

2020	O
.	O

A	O
monolingual	O
approach	O
to	O
contextualized	O
word	O
embeddings	O
for	O
mid	O
-	O
resource	O
languages	O
.	O

InProceedings	O
of	O
the	O
58th	O
Annual	O
Meeting	O
of	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
,	O
pages	O
1703	O
–	O
1714	O
,	O
Online	O
.	O

Association	O
for	O
Computational	O
Linguistics	O
.	O

Matthew	O
E.	O
Peters	O
,	O
Mark	O
Neumann	O
,	O
Mohit	O
Iyyer	O
,	O
Matt	O
Gardner	O
,	O
Christopher	O
Clark	O
,	O
Kenton	O
Lee	O
,	O
and	O
Luke	O
Zettlemoyer	O
.	O

2018	O
.	O

Deep	O
contextualized	O
word	O
representations	O
.	O

In	O
Proceedings	O
of	O
the	O
2018	O
Conference	O
of	O
the	B-MethodName
North	I-MethodName
American	O
Chapter	B-MethodName
of	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
:	O
Human	O
Language	O
Technologies	O
,	O
Volume	O
1	O
(	O
Long	O
Papers	O
)	O
,	O
pages	O
2227–2237	O
,	O
New	O
Orleans	O
,	O
Louisiana	O
.	O
Association	O
for	O
Computational	O
Linguistics	O
.	O

Marco	O
Polignano	O
,	O
Pierpaolo	O
Basile	O
,	O
Marco	O
de	O
Gemmis	O
,	O
Giovanni	O
Semeraro	O
,	O
and	O
Valerio	O
Basile	O
.	O

2019	O
.	O

Alberto	B-MethodName
:	O
Italian	O
bert	O
language	O
understanding	O
model	O
for	O
nlp	O
challenging	O
tasks	O
based	O
on	O
tweets	O
.	O

Alec	O
Radford	O
and	O
Ilya	O
Sutskever	O
.	O

2018	O
.	O

Improving	O
language	O
understanding	O
by	O
generative	O
pre	O
-	O
training	O
.	O
Inarxiv	O
.	O

Colin	O
Raffel	O
,	O
Noam	O
Shazeer	O
,	O
Adam	O
Roberts	O
,	O
Katherine	O
Lee	O
,	O
Sharan	O
Narang	O
,	O
Michael	O
Matena	O
,	O
Yanqi	O
Zhou	O
,	O
Wei	O
Li	O
,	O
and	O
Peter	O
J.	O
Liu	O
.	O
2020	O
.	O

Exploring	O
the55limits	O
of	O
transfer	O
learning	O
with	O
a	O
unified	O
text	O
-	O
to	O
-	O
text	O
transformer	O
.	O

Journal	O
of	O
Machine	O
Learning	O
Research	O
,	O
21(140):1–67	O
.	O

Pranav	O
Rajpurkar	O
,	O
Jian	O
Zhang	O
,	O
Konstantin	O
Lopyrev	O
,	O
and	O
Percy	O
Liang	O
.	O

2016	O
.	O

SQuAD	B-DatasetName
:	O
100,000	O
+	O
questions	O
for	O
machine	O
comprehension	O
of	O
text	O
.	O

In	O
Proceedings	O
of	O
the	O
2016	O
Conference	B-DatasetName
on	O
Empirical	B-MethodName
Methods	O
in	O
Natural	O
Language	O
Processing	O
,	O
pages	O
2383–2392	O
,	O
Austin	O
,	O
Texas	O
.	O
Association	O
for	O
Computational	O
Linguistics	O
.	O
Piotr	O
Rybak	O
,	O
Robert	O
Mroczkowski	O
,	O
Janusz	O
Tracz	O
,	O
and	O
Ireneusz	O
Gawlik	O
.	O

2020	O
.	O

KLEJ	B-MethodName
:	O
Comprehensive	O
benchmark	O
for	O
Polish	O
language	O
understanding	O
.	O

In	O
Proceedings	O
of	O
the	O
58th	O
Annual	O
Meeting	O
of	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
,	O
pages	O
1191	O
–	O
1201	O
,	O
Online	O
.	O

Association	O
for	O
Computational	O
Linguistics	O
.	O

Shoval	O
Sadde	O
,	O
Amit	O
Seker	O
,	O
and	O
Reut	O
Tsarfaty	O
.	O
2018	O
.	O

The	O
hebrew	O
universal	O
dependency	O
treebank	O
:	O

Past	O
present	O
and	O
future	O
.	O

In	O
Proceedings	O
of	O
the	O
Second	O
Workshop	B-DatasetName
on	O
Universal	B-MethodName
Dependencies	O
,	O
UDW@EMNLP	O
2018	O
,	O
Brussels	O
,	O
Belgium	O
,	O
November	O
1	O
,	O
2018	O
,	O
pages	O
133–143	O
.	O

Gözde	O
Gül	O
¸	O
Sahin	O
,	O
Clara	O
Vania	O
,	O
Ilia	O
Kuznetsov	O
,	O
and	O
Iryna	O
Gurevych	O
.	O

2019	O
.	O

LINSPECTOR	B-MethodName
:	O
multilingual	O
probing	O
tasks	O
for	O
word	O
representations	O
.	O

CoRR	B-DatasetName
,	O
abs/1903.09442	O
.	O
Djamé	O
Seddah	O
,	O
Reut	O
Tsarfaty	O
,	O
Sandra	O
Kübler	O
,	O
Marie	O
Candito	O
,	O
Jinho	O
D.	O
Choi	O
,	O
Richárd	O
Farkas	O
,	O
Jennifer	O
Foster	O
,	O
Iakes	O
Goenaga	O
,	O
Koldo	O
Gojenola	O
Galletebeitia	O
,	O
Yoav	O
Goldberg	O
,	O
Spence	O
Green	O
,	O
Nizar	O
Habash	O
,	O
Marco	O
Kuhlmann	O
,	O
Wolfgang	O
Maier	O
,	O
Joakim	O
Nivre	O
,	O
Adam	O
Przepiórkowski	O
,	O
Ryan	O
Roth	O
,	O
Wolfgang	O
Seeker	O
,	O
Yannick	O
Versley	O
,	O
Veronika	O
Vincze	O
,	O
Marcin	O
Wolin	O

ski	B-MethodName
,	O
Alina	O
Wróblewska	O
,	O
and	O
Éric	O
Villemonte	O
de	O
la	O
Clergerie	O
.	O

2013	O
.	O

Overview	O
of	O
the	O
SPMRL	O
2013	O
shared	O
task	O
:	O
A	O
cross	O
-	O
framework	O
evaluation	O
of	O
parsing	O
morphologically	O
rich	O
languages	O
.	O

In	O
Proceedings	O
of	O
the	O
Fourth	O
Workshop	B-DatasetName
on	O
Statistical	O
Parsing	O
of	O
Morphologically	O
-	O
Rich	O
Languages	O
,	O
SPMRL@EMNLP	B-DatasetName
2013	O
,	O
Seattle	O
,	O
Washington	O
,	O
USA	O
,	O
October	O
18	O
,	O
2013	O
,	O
pages	O
146–182	O
.	O
Amit	O
Seker	O
and	O
Reut	O
Tsarfaty	O
.	O

2020	O
.	O

A	O
pointer	O
network	O
architecture	O
for	O
joint	O
morphological	O
segmentation	O
and	O
tagging	O
.	O

In	O
Findings	O
of	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
:	O
EMNLP	O
2020	O
,	O
pages	O
4368–4378	O
,	O
Online	O
.	O

Association	O
for	O
Computational	O
Linguistics	O
.	O

Rico	O
Sennrich	O
,	O
Barry	O
Haddow	O
,	O
and	O
Alexandra	O
Birch	O
.	O
2016	O
.	O

Neural	O
machine	O
translation	O
of	O
rare	O
words	O
with	O
subword	O
units	O
.	O

In	O
Proceedings	O
of	O
the	O
54th	O
Annual	O
Meeting	O
of	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
(	O
Volume	O
1	O
:	O
Long	O
Papers	O
)	O
,	O
pages	O
1715–1725	O
,	O
Berlin	O
,	O
Germany	O
.	O

Association	O
for	O
Computational	O
Linguistics	O
.	O

Reut	O
Tsarfaty	O
,	O
Dan	O
Bareket	O
,	O
Stav	O
Klein	O
,	O
and	O
Amit	O
Seker	O
.	O
2020	O
.	O

From	O
SPMRL	B-DatasetName
to	O
NMRL	B-TaskName
:	O
what	O
did	O
we	O
learn(and	O
unlearn	O
)	O
in	O
a	O
decade	O
of	O
parsing	O
morphologicallyrich	O
languages	O
(	O
mrls	O
)	O
?	O

In	O
Proceedings	O
of	O
the	O
58th	O
Annual	O
Meeting	O
of	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
,	O
ACL	B-MethodName
2020	B-DatasetName
,	O
Online	O
,	O
July	O
5	O
-	O
10	O
,	O
2020	O
,	O
pages	O
7396–7408	O
.	O
Antti	O
Virtanen	O
,	O
Jenna	O
Kanerva	O
,	O
Rami	O
Ilo	O
,	O
Jouni	O
Luoma	O
,	O
Juhani	O
Luotolahti	O
,	O
Tapio	O
Salakoski	O
,	O
Filip	O
Ginter	O
,	O
and	O
Sampo	O
Pyysalo	O
.	O

2019	O
.	O

Multilingual	O
is	O
not	O
enough	O
:	O
Bert	B-MethodName
for	O
finnish	O
.	O

Alex	O
Wang	O
,	O
Amanpreet	O
Singh	O
,	O
Julian	O
Michael	O
,	O
Felix	O
Hill	O
,	O
Omer	O
Levy	O
,	O
and	O
Samuel	O
Bowman	O
.	O

2018	O
.	O

GLUE	B-DatasetName
:	O
A	O
multi	O
-	O
task	O
benchmark	O
and	O
analysis	O
platform	O
for	O
natural	O
language	O
understanding	O
.	O

In	O
Proceedings	O
of	O
the	O
2018	O
EMNLP	O
Workshop	B-DatasetName
BlackboxNLP	I-DatasetName
:	O
Analyzing	O
and	O
Interpreting	O
Neural	B-MethodName
Networks	I-MethodName
for	O
NLP	O
,	O
pages	O
353–355	O
,	O
Brussels	O
,	O
Belgium	O
.	O

Association	O
for	O
Computational	O
Linguistics	O
.	O

Thomas	O
Wolf	O
,	O
Lysandre	O
Debut	O
,	O
Victor	O
Sanh	O
,	O
Julien	O
Chaumond	O
,	O
Clement	O
Delangue	O
,	O
Anthony	O
Moi	O
,	O
Pierric	O
Cistac	O
,	O
Tim	O
Rault	O
,	O
Rémi	O
Louf	O
,	O
Morgan	O
Funtowicz	O
,	O
Joe	O
Davison	O
,	O
Sam	O
Shleifer	O
,	O
Patrick	O
von	O
Platen	O
,	O
Clara	O
Ma	O
,	O
Yacine	O
Jernite	O
,	O
Julien	O
Plu	O
,	O
Canwen	O
Xu	O
,	O
Teven	O
Le	O
Scao	O
,	O
Sylvain	O
Gugger	O
,	O
Mariama	O
Drame	O
,	O
Quentin	O
Lhoest	O
,	O
and	O
Alexander	O
M.	O
Rush	O
.	O

2020	O
.	O

Transformers	O
:	O
State	O
-	O
of	O
-	O
the	O
-	O
art	O
natural	O
language	O
processing	O
.	O

In	O
Proceedings	O
of	O
the	O
2020	B-DatasetName
Conference	I-DatasetName
on	O
Empirical	B-MethodName
Methods	O
in	O
Natural	O
Language	O
Processing	O
:	O
System	O
Demonstrations	O
,	O
pages	O
38–45	O
,	O
Online	O
.	O

Association	O
for	O
Computational	O
Linguistics	O
.	O

Rowan	O
Zellers	O
,	O
Yonatan	O
Bisk	O
,	O
Roy	O
Schwartz	O
,	O
and	O
Yejin	O
Choi	O
.	O

2018	O
.	O

SWAG	B-MethodName
:	O

A	O
large	O
-	O
scale	O
adversarial	O
dataset	O
for	O
grounded	O
commonsense	O
inference	O
.	O

In	O
Proceedings	O
of	O
the	O
2018	O
Conference	B-DatasetName
on	O
Empirical	B-MethodName
Methods	O
in	O
Natural	O
Language	O
Processing	O
,	O
pages	O
93–104	O
,	O
Brussels	O
,	O
Belgium	O
.	O

Association	O
for	O
Computational	O
Linguistics	O
.	O

Daniel	O
Zeman	O
,	O
Jan	O
Haji	O
ˇc	O
,	O
Martin	O
Popel	O
,	O
Martin	O
Potthast	O
,	O
Milan	O
Straka	O
,	O
Filip	O
Ginter	O
,	O
Joakim	O
Nivre	O
,	O
and	O
Slav	O
Petrov	O
.	O

2018	O
.	O

CoNLL	O
2018	O
shared	O
task	O
:	O
Multilingual	O
parsing	O
from	O
raw	O
text	O
to	O
Universal	O
Dependencies	O
.	O

In	O
Proceedings	O
of	O
the	O
CoNLL	B-DatasetName
2018	O
Shared	O
Task	O
:	O
Multilingual	O
Parsing	O
from	O
Raw	O
Text	O
to	O
Universal	O
Dependencies	O
,	O
pages	O
1–21	O
,	O
Brussels	O
,	O
Belgium	O
.	O

Association	O
for	O
Computational	O
Linguistics.56	O

Proceedings	O
of	O
the	O
59th	O
Annual	O
Meeting	O
of	O
the	O
Association	O
for	O
Computational	O
Linguistics	O
and	O
the	O
11th	O
International	B-DatasetName
Joint	I-DatasetName
Conference	I-DatasetName
on	O
Natural	O
Language	O
Processing	O
,	O
pages	O
5882–5893	O
August	O
1–6	O
,	O
2021	O
.	O

©	O
2021	O
Association	O
for	O
Computational	O
Linguistics5882SMedBERT	O
:	O
A	O
Knowledge	O
-	O
Enhanced	O
Pre	O
-	O
trained	O
Language	O
Model	O
with	O
Structured	O
Semantics	O
for	O
Medical	O
Text	O
Mining	O
Taolin	O
Zhang1;2;3	O
,	O
Zerui	O
Cai4	O
,	O
Chengyu	O
Wang2	O
,	O
Minghui	O
Qiu2	O
Bite	O
Yang5,Xiaofeng	O
He4;6	O
1School	O
of	O
Software	O
Engineering	O
,	O
East	O
China	O
Normal	O
University2Alibaba	O
Group	O
3Shanghai	O
Key	O
Laboratory	O
of	O
Trsustworthy	O
Computing	O
4School	O
of	O
Computer	O
Science	O
and	O
Technology	O
,	O
East	O
China	O
Normal	O
University5DXY	O
6Shanghai	O
Research	O
Institute	O
for	O
Intelligent	O
Autonomous	O
Systems	O
zhangtl0519@gmail.com	O
,	O
zrcai	O
flow@126.com	O
,	O
yangbt@dxy.cn	O
fchengyu.wcy	O
,	O
minghui.qmh	O
g@alibaba-inc.com	O
,	O
hexf@cs.ecnu.edu.cn	O
Abstract	O

Recently	O
,	O
the	O
performance	O
of	O
Pre	O
-	O
trained	O
Language	O
Models	B-MethodName
(	O
PLMs	O
)	O
has	O
been	O
signiﬁcantly	O
improved	O
by	O
injecting	O
knowledge	O
facts	O
to	O
enhance	O
their	O
abilities	O
of	O
language	O
understanding	O
.	O

For	O
medical	O
domains	O
,	O
the	O
background	O
knowledge	O
sources	O
are	O
especially	O
useful	O
,	O
due	O
to	O
the	O
massive	O
medical	O
terms	O
and	O
their	O
complicated	O
relations	O
are	O
difﬁcult	O
to	O
understand	O
in	O
text	O
.	O

In	O
this	O
work	O
,	O
we	O
introduce	O
SMedBERT	B-MethodName
,	O
a	O
medical	O
PLM	O
trained	O
on	O
large	O
-	O
scale	O
medical	O
corpora	O
,	O
incorporating	O
deep	O
structured	O
semantics	O
knowledge	O
from	O
neighbours	O
of	O
linked	O
-	O
entity	O
.	O

In	O
SMedBERT	B-MethodName
,	O
the	O
mention	O
-	O
neighbour	O
hybrid	O
attention	O
is	O
proposed	O
to	O
learn	O
heterogeneousentity	O
information	O
,	O
which	O
infuses	O
the	O
semantic	O
representations	O
of	O
entity	O
types	O
into	O
the	O
homogeneous	O
neighbouring	O
entity	O
structure	O
.	O

Apart	O
from	O
knowledge	O
integration	O
as	O
external	O
features	O
,	O
we	O
propose	O
to	O
employ	O
the	O
neighbors	O
of	O
linked	O
-	O
entities	O
in	O
the	O
knowledge	O
graph	O
as	O
additional	O
global	O
contexts	O
of	O
text	O
mentions	O
,	O
allowing	O
them	O
to	O
communicate	O
via	O
shared	O
neighbors	O
,	O
thus	O
enrich	O
their	O
semantic	O
representations	O
.	O

Experiments	O
demonstrate	O
that	O
SMedBERT	B-MethodName
significantly	O
outperforms	O
strong	O
baselines	O
in	O
various	O
knowledge	O
-	O
intensive	O
Chinese	O
medical	O
tasks	O
.	O

It	O
also	O
improves	O
the	O
performance	O
of	O
other	O
tasks	O
such	O
as	O
question	O
answering	O
,	O
question	O
matching	O
and	O
natural	O
language	O
inference.1	O
1	O
Introduction	O
Pre	O
-	O
trained	O
Language	O
Models	O
(	O
PLMs	O
)	O
learn	O
effective	O
context	O
representations	O
with	O
self	O
-	O
supervised	O
tasks	O
,	O
spotlighting	O
in	O
various	O
NLP	O
tasks	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2019a	O
;	O

Nan	O
et	O
al	O
.	O
,	O
2020	O
;	O
Liu	O
et	O
al	O
.	O
,	O
2020a	O
)	O
.	O

In	O
addition	O
,	O
Knowledge	O
-	B-MethodName
Enhanced	B-DatasetName
PLMs	I-DatasetName
(	O
KEPLMs	B-MethodName
)	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2019	O
;	O
Liu	O
et	O
al	O
.	O
,	O
2020b	O
;	O
Wang	O
et	O
al	O
.	O
,	O
2019b	O
)	O
further	O
beneﬁt	O
language	O
understanding	O
by	O
Corresponding	O
author	O
.	O

1The	O
code	O
and	O
pre	O
-	O
trained	O
models	O
will	O
be	O
available	O
at	O
https://github.com/MatNLP/SMedBERT	O
.	O

ఉᆆ	B-MethodName
⯲	O
(	O
sore	O
throat	O
)	O

᯦ශߖ	O
⣬	O
⯻	O
∈	O
(	O
novel	O
coronavirus	O
)	O

Symptom	O
Cause	O
of	O
disease	O
લ੮᝕ḉ	O
(	O
respiratory	O
infection	O
)	O

Disease	O
લ੮㔲ਾᖷ	O
(	O
respiratory	O
syndrome	O
)	O
Symptom	O
-	O
Disease	O
Cause	O
-	O
Disease	O
Entity	O
Type	O
Relation	O
COVID-19	O
,QSXW7H[W	O
䓡։	O
ਇ	O
✣	O
θఉᆆ	O
⯲	O
θ㞯	O
⌱	O
ᱥ᝕ḉ	O
᯦ශߖ	O
⣬	O
⯻	O
∈	O
(	O
COVID-19	O
)	O
Ⲻ	O
⯽	O
⣬	O
Ⱦ	O
(	O
Fever	O
,	O
sore	O
throat	O
,	O
and	O
diarrhea	O
are	O
symptoms	O
of	O
novel	O
coronavirus	O
(	O
COVID-19	O
)	O
.	O
)	O

1HLJKERULQJ(QWLW\IURP';<.	O
*	O
/LQNHG(QWLW\	O

㛰	O
⛄	O
(	O
pneumonia	O
)	O
᝕ḉ〇	O
(	O
infectious	O
Department	O
)	O
Symptom	O
-	O
Symptom	O
Medical	O
Department	O
ਇ	O
✝	O
(	O
fever	O
)	O
Cause	O
-	O
Department	O
Figure	O
1	O
:	O
Example	O
of	O
neighboring	O
entity	O
information	O
in	O
medical	O
text	O
.	O

(	O
Best	O
viewed	O
in	O
color	O
)	O
grounding	O
these	O
PLMs	O
with	O
high	O
-	O
quality	O
,	O
humancurated	O
knowledge	O
facts	O
,	O
which	O
are	O
difﬁcult	O
to	O
learn	O
from	O
raw	O
texts	O
.	O

In	O
the	O
literatures	O
,	O
a	O
majority	O
of	O
KEPLMs	B-MethodName
(	O
Zhang	O
et	O
al	O
.	O
,	O
2020a	O
;	O
Hayashi	O
et	O
al	O
.	O
,	O
2020	O
;	O
Sun	O
et	O
al	O
.	O
,	O
2020	O
)	O
inject	O
information	O
of	O
entities	O
corresponding	O
to	O
mention	O
-	O
spans	O
from	O
Knowledge	B-DatasetName
Graphs	I-DatasetName
(	O
KGs	O
)	O
into	O
contextual	O
representations	O
.	O

However	O
,	O
those	O
KEPLMs	B-MethodName
only	O
utilize	O
linked	O
-	O
entity	O
in	O
the	O
KGs	O
as	O
auxiliary	O
information	O
,	O
which	O
pay	O
little	O
attention	O
to	O
the	O
neighboring	O
structured	O
semantics	O
information	O
of	O
the	O
entity	O
linked	O
with	O
text	O
mentions	O
.	O

In	O
the	O
medical	O
context	O
,	O
there	O
exist	O
complicated	O
domain	O
knowledge	O
such	O
as	O
relations	O
and	O
medical	O
facts	O
among	O
medical	O
terms	O
(	O
Rotmensch	O
et	O
al	O
.	O
,	O
2017	O
;	O
Li	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
which	O
are	O
difﬁcult	O
to	O
model	O
using	O
previous	O
approaches	O
.	O

To	O
address	O
this	O
issue	O
,	O
we	O
consider	O
leveraging	O
structured	O
semantics	O
knowledge	O
in	O
medical	O
KGs	O
from	O
the	O
two	O
aspects	O
.	O

(	O
1	O
)	O
Rich	O
semantic	O
information	O
from	O
neighboring	O
structures	O
of	O
linked	O
-	O
entities	O
,	O
such	O
as	O
entity	O
types	O
and	O
relations	O
,	O
are	O
highly	O
useful	O
for	O
medical	O
text	O
understanding	O
.	O

As	O
in	O
Figure	O
1	O
,	O
“	O
°	O
	O
 	O
¶ÅÒ	O
”	O
(	O
novel	O
coronavirus	O
)	O
can	O
be	O
the	O
cause	O
of	O
many	O
diseases	O
,	O
such	O
as	O
“	O
º	O
”	O
(	O
pneumonia	O
)	O
and	O
“	O
|8ü”5883(respiratory	O
syndrome).2(2	O
)	O
Additionally	O
,	O
we	O
leverage	O
neighbors	O
of	O
linked	O
-	O
entity	O
as	O
global	O
“	O
contexts	O
”	O
to	O
complement	O
plain	O
-	O
text	O
contexts	O
used	O
in	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013a	O
;	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
.	O

The	O
structure	O
knowledge	O
contained	O
in	O
neighbouring	O
entities	O
can	O
act	O
as	O
the	O
“	O
knowledge	O
bridge	O
”	O
between	O
mention	O
-	O
spans	O
,	O
facilitating	O
the	O
interaction	O
of	O
different	O
mention	O
representations	O
.	O

Hence	O
,	O
PLMs	O
can	O
learn	O
better	O
representations	O
for	O
rare	O
medical	O
terms	O
.	O

In	O
this	O
paper	O
,	O
we	O
introduce	O
SMedBERT	B-MethodName
,	O
a	O
KEPLM	B-MethodName
pre	O
-	O
trained	O
over	O
large	O
-	O
scale	O
medical	O
corpora	O
and	O
medical	O
KGs	O
.	O

To	O
the	O
best	O
of	O
our	O
knowledge	O
,	O
SMedBERT	B-MethodName
is	O
the	O
ﬁrst	O
PLM	O
with	O
structured	O
semantics	O
knowledge	O
injected	O
in	O
the	O
medical	O
domain	O
.	O

Speciﬁcally	O
,	O
the	O
contributions	O
of	O
SMedBERT	B-MethodName
mainly	O
include	O
two	O
modules	O
:	O
Mention	O
-	O
neighbor	O
Hybrid	O
Attention	O
:	O
We	O
fuse	O
the	O
embeddings	O
of	O
the	O
node	O
and	O
type	O
of	O
linkedentity	O
neighbors	O
into	O
contextual	O
target	O
mention	O
representations	O
.	O

The	O
type	O
-	O
level	O
and	O
node	O
-	O
level	O
attentions	O
help	O
to	O
learn	O
the	O
importance	O
of	O
entity	O
types	O
and	O
the	O
neighbors	O
of	O
linked	O
-	O
entity	O
,	O
respectively	O
,	O
in	O
order	O
to	O
reduce	O
the	O
knowledge	O
noise	O
injected	O
into	O
the	O
model	O
.	O

The	O
type	O
-	O
level	O
attention	O
transforms	O
the	O
homogeneous	O
node	O
-	O
level	O
attention	O
into	O
a	O
heterogeneous	O
learning	O
process	O
of	O
neighboring	O
entities	O
.	O

Mention	O
-	O
neighbor	O
Context	O
Modeling	O
:	O
We	O
propose	O
two	O
novel	O
self	O
-	O
supervised	O
learning	O
tasks	O
for	O
promoting	O
interaction	O
between	O
mention	O
-	O
span	O
and	O
corresponding	O
global	O
context	O
,	O
namely	O
masked	O
neighbor	O
modeling	O
and	O
masked	O
mention	O
modeling	O
.	O

The	O
former	O
enriches	O
the	O
representations	O
of	O
“	O
context	O
”	O
neighboring	O
entities	O
based	O
on	O
the	O
well	O
trained	O
“	O
target	O
word	O
”	O
mention	O
-	O
span	O
,	O
while	O
the	O
latter	O
focuses	O
on	O
gathering	O
those	O
information	O
back	O
from	O
neighboring	O
entities	O
to	O
the	O
masked	O
target	O
like	O
low	O
-	O
frequency	O
mention	O
-	O
span	O
which	O
is	O
poorly	O
represented	O
(	O
Turian	O
et	O
al	O
.	O
,	O
2010	O
)	O
.	O

In	O
the	O
experiments	O
,	O
we	O
compare	O
SMedBERT	B-MethodName
against	O
various	O
strong	O
baselines	O
,	O
including	O
mainstream	O
KEPLMs	B-MethodName
pre	O
-	O
trained	O
over	O
our	O
medical	O
resources	O
.	O

The	O
underlying	O
medical	O
NLP	O
tasks	O
include	O
:	O
named	O
entity	O
recognition	O
,	O
relation	O
extraction	O
,	O
question	O
answering	O
,	O
question	O
matching	O
and	O
natural	O
language	O
inference	O
.	O

The	O
results	O
show	O
that	O
SMedBERT	B-MethodName
consistently	O
outperforms	O
all	O
the	O
baselines	O
on	O
these	O
tasks	O
.	O

2Although	O
we	O
focus	O
on	O
Chinese	O
medical	O
PLMs	O
here	O
.	O

The	O
proposed	O
method	O
can	O
be	O
easily	O
adapted	O
to	O
other	O
languages	O
,	O
which	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
work.2	O
Related	O
Work	O
PLMs	O
in	O
the	O
Open	O
Domain	O
.	O

PLMs	O
have	O
gained	O
much	O
attention	O
recently	O
,	O
proving	O
successful	O
for	O
boosting	O
the	O
performance	O
of	O
various	O
NLP	O
tasks	O
(	O
Qiu	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

Early	O
works	O
on	O
PLMs	O
focus	O
on	O
feature	O
-	O
based	O
approaches	O
to	O
transform	O
words	O
into	O
distributed	O
representations	O
(	O
Collobert	O
and	O
Weston	O
,	O
2008	O
;	O
Mikolov	O
et	O
al	O
.	O
,	O
2013b	O
;	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
;	O
Peters	O
et	O
al	O
.	O
,	O
2018	O
)	O
.	O

BERT	B-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
(	O
as	O
well	O
as	O
its	O
robustly	O
optimized	O
version	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O
2019b	O
)	O
)	O
employs	O
bidirectional	O
transformer	O
encoders	O
(	O
Vaswani	O
et	O
al	O
.	O
,	O
2017	O
)	O
and	O
self	O
-	O
supervised	O
tasks	O
to	O
generate	O
context	O
-	O
aware	O
token	O
representations	O
.	O

Further	O
improvement	O
of	O
performances	O
mostly	O
based	O
on	O
the	O
following	O
three	O
types	O
of	O
techniques	O
,	O
including	O
self	O
-	O
supervised	O
tasks	O
(	O
Joshi	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
transformer	O
encoder	O
architectures	O
(	O
Yang	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
multi	O
-	O
task	O
learning	O
(	O
Liu	O
et	O
al	O
.	O
,	O
2019a	O
)	O
.	O

Knowledge	O
-	O
Enhanced	O
PLMs	O

.	O

As	O
existing	O
BERTlike	B-MethodName
models	O
only	O
learn	O
knowledge	O
from	O
plain	O
corpora	O
,	O
various	O
works	O
have	O
investigated	O
how	O
to	O
incorporate	O
knowledge	O
facts	O
to	O
enhance	O
the	O
language	O
understanding	O
abilities	O
of	O
PLMs	O
.	O

KEPLMs	B-MethodName
are	O
mainly	O
divided	O
into	O
the	O
following	O
three	O
types	O
.	O

(	O
1	O
)	O
Knowledge	O
-	O
enhanced	O
by	O
Entity	O
Embedding	O
:	O
ERNIE	O
-	O
THU	B-MethodName
(	O
Zhang	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
KnowBERT	B-MethodName
(	O
Peters	O
et	O
al	O
.	O
,	O
2019	O
)	O
inject	O
linked	O
-	B-MethodName
entity	O
as	O
heterogeneous	O
features	O
learned	O
by	O
KG	O
embedding	O
algorithms	O
such	O
as	O
TransE	O
(	O
Bordes	O
et	O
al	O
.	O
,	O
2013	O
)	O
.	O

(	O
2	O
)	O
Knowledge	O
-	O
enhanced	O
by	O
Entity	O
Description	O
:	O
EBERT	B-MethodName
(	O
Zhang	O
et	O
al	O
.	O
,	O
2020a	O
)	O
and	O
KEPLER	B-MethodName
(	O
Wang	O
et	O
al	O
.	O
,	O
2019b	O
)	O
add	O
extra	O
description	O
text	O
of	O
entities	O
to	O
enhance	O
semantic	O
representation	O
.	O

(	O
3	O
)	O
Knowledgeenhanced	O
by	O
Triplet	O
Sentence	O
:	O
K	B-MethodName
-	I-MethodName
BERT	I-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O
2020b	O
)	O
and	O
CoLAKE	B-MethodName
(	O
Sun	O
et	O
al	O
.	O
,	O
2020	O
)	O
convert	O
triplets	O
into	O
sentences	O
and	O
insert	O
them	O
into	O
the	O
training	O
corpora	O
without	O
pre	O
-	O
trained	O
embedding	O
.	O

Previous	O
studies	O
on	O
KG	B-MethodName
embedding	O
(	O
Nguyen	O
et	O
al	O
.	O
,	O
2016	O
;	O
Schlichtkrull	O
et	O
al	O
.	O
,	O
2018	O
)	O
have	O
shown	O
that	O
utilizing	O
the	O
surrounding	O
facts	O
of	O
entity	O
can	O
obtain	O
more	O
informative	O
embedding	O
,	O
which	O
is	O
the	O
focus	O
of	O
our	O
work	O
.	O
PLMs	O
in	O
the	O
Medical	O
Domain	O
.	O

PLMs	B-MethodName
in	O
the	O
medical	O
domain	O
can	O
be	O
generally	O
divided	O
into	O
three	O
categories	O
.	O

(	O
1	O
)	O
BioBERT	B-MethodName
(	O
Lee	O
et	O
al	O
.	O
,	O
2020	O
)	O
,	O
BlueBERT	B-MethodName
(	O
Peng	O
et	O

al	O
.	O
,	O
2019	O
)	O
,	O
SCIBERT	O
(	O
Beltagy	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
ClinicalBert	B-MethodName
(	O
Huang	O
et	O
al	O
.	O
,	O
2019	O
)	O

apply	O
continual	O
learning	O
on	O
medical	O
domain	O
texts	O
,	O
such	O
as	O
PubMed	B-MethodName
abstracts	O
,	O
PMC	O
full	O
-	O
text	O
articles	O
and	O
MIMIC	B-DatasetName
-	I-DatasetName
III	O
clinical	O
notes	O
.	O

(	O
2	O
)	O
PubMedBERT5884	O
DƵůƚŝͲ,ĞĂĚ	O
^ĞůĨͲƚƚĞŶƚŝŽŶ	O
&	O
ĞĞĚ&ŽƌǁĂƌĚ	O
>	O
ĂǇĞƌ	O
dŽŬĞŶ/ŶƉƵƚ	O
Dǆ	O
dͲ	O
	O
ŶĐŽĚĞƌ	O
<	O
Ͳ	O
	O
ŶĐŽĚĞƌ	O
Eǆ	O
DƵůƚŝͲ,ĞĂĚ	O
^ĞůĨͲƚƚĞŶƚŝŽŶ	O
,	O
ǇďƌŝĚƚƚĞŶƚŝŽŶ	O
EĞƚǁŽƌŬ	O
,	O
ĞƚĞƌŽŐĞŶĞŽƵƐ/ŶĨŽƌŵĂƚŝŽŶ&ƵƐŝŽŶ	O
WƌĞͲƚƌĂŝŶŝŶŐ	O
dĂƐŬƐ	O

D	O
>	O
D	O
DĞŶƚŝŽŶͲŶĞŝŐŚďŽƌŽŶƚĞǆƚDŽĚĞůŝŶŐ	O
;ĂͿ	O
^DĞĚ	O
	O
ZdƌĐŚŝƚĞĐƚƵƌĞ	O

<	O
ŶŽǁůĞĚŐĞ/ŶƉƵƚ	O
DĞĚŝĐĂů	O
<	O
'	O
ᯠߐ	O
⯵	O
∂	O
˄Ks/Ͳϭϵ	O

˅	O
EĞŝŐŚďŽƌŝŶŐ	O
ŶƚŝƚŝĞƐ	O
dǇƉĞ	O

EŽĚĞ	O
EŽĚĞ	O
W	O
	B-DatasetName
WZ	I-DatasetName
dǇƉĞ	O

J	B-MethodName
Ê	O
,	O
ǇďƌŝĚƚƚ͘Θ/ŶĨŽƌ͘&ƵƐŝŽŶDŽĚƵůĞƐ	O
ÊDĞŶƚŝŽŶͲŶĞŝŐŚďŽƌ	O
ŽŶƚĞǆƚDŽĚĞůŝŶŐ	O
༃	O
DĂƐŬĞĚEĞŝŐŚďŽƌDŽĚĞůŝŶŐ	O
U	O
UU	O
ŬŶŽǁůĞĚŐĞĐŽŶƚĞǆƚ	O
ŬŶŽǁůĞĚŐĞĐŽŶƚĞǆƚ	O
U΀D^<΁	O
ĸDĂƐŬĞĚDĞŶƚŝŽŶDŽĚĞůŝŶŐ	O
W	O
>	O
DƐ	O
ƚŽŬĞŶƐ	O
PO	O
POPOFigure	O
2	O
:	O
Model	O
overview	O
of	O
SMedBERT	B-MethodName
.	O

The	O
left	O
part	O
is	O
our	O
model	O
architecture	O
and	O
the	O
right	O
part	O
is	O
the	O
details	O
of	O
our	O
model	O
including	O
hybrid	O
attention	O
network	O
and	O
mention	O
-	O
neighbor	O
context	O
modeling	O
pre	O
-	O
training	O
tasks	O
.	O

(	O
Gu	O
et	O
al	O
.	O
,	O
2020	O
)	O
learns	O
weights	O
from	O
scratch	O
using	O
PubMed	B-MetricName
data	O
to	O
obtain	O
an	O
in	O
-	O
domain	O
vocabulary	O
,	O
alleviating	O
the	O
out	O
-	O
of	O
-	O
vocabulary	O
(	O
OOV	O
)	O
problem	O
.	O

This	O
training	O
paradigm	O
needs	O
the	O
support	O
of	O
largescale	O
domain	O
data	O
and	O
resources	O
.	O

(	O
3	O
)	O
Some	O
other	O
PLMs	O
use	O
domain	O
self	O
-	O
supervised	O
tasks	O
for	O
pretraining	O
.	O

For	O
example	O
,	O
MC	B-MethodName
-	I-MethodName
BERT	I-MethodName
(	O
Zhang	O
et	O
al	O
.	O
,	O
2020b	O
)	O
masks	O
Chinese	O
medical	O
entities	O
and	O
phrases	O
to	O
learn	O
complex	O
structures	O
and	O
concepts	O
.	O

DiseaseBERT	B-MethodName
(	O
He	O
et	O
al	O
.	O
,	O
2020	O
)	O
leverages	O
the	O
medical	O
terms	O
and	O
its	O
category	O
as	O
the	O
labels	O
to	O
pre	O
-	O
train	O
the	O
model	O
.	O

In	O
this	O
paper	O
,	O
we	O
utilize	O
both	O
domain	O
corpora	O
and	O
neighboring	O
entity	O
triplets	O
of	O
mentions	O
to	O
enhance	O
the	O
learning	O
of	O
medical	O
language	O
representations	O
.	O

3	O
The	O
SMedBERT	B-MethodName
Model	O
3.1	O
Notations	O
and	O
Model	O
Overview	O
In	O
the	O
PLM	O
,	O
we	O
denote	O
the	O
hidden	O
feature	O
of	O
each	O
tokenfw1;:::;wNgasfh1;h2;:::;hNgwhereN	O
is	O
the	O
maximum	O
input	O
sequence	O
length	O
and	O
the	O
total	O
number	O
of	O
pre	O
-	O
training	O
samples	O
as	O
M.	O
LetEbe	O
the	O
set	O
of	O
mention	O
-	O
span	O
emin	O
the	O
training	O
corpora	O
.	O

Furthermore	O
,	O
the	O
medical	O
KG	B-MethodName
consists	O
of	O
the	O
entities	O
setEand	O
the	O
relations	O
set	O
R.	O

The	O
triplet	O
set	O
isS	O
=	O
f(h;r;t	O
)	O

jh2E;r2R;t2Eg	O
,	O
whereh	O
is	O
the	O
head	O
entity	O
with	O
relation	O
rto	O
the	O
tail	O
entity	O
t.	O

The	O
embeddings	O
of	O
entities	O
and	O
relations	O
trained	O
on	O
KG	B-MethodName
by	O
TransR	O
(	O
Lin	O
et	O
al	O
.	O
,	O
2015	O
)	O
are	O
represented	O
as entand rel	O
,	O
respectively	O
.	O

The	O
neighboring	O
entity	O
set	O
recalled	O
from	O
KG	B-MethodName
by	O
emis	O
denoted	O
as	O
Nem	O
=	O
fe1	O
m;e2	O
m;:::;eK	O
mgwhereKis	O
the	O
threshold	O
of	O
our	O
PEPR	O
algorithm	O
.	O

We	O
denote	O
the	O
number	O
ofentities	O
in	O
the	O
KG	O
as	O
Z.	O

The	O
dimensions	O
of	O
the	O
hidden	O
representation	O
in	O
PLM	O
and	O
the	O
KG	O
embeddings	O
ared1andd2	O
,	O
respectively	O
.	O

The	O
main	O
architecture	O
of	O
the	O
our	O
model	O
is	O
shown	O
in	O
Figure	O
2	O
.	O
SMedBERT	B-MethodName
mainly	O
includes	O
three	O
components	O
:	O
(	O
1	O
)	O
Top	O
-	O
K	B-MethodName
entity	O
sorting	O
determine	O
which	O
K	O
neighbour	O
entities	O
to	O
use	O
for	O
each	O
mention	O
.	O

(	O
2	O
)	O
Mention	O
-	O
neighbor	B-MethodName
hybrid	O
attention	O
aims	O
to	O
infuse	O
the	O
structured	O
semantics	O
knowledge	O
into	O
encoder	O
layers	O
,	O
which	O
includes	O
type	O
attention	O
,	O
node	O
attention	O
and	O
gated	O
position	O
infusion	O
module	O
.	O

(	O
3	O
)	O
Mention	O
-	O
neighbor	O
context	O
modeling	O
includes	O
masked	O
neighbor	O
modeling	O
and	O
masked	O
mention	O
modeling	O
aims	O
to	O
promote	O
mentions	O
to	O
leverage	O
and	O
interact	O
with	O
neighbour	O
entities	O
.	O

3.2	O
Top	B-MethodName
-	O
K	O
Entity	O
Sorting	B-MethodName
Previous	O
research	O
shows	O
that	O
simple	O
neighboring	O
entity	O
expansion	O
may	O
induce	O
knowledge	O
noises	O
during	O
PLM	O
training	O
(	O
Wang	O
et	O
al	O
.	O
,	O
2019a	O
)	O
.	O

In	O
order	O
to	O
recall	O
the	O
most	O
important	O
neighboring	O
entity	O
set	O
from	O
the	O
KG	O
for	O
each	O
mention	O
,	O
we	O
extend	O
the	O
Personalized	O
PageRank	B-DatasetName
(	O
PPR	O
)	O
(	O
Page	O
et	O
al	O
.	O
,	O
1999	O
)	O
algorithm	O
to	O
ﬁlter	O
out	O
trivial	O
entities.3Recall	O
that	O
the	O
iterative	O
process	O
in	O
PPR	O
is	O
Vi=	O
(	O
1 	O
	O
)	O
AVi 1	O
+	O
	O
P	O
whereAis	O
the	O
normalized	O
adjacency	O
matrix	O
,	O
is	O
the	O
damping	O
factor	O
,	O
Pis	O
uniformly	O
distributed	O
jump	O
probability	O
vector	O
,	O
and	O
Vis	O
the	O
iterative	O
score	O
vector	O
for	O
each	O
entity	O
.	O

PEPR	B-MethodName
speciﬁcally	O
focuses	O
on	O
learning	O
the	O
weight	O
for	O
the	O
target	O
mention	O
span	O
in	O
each	O
iteration	O
.	O

It	O
3We	O
name	O
our	O
algorithm	O
to	O
be	O
Personalized	O
Entity	O
PageRank	B-DatasetName
,	O
abbreviated	O
as	O
PEPR.5885assigns	O
the	O
span	O
ema	O
higher	O
jump	O
probability	O
1	O
in	O
Pwith	O
the	O
remaining	O
as1	O
Z.	O

It	O
also	O
uses	O
the	O
entity	O
frequency	O
to	O
initialize	O
the	O
score	O
vector	O
V	O
:	O
Vem=(tem	O
Tem2E	O
1	O
Mem=2E(1	O
)	O
where	O
Tis	O
the	O
sum	O
of	O
frequencies	O
of	O
all	O
entities	O
.	O

temis	O
the	O
frequency	O
of	O
emin	O
the	O
corpora	O
.	O

After	O
sorting	O
,	O
we	O
select	O
the	O
top-	B-MethodName
Kentity	I-MethodName
setNem	O
.	O

3.3	O
Mention	O
-	O
neighbor	B-MethodName
Hybrid	O
Attention	O
Besides	O
the	O
embeddings	O
of	O
neighboring	O
entities	O
,	O
SMedBERT	B-MethodName
integrates	O
the	O
type	O
information	O
of	O
medical	O
entities	O
to	O
further	O
enhance	O
semantic	O
representations	O
of	O
mention	O
-	O
span	O
.	O

3.3.1	O
Neighboring	O
Entity	O
Type	O
Attention	O
Different	O
types	O
of	O
neighboring	O
entities	O
may	O
have	O
different	O
impacts	O
.	O

Given	O
a	O
speciﬁc	O
mention	O
-	O
span	O
em	O
,	O
we	O
compute	O
the	O
neighboring	O
entity	O
type	O
attention	O
.	O

Concretely	O
,	O
we	O
calculate	O
hidden	O
representation	O
of	O
each	O
entity	O
type	O
	O
ash	O
	O
=	O
P	O
eim2E	O
	O
mheim	O
.	O

E	O
mare	O
neighboring	O
entities	O
of	O
emwith	O
the	O
same	O
type	O
	O
andheim=	O
 ent 	O
ei	O
m	O
2Rd2	O
.	O

h0	O
em	O
=	O
LN((fsp(hi;:::;hj)Wbe	O
)	O
)	O
(	O
2	O
)	O
wherefspis	O
the	O
self	O
-	O
attentive	O
pooling	O
(	O
Lin	O
et	O
al	O
.	O
,	O
2017	O
)	O
to	O
generate	O
the	O
mention	O
-	O
span	O
representation	O
hem2Rd1and	O
the	O
(	O
hi;hi+1;:::;hj)is	O
the	O
hidden	O
representation	O
of	O
tokens	O
(	O
wi;wi+1;:::;wj)in	O
mention	O
-	O
span	O
emtrained	O
by	O
PLMs	O
.	O

h0	O
em2Rd2	O
is	O
obtained	O
by	O
()non	O
-	O
linear	O
activation	O
function	O
GELU	O
(	O
Hendrycks	O
and	O
Gimpel	O
,	O
2016	O
)	O
and	O
the	O
learnable	O
projection	O
matrix	O
Wbe2Rd1d2.LN	O
is	O
the	O
LayerNorm	O
function	O
(	O
Ba	O
et	O
al	O
.	O
,	O
2016	O
)	O
.	O

Then	O
,	O
we	O
calculate	O
the	O
each	O
type	O
attention	O
weight	O
using	O
the	O
type	O
representation	O
h	O
	O
2Rd2and	O
the	O
transformed	O
mention	O
-	O
span	O
representation	O
h0	O
em	O
:	O
	O
0	O
	O
=	O
tanh 	O
h0	O
emWt+h	O
	O
Wt0	O
Wa	O
(	O
3	O
)	O
whereWt2Rd2d2,Wt02Rd2d2andWa2	O
Rd21	O
.	O

Finally	O
,	O
the	O
neighboring	O
entity	O
type	O
attention	O
weights	O
	O
are	O
obtained	O
by	O
normalizing	O
the	O
attention	O
score	O
	O
0	O
	O
among	O
all	O
entity	O
types	O
T.	O
3.3.2	O
Neighboring	O
Entity	O
Node	O
Attention	O
Apart	O
from	O
entity	O
type	O
information	O
,	O
different	O
neighboring	O
entities	O
also	O
have	O
different	O
inﬂuences	O
.	O

Speciﬁcally	O
,	O
we	O
devise	O
the	O
neighboring	O
entity	O
nodeattention	O
to	O
capture	O
the	O
different	O
semantic	O
inﬂuences	O
from	O
neighboring	O
entities	O
to	O
the	O
target	O
mention	O
span	O
and	O
reduce	O
the	O
effect	O
of	O
noises	O
.	O

We	O
calculate	O
the	O
entity	O
node	O
attention	O
using	O
the	O
mentionspan	O
representation	O
h0	O
emand	O
neighboring	O
entities	O
representation	O
heimwith	O
entity	O
type	O
	O
as	O
:	O
	O
0	O
emeim= 	O
h0	O
emWq 	O
heimWkT	O
pd2	O
(	O
4	O
)	O
	O
emeim	O
=	O
exp	O
	O
0	O
emeim	O
P	O
eim2Nemexp	O
	O
0	O
emeim	O
(	O
5	O
)	O
whereWq2Rd2d2andWk2Rd2d2are	O
the	O
attention	O
weight	O
matrices	O
.	O

The	O
representations	O
of	O
all	O
neighboring	O
entities	O
in	O
Nemare	O
aggregated	O
to	O
h0	O
em2Rd2	O
:	O
bh0	O
em	O
=	O
X	O
eim2Nem	O
	O
emeim 	O
heimWv+bv	O
(	O
6	O
)	O
h0	O
em	O

=	O
LN	O
bh0	O
em+	O
	O
bh0	O
emWl1+bl1	O
Wl2	O
(	O
7	O
)	O
whereWv2Rd2d2,Wl12Rd24d2,Wl22	O
R4d2d2.bv2Rd2andbl12R4d2are	O
the	O
bias	O
vectors	O
.	O

h0	O
emis	O
the	O
mention	O
-	O
neighbor	O
representation	O
from	O
hybrid	O
attention	O
module	O
.	O

3.3.3	O
Gated	O
Position	O
Infusion	O
Knowledge	O
-	O
injected	O
representations	O
may	O
divert	O
the	O
texts	O
from	O
its	O
original	O
meanings	O
.	O

We	O
further	O
reduce	O
knowledge	O
noises	O
via	O
gated	O
position	O
infusion	O
:	O
h0	O
emf= h0	O
emkh0	O
em	O
Wmf+bmf	O
(	O
8)	O
eh0	O
emf	O
=	O
LN(h0	O
emfWbp+bbp	O
)	O
(	O
9	O
)	O
whereWmf2R2d22d2,Wbp2R2d2d1,bmf2	O
R2d2,bbp2Rd1.h0	O
emf2R2d2is	O
the	O
span	O
-	O
level	O
infusion	O
representation	O
.	O

“	O
k	O
”	O
means	O
concatenation	O
operation.eh0	O
emf2Rd1is	O
the	O
ﬁnal	O
knowledgeinjected	O
representation	O
for	O
mention	O
em	O
.	O

We	O
generate	O
the	O
output	O
token	O
representation	O
hifby4	O
:	O
gi=	O
tanhh	O
hikeh0	O
emfi	O
Wug+bug	O
(	O
10	O
)	O
hif=h	O
hikgieh0	O
emfi	O
Wex+bex	O

+	O
hi	O
(	O
11	O
)	O
whereWug	O
,	O
Wex2R2d1d1.bug;bex2Rd1	O
.	O

“	O
	O
”	O
means	O
element	O
-	O
wise	O
multiplication	O
.	O

4We	O
ﬁnd	O
that	O
restricting	O
the	O
knowledge	O
infusion	O
position	O
to	O
tokens	O
is	O
helpful	O
to	O
improve	O
performance.58863.4	O
Mention	O
-	O
neighbor	O
Context	O
Modeling	O
To	O
fully	O
exploit	O
the	O
structured	O
semantics	O
knowledge	O
in	O
KG	O
,	O
we	O
further	O
introduce	O
two	O
novel	O
selfsupervised	O
pre	O
-	O
training	O
tasks	O
,	O
namely	O
Masked	O
Neighbor	B-MethodName
Modeling	O
(	O
MNeM	O
)	O
and	O
Masked	B-MethodName
Mention	O
Modeling	O
(	O
MMeM	O
)	O
.	O

3.4.1	O
Masked	O
Neighbor	O
Modeling	O
Formally	O
,	O
let	O
rbe	O
the	O
relation	O
between	O
the	O
mentionspanemand	O
a	O
neighboring	O
entity	O
ei	O
m	O
:	O
hmf	O
=	O
LN((fsp(hif;:::;hjf)Wsa))(12	O
)	O
wherehmfis	O
the	O
mention	O
-	O
span	O
hidden	O
features	O
based	O
on	O
the	O
tokens	O
hidden	O
representation 	O
hif;h(i+1)f;:::;hjf	O
.hr=	O
 rel(r)2Rd2is	O
the	O
relationrrepresentation	O
and	O
Wsa2Rd1d2is	O
a	O
learnable	O
projection	O
matrix	O
.	O

The	O
goal	O
of	O
MNeM	B-MethodName
is	O
leveraging	O
the	O
structured	O
semantics	O
in	O
surrounding	O
entities	O
while	O
reserving	O
the	O
knowledge	O
of	O
relations	O
between	O
entities	O
.	O

Considering	O
the	O
object	O
functions	O
of	O
skip	O
-	O
gram	O
with	O
negative	O
sampling	O
(	O
SGNS	O
)	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013a	O
)	O
and	O
score	O
function	O
of	O
TransR	O
(	O
Lin	O
et	O
al	O
.	O
,	O
2015	O
):	O
LS=	O
logfs(w;c	O
)	O
+	O
kEcnPD[logfs(w; cn	O
)	O
]	O
(	O
13	O
)	O
ftr(h;r;t	O
)	O

=	O
khMr+r tMrk	O
(	O
14	O
)	O
where	O
thewinLSis	O
the	O
target	O
word	O
of	O
context	O
c.fs	O
is	O
the	O
compatibility	O
function	O
measuring	O
how	O
well	O
the	O
target	O
word	O
is	O
ﬁtted	O
into	O
the	O
context	O
.	O

Inspired	O
by	O
SGNS	O
,	O
following	O
the	O
general	O
energy	O
-	O
based	O
framework	O
(	O
LeCun	O
et	O
al	O
.	O
,	O
2006	O
)	O
,	O
we	O
treat	O
mention	O
-	O
spans	O
in	O
corpora	O
as	O
“	O
target	O
words	O
”	O
,	O
and	O
neighbors	O
of	O
corresponding	O
entities	O
in	O
KG	B-MethodName
as	O
“	O
contexts	O
”	O
to	O
provide	O
additional	O
global	O
contexts	O
.	O

We	O
employ	O
the	O
Sampled	B-MethodName
-	O
Softmax	B-MethodName
(	O
Jean	O
et	O
al	O
.	O
,	O
2015	O
)	O
as	O
the	O
criterionLMNeM	O
for	O
the	O
mention	O
-	O
span	O
em	O
:	O
X	O
Nemlogexp(fs(	O
)	O
)	O
exp(fs(	O
)	O
)	O

+	O
KEenQ(en)[exp(fs(0	O
)	O
)	O
]	O
(	O
15	O
)	O
wheredenotes	O
the	O
triplet	O
(	O
em;r;ei	O
m),ei	O
m2Nem	O
.	O

0is	O
the	O
negative	O
triplets	O
(	O
em;r;en	O
)	O
,	O
andenis	O
negative	O
entity	O
sampled	O
with	O
Q(ei	O
m)detailed	O
in	O
Appendix	O
B.	O
To	O
keep	O
the	O
knowledge	O
of	O
relations	O
between	O
entities	O
,	O
we	O
deﬁne	O
the	O
compatibility	O
function	O
as	O
:	O
fs 	O
em;r;ei	O

m	O
=	O
hmfMr+hr	O
jjhmfMr+hrjj(heimMr)T	O
jjheimMrjj	O
(	O
16)whereis	O
a	O
scale	O
factor	O
.	O

Assuming	O
the	O
norms	O
of	O
bothhmfMr+hrandheimMrare	O
1,we	O
have	O
:	O
fs 	O
em;r;ei	O
m	O
=	O
()ftr(hmf;hr;heim	O
)	O
=	O
0	O
(	O
17	O
)	O
which	O
indicates	O
the	O
proposed	O
fsis	O
equivalence	O
with	O
ftr	O
.	O

BecausejhenMrjneeds	O
to	O
be	O
calculated	O
for	O
eachen	O
,	O
the	O
computation	O
of	O
the	O
score	O
function	O
fs	O
is	O
costly	O
.	O

Hence	O
,	O
we	O
transform	O
part	O
of	O
the	O
formula	O
fsas	O
follows	O
:	O
(	O
hmfMr+hr)(henMr)T=	O
	O
hmf	O
1Mr	O
hrMr	O
hrT	O
hen0T	O
=	O
	O
hmf	O
1	O
MPr	O
hen0T	O
(	O
18	O
)	O

In	O
this	O
way	O
,	O
we	O
eliminate	O
computation	O
of	O
transforming	O
eachhen	O
.	O

Finally	O
,	O
to	O
compensate	O
the	O
offset	O
introduced	O
by	O
the	O
negative	O
sampling	O
function	O
Q(ei	O
m	O
)	O
(	O
Jean	O
et	O
al	O
.	O
,	O
2015	O
)	O
,	O
we	O
complement	O
fs(em;r;ei	O
m	O
)	O
as	O
:	O
	O
hmf	O
1	O
MPr	O
k	O
hmf	O
1	O
MPrkheim0	O

kheimk logQ(ei	B-DatasetName
m	O
)	O
(	O

19	O
)	O
3.4.2	O
Masked	O
Mention	O
Modeling	O
In	O
contrast	O
to	O
MNeM	B-DatasetName
,	O
MMeM	B-DatasetName
transfers	O
the	O
semantic	O
information	O
in	O
neighboring	O
entities	O
back	O
to	O
the	O
masked	O
mention	O
em	O
.	O

Ym	O
=	O
LN((fsp(hip;:::;hjp)Wsa	O
)	O
)	O
(	O
20	O
)	O
whereYmis	O
the	O
ground	O
-	O
truth	O
representation	O
of	O
em	O
andhip=	O
 p(wi)2Rd2. pis	O
the	O
pre	O
-	O
trained	O
embedding	O
of	O
BERT	B-MethodName
in	O
our	O
medical	O
corpora	O
.	O

The	O
mention	O
-	O
span	O
representation	O
obtained	O
by	O
our	O
model	O
ishmf	O
.	O

For	O
a	O
sample	O
s	O
,	O
the	O
loss	O
of	O
MMeM	O
LMMeM	O
is	O
calculated	O
via	O
Mean	O
-	O
Squared	O
Error	O
:	O
LMMeM	O

=	O
MsX	O
mikhmif Ymik2(21	O
)	O
whereMsis	O
the	O
set	O
of	O
mentions	O
of	O
sample	O
s.	O
3.5	O
Training	O
Objective	O
In	O
SMedBERT	B-MethodName
,	O
the	O
training	O
objectives	O
mainly	O
consist	O
of	O
three	O
parts	O
,	O
including	O
the	O
self	O
-	O
supervised	O
loss	O
proposed	O
in	O
previous	O
works	O
and	O
the	O
mentionneighbor	O
context	O
modeling	O
loss	O
proposed	O
in	O
our	O
work	O
.	O

Our	O
model	O
can	O
be	O
applied	O
to	O
medical	O
text	O
pre	O
-	O
training	O
directly	O
in	O
different	O
languages	O
as	O
long5887as	O
high	O
-	O
quality	O
medical	O
KGs	O
can	O
be	O
obtained	O
.	O

The	O
total	O
loss	O
is	O
as	O
follows	O
:	O
Ltotal	O
=	O
LEX+1LMNeM	O
+	O
2LMMeM	O
(	O
22	O
)	O
whereLEXis	O
the	O
sum	O
of	O
sentence	O
-	O
order	O
prediction	O
(	O
SOP	O
)	O
(	O
Lan	O
et	O
al	O
.	O
,	O
2020	O
)	O
and	O
masked	O
language	O
modeling.1and2are	O
the	O
hyperparameters	O
.	O

4	O
Experiments	O
4.1	O
Data	O
Source	O
Pre	O
-	O
training	O
Data	O
.	O

The	O
pre	O
-	O
training	O
corpora	O
after	O
pre	O
-	O
processing	O
contains	O
5,937,695	O
text	O
segments	O
with	O
3,028,224,412	O
tokens	O
(	O
4.9	O
GB	O
)	O
.	O

The	O
KGs	O
embedding	O
trained	O
by	O
TransR	O
(	O
Lin	O
et	O
al	O
.	O
,	O
2015	O
)	O
on	O
two	O
trusted	O
data	O
sources	O
,	O
including	O
the	O
SymptomIn	O
-	O
Chinese	B-MethodName
from	O
OpenKG5and	O
DXY	O
-	O
KG6containing	O
139,572	O
and	O
152,508	O
entities	O
,	O
respectively	O
.	O

The	O
number	O
of	O
triplets	O
in	O
the	O
two	O
KGs	B-MethodName
are	O
1,007,818	O
and	O
3,764,711	O
.	O

The	O
pre	O
-	O
training	O
corpora	O
and	O
the	O
KGs	B-MethodName
are	O
further	O
described	O
in	O
Appendix	O
A.1	O
.	O

Task	O
Data	O
.	O

We	O
use	O
four	O
large	O
-	O
scale	O
datasets	O
in	O
ChineseBLUE	B-MethodName
(	O
Zhang	O
et	O
al	O
.	O
,	O
2020b	O
)	O
to	O
evaluate	O
our	O
model	O
,	O
which	O
are	O
benchmark	O
of	O
Chinese	O
medical	O
NLP	O
tasks	O
.	O

Additionally	O
,	O
we	O
test	O
models	O
on	O
four	O
datasets	O
from	O
real	O
application	O
scenarios	O
provided	O
by	O
DXY	O
company7and	O
CHIP8	O
,	O
i.e.	O
,	O
Named	O
Entity	O
Recognition	B-DatasetName
(	O
DXY	O
-	O
NER	O
)	O
,	O
Relation	O
Extraction	O
(	O
DXY	O
-	O
RE	O
,	O
CHIP	O
-	O
RE	O
)	O
and	O
Question	O
Answer	O
(	O
WebMedQA	O
(	O
He	O
et	O
al	O
.	O
,	O
2019	O
)	O
)	O
.	O

For	O
other	O
information	O
of	O
the	O
downstream	O
datasets	O
,	O
we	O
refer	O
readers	O
to	O
Appendix	O
A.2	O
.	O

4.2	O
Baselines	O
In	O
this	O
work	O
,	O
we	O
compare	O
SMedBERT	B-MethodName
with	O
general	O
PLMs	O
,	O
domain	O
-	O
speciﬁc	O
PLMs	O
and	O
KEPLMs	B-MethodName
with	O
knowledge	O
embedding	O
injected	O
,	O
pre	O
-	O
trained	O
on	O
our	O
Chinese	O
medical	O
corpora	O
:	O
General	O
PLMs	O
:	O

We	O
use	O
three	O
Chinese	O
BERT	B-MethodName
-	O
style	O
models	O
,	O
namely	O
BERT	B-MethodName
-	I-MethodName
base	I-MethodName
(	O
Devlin	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
BERT	B-MethodName
-	I-MethodName
wwm	I-MethodName
(	O
Cui	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
RoBERTa	B-MethodName
(	O
Liu	O
et	O
al	O
.	O
,	O
2019b	O
)	O
.	O

All	O
the	O
weights	O
are	O
initialized	O
from	O
(	O
Cui	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

Domain	O
-	O
speciﬁc	O
PLMs	O
:	O

As	O
very	O
few	O
PLMs	O
in	O
the	O
Chinese	O
medical	O
domain	O
are	O
available	O
,	O
we	O
consider	O
the	O
following	O
models	O
.	O

MC	B-MethodName
-	O
BERT	B-MethodName
(	O
Zhang	O
et	O
al	O
.	O
,	O
5http://www.openkg.cn/dataset/	O
symptom	O
-	O
in	O
-	O
chinese	O
6https://portal.dxy.cn/	O
7https://auth.dxy.cn/accounts/login	O
8http://www.cips-chip.org.cn:8088/homeModel	O
D1	O
D2	O
D3	O
SGNS	B-MethodName
-	O
char	O
-	O
med	O
27.21	O
%	O
27.16	O
%	O
21.72	O
%	O
SGNS	O
-	O
word	O
-	O
med	O
24.64	O
%	O
24.95	O
%	O
20.37	O
%	O
GLOVE	O
-	O
char	O
-	O
med	O
27.24	O
%	O
27.12	O
%	O
21.91	O
%	O
GLOVE	O
-	O
word	O
-	O
med	O
24.41	O
%	O
23.89	O
%	O
20.56	O
%	O
BERT	B-MethodName
-	O
open	O
29.79	O
%	O
29.41	O
%	O
21.83	O
%	O
BERT	B-MethodName
-	O
wwm	O
-	O
open	O
29.75	O
%	O
29.55	O
%	O
21.97	O
%	O
RoBERTa	B-MethodName
-	O
open	O
30.84	O
%	O
30.56	O
%	O
21.98	O
%	O
MC	B-MethodName
-	O
BERT	B-MethodName
30.63	O
%	O
30.34	O
%	O
22.65	O
%	O
BioBERT	B-MethodName
-	O
zh	O
30.84	O
%	O
30.69	O
%	O
22.71	O
%	O
ERNIE	O
-	O
med	O
30.97	O
%	O
30.78	O
%	O
22.99	O
%	O
KnowBERT	O
-	O
med	O
30.95	O
%	O
30.77	O
%	O
23.07	O
%	O
SMedBERT	O
31.81	O
%	O
32.14	O
%	O
24.08	O
%	O
Table	O
1	O
:	O
Results	O
of	O
unsupervised	O
semantic	O
similarity	O
task	O
.	O

“	O
med	O
”	O
refers	O
to	O
models	O
continually	O
pre	O
-	O
trained	O
on	O
medical	O
corpora	O
,	O
and	O
“	O
open	O
”	O
means	O
open	O
-	O
domain	O
corpora	O
.	O

“	O
char	O
’	O
and	O
“	O
word	O
”	O
refer	O
to	O
the	O
token	O
granularity	O
of	O
input	O
samples	O
.	O
2020b	O
)	O
is	O
pre	O
-	O
trained	O
over	O
a	O
Chinese	O
medical	O
corpora	O
via	O
masking	O
different	O
granularity	O
tokens	O
.	O

We	O
also	O
pre	O
-	O
train	O
BERT	B-MethodName
using	O
our	O
corpora	O
,	O
denoted	O
as	O
BioBERT	O
-	O
zh	O
.	O
KEPLMs	O
:	O

We	O
employ	O
two	O
SOTA	B-MethodName
KEPLMs	O
continually	O
pre	O
-	O
trained	O
on	O
our	O
medical	O
corpora	O
as	O
our	O
baseline	O
models	O
,	O
including	O
ERNIE	O
-	B-MethodName
THU	I-MethodName
(	O
Zhang	O
et	O
al	O
.	O
,	O
2019	O
)	O
and	O
KnowBERT	B-MethodName
(	O
Peters	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

For	O
a	O
fair	O
comparison	O
,	O
KEPLMs	B-MethodName
use	O
other	O
additional	O
resources	O
rather	O
than	O
the	O
KG	O
embedding	O
are	O
excluded	O
(	O
See	O
Section	O
2	O
)	O
,	O
and	O
all	O
the	O
baseline	O
KEPLMs	B-MethodName
are	O
injected	O
by	O
the	O
same	O
KG	O
embedding	O
.	O

The	O
detailed	O
parameter	O
settings	O
and	O
training	O
procedure	O
are	O
in	O
Appendix	O
B.	O
4.3	O
Intrinsic	O
Evaluation	O
To	O
evaluate	O
the	O
semantic	O
representation	O
ability	O
of	O
SMedBERT	B-MethodName
,	O
we	O
design	O
an	O
unsupervised	O
semantic	O
similarity	O
task	O
.	O

Speciﬁcally	O
,	O
we	O
extract	O
all	O
entities	O
pairs	O
with	O
equivalence	O
relations	O
in	O
KGs	O
as	O
positive	O
pairs	O
.	O

For	O
each	O
positive	O
pair	O
,	O
we	O
use	O
one	O
of	O
the	O
entity	O
as	O
query	O
entity	O
while	O
the	O
other	O
as	O
positive	O
candidate	O
,	O
which	O
is	O
used	O
to	O
sample	O
other	O
entities	O
as	O
negative	O
candidates	O
.	O

We	O
denote	O
this	O
dataset	O
as	O
D1	O
.	O

Besides	O
,	O
the	O
entities	O
in	O
the	O
same	O
positive	O
pair	O
often	O
have	O
many	O
neighbours	O
in	O
common	O
.	O

We	O
select	O
positive	O
pairs	O
with	O
large	O
proportions	O
of	O
common	O
neighbours	O
as	O
D2	O
.	O

Additionally	O
,	O
to	O
verify	O
the	O
ability	O
of	O
SMedBERT	B-MethodName
of	O
enhancing	O
the	O
low	O
-	O
frequency	O
mention	O
representation	O
,	O
we	O
extract	O
all	O
positive	O
pairs	O
that	O
with	O
at	O
least	O
one	O
low	O
-	O
frequency	O
mention	O
as	O
D3	B-MethodName
.	O

There	O
are	O
totally	O
359,358	O
,	O
272,320	O
and	O
41,583	O
samples	O
for	O
D1,D2,D3respectively	O
.	O

We	O
describe	O
the5888Named	O
Entity	O
Recognition	O
Relation	O
Extraction	O
Model	O
cMedQANER	O
DXY	O
-	O
NER	O
Average	O
CHIP	O
-	O
RE	O
DXY	O
-	O
RE	B-MethodName
Average	O
Dev	O
Test	O
Dev	O
Test	O
Test	O
Test	O
Dev	O
Test	O
Test	O
BERT	B-MethodName
-	O
open	O
80.69	O
%	O
83.12	O
%	O
79.12	O
%	O
79.03	O
%	O
81.08	O
%	O
85.86	O
%	O
94.18	O
%	O
94.13	O
%	O
90.00	O
%	O
BERT	B-MethodName
-	O
wwm	O
-	O
open	O
80.52	O
%	O
83.07	O
%	O
79.48	O
%	O
79.29	O
%	O
81.18	O
%	O
86.01	O
%	O
94.35	O
%	O
94.38	O
%	O
90.20	O
%	O
RoBERT	B-MethodName
-	O
open	O
80.92	O
%	O
83.29	O
%	O
79.27	O
%	O
79.33	O
%	O
81.31	O
%	O
86.19	O
%	O
94.64	O
%	O
94.66	O
%	O
90.43	O
%	O
BioBERT	B-MethodName
-	O
zh	O
80.72	O
%	O
83.38	O
%	O
79.52	O
%	O
79.45	O
%	O
81.42	O
%	O
86.12	O
%	O
94.54	O
%	O
94.64	O
%	O
90.38	O
%	O
MC	B-MethodName
-	O
BERT	B-MethodName
81.02	O
%	O
83.46	O
%	O
79.79	O
%	O
79.59	O
%	O
81.53	O
%	O
86.09	O
%	O
94.74	O
%	O
94.73	O
%	O
90.41	O
%	O
KnowBERT	O
-	O
med	O
81.29	O
%	O
83.75	O
%	O
80.86	O
%	O
80.44	O
%	O
82.10	O
%	O
86.27	O
%	O
95.05	O
%	O
94.97	O
%	O
90.62	O
%	O
ERNIE	O
-	O
med	O
81.22	O
%	O
83.87	O
%	O
80.82	O
%	O
80.87	O
%	O
82.37	O
%	O
86.25	O
%	O
94.98	O
%	O
94.91	O
%	O
90.58	O
%	O
SMedBERT	B-MethodName
82.23	O
%	O
84.75	O
%	O
83.06	O
%	O
82.94	O
%	O
83.85	O
%	O
86.95	O
%	O
95.73	O
%	O
95.89	O
%	O
91.42	O
%	O
Table	O
2	O
:	O
Performance	O
of	O
Named	O
Entity	O
Recognition	O
(	O
NER	O
)	O
and	O
Relation	O
Extraction	O
(	O
RE	O
)	O
tasks	O
in	O
terms	O
of	O
F1	O
.	O

The	O
Development	O
data	O
of	O
CHIP	B-MethodName
-	I-MethodName
RE	I-MethodName
is	O
unreleased	O
in	O
public	O
dataset	O
.	O

Question	O
Answering	O
Question	O
Matching	O
Natural	O
Lang	O
.	O

Infer	O
.	O

Model	O
cMedQA	O
WebMedQA	O
Average	O
cMedQQ	O
cMedNLI	B-MethodName
Dev	O
Test	O
Dev	O
Test	O
Test	O
Dev	O
Test	O
Dev	O
Test	O
BERT	B-MethodName
-	O
open	O
72.99	O
%	O
73.82	O
%	O
77.20	O
%	O
79.72	O
%	O
76.77	O
%	O
86.74	O
%	O
86.72	O
%	O
95.52	O
%	O
95.66	O
%	O
BERT	B-MethodName
-	O
wwm	O
-	O
open	O
72.03	O
%	O
72.96	O
%	O
77.06	O
%	O
79.68	O
%	O
76.32	O
%	O
86.98	O
%	O
86.82	O
%	O
95.53	O
%	O
95.78	O
%	O
RoBERT	O
-	O
open	O
72.22	O
%	O
73.18	O
%	O
77.18	O
%	O
79.57	O
%	O
76.38	O
%	O
87.24	O
%	O
86.97	O
%	O
95.87	O
%	O
96.11	O
%	O
BioBERT	O
-	O
zh	O
74.32	O
%	O
75.12	O
%	O
78.04	O
%	O
80.45	O
%	O
77.79	O
%	O
87.30	O
%	O
87.06	O
%	O
95.89	O
%	O
96.04	O
%	O
MC	B-MethodName
-	O
BERT	B-MethodName
74.40	O
%	O
74.46	O
%	O
77.85	O
%	O
80.54	O
%	O
77.50	O
%	O
87.17	O
%	O
87.01	O
%	O
95.81	O
%	O
96.06	O
%	O
KnowBERT	B-MethodName
-	O
med	O
74.38	O
%	O
75.25	O
%	O
78.20	O
%	O
80.67	O
%	O
77.96	O
%	O
87.25	O
%	O
87.14	O
%	O
95.96	O
%	O
96.03	O
%	O
ERNIE	O
-	O
med	O
74.37	O
%	O
75.22	O
%	O
77.93	O
%	O
80.56	O
%	O
77.89	O
%	O
87.34	O
%	O
87.20	O
%	O
96.02	O
%	O
96.25	O
%	O
SMedBERT	B-MethodName
75.06	O
%	O
76.04	O
%	O
79.26	O
%	O
81.68	O
%	O
78.86	O
%	O
88.13	O
%	O
88.09	O
%	O
96.64	O
%	O
96.88	O
%	O
Table	O
3	O
:	O
Performance	O
of	O
Question	O
Answering	O
(	O
QA	O
)	O
,	O
Question	O
Matching	O
(	O
QM	B-MethodName
)	O
and	O
Natural	O
Language	O
Inference	O
(	O
NLI	O
)	O
tasks	O
.	O

The	O
metric	O
of	O
the	O
QA	O
task	O
is	O
Acc@1	O
and	O
those	O
of	O
QM	O
and	O
NLI	B-DatasetName
are	O
F1	O
.	O

details	O
of	O
collecting	O
data	O
and	O
embedding	O
words	O
in	O
Appendix	O
C.	O

In	O
this	O
experiments	O
,	O
we	O
compare	O
SMedBERT	B-MethodName
with	O
three	O
types	O
of	O
models	O
:	O
classical	O
word	O
embedding	O
methods	O
(	O
SGNS	B-MethodName
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013a	O
)	O
,	O
GLOVE	B-MethodName
(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
)	O
,	O
PLMs	O
and	O
KEPLMs	B-MethodName
.	O

We	O
compute	O
the	O
similarity	O
between	O
the	O
representation	O
of	O
query	O
entities	O
and	O
all	O
the	O
other	O
entities	O
,	O
retrieving	O
the	O
most	O
similar	O
one	O
.	O

The	O
evaluation	O
metric	O
is	O
top-1	O
accuracy	O
(	O
Acc@1	O
)	O
.	O

Experiment	O
results	O
are	O
shown	O
in	O
Table	O
1	O
.	O

From	O
the	O
results	O
,	O
we	O
observe	O
that	O
:	O
(	O
1	O
)	O
SMedBERT	B-MethodName
greatly	O
outperforms	O
all	O
baselines	O
especially	O
on	O
the	O
dataset	O
D2	O
(	O
+1.36	O
%	O
)	O
,	O
where	O
most	O
positive	O
pairs	O
have	O
many	O
shared	O
neighbours	O
,	O
demonstrating	O
that	O
ability	O
of	O
SMedBERT	B-MethodName
to	O
utilize	O
semantic	O
information	O
from	O
the	O
global	O
context	O
.	O

(	O
2	O
)	O
In	O
dataset	O
D3	O
,	O
SMedBERT	B-MethodName
improve	O
the	O
performance	O
signiﬁcantly	O
(	O
+1.01	O
%	O
)	O
,	O
indicating	O
our	O
model	O
is	O
effective	O
to	O
enhance	O
the	O
representation	O
of	O
low	O
-	O
frequency	O
mentions	O
.	O

4.4	O
Results	O
of	O
Downstream	B-MethodName
Tasks	O
We	O
ﬁrst	O
evaluate	O
our	O
model	O
in	O
NER	B-TaskName
and	O
RE	B-MethodName
tasks	O
that	O
are	O
closely	O
related	O
to	O
entities	O
in	O
the	O
input	O
texts	O
.	O

Table	O
2	O
shows	O
the	O
performances	O
on	O
medical	O
NER	B-TaskName
and	O
RE	B-MethodName
tasks	O
.	O

In	O
NER	B-TaskName
and	O
RE	B-MethodName
tasks	O
,	O
we	O
can	O
observe	O
from	O
the	O
results	O
:	O
(	O
1	O
)	O
Compared	O
with	O
PLMs	O
trained	O
in	O
open	O
-	O
domain	O
corpora	O
,	O
KEPLMs	B-MethodName
with	O
medical	O
corpora	O
and	O
knowledge	O
facts	O
achieve	O
better	O
results	O
.	O

(	O
2	O
)	O
The	O
performance	O
of	O
SMedBERT	B-MethodName
is	O
greatly	O
improved	O
compared	O
with	O
the	O
strongest	O
baseline	O
in	O
two	O
NER	B-TaskName
datasets	O
(	O
+0.88	O
%	O
,	O
+2.07	O
%	O
)	O
,	O
and	O
(	O
+0.68	O
%	O
,	O
+0.92	O
%	O
)	O
on	O
RE	B-MethodName
tasks	O
.	O

We	O
also	O
evaluate	O
SMedBERT	B-MethodName
on	O
QA	B-TaskName
,	O
QM	B-DatasetName
and	O
NLI	B-DatasetName
tasks	O
and	O
the	O
performance	O
is	O
shown	O
in	O
Table	O
3	O
.	O

We	O
can	O
observe	O
that	O
SMedBERT	B-MethodName
improve	O
the	O
performance	O
consistently	O
on	O
these	O
datasets	O
(	O
+0.90	O
%	O
on	O
QA	O
,	O
+0.89	O
%	O
on	O
QM	B-DatasetName
and	O
+0.63	O
%	O
on	O
NLI	B-DatasetName
)	O
.	O

In	O
general	O
,	O
it	O
can	O
be	O
seen	O
from	O
Table	O
2	O
and	O
Table	O
3	O
that	O
injecting	O
the	O
domain	O
knowledge	O
especially	O
the	O
structured	O
semantics	O
knowledge	O
can	O
improve	O
the	O
result	O
greatly	O
.	O

4.5	O
Inﬂuence	O
of	O
Entity	O
Hit	O
Ratio	O
In	O
this	O
experiment	O
,	O
we	O
explore	O
the	O
model	O
performance	O
in	O
NER	B-TaskName
and	O
RE	B-MethodName
tasks	O
with	O
different	O
entity	O
hit	O
ratios	O
,	O
which	O
control	O
the	O
proportions	O
of	O
knowledgeenhanced	O
mention	O
-	O
spans	O
in	O
the	O
samples	O
.	O

The	O
aver-5889	O
Figure	O
3	O
:	O
Entity	B-MethodName
hit	O
ratio	O
results	O
of	O
SMedBERT	B-MethodName
and	O
ERNIE	O
in	O
NER	B-TaskName
and	O
RE	B-MethodName
tasks	O
.	O

Figure	O
4	O
:	O
The	O
inﬂuence	O
of	O
different	O
K	O
values	O
in	O
results	O
.	O

age	O
number	O
of	O
mention	O
-	O
spans	O
in	O
samples	O
is	O
about	O
40	O
.	O

Figure	O
3	O
illustrates	O
the	O
performance	O
of	O
SMedBERT	B-MethodName
and	O
ERNIE	O
-	B-MethodName
med	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

From	O
the	O
result	O
,	O
we	O
can	O
observe	O
that	O
:	O
(	O
1	O
)	O
The	O
performance	O
improves	O
signiﬁcantly	O
at	O
the	O
beginning	O
and	O
then	O
keeps	O
stable	O
as	O
the	O
hit	O
ratio	O
increases	O
,	O
proving	O
the	O
heterogeneous	O
knowledge	O
is	O
beneﬁcial	O
to	O
improve	O
the	O
ability	O
of	O
language	O
understanding	O
and	O
indicating	O
too	O
much	O
knowledge	O
facts	O
are	O
unhelpful	O
to	O
further	O
improve	O
model	O
performance	O
due	O
to	O
the	O
knowledge	O
noise	O
(	O
Liu	O
et	O
al	O
.	O
,	O
2020b	O
)	O
.	O

(	O
2	O
)	O
Compared	O
with	O
previous	O
approaches	O
,	O
our	O
SMedBERT	B-MethodName
model	O
improves	O
performance	O
greatly	O
and	O
more	O
stable	O
.	O

4.6	O
Inﬂuence	O
of	O
Neighboring	O
Entity	O
Number	O
We	O
further	O
evaluate	O
the	O
model	O
performance	O
under	O
differentKover	O
the	O
test	O
set	O
of	O
DXY	O
-	B-MethodName
NER	O
and	O
DXY	O
-	O
RE	O
.	O

Figure	O
4	O
shows	O
the	O
the	O
model	O
result	O
with	O
K	O
=	O
f5;10;20;30	O
g.	O
In	O
our	O
settings	O
,	O
the	O
SMedBERT	B-MethodName
can	O
achieve	O
the	O
best	O
performance	O
in	O
different	O
tasks	O
around	O
K=	O
10	O
.	O

The	O
results	O
of	O
SMedBERT	B-MethodName
show	O
that	O
the	O
model	O
performance	O
increasing	O
ﬁrst	O
and	O
then	O
decreasing	O
with	O
the	O
increasing	O
of	O
K.	O

This	O
phenomenon	O
also	O
indicates	O
the	O
knowledge	O
noise	O
problem	O
that	O
injecting	O
too	O
much	O
knowledge	O
of	O
neighboring	O
entities	O
may	O
hurt	O
the	O
performance	O
.	O

4.7	O
Ablation	O
Study	O
In	O
Table	O
4	O
,	O
we	O
choose	O
three	O
important	O
model	O
components	O
for	O
our	O
ablation	O
study	O
and	O
report	O
the	O
testModel	O
D5	O
D6	O
D7	O
D8	O
SMedBERT	B-MethodName
84.75	O
%	O
82.94	O
%	O
86.95	O
%	O
95.89	O
%	O
ERNIE	O
-	O
med	O
83.87	O
%	O
80.87	O
%	O
86.25	O
%	O
94.91	O
%	O
-Type	O
Att	O
.	O
84.25	O
%	O
81.99	O
%	O
86.61	O
%	O
95.29	O
%	O
-Hybrid	O
Att	O
.	O
83.71	O
%	O
80.85	O
%	O
86.46	O
%	O
95.20	O
%	O
-Know	O
.	O

Loss	O
84.31	O
%	O
82.12	O
%	O
86.50	O
%	O
95.43	O
%	O
Table	O
4	O
:	O
Ablation	O
study	O
of	O
SMedBERT	B-MethodName
on	O
four	O
datasets	O
(	O
testing	O
set	O
)	O
.	O

Due	O
to	O
the	O
space	O
limitation	O
,	O
we	O
use	O
the	O
abbreviations	O
“	O
D5	O
”	O
,	O
“	O
D6	O
”	O
,	O
“	O
D7	O
”	O
,	O
and	O
“	O
D8	O
”	O
to	O
represent	O
the	O
cMedQANER	O
,	O
DXY	B-DatasetName
-	B-MethodName
NER	O
,	O
CHIP	B-MethodName
-	I-MethodName
RE	O
,	O
and	O
DXYRE	O
datasets	O
respectively	O
.	O

set	O
performance	O
on	O
four	O
datasets	O
of	O
NER	B-TaskName
and	O
RE	B-MethodName
tasks	O
that	O
are	O
closely	O
related	O
to	O
entities	O
.	O

Speciﬁcally	O
,	O
the	O
three	O
model	O
components	O
are	O
neighboring	O
entity	O
type	O
attention	O
,	O
the	O
whole	O
hybrid	O
attention	O
module	O
,	O
and	O
mention	O
-	O
neighbor	O
context	O
modeling	O
respectively	O
,	O
which	O
includes	O
two	O
masked	O
language	O
model	O
lossLMNeM	O
andLMMeM	O
.	O

From	O
the	O
result	O
,	O
we	O
can	O
observe	O
that	O
:	O
(	O
1	O
)	O
Without	O
any	O
of	O
the	O
three	O
mechanisms	O
,	O
our	O
model	O
performance	O
can	O
also	O
perform	O
competitively	O
with	O
the	O
strong	O
baseline	O
ERNIE	O
-	B-MethodName
med	B-DatasetName
(	O
Zhang	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

(	O
2	O
)	O
Note	O
that	O
after	O
removing	O
the	O
hybrid	O
attention	O
module	O
,	O
the	O
performance	O
of	O
our	O
model	O
has	O
the	O
greatest	O
decline	O
,	O
which	O
indicates	O
that	O
injecting	O
rich	O
heterogeneous	O
knowledge	O
of	O
neighboring	O
entities	O
is	O
effective	O
.	O

5	O
Conclusion	O
In	O
this	O
work	O
,	O
we	O
address	O
medical	O
text	O
mining	O
tasks	O
with	O
the	O
structured	O
semantics	O
KEPLM	B-MethodName
proposed	O
named	O
SMedBERT	B-MethodName
.	O

Accordingly	O
,	O
we	O
inject	O
entity	O
type	O
semantic	O
information	O
of	O
neighboring	O
entities	O
into	O
node	O
attention	O
mechanism	O
via	O
heterogeneous	O
feature	O
learning	O
process	O
.	O

Moreover	O
,	O
we	O
treat	O
the	O
neighboring	O
entity	O
structures	O
as	O
additional	O
global	O
contexts	O
to	O
predict	O
the	O
masked	O
candidate	O
entities	O
based	O
on	O
mention	O
-	O
spans	O
and	O
vice	O
versa	O
.	O

The	O
experimental	O
results	O
show	O
the	O
signiﬁcant	O
improvement	O
of	O
our	O
model	O
on	O
various	O
medical	O
NLP	O
tasks	O
and	O
the	O
intrinsic	O
evaluation	O
.	O

There	O
are	O
two	O
research	O
directions	O
that	O
can	O
be	O
further	O
explored	O
:	O
(	O
1	O
)	O
Injecting	O
deeper	O
knowledge	O
by	O
using	O
“	O
farther	O
neighboring	O
”	O
entities	O
as	O
contexts	O
;	O
(	O
2	O
)	O
Further	O
enhancing	O
Chinese	O
medical	O
long	O
-	O
tail	O
entity	O
semantic	O
representation	O
.	O

Acknowledgements	O
We	O
would	O
like	O
to	O
thank	O
anonymous	O
reviewers	O
for	O
their	O
valuable	O
comments	O
.	O

This	O
work	O
is	O
supported	O
by5890the	O
National	O
Key	O
Research	O
and	O
Development	O
Program	O
of	O
China	O
under	O
Grant	O
No	B-DatasetName
.	O
2016YFB1000904	O
,	O
and	O
Alibaba	O
Group	O
through	O
Alibaba	O
Research	O
Intern	O
Program	O
.	O

References	O
Lei	O
Jimmy	O
Ba	O
,	O
Jamie	O
Ryan	O
Kiros	O
,	O
and	O
Geoffrey	O
E.	O
Hinton	O
.	O

2016	O
.	O

Layer	O
normalization	O
.	O

CoRR	B-DatasetName
,	O
abs/1607.06450	O
.	O

Iz	O
Beltagy	O
,	O
Kyle	O
Lo	O
,	O
and	O
Arman	O
Cohan	O
.	O

2019	O
.	O

Scibert	B-MethodName
:	O
A	O
pretrained	O
language	O
model	O
for	O
scientiﬁc	O
text	O
.	O

In	O
EMNLP	O
,	O
pages	O
3613–3618	O
.	O
Antoine	O
Bordes	O
,	O
Nicolas	O
Usunier	O
,	O
Alberto	O
Garc	O
´	O
ıaDur´an	O
,	O
Jason	O
Weston	O
,	O
and	O
Oksana	O
Yakhnenko	O
.	O
2013	O
.	O

Translating	O
embeddings	O
for	O
modeling	O
multirelational	O
data	O
.	O

In	O
NIPS	B-TaskName
,	O
pages	O
2787–2795	O
.	O

Ronan	O
Collobert	O
and	O
Jason	O
Weston	O
.	O

2008	O
.	O

A	O
uniﬁed	O
architecture	O
for	O
natural	O
language	O
processing	O
:	O
deep	O
neural	O
networks	O
with	O
multitask	O
learning	O
.	O

In	O
ICML	B-DatasetName
,	O
pages	O
160–167	O
.	O

Yiming	O
Cui	O
,	O
Wanxiang	O
Che	O
,	O
Ting	O
Liu	O
,	O
Bing	O
Qin	O
,	O
Shijin	O
Wang	O
,	O
and	O
Guoping	O
Hu	O
.	O
2020	O
.	O

Revisiting	O
pretrained	O
models	O
for	O
chinese	O
natural	O
language	O
processing	O
.	O

In	O
EMNLP	O
,	O
pages	O
657–668	O
.	O

Yiming	O
Cui	O
,	O
Wanxiang	O
Che	O
,	O
Ting	O
Liu	O
,	O
Bing	O
Qin	O
,	O
Ziqing	O
Yang	O
,	O
Shijin	O
Wang	O
,	O
and	O
Guoping	O
Hu	O
.	O
2019	O
.	O

Pre	O
-	O
training	O
with	O
whole	O
word	O
masking	O
for	O
chinese	O
BERT	B-MethodName
.	O

CoRR	O
,	O
abs/1906.08101	O
.	O

Jacob	O
Devlin	O
,	O
Ming	O
-	O
Wei	O
Chang	O
,	O
Kenton	O
Lee	O
,	O
and	O
Kristina	O
Toutanova	O
.	O
2019	O
.	O

BERT	B-MethodName
:	O
pre	O
-	O
training	O
of	O
deep	O
bidirectional	O
transformers	O
for	O
language	O
understanding	O
.	O

In	O
NAACL	B-TaskName
,	O
pages	O
4171–4186	O
.	O

Yu	O
Gu	O
,	O
Robert	O
Tinn	O
,	O
Hao	O
Cheng	O
,	O
Michael	O
Lucas	O
,	O
Naoto	O
Usuyama	O
,	O
Xiaodong	O
Liu	O
,	O
Tristan	O
Naumann	O
,	O
Jianfeng	O
Gao	O
,	O
and	O
Hoifung	O
Poon	O
.	O

2020	O
.	O

Domain	O
-	O
speciﬁc	O
language	O
model	O
pretraining	O
for	O
biomedical	O
natural	O
language	O
processing	O
.	O

CoRR	O
,	O
abs/2007.15779	O
.	O

Hiroaki	O
Hayashi	O
,	O
Zecong	O
Hu	O
,	O
Chenyan	O
Xiong	O
,	O
and	O
Graham	O
Neubig	O
.	O

2020	O
.	O

Latent	O
relation	O
language	O
models	O
.	O

InAAAI	O
,	O
pages	O
7911–7918	O
.	O

Junqing	O
He	O
,	O
Mingming	O
Fu	O
,	O
and	O
Manshu	O
Tu	O
.	O

2019	O
.	O

Applying	O
deep	O
matching	O
networks	O
to	O
chinese	O
medical	O
question	O
answering	O
:	O
a	O
study	O
and	O
a	O
dataset	O
.	O

BMC	B-DatasetName
Medical	O
Informatics	O
Decis	O
.	O

Mak	O
.	O
,	O
19	O
-	O
S(2):91–100	O
.	O

Yun	O
He	O
,	O
Ziwei	O
Zhu	O
,	O
Yin	O
Zhang	O
,	O
Qin	O
Chen	O
,	O
and	O
James	O
Caverlee	O
.	O

2020	O
.	O

Infusing	O
disease	O
knowledge	O
into	O
BERT	B-MethodName
for	O
health	O
question	O
answering	O
,	O
medical	O
inference	O
and	O
disease	O
name	O
recognition	O
.	O

In	O
EMNLP	O
,	O
pages	O
4604–4614	O
.	O

Dan	O
Hendrycks	O
and	O
Kevin	O
Gimpel	O
.	O

2016	O
.	O

Gaussian	B-MetricName
error	O
linear	O
units	O
(	O
gelus	O
)	O
.	O

arXiv:1606.08415	B-DatasetName
.Kexin	O

Huang	O
,	O
Jaan	O
Altosaar	O
,	O
and	O
Rajesh	O
Ranganath	O
.	O

2019	O
.	O

Clinicalbert	O
:	O
Modeling	O
clinical	O
notes	O
and	O
predicting	O
hospital	O
readmission	O
.	O

CoRR	O
,	O
abs/1904.05342	O
.	O

Paul	O
Jaccard	O
.	O

1912	O
.	O

The	O
distribution	O
of	O
the	O
ﬂora	O
in	O
the	O
alpine	O
zone	O
.	O

New	O
Phydvtologist	O
,	O
11(2):37–50	O
.	O

S´ebastien	O
Jean	O
,	O
KyungHyun	O
Cho	O
,	O
Roland	O
Memisevic	O
,	O
and	O
Yoshua	O
Bengio	O
.	O
2015	O
.	O

On	O
using	O
very	O
large	O
target	O
vocabulary	O
for	O
neural	O
machine	O
translation	O
.	O

In	O
ACL	O
,	O
pages	O
1–10	O
.	O

Mandar	O
Joshi	O
,	O
Danqi	O
Chen	O
,	O
Yinhan	O
Liu	O
,	O
Daniel	O
S.	O
Weld	O
,	O
Luke	O
Zettlemoyer	O
,	O
and	O
Omer	O
Levy	O
.	O

2020	O
.	O

Spanbert	B-MethodName
:	O
Improving	O
pre	O
-	O
training	O
by	O
representing	O
and	O
predicting	O
spans	O
.	O

Trans	O
.	O

Assoc	O
.	O

Comput	O
.	O

Linguistics	O
,	O
8:64–77	O
.	O

Zhenzhong	O
Lan	O
,	O
Mingda	O
Chen	O
,	O
Sebastian	O
Goodman	O
,	O
Kevin	O
Gimpel	O
,	O
Piyush	O
Sharma	O
,	O
and	O
Radu	O
Soricut	O
.	O
2020	O
.	O

ALBERT	B-MethodName
:	O

A	O
lite	B-MethodName
BERT	I-MethodName
for	O
self	O
-	O
supervised	O
learning	O
of	O
language	O
representations	O
.	O

In	O
ICLR	B-DatasetName
.	O

Yann	O
LeCun	O
,	O
Sumit	O
Chopra	O
,	O
Raia	O
Hadsell	O
,	O
M	O
Ranzato	O
,	O
and	O
F	O
Huang	O
.	O
2006	O
.	O

A	O
tutorial	O
on	O
energy	O
-	O
based	O
learning	O
.	O

Predicting	O
structured	O
data	O
,	O
1(0	O
)	O
.	O

Jinhyuk	O
Lee	O
,	O
Wonjin	O
Yoon	O
,	O
Sungdong	O
Kim	O
,	O
Donghyeon	O
Kim	O
,	O
Sunkyu	O
Kim	O
,	O
Chan	O
Ho	O

So	O
,	O
and	O
Jaewoo	O
Kang	O
.	O

2020	O
.	O

Biobert	B-MethodName
:	O
a	O
pre	O
-	O
trained	O
biomedical	O
language	O
representation	O
model	O
for	O
biomedical	O
text	O
mining	O
.	O

Bioinform	B-MethodName
.	O
,	O
36(4):1234	O
–	O
1240	O
.	O

Linfeng	O
Li	O
,	O
Peng	O
Wang	O
,	O
Jun	O
Yan	O
,	O
Yao	O
Wang	O
,	O
Simin	O
Li	O
,	O
Jinpeng	O
Jiang	O
,	O
Zhe	O
Sun	O
,	O
Buzhou	O
Tang	O
,	O
TsungHui	O
Chang	O
,	O
Shenghui	O
Wang	O
,	O
and	O
Yuting	O
Liu	O
.	O

2020	O
.	O

Real	O
-	O
world	O
data	O
medical	O
knowledge	O
graph	O
:	O
construction	O
and	O
applications	O
.	O

Artif	O
.	O

Intell	O
.	O

Medicine	O
,	O
103:101817	O
.	O

Yankai	O
Lin	O
,	O
Zhiyuan	O
Liu	O
,	O
Maosong	O
Sun	O
,	O
Yang	O
Liu	O
,	O
and	O
Xuan	O
Zhu	O
.	O
2015	O
.	O

Learning	O
entity	O
and	O
relation	O
embeddings	O
for	O
knowledge	O
graph	O
completion	O
.	O

In	O
AAAI	B-DatasetName
,	O
pages	O
2181–2187	O
.	O

Zhouhan	O
Lin	O
,	O
Minwei	O
Feng	O
,	O
C	O
´	O
ıcero	O
Nogueira	O
dos	O
Santos	O
,	O
Mo	O
Yu	O
,	O
Bing	O
Xiang	O
,	O
Bowen	O
Zhou	O
,	O
and	O
Yoshua	O
Bengio	O
.	O

2017	O
.	O

A	O
structured	O
self	O
-	O
attentive	O
sentence	O
embedding	O
.	O

In	O
ICLR	B-DatasetName
.	O

Dayiheng	O
Liu	O
,	O
Yeyun	O
Gong	O
,	O
Jie	O
Fu	O
,	O
Yu	O
Yan	O
,	O
Jiusheng	O
Chen	O
,	O
Daxin	O
Jiang	O
,	O
Jiancheng	O
Lv	O
,	O
and	O
Nan	O
Duan	O
.	O
2020a	O
.	O

Rikinet	B-MethodName
:	O
Reading	O
wikipedia	O
pages	O
for	O
natural	O
question	O
answering	O
.	O

In	O
ACL	O
,	O
pages	O
6762–6771	O
.	O

Weijie	O
Liu	O
,	O
Peng	O
Zhou	O
,	O
Zhe	O
Zhao	O
,	O
Zhiruo	O
Wang	O
,	O
Qi	O
Ju	O
,	O
Haotang	O
Deng	O
,	O
and	O
Ping	O
Wang	O
.	O

2020b	O
.	O

K	B-MethodName
-	I-MethodName
BERT	I-MethodName
:	O
enabling	O
language	O
representation	O
with	O
knowledge	O
graph	O
.	O

In	O
AAAI	B-DatasetName
,	O
pages	O
2901–2908	O
.	O

Xiaodong	O
Liu	O
,	O
Pengcheng	O
He	O
,	O
Weizhu	O
Chen	O
,	O
and	O
Jianfeng	O
Gao	O
.	O
2019a	O
.	O

Multi	O
-	O
task	O
deep	O
neural	O
networks	O
for	O
natural	O
language	O
understanding	O
.	O

In	O
ACL	O
,	O
pages	O
4487–4496.5891Yinhan	O
Liu	O
,	O
Myle	O
Ott	O
,	O
Naman	O
Goyal	O
,	O
Jingfei	O
Du	O
,	O
Mandar	O
Joshi	O
,	O
Danqi	O
Chen	O
,	O
Omer	O
Levy	O
,	O
Mike	O
Lewis	O
,	O
Luke	O
Zettlemoyer	O
,	O
and	O
Veselin	O
Stoyanov	O
.	O

2019b	O
.	O
Roberta	B-MethodName
:	O

A	O
robustly	O
optimized	O
BERT	B-MethodName
pretraining	O
approach	O
.	O

CoRR	B-DatasetName
,	O
abs/1907.11692	O
.	O

Tom´as	O
Mikolov	O
,	O
Kai	O
Chen	O
,	O
Greg	O
Corrado	O
,	O
and	O
Jeffrey	O
Dean	O
.	O

2013a	O
.	O

Efﬁcient	O
estimation	O
of	O
word	O
representations	O
in	O
vector	O
space	O
.	O

In	O
ICLR	B-DatasetName
.	O

Tom´as	O
Mikolov	O
,	O
Ilya	O
Sutskever	O
,	O
Kai	O
Chen	O
,	O
Gregory	O
S.	O
Corrado	O
,	O
and	O
Jeffrey	O
Dean	O
.	O

2013b	O
.	O

Distributed	O
representations	O
of	O
words	O
and	O
phrases	O
and	O
their	O
compositionality	O
.	O

In	O
NIPS	B-TaskName
,	O
pages	O
3111–3119	O
.	O

Guoshun	O
Nan	O
,	O
Zhijiang	O
Guo	O
,	O
Ivan	O
Sekulic	O
,	O
and	O
Wei	O
Lu	O
.	O
2020	O
.	O

Reasoning	O
with	O
latent	O
structure	O
reﬁnement	O
for	O
document	O
-	O
level	O
relation	O
extraction	O
.	O

In	O
ACL	O
,	O
pages	O
1546–1557	O
.	O

Dat	O
Quoc	O
Nguyen	O
,	O
Kairit	O
Sirts	O
,	O
Lizhen	O
Qu	O
,	O
and	O
Mark	O
Johnson	O
.	O

2016	O
.	O

Neighborhood	O
mixture	O
model	O
for	O
knowledge	O
base	O
completion	O
.	O

In	O
CoNLL	O
,	O
pages	O
40	O
–	O
50	O
.	O

Lawrence	O
Page	O
,	O
Sergey	O
Brin	O
,	O
Rajeev	O
Motwani	O
,	O
and	O
Terry	O
Winograd	O
.	O
1999	O
.	O

The	O
pagerank	O
citation	O
ranking	O
:	O
Bringing	O
order	O
to	O
the	O
web	O
.	O

Technical	O
Report	O
1999	O
-	O
66	O
,	O
Stanford	O
InfoLab	O
.	O

Yifan	O
Peng	O
,	O
Shankai	O
Yan	O
,	O
and	O
Zhiyong	O
Lu	O
.	O

2019	O
.	O

Transfer	O
learning	O
in	O
biomedical	O
natural	O
language	O
processing	O
:	O
An	O
evaluation	O
of	O
BERT	B-MethodName
and	O
elmo	O
on	O
ten	O
benchmarking	O
datasets	O
.	O

In	O
BioNLP	B-DatasetName
,	O
pages	O
58–65	O
.	O

Jeffrey	O
Pennington	O
,	O
Richard	O
Socher	O
,	O
and	O
Christopher	O
D.	O
Manning	O
.	O

2014	O
.	O

Glove	B-MethodName
:	O
Global	O
vectors	O
for	O
word	O
representation	O
.	O

In	O
EMNLP	O
,	O
pages	O
1532–1543	O
.	O
Matthew	O
E.	O
Peters	O
,	O
Mark	O
Neumann	O
,	O
Robert	O
L.	O
Logan	O
IV	O
,	O
Roy	O
Schwartz	O
,	O
Vidur	O
Joshi	O
,	O
Sameer	O
Singh	O
,	O
and	O
Noah	O
A.	O
Smith	O
.	O

2019	O
.	O

Knowledge	O
enhanced	O
contextual	O
word	O
representations	O
.	O

In	O
EMNLP	O
,	O
pages	O
43–54	O
.	O

Matthew	O
E.	O
Peters	O
,	O
Mark	O
Neumann	O
,	O
Mohit	O
Iyyer	O
,	O
Matt	O
Gardner	O
,	O
Christopher	O
Clark	O
,	O
Kenton	O
Lee	O
,	O
and	O
Luke	O
Zettlemoyer	O
.	O

2018	O
.	O

Deep	O
contextualized	O
word	O
representations	O
.	O

In	O
NAACL	B-TaskName
,	O
pages	O
2227–2237	O
.	O

Xipeng	O
Qiu	O
,	O
Tianxiang	O
Sun	O
,	O
Yige	O
Xu	O
,	O
Yunfan	O
Shao	O
,	O
Ning	O
Dai	O
,	O
and	O
Xuanjing	O
Huang	O
.	O
2020	O
.	O

Pre	O
-	O
trained	O
models	O
for	O
natural	O
language	O
processing	O
:	O
A	O
survey	O
.	O

CoRR	O
,	O
abs/2003.08271	O
.	O

Maya	O
Rotmensch	O
,	O
Yoni	O
Halpern	O
,	O
Abdulhakim	O
Tlimat	O
,	O
Steven	O
Horng	O
,	O
and	O
David	O
Sontag	O
.	O
2017	O
.	O

Learning	O
a	O
health	O
knowledge	O
graph	O
from	O
electronic	O
medical	O
records	O
.	O

Scientiﬁc	O
reports	O
,	O
7(1):1–11	O
.	O

Michael	O
Sejr	O
Schlichtkrull	O
,	O
Thomas	O
N.	O
Kipf	O
,	O
Peter	O
Bloem	O
,	O
Rianne	O
van	O
den	O
Berg	O
,	O
Ivan	O
Titov	O
,	O
and	O
Max	O
Welling	O
.	O

2018	O
.	O

Modeling	O
relational	O
data	O
with	O
graph	O
convolutional	O
networks	O
.	O

In	O
ESWC	O
,	O
pages	O
593–607.Tianxiang	O
Sun	O
,	O
Yunfan	O
Shao	O
,	O
Xipeng	O
Qiu	O
,	O
Qipeng	O
Guo	O
,	O
Yaru	O
Hu	O
,	O
Xuanjing	O
Huang	O
,	O
and	O
Zheng	O
Zhang	O
.	O

2020	O
.	O

Colake	B-MethodName
:	O
Contextualized	O
language	O
and	O
knowledge	O
embedding	O
.	O

In	O
COLING	B-DatasetName
,	O
pages	O
3660–3670	O
.	O
Joseph	O
P.	O
Turian	O
,	O
Lev	O
-	O
Arie	O
Ratinov	O
,	O
and	O
Yoshua	O
Bengio	O
.	O

2010	O
.	O

Word	O
representations	O
:	O
A	O
simple	O
and	O
general	O
method	O
for	O
semi	O
-	O
supervised	O
learning	O
.	O

In	O
ACL	O
,	O
pages	O
384–394	O
.	O
Ashish	O
Vaswani	O
,	O
Noam	O
Shazeer	O
,	O
Niki	O
Parmar	O
,	O
Jakob	O
Uszkoreit	O
,	O
Llion	O
Jones	O
,	O
Aidan	O
N.	O
Gomez	O
,	O
Lukasz	O
Kaiser	O
,	O
and	O
Illia	O
Polosukhin	O
.	O
2017	O
.	O

Attention	O
is	O
all	O
you	O
need	O
.	O

In	O
NIPS	B-TaskName
,	O
pages	O
5998–6008	O
.	O

Xiaoyan	O
Wang	O
,	O
Pavan	O
Kapanipathi	O
,	O
Ryan	O
Musa	O
,	O
Mo	O
Yu	O
,	O
Kartik	O
Talamadupula	O
,	O
Ibrahim	O
Abdelaziz	O
,	O
Maria	O
Chang	O
,	O
Achille	O
Fokoue	O
,	O
Bassem	O
Makni	O
,	O
Nicholas	O
Mattei	O
,	O
and	O
Michael	O
Witbrock	O
.	O
2019a	O
.	O

Improving	O
natural	O
language	O
inference	O
using	O
external	O
knowledge	O
in	O
the	O
science	O
questions	O
domain	O
.	O

In	O
AAAI	B-DatasetName
,	O
pages	O
7208–7215	O
.	O

Xiaozhi	O
Wang	O
,	O
Tianyu	O
Gao	O
,	O
Zhaocheng	O
Zhu	O
,	O
Zhiyuan	O
Liu	O
,	O
Juanzi	O
Li	O
,	O
and	O
Jian	O
Tang	O
.	O

2019b	O
.	O

KEPLER	B-MethodName
:	O
A	O
uniﬁed	O
model	O
for	O
knowledge	O
embedding	O
and	O
pre	O
-	O
trained	O
language	O
representation	O
.	O

CoRR	O
,	O
abs/1911.06136	O
.	O

William	O
E	O
Winkler	O
.	O
1990	O
.	O

String	O
comparator	O
metrics	O
and	O
enhanced	O
decision	O
rules	O
in	O
the	O
fellegi	O
-	O
sunter	O
model	O
of	O
record	O
linkage	O
.	O

Liang	O
Xu	O
,	O
Xuanwei	O
Zhang	O
,	O
and	O
Qianqian	O
Dong	O
.	O
2020	O
.	O

Cluecorpus2020	O
:	O

A	O
large	O
-	O
scale	O
chinese	O
corpus	O
for	O
pre	O
-	O
training	O
language	O
model	O
.	O

CoRR	O
,	O
abs/2003.01355	O
.	O

Zhilin	O
Yang	O
,	O
Zihang	O
Dai	O
,	O
Yiming	O
Yang	O
,	O
Jaime	O
G.	O
Carbonell	O
,	O
Ruslan	O
Salakhutdinov	O
,	O
and	O
Quoc	O
V	O
.	O

Le	O
.	O
2019	O
.	O

Xlnet	B-MethodName
:	O

Generalized	O
autoregressive	O
pretraining	O
for	O
language	O
understanding	O
.	O

In	O
NIPS	B-TaskName
,	O
pages	O
5754	O
–	O
5764	O
.	O

Denghui	O
Zhang	O
,	O
Zixuan	O
Yuan	O
,	O
Yanchi	O
Liu	O
,	O
Zuohui	O
Fu	O
,	O
Fuzhen	O
Zhuang	O
,	O
Pengyang	O
Wang	O
,	O
Haifeng	O
Chen	O
,	O
and	O
Hui	O
Xiong	O
.	O
2020a	O
.	O

E	B-MethodName
-	I-MethodName
BERT	I-MethodName
:	O
A	O
phrase	O
and	O
product	O
knowledge	O
enhanced	O
language	O
model	O
for	O
ecommerce	O
.	O

CoRR	O
,	O
abs/2009.02835	O
.	O

Ningyu	O
Zhang	O
,	O
Qianghuai	O
Jia	O
,	O
Kangping	O
Yin	O
,	O
Liang	O
Dong	O
,	O
Feng	O
Gao	O
,	O
and	O
Nengwei	O
Hua	O
.	O
2020b	O
.	O

Conceptualized	O
representation	O
learning	O
for	O
chinese	O
biomedical	O
text	O
mining	O
.	O

CoRR	O
,	O
abs/2008.10813	O
.	O

Sheng	O
Zhang	O
,	O
Xin	O
Zhang	O
,	O
Hui	O
Wang	O
,	O
Jiajun	O
Cheng	O
,	O
Pei	O
Li	O
,	O
and	O
Zhaoyun	O
Ding	O
.	O

2017	O
.	O

Chinese	O
medical	O
question	O
answer	O
matching	O
using	O
end	O
-	O
to	O
-	O
end	O
characterlevel	O
multi	O
-	O
scale	O
cnns	O
.	O

Applied	O
Sciences	O
,	O
7(8):767	O
.	O

Zhengyan	O
Zhang	O
,	O
Xu	O
Han	O
,	O
Zhiyuan	O
Liu	O
,	O
Xin	O
Jiang	O
,	O
Maosong	O
Sun	O
,	O
and	O
Qun	O
Liu	O
.	O
2019	O
.	O

ERNIE	B-MethodName
:	O
enhanced	O
language	O
representation	O
with	O
informative	O
entities	O
.	O

In	O
ACL	O
,	O
pages	O
1441–1451.5892A	O

Data	O
Source	O
A.1	O
Pre	O
-	O
training	O
Data	O
A.1.1	O
Training	O
Corpora	B-MethodName
The	O
pre	O
-	O
training	O
corpora	O
is	O
crawled	O
from	O
DXY	B-DatasetName
BBS	I-DatasetName
(	O
Bulletin	B-DatasetName
Board	I-DatasetName
System)9	I-DatasetName
,	O
which	O
is	O
a	O
very	O
popular	O
Chinese	O
social	O
network	O
for	O
doctors	O
,	O
medical	O
institutions	O
,	O
life	O
scientists	O
,	O
and	O
medical	O
practitioners	O
.	O

The	O
BBS	O
has	O
more	O
than	O
30	O
channels	O
,	O
which	O
contains	O
18	O
forums	O
and	O
130	O
ﬁne	O
-	O
grained	O
groups	O
,	O
covering	O
most	O
of	O
the	O
medical	O
domains	O
.	O

For	O
our	O
pre	O
-	O
training	O
purpose	O
,	O
we	O
crawl	O
texts	O
from	O
channels	O
about	O
clinical	O
medicine	O
,	O
pharmacology	O
,	O
public	O
health	O
and	O
consulting	O
.	O

For	O
text	O
pre	O
-	O
processing	O
,	O
we	O
mainly	O
follow	O
the	O
methods	O
of	O
(	O
Xu	O
et	O
al	O
.	O
,	O
2020	O
)	O
.	O

Additionally	O
,	O
(	O
1	O
)	O
we	O
remove	O
all	O
URLs	O
,	O
HTML	O
tags	O
,	O
e	O
-	O
mail	O
addresses	O
,	O
and	O
all	O
tokens	O
except	O
characters	O
,	O
digits	O
,	O
and	O
punctuation	O
(	O
2	O
)	O
all	O
documents	O
shorter	O
than	O
256	O
are	O
discard	O
,	O
while	O
documents	O
longer	O
than	O
512	O
are	O
cut	O
into	O
shorter	O
text	O
segments	O
.	O

A.1.2	O
Knowledge	O
Graph	O
The	O
DXY	B-DatasetName
knowledge	O
graph	O
is	O
construed	O
by	O
extracting	O
structured	O
text	O
from	O
DXY	B-DatasetName
website10	O
,	O
which	O
includes	O
information	O
of	O
diseases	O
,	O
drugs	O
and	O
hospitals	O
edited	O
by	O
certiﬁed	O
medical	O
experts	O
,	O
thus	O
the	O
quality	O
of	O
the	O
KG	O
is	O
guaranteed	O
.	O

The	O
KG	B-MethodName
is	O
mainly	O
disease	O
-	O
centered	O
,	O
including	O
totally	O
3,764,711	O
triples	O
,	O
152.508	O
unique	O
entities	O
,	O
and	O
44	O
types	O
of	O
relations	O
.	O

The	O
details	O
of	O
Symptom	B-MethodName
-	I-MethodName
InChinese	O
from	O
OpenKG	O
is	O
available11	O
.	O

We	O
ﬁnally	O
get	O
26	O
types	O
of	O
entities	O
,	O
274,163	O
unique	O
entities	O
,	O
56	O
types	O
of	O
relations	O
,	O
and	O
4,390,726	O
triples	O
after	O
the	O
fusion	O
of	O
the	O
two	O
KGs	O
.	O

A.2	O
Task	O
Data	O
We	O
choose	O
the	O
four	O
large	O
-	O
scale	O
datasets	O
in	O
ChineseBlue	B-DatasetName
tasks	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2020b	O
)	O
while	O
others	O
are	O
ignored	O
due	O
to	O
the	O
limitation	O
of	O
datasets	O
size	O
,	O
which	O
are	O
cMedQANER	B-DatasetName
,	O
cMedQQ	B-DatasetName
,	O
cMedQNLI	B-DatasetName
and	O
cMedQA	B-DatasetName
.	O

WebMedQA	B-DatasetName
(	O
He	O
et	O
al	O
.	O
,	O
2019	O
)	O
is	O
a	O
real	O
-	O
world	O
Chinese	O
medical	O
question	O
answering	O
dataset	O
and	O
CHIP	O
-	O
RE	O
dataset	O
are	O
collected	O
from	O
online	O
health	O
consultancy	O
websites	O
.	O

Note	O
that	O
since	O
both	O
the	O
WebMedQA	O
and	O
cMedQA	O
datasets	O
are	O
very	O
large	O
while	O
we	O
have	O
many	O
baselines	O
to	O
be	O
compared	O
,	O
we	O
randomly	O
sample	O
the	O
ofﬁcial	O
training	O
set	O
,	O
development	O
set	O
and	O
test	O
set	O
respectively	O
9https://www.dxy.cn/bbs/newweb/pc/home	O
10https://portal.dxy.cn/	O
11http://openkg.cn/dataset/	O
symptom	O
-	O
in	O
-	O
chineseto	O
form	O
their	O
corresponding	O
smaller	O
version	O
for	O
experiments	O
.	O

DXY	B-DatasetName
-	B-MethodName
NER	I-MethodName
and	O
DXY	O
-	B-MethodName
RE	O
are	O
datasets	O
from	O
real	O
medical	O
application	O
scenarios	O
provided	O
by	O
a	O
prestigious	O
Chinese	O
medical	O
company	O
.	O

The	O
DXY	B-DatasetName
-	I-DatasetName
NER	I-DatasetName
contains	O
22	O
unique	O
entity	O
types	O
and	O
56	O
relation	O
types	O
in	O
the	O
DXY	B-DatasetName
-	B-MethodName
RE	O
.	O

These	O
two	O
datasets	O
are	O
collected	O
from	O
the	O
medical	O
forum	O
of	O
DXY	B-DatasetName
and	O
books	O
in	O
the	O
medical	O
domain	O
.	O

Annotators	O
are	O
selected	O
from	O
junior	O
and	O
senior	O
students	O
with	O
clinical	O
medical	O
background	O
.	O

In	O
the	O
process	O
of	O
quality	O
control	O
,	O
the	O
two	O
datasets	O
are	O
annotated	O
twice	O
by	O
different	O
groups	O
of	O
annotators	O
.	O

An	O
expert	O
with	O
medical	O
background	O
performs	O
quality	O
check	O
manually	O
again	O
when	O
annotated	O
results	O
are	O
inconsistent	O
,	O
whereas	O
perform	O
sampling	O
quality	O
check	O
when	O
results	O
are	O
consistent	O
.	O

Table	O
5	O
shows	O
the	O
datasets	O
size	O
of	O
our	O
experiments	O
.	O

B	O
Model	O
Settings	O
and	O
Training	O
Details	O
Hyper	O
-	O
parameters	O
.	O

d1=768,d2=200,K=10,	O
=	O
10,1=2,2=4	O
.	O

Model	O
Details	O
.	O

We	O
align	O
the	O
all	O
mention	O
-	O
spans	O
to	O
the	O
entity	O
in	O
KG	O
by	O
exact	O
match	O
for	O
comparison	O
purpose	O
with	O
ENIRE	O
-	B-MethodName
THU	B-TaskName
(	O
Zhang	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

The	O
negative	O
sampling	O
function	O
is	O
deﬁned	O
asQ(ei	O
m	O
)	O

=	O
teim	O
Ceim	O
,	O
whereCeimis	O
the	O
sum	O
of	O
frequency	O
of	O
all	O
mentions	O
with	O
the	O
same	O
type	O
of	O
ei	O
m.	O

The	O
Mention	B-MethodName
-	I-MethodName
neighbor	I-MethodName
Hybrid	I-MethodName
Attention	O
module	O
is	O
inserted	O
after	O
the	O
tenth	O
transformer	O
encoder	O
layer	O
to	O
compare	O
with	O
KnowBERT	B-MethodName
(	O
Peters	O
et	O
al	O
.	O
,	O
2019	O
)	O
,	O
while	O
we	O
perform	O
the	O
Mention	B-MethodName
-	O
neighbor	B-MethodName
Context	O
Modeling	O
based	O
on	O
the	O
output	O
of	O
BERT	B-MethodName
encoder	O
.	O

We	O
use	O
all	O
the	O
base	O
-	O
version	O
PLMs	O
in	O
the	O
experiments	O
.	O

The	O
size	O
of	O
SMedBERT	B-MethodName
is	O
474	O
MB	O
while	O
393	O
MB	O
of	O
that	O
are	O
components	O
of	O
BERT	B-MethodName
,	O
and	O
the	O
added	O
81	O
MB	O
is	O
mostly	O
of	O
the	O
KG	B-MethodName
embedding	O
.	O

Results	O
are	O
presented	O
in	O
average	O
with	O
5	O
random	O
runs	O
with	O
different	O
random	O
seeds	O
and	O
the	O
same	O
hyperparameters	O
.	O

Training	O
Procedure	O
.	O

We	O
strictly	O
follow	O
the	O
originally	O
pre	O
-	O
training	O
process	O
and	O
parameter	O
setting	O
of	O
other	O
KEPLMs	B-MethodName
.	O

We	O
only	O
adapt	O
their	O
publicly	O
available	O
code	O
from	O
English	O
to	O
Chinese	O
and	O
use	O
the	O
knowledge	O
embedding	O
trained	O
on	O
our	O
medical	O
KG	O
.	O

To	O
have	O
a	O
fair	O
comparison	O
,	O
the	O
pre	O
-	O
training	O
processing	O
of	O
SMedBERT	B-MethodName
is	O
mostly	O
set	O
based	O
on	O
ENIRE	O
-	B-MethodName
THU	I-MethodName
(	O
Zhang	O
et	O
al	O
.	O
,	O
2019	O
)	O
without	O
layerspecial	O
learning	O
rates	O
in	O
KnowBERT	B-MethodName
(	O
Peters	O
et	O
al	O
.	O
,	O
2019	O
)	O
.	O

We	O
only	O
pre	O
-	O
train	O
SMedBERT	B-MethodName
on	O
the	O
collected	O
medical	O
data	O
for	O
1	O
epoch	O
.	O

In	O
pre	O
-	O
training5893The	O
Dataset	O
Size	O
in	O
Our	O
Experiments	O
Dataset	O
Train	O
Dev	O
Test	O
Task	O
Metric	O
cMedQANER	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2020b)1,673	O
175	O
215	O
NER	B-TaskName
F1	O
cMedQQ	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2020b)16,071	O
1,793	O
1,935	O
QM	O
F1	O
cMedQNLI	O
(	O
Zhang	O
et	O
al	O
.	O
,	O
2020b)80,950	O
9,065	O
9,969	O
NLI	O
F1	O
cMedQA	O
(	O
Zhang	O
et	O
al	O
.	O
,	O

2017)186,771	O
46,600	O
46,600	O
QA	O
Acc@1	O
WebMedQA	O
(	O
He	O
et	O
al	O
.	O
,	O
2019)252,850	O
31,605	O
31,655	O
QA	O
Acc@1	O
CHIP	B-MethodName
-	O
RE43,649	O
-	O
10,622	O
RE	B-MethodName
F1	O
DXY	O
-	O
NER	O
34,224	O
8,576	O
8,592	O
NER	B-TaskName
F1	O
DXY	O
-	O
RE	B-MethodName
141,696	O
35,456	O
35,794	O
RE	B-MethodName
F1	O
CHIP	O
-	O
RE	B-MethodName
dataset	O
is	O
released	O
in	O
CHIP	O
2020	O
.	O

(	O
http://cips-chip.org.cn/2020/eval2	O
)	O
Table	O
5	O
:	O
The	O
statistical	O
data	O
and	O
metric	O
of	O
eight	O
datasets	O
used	O
in	O
our	O
SMedBERT	B-MethodName
model	O
.	O
process	O
,	O
the	O
learning	O
rate	O
is	O
set	O
to	O
5e 5and	O
batch	O
size	O
is	O
512	O
with	O
the	O
max	O
sequence	O
length	O
is	O
512	O
.	O

For	O
ﬁne	O
-	O
tuning	O
,	O
we	O
ﬁnd	O
the	O
following	O
ranges	O
of	O
possible	O
values	O
work	O
well	O
,	O
i.e.	O
,	O
batch	O
size	O
is	O
f8,16	O
g	O
,	O
learning	O
rate	O
(	O
AdamW	O
)	O
is	O
f2e 5,4e 5,6e 5gand	O
the	O
number	O
of	O
epochs	O
is	O
f2,3,4	O
g.	O
Pre	O
-	O
training	O
SMedBERT	B-MethodName
takes	O
about	O
36	O
hours	O
per	O
epoch	O
on	O
2	O
NVIDIA	O
GeForce	B-MethodName
RTX	I-MethodName
3090	O
GPUs	O
.	O

C	O
Data	O
and	O
Embedding	O
of	O
Unsupervised	O
Semantic	O
Similarity	O
Since	O
the	O
KGs	O
used	O
in	O
this	O
paper	O
is	O
a	O
directed	O
graph	O
,	O
we	O
ﬁrst	O
transform	O
the	O
directed	O
”	O
I÷sû	O
”	O
(	O
equivalence	O
relations	O
)	O
pairs	O
to	O
undirected	O
pairs	O
and	O
discard	O
the	O
duplicated	O
pairs	O
.	O

For	O
each	O
positive	O
pairs	O
,	O
we	O
use	O
head	O
and	O
tail	O
as	O
query	O
respectively	O
and	O
sample	O
the	O
negative	O
candidates	O
based	O
on	O
the	O
other	O
.	O

Speciﬁcally	O
,	O
we	O
randomly	O
select	O
19	O
negative	O
entities	O
with	O
the	O
same	O
type	O
and	O
has	O
a	O
JaroWinkle	B-MethodName
similarity	O
(	O
Winkler	O
,	O
1990	O
)	O
bigger	O
0.6	O
with	O
the	O
ground	O
-	O
truth	O
entity	O
.	O

We	O
select	O
from	O
all	O
samples	O
inDataset-1	O
with	O
positive	O
pairs	O
that	O
the	O
neighbours	O
sets	O
of	O
head	O
and	O
tail	O
entity	O
have	O
Jaccard	B-DatasetName
Index	I-DatasetName
(	O
Jaccard	B-MethodName
,	O
1912	O
)	O
no	O
less	O
than	O
0.75	O
and	O
at	O
least	O
3	O
common	O
element	O
to	O
construct	O
the	O
Dataset-2	B-MethodName
.	O

For	O
Dataset-3	B-DatasetName
,	O
we	O
count	O
the	O
frequency	O
of	O
all	O
entity	O
mentions	O
in	O
pretraining	O
corpora	O
,	O
and	O
treat	O
mentions	O
with	O
frequency	O
no	O
more	O
than	O
200	O
as	O
low	O
-	O
frequency	O
mentions	O
.	O

Classic	O
Word	B-MethodName
Representation	B-DatasetName
Embedding	O
:	O
We	O
train	O
the	O
character	O
-	O
level	O
and	O
word	O
-	O
level	O
embedding	O
using	O
SGNS	O
(	O
Mikolov	O
et	O
al	O
.	O
,	O
2013a	O
)	O
and	O
GLOVE	O

(	O
Pennington	O
et	O
al	O
.	O
,	O
2014	O
)	O
model	O
respectively	O
on	O
our	O
medical	O
corpora	O
with	O
open	O
-	O
source	O
toolkits12	O
.	O

We	O
average	O
the	O
character	O
embedding	O
for	O
all	O
tokens	O
in	O
the	O
mention	O
to	O
get	O
the	O
character	O
-	O
level	O
representation	O
.	O

However	O
,	O
since	O
some	O
mentions	O
are	O
very	O
rare	O
in	O
the	O
corpora	O
for	O
word	O
-	O
level	O
representation	O
,	O
we	O
use	O
the	O
character	O
-	O
level	O
representation	O
as	O
their	O
word	O
-	O
level	O
representation	O
.	O

BERT	O
-	O
like	O
Representation	O
Embedding	O
:	O
We	O
extract	O
the	O
token	O
hidden	O
features	O
of	O
the	O
last	O
layer	O
and	O
average	O
the	O
representations	O
of	O
the	O
input	O
tokens	O
except	O
[	O
CLS	O
]	O
and	O
[	O
SEP	O
]	O
tag	O
,	O
to	O
get	O
a	O
vector	O
for	O
each	O
entity	O
.	O

Similarity	O
Measure	O
:	O
We	O
try	O
using	O
the	O
inverse	O
of	O
L2	O
-	O
distance	O
and	O
cosine	O
similarity	O
as	O
measurement	O
,	O
and	O
we	O
ﬁnd	O
that	O
cosine	O
similarity	O
always	O
perform	O
better	O
.	O

Hence	O
,	O
we	O
report	O
all	O
experiment	O
results	O
under	O
the	O
cosine	O
similarity	O
metric	O
.	O

12SGNS	O
:	O
https://github.com/JuGyang/	O
word2vec	O
-	O
SGNS	O
.	O

Glove	B-MethodName
:	O
https://github.com/stanfordnlp/	O
GloVe	O

